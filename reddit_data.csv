data,label
"Colleagues

I am a biomedical engineer and I have a little knowledge in python and SQL, I would like to work as a data scientist because I really like programming more than working as a biomedical.

But I don't know how to start, I've seen job postings and the truth is they don't ask for much but they never call me anyway.

I hope you can give me tips and sources of information to study",datascience
"TL;DR: offered a fully-funded Master’s position with a CS/ML faculty prof, with a project in my area of interest (AI/ML in biomedical science). However, as a biomed major, I’d have to be doing the masters as an MSc in Biology, rather than MSc in CS. How transferable is this to industry, provided I’m okay with another 1-year MSc in (bio)statistics/data science if needed? Alternate plan is to do an accelerated BSc in CS.

Hey everyone. I finished my Bachelor’s in Biomedical Science in Canada, and for a year after graduation, had worked in wet lab research. [After taking the time to explore my careers and interests](https://www.reddit.com/r/datascience/comments/108lp08/computer_science_biostatistics_health_informatics/), I was planning on doing an accelerated bachelor’s in CS + a master's in biostatistics. Through this, my goal was to do research using data science within a biomedical/clinical context, while also having robust credentials for industry if things go awry.

However, I recently got offered a fully-funded Master’s position by one of my professors. They are primarily appointed in the CS department, with research in ML for drug discovery, cancer biology, etc. The project aligns very well with my interests, however, as my current credentials are in biomedical science (not CS), he is only able to offer the position through a Master's in Biology, as opposed to a Master's in CS.

That being said:

* How transferrable would this MSc in Biology be towards the general CS/DS/ML industry? 
* Overall, provided my interests, would this be a wise decision career-wise?

I’m also fine with doing another internship-based MSc in (bio)statistics or data science after this program, if need be (e.g. if the MSc in Bio isn’t recognized). I would have enrolled in those directly if it weren’t for missing prerequisite courses (which I plan on taking in the BSc in CS or MSc in Bio).",datascience
"Junior data scientist here.  I recently started at a new (medium sized 2500 employees, non tech) company, previous company was a startup (only 20 employees). At my previous company I was issued a laptop which had no company software and no system restrictions. It was very easy to download any IDE/software that I needed to do my job. New (larger) company has many system restrictions, can't download IDEs, limited permissions on files, limited cmd line, not using any version control platforms (git, etc). Is this normal among larger corporations?",datascience
"I have been a data analyst for a little over 9 years. Throughout those years, I have worked as an enlisted military member and in government contracting. 

I recently left the data analyst world to work in the policy world. My goal is to gain knowledge so when I go back into the analysis world, I can have a better understanding of what a project manager wants from me or possibly even step into a project management role (it's a long shot, I know)

I love how data can expose gains, losses, and provide backing to a viewpoint. When I go back I don't know to go back into data analytics, data science, project management, or another area. Can I get suggestions on what certs, courses, etc. to obtain?

I have an Associates in Production Management and I'm 1 year away from my Batchelors in Data Science.",datascience
"My organization use mostly Microsoft and I need to deploy my python script so any users can use it. The preference is the cloud to automate processes, so the input/upload of the files from the user is needed. Any suggestions? I can't use third party sites",datascience
"My management is open to us spending our training budgets on conferences again.   I used to really love conferences -- get out of town for a few days, meet people, hear about new stuff, and get excited.

Anybody got recommendations for tech-focused conferences?   I prefer the ones with a broad range of presenters, not the ""summit"" style that are dumbed down and more management oriented.   I'm limited to ""US only"" in this job.",datascience
"Hi,  
Seeking advice which of these two Master's program is better preparation for PhD in Data Science?  
Master in Statistic Science (MS)  
Master in Data Analytics Engineering (MS)

Links:   
[https://catalog.gmu.edu/colleges-schools/engineering-computing/data-analytics-engineering-ms/](https://catalog.gmu.edu/colleges-schools/engineering-computing/data-analytics-engineering-ms/)  
[https://statistics.gmu.edu/academics/ms-statistical-science](https://statistics.gmu.edu/academics/ms-statistical-science)",datascience
Does anyone know how i can get tensorflow running with anaconda on my m2 Mac? I’ve been fighting for my life using google Collab instead of Jupyter notebooks to train my RNN and I’m sick of it.,datascience
"Hi All,

I'm writing this as a student in Data Science with an undergrad in Electrical Engineering. We all know that the job market is currently difficult for new grads in this space. I've been fortunate enough, however, to have been offered a Data Engineering role at a midsized startup.

My question, as the title states, is how this is viewed by the DS community? I wonder what the progression would look like from there and whether it would be realistic for me to move towards Data Scientist/Research Scientist/ML Engineer roles starting from that base.

I know this community has lots of experience and was wondering what peoples thoughts on the subject are. Are data engineers seen as less competent or lower in the chain of command?

Any input is valuable!",datascience
"I had an interview as a data analyst, the first two went great. They gave me an SQL test, I took it and submitted them all. Got the correct outputs on every question. I got rejected because the team reviewed my report and said my script does not aligned to what they are looking for?! 

I’m confused, I thought SQL should be straight forward logic… My script is all Select, From, Where, Order By and some complex aggregation like Count, Min, Max… I’m basically translating its direction to SQL and got correct outputs. What would a team want to see from my script other than correct logic??",datascience
"Received a question in my machine learning course about which functions should not be used as basis functions.

The answer was

* A linear function (such as k \* x + l, where k and l are constants)
* hyperbolic tangent function (such as 2/(1+e\^(-2x))-1)
* A sigmoid function (such as 1/(1+e\^(-x)))
* rectified linear function (such as max(0,x))

should not be used basis functions, and

* A sinc function (such as sin(x)/x)
* A sinusoid (such as sin(2pi\*k\*f\*x), where k is a constant)

can be used as basis functions.

What is the logic here and how would one go about evaluating different functions to determine whether or not they should be used as basis functions?",datascience
"Which specialization has more scope in the future for a data science professional with a decade of work experience, supply chain management or healthcare management?",datascience
"I'm a fresher and trying to break into the field of data. I have degrees in data science and machine learning but it's so hard to find any jobs in this field as someone with no experience. I finally got a job offer as a data analyst in a small advertising & marketing company- it has a good work environment and is also 15 mins from my house. BUT, the employees there are using only Google analytics, Google tags etc. 

I have learnt python, SQL, tableau and ML, but I'm worried that all this learning will go to waste.. feeling a bit discouraged and needed advice from people in the field. If i take this job would it be hard to escalate into a more technical role in the future?",datascience
"Hi. I had a discussion recently with a coworker. We are running a K means clustering algorithm. He said that when dummy variables are made into 1s and 0s,these columns must be scaled in a specific way. If variable A has 17 categories - names of cities. Then after turning into 17 columns of 1s and 0s (Python does it this way). He said that the sum of variances of those columns should be num_cathegories - 1. Thus 16. If the categories are 11, then sum variances should be 10. He proposed centering the dummies for each dummy column by substracting the mean and then dividing by the square root of the columns mean. I have looked into literature for this but haven't found anything. He is senior to me so I did what I was told without understanding it. Can you help?",datascience
"Hi, I am trying to decide between M2 Pro and M2 Max. For those who have used a Mac with these two new chips or M1 Pro and M2 Max, do you find that the extra GPU-core of the Max chips helpful?",datascience
"Hi Reddit community,

I recently wrote a Medium article on using Machine Learning library Pycaret to predict and create a lead scoring model. PyCaret is an open-source machine learning library in Python that makes it easy to build, train and deploy machine learning models.

In the article, I demonstrate how to use PyCaret to build a model that predicts the conversion of the leads and the probability of the conversion. Then, I stored the new leads prediction and probability on a Postgresql database and created a PowerBI Dashboard.

Check it out here: [LINK](https://medium.com/geekculture/beginners-guide-to-machine-learning-and-power-bi-building-a-lead-scoring-dashboard-12664c742bca)

I hope you find the article informative and useful. If you have any feedback or questions, please leave a comment!

Thanks for reading!",datascience
"As healthcare organizations increasingly rely on data to drive clinical and operational decisions, it’s crucial to ensure that the data is protected and in compliance with regulatory requirements. The Health Insurance Portability and Accountability Act (HIPAA) sets the standard for protecting sensitive patient data in the United States, and failure to comply with HIPAA regulations can result in hefty fines and damage to the organization’s reputation. In this blog post, we’ll explore some best practices for implementing DataOps in a healthcare organization to ensure HIPAA compliance.

## Understanding HIPAA Requirements  

The first step in implementing DataOps best practices for HIPAA compliance is to understand the regulations and requirements. HIPAA covers a broad range of topics, from patient privacy to data security to breach notification, so it’s important to have a comprehensive understanding of what the law requires. 

## Identifying Risks and Vulnerabilities  

Once you understand the requirements, the next step is to identify potential risks and vulnerabilities in your data management processes. This could include anything from data breaches to unauthorized access to patient information. 

## Implementing Data Governance  

Data governance is the process of managing the availability, usability, integrity, and security of the data used in an organization. Implementing a comprehensive data governance strategy is essential for maintaining HIPAA compliance and ensuring that data is accurate, complete, and secure. 

## Building a DataOps Team / Managed Services 

DataOps is a methodology that combines data engineering, data integration, and data analytics to streamline data operations and improve data quality. Building a dedicated DataOps team can help ensure that data is managed in a way that complies with HIPAA regulations. ISmile Technologies can help healthcare organizations implement DataOps best practices to improve their data management processes and maintain HIPAA compliance, With a comprehensive data governance strategy, a dedicated DataOps team.

## Using Automated Testing and Deployment  

Automated testing and deployment are key components of the DataOps methodology. By automating these processes, you can reduce the risk of errors and ensure that changes to data management processes are made in a consistent and repeatable way. 

## Conducting Regular Audits and Assessments  

Regular audits and assessments can help identify potential issues before they become problems. Conducting these assessments on a regular basis can help ensure that your data management processes remain in compliance with HIPAA regulations. 

## Training and Education  

Finally, it’s important to provide ongoing training and education to your DataOps team and other employees who handle sensitive patient data. This can help ensure that everyone is aware of the risks and requirements associated with HIPAA compliance and is equipped to handle data in a way that meets those requirements.

## How ISmile Technologies helps in Healthcare

ISmile Technologies is a technology consulting firm that specializes in helping healthcare organizations implement DataOps best practices to improve their data management processes and maintain HIPAA compliance. Here are some ways ISmile Technologies can assist their clients:  


* Data Governance Strategy:   
ISmile Technologies can help healthcare organizations develop and implement a comprehensive data governance strategy that covers data quality, data security, and data privacy. This includes policies and procedures for managing sensitive patient data, identifying potential risks and vulnerabilities, and ensuring compliance with HIPAA regulations. 
* DataOps Managed Services:   
ISmile Technologies can help healthcare organizations build a dedicated DataOps team with our DataOps Managed Services, that can handle data management processes, including data integration, data engineering, and data analytics. We are responsible for implementing automated testing and deployment processes to ensure that changes to data management processes are made in a consistent and consistent way. 
* Technology Implementation:   
ISmile Technologies can help healthcare organizations identify and implement the right technology solutions to support their DataOps best practices. This includes selecting and implementing data integration tools, data quality tools, and analytics platforms that meet the organization’s needs and comply with HIPAA regulations. 
* Training and Education:   
ISmile Technologies provides training and education to healthcare organizations’ employees on HIPAA regulations and DataOps best practices. This includes training on data management processes, security protocols, and incident response plans. 
* Ongoing Support:   
ISmile Technologies DataOps managed services provides ongoing support to healthcare organizations to ensure that their data management processes remain in compliance with HIPAA regulations and continue to meet the organization’s needs. This includes regular audits and assessments of data management processes and providing support and guidance on any issues that arise. 

[Read More ...](https://www.ismiletechnologies.com/cloud/dataops-best-practices-for-maintaining-hipaa-compliance-in-healthcare/)",datascience
"This is a [good blog post](https://ludic.mataroa.blog/blog/most-data-work-seems-fundamentally-worthless/) I recently read. Much of my career has been either fighting against this, or seeking out places where it's not true.

Most organizations want to APPEAR to be data-driven, but actually BEING data-driven is much harder, and usually not a priority.

Good quote from the article:

> Piles of money + unclear outcomes = every grifter under the sun begins to migrate to your organisation. It is very hard to keep them all out, and they naturally begin to let other grifters in because they all run interference for each other. Sure, they might betray each other constantly, but they won't challenge the social fiction that some sort of meaningful work is happening.",datascience
"Hello, i am looking for a DA/DS Mentor to help me in my journey. Somethings about myself , I have a bachelor in Management Science & Technology I have knowledge of SQL and some other tools and i am learning python at the moment. I am from Greece and it would help if the mentor is Europe based. I am willing to put effort and i would like guidance with projects etc.",datascience
"We have a timeseries dataset and we observe a normal distribution around its mean and median for one month, but for the next month its mean and median has deviated because of some external force,  I need to know how is that change in distribution is called",datascience
" Hello,

Basically, I'm trying to build an architecture for a training composed of :

Producer .py ---> Kafka broker ---> SparkStreaming.scala ---> Consumer .py

So, I want to produce data as json send it to sparkstreaming where ill filter it (combine the data of the last 30sec and send the mean value of one of the variables) and then stream it to the consumer, but when i did that, i'm have a 6min delay when 1st execute, even tho i specified to execute the filtering every 30sec.

So, my question is why i'm getting that 6min delay? Am I missing an information about spark streaming?",datascience
"I'm currently looking at potential low/no code ML solutions that the rest of the business can use to reduce the load on the Data Science team. Rather than just rely on the demos from the salespeople I'm looking a creating/sourcing a small test suite that we can use to assess the tools.

Anyone have an suggestions/experience with this sort of thing?",datascience
"I am pursuing my masters in management from ESSEC, France and working at a startup as a data scientist (work-study programme). I will be graduating next year with 2 years of data science experience, with 2 years of pre-masters experience in product management. Hence, I am trying to figure out career options to target for my full time role that not only involves just analysis part but also some form of managerial responsibilities as well. Thank you for the help",datascience
"Hey everyone so I’ve been at my job as a data analyst for a little more than 1 year. I got my masters in statistics and wanted to get into the data analyst field since my bachelors so I was really excited when I got my job. Since the last year my boss has been shooting down basically most of my ideas. 

They gave me complex survey design data to work with and at first I suggested doing some models but I was told that was too advance for public health stakeholders and just should stick to descriptive statistics. Well when we presented to stake holders they ended up asking for more advanced statistics so after that I was allowed to do a manuscript. 

As I was writing it, I went based of their goals from their survey. I wasn’t part of the development of the survey and was just responsible for finding something interesting based on their goals. But when my boss saw it she just asked me why I did it that way, then I explained to her my rationale for using a logistic regression but then she started questioning why I even chose those questions as part of the goals which i responded as those were the questions defined in their documentation(she was part of the survey development) as part of the goals.  She said I still hadn’t explained why I did it so she  asked me if I had a comprehensive report of all the survey questions which I  didn’t. Because my first task had been to do descriptive statistics.  But now I was now being tasked to do some modeling not to do a bunch of cross tabulations.  I had already written over 40 pages of a manuscript and she basically asked me to start over and so then I asked for clarification if they wanted to do every possible combination of 50 questions or a summary of what I found interesting? I thought I was going base off their goals and wasn’t sure what she wanted from me. Which then I was told to do a comprehensive report of demographics against the 50 survey questions. 

So I did it in a dashboard and which was the first time this department had even used a dashboard. I was pretty happy and excited to learn how to do a dashboard.  When I presented it to her she praised it but then started having so many edits and changes. I didn’t feel like she was satisfied with my work. Some of her feedback was that she didn’t like that I had used the questions as titles and wanted them now as statements, and she said it was personal preference but yet she continued going through a list of these things I need to change in the graphs. This has been going for 5 months and I can’t move forward with anything in this project . 

Everytime I try to do what she wants and present it to her she asks me if I have done something else and now I have to do that something else and it just keeps on repeating. I’ve asked her to give me a template of what she wants but she doesn’t have an answer. She just tells me to ask myself what is this telling people. In the comprehensive report/dashboard she asked me to remove questions because she said “you have to ask yourself, does this question have any meaning?” I was told to do a comprehensive report so I didn’t remove any questions and I didn’t write the questions myself she and the team wrote the questions so I’m confused why they wrote questions they don’t think are meaningful to begin with? 

I guess the team has noticed I’ve been “arguing” with her but all I think I’ve done is ask for clarification so I can have some progress and be able to satisfy their needs. Ever since then she’s been asking me to email her everyday what I’m going to do and when I’m clocking in and when I’m clocking out. This started about 4 months ago. Even on her days off/ vacation she’ll ask me to send her an email CCing her and my other supervisor. I understand CCing my present supervisor but I’m not sure why her as well since she’s on her days off. 

I’m looking for another job rn and feel so disillusioned in this job. I felt like none of my work was satisfactory and was constantly questioned for what I had done. I started wondering if maybe I’m an idiot and couldn’t understand what they wanted or if they were not being clear. I’ve spoke to another data analyst in the team and she told me she felt the same way. She doesn’t like working with our boss because she constantly interrupts and has also shot down all of her potential publications. My only fear with this is that I know I only have a little over a year of experience and I’m missing something and go to another job not learning how I can be a better data analyst. Does this sound like micromanaging behavior or does it sound like lack of experience?",datascience
" 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",datascience
"Hello All,

I am a product owner of DS & DE at a <200 startup (going on 4 months now) and have had great success in pushing an agile-lite (not 100% strict) process into place that has received positive feedback from my DS & DE team members.

The problem I am looking to solve now is some DS team members have approached me wishing to pursue passion projects that are not product-driven, but have a good deal of future value to them for the company.

I wanted to ask how yall think I should handle these requests. My current idea would be on allowing those who wish to pursue such projects do so on a time-constraint basis (such as 5-10hr per week) as long as they are getting other work done on time. I don't want to push a bad work-life balance, so I would like to put the impetus on them with the amount they feel comfortable with pursuing, but make sure it doesn't go overboard by providing a check when it feels necessary.

Any suggestions?",datascience
"Hello, feel free to delete if not appropriate.  

I am looking for direction on a project.  I am working to create a budgeting tool. I am proficient in Excel, and am comfortable with VBA and Python.  Fully self taught.  The team I am working with are Excel beginners to intermediates.  None of them are familiar with Power Query.

The team works with budgets that vary based on timelines, number of employees, inflation, and other factors such as number of miles traveled.

The current tool is in Excel, and uses fairly extensive and heavy formulas across several sheets.  The person that built the previous tool did their best, but the workbook is a mess, so they want me to make a new tool.

The team uses to tool to pull a budget, make updates to some of the specifications, such as length of time of the project, number of employees, etc.  the tool then outputs a budget with updated values based on the changes.  The next time there needs to be an update to the budget, a team member pulls the most recent budget, and starts the process over again. 

I have previously built similar tools in the same industry at a different company.  I used Power Query to transform the data,  and macros/VBA to format the output.  But eventually the whole got very heavy and would take up to 20-30 minutes to run.  While this was far less time than it would take to do manually, it was not ideal.  And it made it clear that the limits of Excel were being pushed.

The team is pretty set on this being based on Excel, but could possibly be persuaded otherwise if the solution is user friendly.  They have no knowledge of Python or anything like that.  My initial thought was to store the data in a database, and pull budgets into Excel using SQL, allow users to input changes to project specifications, then push back to the SQL server, and do transformations based on users updated specs there.  I have little to no knowledge about SQL.  I am confident I could learn it well enough to implement a solution if this is the best use for this case.

I am hoping that someone with a greater data science understanding than I do can point me in the right direction.",datascience
"Hey! While browsing Reddit, I saw this Youtuber name **Krish Naik** mentioned various times, and how he has helped many people get a good understanding of Data Science and Machine Learning. In his 6-Month Data Science Roadmap video, he mentions a course he sells called Data Science Masters with a 7-8 month duration ($50), which goes over a lot of topics and it just started a month ago. I don't want to link the course so this doesn't go as a promotion, but wanted to know if anybody thinks it might be worth it, as it seems like it has a very good structure with various projects.

#",datascience
"I am currently a master's student taking my first ""true"" ML class. Its one of those extremely math heavy classes where you see more Greek than English. The class is fast paced-- like one algorithm a lecture fast-- and although I do feel like I understand a decent amount of the content, I can say that some of these topics absolutely make my brain melt. Trying to understand EM/GMM makes me want to curl up into a ball and hide in a corner. I've noticed that this subreddit really stresses the importance of having a deep understanding of the mathematical intuition behind these algorithms, and although I definitely do understand at a high level what is going on, I am worried that I won't be able to fully internalize all of these proofs and derivations before the job search. My mathematical intuition is just not that strong. I was wondering if anyone who currently works in the field feels, or has felt the same way. When you started looking for jobs did you feel rock solid in your deep-level understanding of these machine learning algorithms? Or did you find that you became more confident in your understanding through working in the field. To be honest, I really am starting to question whether or not I'm cut out for data science, and I just want to know if anyone who has had similar struggles managed to find their way down the line.",datascience
"I'm always curious about new programming languages and [Nim](https://nim-lang.org/), a systems language, said to marry the speed of C and the syntax of Python has always been on my radar.

I came across this blog post today: [https://benjamindlee.com/posts/2021/why-i-use-nim-instead-of-python-for-data-processing/#:\~:text=Nim%20allows%20run%2Dtime%20constants,file%20line%2Dby%2Dline](https://benjamindlee.com/posts/2021/why-i-use-nim-instead-of-python-for-data-processing/#:~:text=Nim%20allows%20run%2Dtime%20constants,file%20line%2Dby%2Dline). and got quite excited about the possibility of using Nim for data science tasks.

Wanted to ask the thread if anyone else has experimented with Nim for their Data Science tasks.",datascience
"My professor said to me it is hard to find remotely jobs cause the company can't give you a secret informations
And just begin work .",datascience
"For me, I know these roles don’t pay well, and so my way of continuing this passion of mine is to do my own projects on sports (nba is my sport) and then post them on my blog. Usually I’ll ask a question of interest (what makes a scorer truly lethal?) and I’ll try and do some analysis to try and answer the question and talk about it in a nontechnical sense etc. but that’s about it. 

I wanted to maybe take things to the next step, and my dream is to work as a statistician in AWS next gen stats or something, but that seems like only a dream for now. 

What did you sports data nerds here do with your passion? You knew you couldn’t get a reasonably well paying job in sports analytics, so how do you keep up your passion?",datascience
"I am 22 years old and I am attending an evening computer science high school made for adults who dropped out in their teens. In two and a half years I will finish it and then go to University (not sure if computer science, statistics or something else).

For now in school we are studying computer architecture, ASM(on emu8086) and C programming. These are not things inherent to possible jobs I would like to do but they are giving me a very good foundation on which to build. At home I delve a lot into programming again with C and I am studying in depth mathematics and then starting a course on the mathematics needed for the study of artificial intelligence.

I'm currently working a part-time job that has nothing to do with what I study, it's just to have a minimum of independence while I go on with my studies.

I inquired about possible avenues to pursue but found a lot of confusion. So I really like both theory, math and then subjects more related to data science etc. Both the application and engineering of things. I would like to take the best things from data science and merge them with the best things from software engineering.... Because I know that in both of these professions sometimes the inherent job ads put this label on it but then what you go and do are basic, boring things...

I am a fan of artificial intelligence, for now I use the products of it but I want to get to the point where I know how they were built.

At first I considered data analyst and then scaled up in position and backend developer.

The former I discarded because even though it could lead me to data scientist it would lack the engineering part while the latter I discarded because most of the jobs make it explicit that I will only go to CRUD most of the time...

What job positions exist that taking the best of both worlds and are very much related to artificial intelligence and that you need to have very good theoretical knowledge and great engineering skills ?

They don't have to be entry level, but if not can you tell me what kind of entry level work will get me to the job you mention ?",datascience
"I’m in an online statistics class in a data science master’s. I don’t have a lot of background in statistics. I feel like I’m tracking along with all of the reading and coursework. But when it comes to the questions on quizzes and tests, I feel like I always need help to get started on answer.

Oh, is this normal? I feel like I am either cheating (not real cheating, but cheating myself) or not really learning what I need to learn.  

Any advice on learning statistics specifically for data science?

Am I alone in this?

Thanks!",datascience
"Hello!
I am preparing for a case study interview and would appreciate if anyone can share good resources for space optimization case studies with Python. Internet search didn’t help me with helpful resources.

Although I did learn that linear programming approach is used over ML. I personally have never worked with optimization projects. 

If anyone working in this field can guide me for preparation resources, I would really appreciate it.

Thanks.",datascience
"As the caption says.

I (25m) find myself asking for a lil bit of a career guidance. 

I'm about to finish my bachelor in Psychology with and underlying bad taste in my mouth. I don't feel like I picked the right bachelor for me and most importantly like I learnt anything useful that i can directly apply.

I've alway been a geek type of guy, fascinated by business, trends and technology. Last year I was trying to approach python and html and abandoned them both prematurely due to life happening. I love the idea of understanding trends and use them in order to get better results in a business.

I've never liked math since i didn't see the utility in the context it was taught to me. All my professors would mindlessly tell me to remeber a set amount of formulas and use them without any context nor purpose. Now what I'd love as a data analyst would be to apply the gathered information and find a route to optimize a business.

Do you think that a good feeling and the tought of something I could  like would help me in a sudden change of route like this one?

underneath you'll find the table of contents of the course i wanted to take which apparently doesn't require a previous field related  bachelor degree

[https://www.in.tum.de/en/in/current-students/masters-programs/data-engineering-and-analytics/compulsory-elective-and-support-elective-modules/fpso-2018/](https://www.in.tum.de/en/in/current-students/masters-programs/data-engineering-and-analytics/compulsory-elective-and-support-elective-modules/fpso-2018/)

Just reading the contents i feel a weird mixture of fear and excitment.

I started with the idea of marketing research analyst but the courses in germany (where I wanted to study abroad) seemed to lack to give more space to strictly scientific courses.

&#x200B;

* **Do you believe someone with no previous knowledge on the field could approach a course like this?**
* **What was the biggest difficulty you encountered when you first approached data science?**
* **Is It a field with a steep learning curve?**
* **Would a possible employer laugh at my resume seeing no good foundations with a Msc like this one?**

**TLDR;**

Can someone with a psychology degree approach a Msc data science ?",datascience
"I just received my PhD in evolutionary biology a couple of months ago. My dissertation topic was the statistical modeling of antipredator behavior, so it had a roughly balanced statistical and biological content. I currently work full-time as a biomedical statistician at my university. While I love the job, the pay is low, and the lab has virtually no opportunities for professional advancement. I have been applying for biostatistics jobs for a short time, but I see very few entry-level jobs. Most positions are director or senior level, which disqualifies me immediately due to my lack of on-site experience.

I'd like to know if anyone has advice for someone with statistics chops and a genuine passion for the discipline, but without the specific degree and experience employers seem to want.",datascience
"Hello. I am new to machine learning. And would like to ask you to take a look at my notebooks on kaggle if you have some free time. I would be happy to see feedback and suggestions on how to improve my code.


I really appreciate any help you can provide.

My kaggle profile: https://www.kaggle.com/crn4tww",datascience
"I have tried other subs but none have answered my question so I thought to ask here.

If you could create a search engine today how would you create it now compared to how google and the rest started off. Taking ML and A.I in to account, what will be some of the best ways to collect and store data? Still using RDBMS or is there a better way? Would the crawlers change the way they behave? What else would be added?

I would like to know how people think so I can learn some deeper concepts.

This is one resource but I would like to know why these ways and not another way?

[https://github.com/codecrafters-io/build-your-own-x#build-your-own-search-engine](https://github.com/codecrafters-io/build-your-own-x#build-your-own-search-engine)",datascience
"I'm working on a research project for which I'm supposed to fit multiple models on data that is around 60 GB. The models I am working with are also computationally expensive. 

My first idea was to ask for access to my institution's high performance cluster, but it seems that my supervisor is not facilitating that. I've worked with one in the past, and I'd also prefer having a user interface and not having to mess around with bash files too much if possible.

&#x200B;

So, what you all use for such tasks? I'm happy paying a subscription to a service for a few months, but I need to be able to run my computations as if they were small data.",datascience
"I was hired as a graduate from a machine learning master during the pandemic, after coming from a computer science background. I am at an organisation of about 350 staff and work mostly by myself, a couple of other guys do a bit of data stuff and we have no project manager.

My actual boss has no clue about Data Science or what is needed to deliver models to production. I have tried to express that the team needs some leadership but he says it will not happen until I can prove ML is useful. I am under a fair amount of pressure to deliver something useful.

Is this sort of chaos normal in the Data Science world? Thinking about ditching it and going back to software engineering or data engineering.",datascience
"Looking for roles in NY as a PhD in physics (although, my research relies on DS), however I can't find any junior DS roles or junior analytics roles that use Python + ML + stats. What's wrong with the market in New York? Whenever I look for ""junior ds roles"" i see a lot of senior places, but what am I supposed to do? Or should I consider myself as a specialist having N years of experiecne?  


Ty for any help",datascience
"Hello,

My goal is to specialize in Data Science/Analytics in the domain of manufacturing & supply chain. So far I have done work in predictive/preventive maintenance but I feel like that could be too narrow of a focus. I now have two questions regarding this:

* What parts of data science should someone focus on to be valuable for clients in this domain. This includes topics as well as technologies that one should deep dive in in addition to be able to solve standard analytics problems. So far I have thought about:

1. Time Series Analysis, Forecasting
2. IOT --> Any tipps how to get started here?
3. Computer vision? Is it worth getting started because this is such a big field?
4. Work with streaming data instead of batch?
5. Running Models in an Embedded Environment (to stop production if fault is predicted)?
6. Suggestions on what I have overlooked?

&#x200B;

* Is it realistic to do freelance consulting in this area? I feel like building a client base could even be more difficult than in other areas, as manufacturing companies always have a certain size. Data-driven insights into logistics could maybe also help small ecommerce businesses.

&#x200B;

I know the question is a bit scattered, but thankful for any advice.",datascience
"Hello everyone i have been recently trying to create a absa system for a fixed number of aspects currently the strategy i have finalized is to train a ner system to identify certain entities like battery, screen etc and find their sentiments using Spacey dependency parser and depending on the entities i will map them to a aspect for example if the entities are screen and battery i will map them to then mobile hardware aspect is this a good approach or is there any other better way of doing absa ?",datascience
I mentioned I occasionally read the blogs there on another redditor’s post and it was quickly downvoted. I would like to know if it’s really shunned in the DS community. I’d not heard it was some running joke in DS communities that TDS was trash. I don’t want to be learning from a bad resource. Barring actual research papers and journals what’s a better blog site alternative?,datascience
"I'm a high school senior, considering whether or not to attend med school. I'm hesitating between medicine and data science.

I'm interested in medicine 1.) I love!!! biology, but not as much for research, so being a medical doctor entitles me to both learn life science and associate with people. 2.) They do make a lot, and I have to admit that I do so partially out of money. 3.) I've been thinking about medicine from elementary school.

I've also been considering data science since 1.) I'm also interested in it; I've taken Calc III & linear algebra in my local cc and doing pretty well, current self-studying differential equation (they don't allow me to take this course during break) 2.) data scientists also make 300k or above, particularly here in the bay area.

My parents do not care what major I choose, and they are willing to supply half of all my med school tuition and fee (if I'm for medicine). But that's still a lot, for both my family and me. Plus, I have to do tons of volunteering (I'm OK with volunteering, but don't feel like doing it if pushed), extra money on transportation (flying to interview spots), living expenses (MCAT review material, booking hotel).

Hence, I'm wondering if I should continue to pursue medicine. Thank you for your valuable advice.",datascience
" I was a noob in R and using it to teach a graduate econometrics class. (No more STATA!)

One thing I did not realize at first was how easy it is to do parallel computing. Thank goodness for students who knew more coding than me!

For those who don't know, parallel computing is when your computer uses multiple processors to speed up a large loop. My computer has 8 cores but rarely uses most of them.

Using more can speed up your code, but it is also possible to use too many.",datascience
Looking to get some sense about how much value do fellow  DS people see in their works. I personally don't see much significant impact which is mostly because of the industry I'm working in and the org culture. So trying get some understanding about the outside world...,datascience
"I was given a pretty open ended problem that involves recommending brands to customers in a bank. Note that the brands we can recommend are brands that the customer has not yet bought from. 

Essentially I have 4 columns  \[customer\_id, brand\_id, amount, date\] that represent single transactions made per client in a brand. The volume of data is huge (33 million rows) so there is no data scarcity and these transactions are in a 6 month window.

Here's what we plan on doing:

1. First 4 months for training and last 2 months for testing
2. Encode labels (in the training set) for pairs (customer, brand) so that it's 1 if the customer has spent over a certain threshold and 0 otherwise - we ignore brands the customer has not bought from. This threshold would be a different median for each client so that we have the same number of 0's and 1's. In practice the thresholds/medians we obtain are around 20 dollars so the 0's and 1's could be interpreted as the client not liking or liking a brand, respectively.
3. Train a collaborative filtering model (like the SVD) on the train data.
4. Make predictions for the test set and use the Mean Average Precision metric as evaluation metric.

Would these steps be a valid way of building and evaluating a recommender system model?",datascience
"So seems like every 5 years i reach the point where i start looking into different career and this time my eyes are on data analyst, scientist position.
So was an owner of trucking business and 100% of my time was in home office dealing with customers, drivers, accounting and didn’t like the job at all, so when i had an opportunity i sold an moved on.
Then i became an HVAC (heating ventilation air conditioning technician). I enjoy this a lot as most of the time i work individualy on repairs or smaller projects. However, this role i very seasonal and “on call”. A lot of emergencies so you never know when you are going to finish your day (does not help when you have family) Also i like moving around all day but it is physical and years goes by i can tell it is getting tough on the body.

So im slowly looking to possibly change my career where im more flexible and can do the job from office or home within business hours instead of being called to hear i need you NOW. 

I am not religious because i don’t feel like there is any proof. I like to double check facts and confirm them. I always ask why to anything 💁‍♂️. 
I have degree in BSc Exercise, nutrition & Health which is not very relevant. However i remember having classes of “Research Skills” that involved Excel formulas, charts etc that i enjoyed a lot.

My question is if there is anybody in Eastern Europe hired as one and what salary range companies are able to offer there? Im currently in USA but we are planning on moving back to out home country Lithuania. All of the information on youtube etc mostly talking about USA market sheb it comes to salary.",datascience
"I am a first semester computational science student in Germany with some experience of Python programming, mainly for numerical method implementation. I also recently started using C++ and Matlab for my courses.

I am interested in learning skills/concepts for data science (stuff like stats, ml, data exploration, sql, etc). I think a bootcamp would be an ideal environment to learn a lot in a short amount of time. 

Can anyone suggest bootcamps/short course/certificate courses offered by institutions in Germany, especially German universities that I can take. 

I emphasize on German institutions because I have heard that employers here only accept certificates of skills from German institutions. 

Anyways, please suggest me some bootcamps or short courses to use my semester break.",datascience
Let’s say I have a huge collection of document and I’m building an information retrieval system on these documents. What would be the best way to store these data in this kind of applications? Would a data lake built with let’s say AWS S3 be the best option or are there better ones?,datascience
"I work on US-based US candidate roles, so I don’t really have a lot of insight on candidates coming from outside of the United States. 


That may be something you’d have to look at recruitment firms outside of the US or other recruitment firms that work on corporation to corporation roles.",datascience
"So I have an interesting question. I’m in my mid 30s and I’m currently a DS working on security issues for a tech company. Before making the transition to DS, I worked in management both in government and in hospitality. I made the jump to DS and got a masters in it because (outside the income) I wanted to enrich my management skills with data driven understanding.

When I finished my graduate degree I made a financial decision to get the DS experience instead of returning to government or hospitality leadership. 

I got my shot in tech and I love working here as an IC but I’d like to move to leadership based on my managerial background and my management experience isn’t in tech. 

Should I continue the grind here or look to bounce to leadership at another company now that I have both DS and management background? 

Just curious if anyone has run into this before.",datascience
"The degree I’m pursuing has specialization with quantitative analysis on it but I feel like I was taught only surface level things about stats. I only needed to take two classes for the specialization where I learned about econometrics, and very little about linear algebra and matrix algebra. I’ve also taken two stats courses on the side where I learned to code with R and do some data visualizations in one class and the other class was just an intro to stats. Also, I’ve been learning how to code with SQL but all I’ve learned was syntax so far; I haven’t done much with it. 

I’m supposed to graduate this semester but I’m just not sure if what I know is enough for an entry level data analyst or data scientist job. And what I’m mainly worried about is if what I learned in stats enough.

Should I just major in stats as well?",datascience
"Hey everyone! Working on a research project and I want to describe my data to be a live snapshot of performance, although data is pulled on a monthly basis for a system dashboard.

I was reading the definition of Near-Real time data processing and I noticed they did not provide an exact cut off of how long it takes from pulling data to publishing it. I feel batch data processing best explains my process but doesn’t emphasis the dynamics and benefits of monthly data pulls.

TLDR; would it be wrong to consider monthly data pulls near real time data, when nothing like that’s has ever been done before in the department?",datascience
"Looking for basic mathematical models used in f1, not involving algorithms (or simulations), and can be derived by a high school student grade 12 math honours. 

so far I've looked into tire degradation or using velocity/time telemetry to solve kinematics for various drivers and compare who is the better driver on respective sections of a track by taking derivatives and integrals of polynomials  but that might be too simplistic, and I'm looking for something involving a bit more complex math, something where i can apply calculus for optimization. I've thought of homogenous equations for modelling tire degradation, but not sure how to proceed or if my model is even correct \[ dw/dt=a\*v''+b\* ∫ v+c\*v'\], w is tire degradation t is time and v is velocity as a function of time",datascience
I am starting my first job as a DS after graduating and was wondering how do you stay up to date with all the new stuff after university? Especially if your job is focused on only one are of DS (e.g. you only do NLP) in terms of techniques used on a daily basis.,datascience
"Hi everyone. I work as a research assistant in a neuroscience lab. Recently, I fit an HMM and a SHMM to a multivariate time series dataset to model temporal dynamics in neural activity. Their output consists of a Markov chain, or a succession of states whose respective parameters the model inferred. I'm still learning how to work with HMMs/SHMMs and how to analyze their output, but it occurred to me that they're wonderfully good at simplifying multivariate datasets for analysis. I imagine their output could even be fed into an ML algorithm for forecasting.

Has anyone seen them used in industry? I suppose, I may be too amazed by the tool for my own good, but their simplicity is striking",datascience
"When I have to make some ML model, one of the first things I try is using some gradient boosting model (e.g. xgboost). Then I try to manually tune the parameters, engineer some features, and then maybe try other types of models.

I discovered libraries such as optuna not too long ago, and I use them to help to optimize the model parameters. Then, quite recently, I discovered that the use of AutoML libraries can be very helpful (I knew they existed some time ago but didn't really try them).

So, which are your favorite AutoML libraries and why?",datascience
"I have \~4-5 YOE in analytics, most of which as a data scientist. I am almost done with my masters in CS, and currently a senior DS. However, my position will be eliminated soon so im trying to be proactive in my job search.

I am really struggling to get interviews. So I was wondering if someone more senior would be willing to review my resume. Here's a link to it: [https://imgur.com/a/r9SQnq8](https://imgur.com/a/r9SQnq8)",datascience
"Hey all, was hoping for some thoughts on how to approach the long-term benefits of a successful AB test.

Currently I have a new client that reports the improvements from AB tests as permanent benefits, so a successful test we ran recently lifted revenue and the clients analytics team estimated the change would increases their MRR by $1.2m per month. 

I have tried to move them away from this line of thinking but don’t really have a good process or framework to substitute their current process, so I was hoping to gather some thoughts to see what I could recommend/deploy for them going forward. 

My current thought is a model that regresses the impact over time, like a regression to the mean. Maybe over six months or a year depending on traffic volume?

I am working to educate them on gains realized from AB tests are not absolute, but that’s a different battle…

Any thoughts, recommendations or opinions from prior experiences will be greatly appreciated",datascience
"More data analytics than DS but I hope this sub is still a good place to ask this. I apologize in advance if it's not:)

Long story short: I have a BA in Business, a MA in Political Science, got a few a job offers after graduation but I realized the jobs; especially their environments, don't interest me much. Thinking of switching to data analytics for different reasons, the main ones being that I am great at extracting insights from information, I like data in general and my long term goal is to work freelance.

I have been thinking lately about how I could frame my career and how i could sell myself as a data analyst without degrees and a strong background in the field. I came up with the idea of marketing myself as a data analyst capable of covering 3 different fields: 

\- Data analysis: I am self learning SQL, Python. Planning to learn Tableau and/or Power BI. Decent with Excel.

\- Data visualization: I am self taught graphic designer (Photoshop, Illustrator, Figma), web dev (HTML, CSS, limited JS) and I could self learn other softwares to get more advanced with charts/business tools.

\- Data storytelling: I believe my social science Master gives me a key strength in the fact I am able to write nicely, I am good at structuring my thoughts and I have an ability to convey messages/findings to stakeholders. I have been writing essays and articles my whole life. 

I would be self teaching for 5 or 6 months and then I am hopeful to find an entry level position. Do you think this would be realistic and how attractive this profile could be?

(I am 26, I live in the EU but I have a work visa in Canada and the UK as well)

Thanks a lot!",datascience
"Hi, sorry for the likely to be dumb question.. I'm relatively new to these topics.

I have a file containing rows with variable length and a class (defined by value 0 or 1).

Is it possible (and it makes sense?) to use k-nearest neighbors classifier to classify variable input lenght data? the file is something like this: [https://gist.github.com/edoardottt/46dd13c60408e95c1685ee88b5f6ace8](https://gist.github.com/edoardottt/46dd13c60408e95c1685ee88b5f6ace8)",datascience
"I’m in a bit of a weird position. I graduated from a data science boot camp and was able to land a data scientist role for a small start-up, and I loved what I was doing. I was not only able to apply what I learned from the boot camp but I was able to learn and grow from the more senior data scientists on my team. A year has past and due to the economic situation the tech industry is facing, I got laid off and I quickly scrambled to find a new role.

Perhaps I could’ve / should’ve looked for a data scientist role, but I settled for a senior data analyst role (needed a new role fast for financial reasons) from which they advertised could grow into a data scientist role. I know title shouldn’t matter, but I really liked the kind of work/research and presentations I was doing at my previous company and felt pretty good at it.

Now 6 months has passed working in the senior data analyst role, but I’ve only been working in Tableau, running SQL queries for partner teams, but not doing anything close to what I had done before, and so I kind of feel my skills are not fully utilized? Maybe I’m being arrogant / impatient, but how do show I can provide more value doing more advanced analytics? I’m trying to make it work, and find use cases and such to showcase value from the more advanced analytics I used to do, but my manager is ruling out that it’s not needed right now.

Did I choose the wrong role? Should I look again for a new role?",datascience
"I am trying to analyze data from search queries on my website. I have a database table of queries with timestamps and a parameter that indicates whether the user clicked on one of the search results (supposedly successfully finding the information they need). I am currently thinking of identifying the queries that do not produce the desired outcomes (which means that I might need to adjust the “searchability” of the associated pages/resources on my website). To achieve this, I am planning on splitting each query into separate words, and then counting the number of times each word appeared in successful queries. The ratio of successful queries to all queries with that word should indicate whether information tied to it is difficult to find. However, I am not sure how to adjust for noise - for example, if a certain word appeared in only one search and it was unsuccessful, it will have a 0 success rate, however, since it is an extremely rare query, this result is not that important. So my question is, what would be a good way to adjust for the frequency of words, prioritizing the more frequent queries?",datascience
"I have the opportunity to attend a free 17-week software development program through the military after I complete my masters in data analysis. I don't have much DS experience other than some basic analysis, and knowledge management  so far.

Do you think this would be valuable to a beginner data guy looking to be valuable in the civilian market?",datascience
"I get the impression that some have autonomy when determining what the ""main takeaways"" are from an analysis while others prepare the results for a supervisor to interpret and communicate. 

I have found that some data science managers pay lip service to the idea of collaborative storytelling but find it much more convenient to skip the collaborative part. It's time consuming and challenging to resolve disagreeing narratives about a problem. But certain narratives may also present inconvenient truths that managers prefer not to communicate for one reason or another. 

How does it work on your team? What's effective and what isn't for you and your situation?",datascience
"GPT3 is great but not cheap, I also have privacy concerns using their API. Is there any LLM offers similar performance but allows self hosting?",datascience
"Hello all, getting into data analysis and taking a few courses online and for one of them I've been given the freedom to find and analyse any dataset with R but finding it a bit difficult to settle on a research topic, hoping I can find some inspiration here. Any advise would help, mostly looking for trade and commerce research topics but I'm open to virtually anything.",datascience
"Hi all, I recently got an interview for a data science internship and there’s 1 problem: I don’t know anything about data science. I’m a Finance major and I applied because the description sounded interesting. Are they any industry basic technical questions I should know?",datascience
"Hey everyone 👋 I’m Ian — I used to work on data tooling at Stripe. My friend Justin (ex data science at Cruise) and I have been building a new free local editor made specifically for dbt core called Turntable ([https://www.turntable.so/](https://www.turntable.so/))

I love VS Code and other local IDEs, but they don’t have some core features I need for dbt development. Turntable has visual lineage, query preview, and more built in (quick [demo](https://www.loom.com/share/8db10268612d4769893123b00500ad35) below).

Next, we’re planning to explore column-level lineage and code/yaml autocomplete using AI. I’d love to hear what you think and whether the problems / solution resonates. And if you want to try it out, comment or send me a DM… thanks!

[https://www.loom.com/share/8db10268612d4769893123b00500ad35](https://www.loom.com/share/8db10268612d4769893123b00500ad35)",datascience
"Context: I'm a Senior DS and have been give an option two years age to decide between doing IC or management as it will determine my promotion and development path.

Within my company, shifting to management requires demonstration of leadership, vision and driving strategy. What this exactly translates to is sitting in a lot of meetings, delivering presentations on where to take the data and how to maximize its potential, and then building capital planning documents for budget approval.

The work I shared above is tremendously people intensive, and it takes away a lot of time from doing Data Science ground work. And the risk is once I take this path, my core Data Science skills shrink tremendously. I understand how to solve a problem and can provide guidance, but I lose the details of the process. The past two years I've been yo-yoing between IC and Management, and it's been very difficult to know whether taking the management path is worth the sacrifice of losing technical expertise, or if I should resume in IC.

Within my company, high level ICs hit a plateau in terms of leveling (our department maxes out at Staff), and then one can rest and vest. They do incredible work and partake only complex projects, but the visibility they receive is minimal: they're basically VPs on-call Data Scientists when they need the most expert candidate to answer a problem, but those instances are rare for strategic decisions where you can receive directional information using simple models.

Luckily in my org the managers are incredible managers, and each of them carry a unique trait and are one of the best in terms of unblocking problems and protecting the team from politics. But because of those activities, they're basically full-time people managers. While they help prioritize tasks and provide advice, they do very little of Data Science itself despite their incredible qualification. And all, if not most, admit that all the skill they have left now is SQL and basic python coding to vet and understand their direct report's work. Nothing beyond that as they rely on their team to communicate their approach well.

If you have been at this crossroad between IC and Management, what helped you decide which path to take? And was the sacrifice of losing technical skills or losing visibility worth it in the long run?

EDIT: Just to note, pursuing management and shifting back to IC or vice versa is a very rocky road (annual performance gets delayed as they have to re-adjust their development metric, a lot of paper work, and a quarter or two quarters of re-org to help move direct reports/fill in M or IC role)",datascience
"I am stuck between two courses. 

1. Health Informatics and Data Science MSc
2. Urban Data Science and Analytics MSc

Both are at The University of Leeds. 

I have a BSc in Psychology. 

Admittedly, option 2 seems the most interesting to me (from looking at the course details) but I am more concerned with job prospects. I made a mistake with doing psychology and so I don’t want to make the same mistake twice. 


Would really love to hear from people with experience in these areas. Thanks!",datascience
"I obviously have no relevant work experience as I am trying to transition careers and I am running into some friction. 

I have completed some college but I dropped out before I could get an associates degree even due to lack of interest in my major at the time (business marketing). I knew this would be a major disadvantage for me but I plan on going back for a computer science degree and I would like to find a job that offers a tuition reimbursement program. 

I do however have the Google Data Analytics career path certificate and the IBM Data Science and ML career path certificate for whatever that is worth. 

I was able to find quite a few job listings for remote work that only require a high school diploma but I haven’t had any luck yet. So I’m hoping to find someone here who can help guide me in the right direction and answer a few questions: 

I think I have a pretty decent resume but I don’t have a cover letter. Is that something I should write? 

I’ve been asking for the $16-17 starting wage range. Is that too high? 

Should I apply to different positions like Data Entry instead of Data Analysis?",datascience
"As title says. Just out of curiosity. Do we have here anybody who has some knowledge about sports analytics jobs? 

Is it much harder to get into this field that into other data related fields?

Is the job market destroyed and therefore are the salaries a certain % lower than in other fields for the people with the same experience/skills?

Edit: Thank you all for responses. Some of them are very useful and cover the aspects that I didn’t thought about.",datascience
"At my current company, we pay lip-service to it, but ultimately the exec suite has established perverse incentives by measuring success in terms of # of releases (not explicitly, but it's evident come performance review time). E.g. if a new feature gets A/B tested and performs poorly, PMs are still highly incentivized to ignore those results and release the feature, because it's better for their careers.

Also we have a lot of top-down interference: constantly redefining metrics to say we've hit ridiculous OKRs, extrapolating revenue increases from increases in unrelated metrics, changing our product roadmap that was drafted after being informed by data to cater to exec personal preferences, etc. We also use ML for the sake of it; it's a shiny object to brag about in meetings even though our models are expensive and generally wouldn't beat a basic set of heuristics.

Are many companies like this? How do y'all deal with being pressured to do low quality work?",datascience
"I am creating a database that contains links to websites of news articles, government websites etc as supporting evidence for some of the data in the database. I am concerned that some months down the line the links may be broken and become 404. I wanted to ask what is standard or good practice for storing such evidence? Should I download the website as an HTML or take screenshots of the website so that when the data is being verified and validated or when someone is checking the data years down the line, the evidence is preserved ? Or do people use the wayback machine for this in their databases? Would appreciate if someone can guide me on good or best practices here. Thank you!",datascience
"The first release of Chat-GPT is already hugely transformative. We are kidding ourselves that within a few years, AI won’t be able to go from prompt to hundreds of line of working code in seconds.

The only limitation of AI currently is navigation of the physical world, which won’t be more than 10 years away.

Seriously, what was once a niche field for 1% academics is about to be taken over by a much smarter, faster AI. In hindsight, quite obviously AI would be able to navigate code and data better than us. 

The ONLY thing we have left is understanding and translating business needs of humans. We navigate complex social structures in a physical world and translating human needs into technical requirements is all we have left.

Someone prove me wrong please.",datascience
"I'm an oldish guy that use to make custom database apps and custom reports with MS Access and VBA.

I've been out of the tech loop for a while but I'm still somewhat fluent in Python, SQL and could pick up C# again if needed. 

What tools (free or paid) are available today to manipulate data and that can act as a hybrid development, reporting, data entry tool?

I'm trying to reduce the amount of custom handwritten code for a new project and would rather build my data manipulation pipeline with some visual tools.

Edit: added SQL",datascience
"Let's say we have a model and its features (feature\_a, feature\_b, and feature\_c). Then, we have training data feed into it and it scores these features accordingly.

So there will be like data that the model scored features like

feature\_a = 0.6, feature\_b = 1.3, and feature\_c = -2.3

And we have many data of these for many different feature values.

Therein lies my question, is there like a good way to analyze these? I was kind off thinking maybe perform quartile analyzing and set thresholds. But I'm stuck right now to provide something meaningful. 

The end goal is to maybe find some anomalies and maybe check how we can improve the performance of the model by tweaking the data we feed into it.",datascience
"So for some background, I've decided to change careers to get into data analysis/science and will be attending college soon for a BA in data science. But the market seems... Flooded. Am I late to the game or is this still a valid career track?",datascience
"My manager asked me to “run multivariate analysis on all of our customers to identify what factors make them churn.” The dataset I extracted has 700,000 rows and 100 variables (~15 are numeric, ~15 are Y/N), the target variable is Boolean (Active/Not Active).

I feel like a fool asking this, but:

1) What would be the most straightforward way to generate some kind of a graphical representation of how these variables affect churn? 
Right now, my setup is: a) remove outliers, drop redundant columns, drop NAs or replace them where applicable, b) hot encode Y/N variables and then check for collinearity by populating a correlation matrix with all numerical variables and dropping anything that stands out, c) dummify categorical variables, d) run decision tree and random forest. Any recommendations/feedback? My manager loves decision tree because it’s easy to interpret and hates logistic regression because “black box algorithms cannot be interpreted.” 

2) I have a couple of limitations, the main one being is that I’m not allowed to drop ANY records from the dataset (“they are our customers and we cannot exclude them.”) I can potentially renegotiate this part especially since I already wrote ~200 lines to make this data look tidy. What would be the best argument to make or is my manager right? At the very least, there are records with insane values (120 years of age, >100 years of being the customer etc.), so I feel like it’s best to get rid of them.

Thank you in advance! The company computer I have is quite old (8GB Ram) and slow, so unfortunately I cannot experiment with a lot of different methods since I have to present the results to the stakeholders at the marketing department on Monday. 

There’s no one at my company that knows R so any little tip is appreciated :)

EDIT: I do realize that logistic regression is not a black box algorithm at all, it’s just my manager’s strong opinion on it (hence the quotes.) Thought it would add more context as to why I’m hesitant to use it as is",datascience
"I’m interested in DS and working towards getting a bachelors in stats. I’m curious what are optimization problems you all face. For example increasing speed of calculations, determining minimum variables needed, etc. I’m trying to make the eventual job search easy by getting as much hands on experience and expertise. Doing well in classes doesn’t always translate well to career skills and I want to avoid that.",datascience
"Personally , I think the best measure is dollars. Dollars really hits home when a company is making cuts. Hey I made some dashboards doesn’t hit as hard as ‘I saved 500k by improving efficiency etc’. Sometimes it’s direct sometimes not.

Ex. Saves a team of 5, 1 hour a day each avg salary of 100k then the project is worth 62k (100000/2080=48) 48x5x260

If it’s more complicated/less tangible I think ‘can they do their job without it?’ Vs a ‘nice to have’ on the other end as well as ‘How widespread is the impact? (Team/group/org/company). I feel this can still be approximated to dollars tho.",datascience
"Trying to find the drawbacks (besides time to run). Are there inefficiencies? Does it miss certain values of hyperparameters? Is it the best thing since sliced bread? 

So far it’s saved me a lot of time from manually doing grid searching/ cross validating for best set of hyperparameters and the best model.",datascience
"I’m a data engineer with 3yrs experience learning data science in order to switch my role.
As a DE in my past two previous companies, I did not get a great satisfaction of the work which I have done. We build pipelines, optimisation is bit interesting and designing. But never did I feel that I’m doing something useful, great like which requires lots of thought process, analysing. 
I love mathematics. While learning these statistics and other data science concepts, I am getting a motivation in me like yeah let’s do this. But I want to know what kind of work you guys do as a data scientist? Is it exciting?",datascience
"Hi Everyone

I am new to forecasting. I am working on a project for which the goal is to forecast employees' population in the next five years in different age groups. The primary focus is to reflect regional's population growth for different age groups on the forecast. I have the headcount from previous years for each category and the hiring and termination rates. I have other variables as well, such as the regional unemployment rate for each age category.

I have started using fb prophet with multiple regressors to do the forecast. I would like to know if there are other variables and models I could consider. 


Thank you",datascience
" 

It was a normal day in university.. and I NOTICED SOMEONE COPY AND PASTE THEIR DATA INTO AN EXCEL TABLE. Obviously it looked horrible, and they spent the next 10 minutes formatting the data manually into it's specific cells.

Here is how you extract data from an external source into excel:

[https://www.youtube.com/watch?v=lRpZjz3nKs8](https://www.youtube.com/watch?v=lRpZjz3nKs8)

Happy analysis :)",datascience
"I come from a CS background and have always been interested in AI and statistics. Also, I have been working as a Software Engineer for the past year. However, seeing the data scientists in my company doing so much research and data analysis, and so little AI, I think a role as an ML Engineer would suit me better.

Do you think it is worth doing a MSc in AI to have a better CV? Or having SE experience is enough?

Moreover, I have heard that ML Engineer roles are not very common and with the latest layoffs and cuts in budgets, it may be unlikely that I can find such position. What so you think about this? Is MLE a role with future?",datascience
"Hi, I was following a [tutorial](https://docs.delta.io/latest/delta-batch.html#append) on writing to a delta table. I know that normally you would set the path to be to a  container in your storage account, i.e. `table_path = f""abfss://{container}@{adl_in}.dfs.core.windows.net/delta/tables/{table_name}""`

But I just followed the tutorial, and set the path to some ""tmp"" folder:

`df_by_node.write \`  
  `.mode('append') \`  
  `.format('delta') \`  
  `.option('mergeSchema', 'true') \`  
  `.partitionBy('__year', '__month', '__day', '__hour') \`  
  `.save(""/tmp/delta/aggregated_features"")`  


I am running my code in a Synapse workspace, connected to a spark session, and it runs fine, but I was surprised to see that it seems like the `/tmp/delta/aggregated_features` table still exists even when I restart the session. Anyone know anything about the Delta lake defaults? I don't see this path in the ""Data"" tab under ""Workspace"" or ""Linked"" (in the Synapse workspace UI). ? How long does this table last? Is it visible anywhere in the Synpase UI?",datascience
"Hey guys. Just about to transition into the sales department at my company as a data scientist after working for about a year as a data scientist in a different department. Just wondering what your guys’ experiences have been that have worked in building AI/ML models for sales before. What kind of work to expect, whether I’ll be able to work on deploying my models, etc. I’d like to get more experience with devops but I’m not sure whether the sales department would have models running in apps or they’ll just want us to run our models and write reports on who they should target their advertising to. I have a meeting with my boss next week about it just impatient lol.",datascience
"Hi all,

I am a senior data scientist working with Python.
We have a request to build a small web-app that has nothing to do with DS.
The web-app should allow multiple users (<50) to edit data fields that are stored in a SQL database. It should have access control (user should only see their own data) and in the end a report will be generated from all user inputs.

Now my question is, what technologies would you recommend to use to be as efficient as possible. I don't want to spend hours doing web development and I only know Python / SQL / basic HTML.
We have Office 365, a Kubernetes Cluster, Hadoop and Tableau as environments / tech.
Appreciate your help!",datascience
"In the code below, is the first layer with 6 neurone the input layer? or the first hidden layer?

&#x200B;

      model <- keras_model_sequential() %>%
        #norm() %>%
        layer_dense(6, activation = 'relu') %>%
        layer_dense(4, activation = 'relu') %>%
        layer_dropout(0.1) %>%
        layer_dense(1)",datascience
"My bachelors is in Arts and Social Sciences but a specialty in data analysis so I know R programming and I’m teaching myself SQL. 

Any advice for internships? What was your experience? I’ve applied to 30+ and most have come back with a no. 

Most of the ones I’ve applied are outright Data Science/Business Analytics and of course they will choose someone with a business or computer science degree. 

I’m wondering if technical writer internships will be a good entry point, any suggestions? Thank you!",datascience
" It's time to start planning for the future of analytics! Join us as we explore the latest trends and breakthroughs in analytics, AI/ML, and data science. Get a head start on your 2023 analytics goals with Grazitti! #Analytics #AIML #DataScience #Grazitti  
 

Know More: https://www.grazitti.com/analytics-outlook-2023/",datascience
"Hello guys..

I have learnt the Arima model. But i want to learn the parameter estimation part. Like the calculations happen in the background of optimization of the parameters .


Can you guys please suggest me some videos or study material to learn it??


Thanks in advance",datascience
"I haven't programmed in close to 20 years but just picked it back up. ChatGPT and Github Copilot have made it shockingly easy to figure out how to build things, pick up new languages, and efficiently leverage prebuilt libraries.

My current project is trying to use my own data source to feed into a GPT model. I've now got all my data cleansed, processed, loaded in a DB, and tokenized.

The question I have is what is the next step? I was under the impression I could use ChatGPT API to leverage their model with my data and it would be useful... but it seems all you can really do is fine tune ChatGPT so it gives more relevant answers to your prompt? Is that correct?

My project is to use about 5,000 pages of press conference (within my industry) transcripts to create a conversational querying functionality with ChatGPT like dynamism.

So could ask something like ""what was the most common product launched in 2019"". ""What were some of the companies that launched X product and what were the main features they were highlighting for the consumer?""",datascience
I just graduated college and have been looking for a job as a business analyst or data analyst preferably in an entry-level position and have had no luck. I graduated back in June 2022 and have been constantly applying with no luck. Does anyone have any tips for helping me find my first job in data science?,datascience
"I have about one month to analyze about 1 billion pieces of reservoir monitoring data, but I have never conducted such a large-scale data analysis before, and I am familiar with python and SQL language. Mainly used pandas matplotlib and other commonly used data analysis modules. But I have dealt with millions of data at most, and this time I need to look for anomalies in the monitoring data, find out some features, and analyze whether there are data falsifications or reservoir safety problems. Are there any tutorials or tool recommendations that can help me get started in the analysis better? Is there any case experience in similar analysis? I really need a correct guide to improve efficiency, and I am very grateful to everyone who can give me suggestions on how to carry out the above work.",datascience
"I’ve been working at a consultancy in the UK for almost two years starting on their analytics grad scheme before getting promoted recently to a Data Science consultant. Currently I don’t see myself staying here long time (mainly due to an often unpredictable worklife balance) and I’m curious to understand what the bar is like for data science within industry.

In the time before my promotion unfortunately I haven’t had much direct DS experience and I’m now looking to address that however what I have managed to build up is Python, cloud experience (azure, gcp) and some basic mlops. Despite this given the nature of consulting I ended up in business analyst roles predominantly. 

I find myself in the dilemma that whilst I have a decent understanding of the ‘fundamentals’ the company is very much pushing towards deep learning for which I have very little experience and as a ‘career changer’ I often feel out of my depth when compared to my peers who typically have masters in data science.

I feel like I would struggle to get roles within industry that pay around the same or higher (currently a bit more than £50k) but idk if I’m undervaluing myself, if it’s normal and I just need to stick it out while I build skills or if I’m in a bubble due to me working at a consultancy and industry roles have not quite caught up for the most part.",datascience
"So I graduated with MS in Data Science recently and almost every job post I see for Analyst or Scientist or ML position ask to have some form of PowerBI, Tableau or something similar. But I was never taught an ounce of it in school. Tje professor said he might try and teach Tableau for a couple of weeks but it never came into fruition. 
Now I want to learn. How can I go about it? I'm in the job market with no luck, been applying for past 3 months. I kniw the job market isn't good but having no prior experience isn't helping either. I did my undergrad in BBA. 
Now I just want to improve my skills. 
If anyone has any suggestions please send it my way. Thank you!!",datascience
"Good day all!

 I'm in search of any information and/or opinions y'all might have on three Masters programs' courses that I am considering. They're all similar but very different in their own ways and I can't determine what is important for a career. I will be doing this at the cost of my employer so I'm not super hung up on cost but I also don't want to waste their money, as well as my time. I've also considered Arizona State Uni but hate when they limit the information on their website, it's just so they can call you every other day and harass you (like Maryville Uni) .. as if that's convincing. I haven't found much in terms of reviews so I appreciate any help with this!

Denver University - Masters in DS (\~$67K) 15 courses over three and a half years

COMP SCI PROGRAMMING BASICS/ CALCULUS FOR DS/ DISCRETE MATH & LINEAR ALG/ PYTHON SOFTWARE DEV/ INTRO TO DATAB MGMT SYS/ PARALLEL & DISTRIBUTED COMPUTING FOR DS/ DATA MINING/ MACHINE LEARNING/ DATA VISUALIZATION/ INTRO TO PROBABILITY & STATS FOR DS/ ADVANCED PROB AND STATS FOR DS/ DS TOOLS 1/ DS TOOLS 2/ ALGORITHS FOR DS/ CAPSTONE PROJECT IN DS

Syracuse University - MS in Applied DS (\~$62K) 12 courses over three years ( I think)

DATA ADMIN CONCEPTS AND DB MGMT/ INTRO TO DS/ APPLIED MACHINE LEARNING/ BIG DATA ANALYTICS/ QUANT REASONING FOR DS/ BUSINESS ANALYTICS/ SCRIPTING FOR DA/ DATA WAREHOUSE/ ADVANCED BIG DATA MGMT/ CLOUD MGMT/ INTRO TO INFO SECURITY/ APPLIED DS PORTFOLIO

Clemson University - MS in DS and Analytics (\~$38K) 10 courses over three years

ANALYTICS BOOTCAMP/ STAT METHODS 1/ PROGAMMING IN DS/ STAT METHODS 2/ ADV MATH PROGRAMMING/ STAT COMPUTING/ MULTIVARIATE ANALYSIS/ DATA MGMT AND WAREHOUSING/ BUSINESS ANALYTICS APPLICATION/ ADVANCED DA

Uni of Denver",datascience
"Hi, I’m a professional trader and throughout the years I’ve learned different strategies and gathered data about the financial markets. Now, I’d like to transform one of my strategies into a machine learning software that recognises patterns, selects the ones with the highest probability setups and places trades based on specific parameters. Where do I start? Any suggestion about the topic will be gladly accepted.",datascience
"Would all the processes of data science, without the use of machine learning be considered a mixture of data analysis and data engineering? Are there any processes unique to data science that are not machine learning based?",datascience
"I am a beginner at NLP and Data Science and I am looking for an LFQA model for a chatbot creation. The problem is that I cannot find a pre-trained model that can be tuned to NOT give any answer if it cannot find an answer in the provided articles/context. It would also work if the model could provide a score for the answer.

Do you guys have any suggestions?",datascience
"Hello , this is [datamastery.io](https://datamastery.io) team. We help students become professional data scientists. We are a registered, Canadian non-profit . Please read our website if you are all interested in learning more.  We cost $1000 CAD and offer live lectures and labs  three times weekly as well as offer employment assistance one year post graduation. 

Our next cohort is on April 3rd and we would love to for you to join. Feel free to contact us for a meeting and we can go from there. 

Thanks !",datascience
"As I am studying SQL and Python, I want to start practicing what I am learning with real-life's data sets.

I am curious to check some data sets applying Benford's law. 

Any idea of what type of data sets might give the best insights? 

TIA",datascience
"The batch size, output size etc. are divisible by 8 most of the times(32,128). The neural networks work better in this. What's the reason ?",datascience
"Hi guys I am starting a remote internship soon. How do I go about protecting my data if the company is not giving me a work laptop? Do I use a vpn, get another laptop, create a separate user on my laptop for work stuff only? Please let me know.",datascience
"Dear community 

This post is coming after almost 5 months of constant efforts in trying to switch my job. I currently work in Milan in a small consultancy firm which I joined as a junior data scientist 4 years back. Day by day I am getting frustrated mostly because of the nature of the consultancy industry. In these years I have grown a lot, especially in my programming skills. I try my best to write clean code, always pro-object oriented style, and lately functional in python too.  I have over a year experience of working with the hexagonal architecture for microservices. I am pretty sure not every data scientist goes further deep into such concepts. Got my hands dirty with apache kafka too. For ML, I have experience working with niche things such as bayesian non parametrics. 

All I am trying to say somehow something is not clicking. I am trying to search for ML engineer positions hoping to have the perfect world where I can actually build nice scalable applications that are based on some cool ML models. Do recruiters not look at these things? How is it that for every job I apply to I get the response we are moving forward with other candidates who are a better fit? And then also it is literally impossible to ask for feedback because the email is a no-fucking-reply mail. I just wanted to share my feelings with the community. Maybe some of you have been in the same position before. If so how do you keep going forward, how do you not get depressed by every rejection?",datascience
"I don't know what to call this process. Here's the breakdown of my question: 

- I've realized most of my work requires me to work with absolutely messy datasets. I'm talking column names that don't make any sense, typos, dates that are in varying formats like ""Sat April 3rd"" or ""2019-04-03"", columns broken down into three other columns when they can just be one, etc. 

My work means I have to find these datasets and work with them. But about 70-80 per cent of my time is spent on just cleaning the data to have it look exactly how I want it to look. 

For example, those date columns with values like ""Sat April 3rd"", I will have to break into two columns one for day and one for the date. Then lots of pivot tables, group by's with functions that feed into a dictionary of aggregate functions because I have many columns that work completely differently, etc. 

I usually do all of this using Python. It's not easy, but it gets the job done for the most part. Some parts of this task/process is easier in Excel: replacing nonsensical values with average (I do ctrl + shift + L, select everything but the nonsense, then find the average of the useful information, unselect, then do a replace all for the nonsense). 

I find this a tad bit tedious. Especially if the dataset is too large (say 2000 columns, a couple hundred thousand rows) Excel gets laggy, I have to do the same thing for 2000 columns, etc. Even python sometimes fails to do what I need it to do. 

Say for example I have a dataset that has 30 columns for days of a month. I want those column names (Apr 1st, Apr 2nd, ...) to go into just one column as values of ""date"" and aggregate the information by those days somehow. 

To do this in python, I have to take so many steps to get there. Like, making the transpose of the df using joins and grouobys and merges. 

Since a lot of my work is this, is there a software or tool that does the best job at things like this? I'm comfortable with SQL as I used it during an internship 4 years ago. I just need to refresh my mind. I don't like R, but will learn if I need to. I'm most comfortable with Python. Excel, I know enough to get by but I think it's easy enough to learn. 

I'm not familiar with any other tools. I appreciate any help. I feel like I'm doing way too much to get the result that I want when the task isn't that difficult imo. I just want a more straightforward way to handle these things. If SQL does this better, then I'll get started on that. If it's python, then I'd appreciate some resources on this. Also, what exactly is this process? Is it just pre-processing and data cleaning? Am I right in calling it data engineering? Because, I usually do end up having various versions of the same CSV file for different tasks. I feel like I get a mess of data and give back tabular pretty dataframes.",datascience
"I live in a very small area that happens to have businesses that generate $1 billion+ in revenue per year. I know for a fact that they do not utilize data science and/or data pipeline automation to drive business decisions because I have worked for these companies in a business intelligence capacity.

I also know that these businesses don't want to hire a full-time data scientist because of the cost.

As an example of how behind-the-curve some of these companies are, some still use written time sheets for employee time keeping. One of the companies I worked for 2 years ago ($1.5 billion in revenue) was still using Excel workbooks as its ""database"" and using VB to edit the data which had a full team (5+ members) working on the VB code daily.

I personally have 7+ years of analytics and data science experience with small local companies up to  fortune 50 companies as well as a bachelors in math and a masters in Statistics so I am no stranger to data science and it's applications to business. I also have college-level teaching experience from being a TA in my Masters program.

There is no other consulting/services company around within 50 miles for data analytics/business intelligence.

Therefore, I was wondering if anyone had any opinions as to whether or not this sounds like a good area of opportunity for a consulting firm/freelancer in the data science/business intelligence space.

Edit - I forgot to mention that I absolutely love teaching. So this could potentially be a Non-Profit Idea that helps drive economic advancement in this small community.",datascience
"Hi all

First of all I’m not a data scientist. I’m a software engineer. I have an idea of the app where people would get personalized recommendations(predictions). But I think in order to achieve it I’ll have to store and regularly retrain per user models otherwise recommendations will not be actually personalized. Is it maintainable at all? I’m also trying to understand how social network recommendations work in such situations. My use case is similar I think. But the idea of having a model(potentially several) for each user seems wrong to me. Or no? Could anyone please suggest? 
Thank you!",datascience
"Fellow scientists... On the second year of data science i've become increasingly confused as to when, what and why to use certain algorithms... Specially in a field as rapidly developing as ours.

therefore i ask you if any of you got an overview of what the current best algorithms is according to their uses? Is there a systemisc ""cheat-sheet"" or overview of the current best libraries to import for X types of data and analysis?

For example learning NLP. First you learn Bag of Word models, then IDF-TF models. Then you learn thats outdated and people use Word2Vec models that retain semantic meaning. Then you learn thats outdated and that people use Bert models. But now even that in competition with transformer-based models as ChatGPT is based on etc.

Same thing with time series. First you like ARIMA, then you learn RNN, then LSTM, then Autoencoders, then to use autoencoders as unsupervised feature extraction into a supervised model etc.

And the more complicated you modern you get, the more specific becomes that imported libraries. Anomaly detection? Go PyOD/Prophet/PySAD etc.

How do you guys navigate in this? Is there some conversative principles that would keep your methods within some kind of boundary of usefullnes? I miss the old days of only knowing basic sklearn and basic naive bayes and clustering algorithms lol. On the otherhand if state of the art just means importing and fitting the right algorithm then i guess it ain't so bad.

tl;dr: Is there a cheat-sheet of X-kind of analsis then Y-kind of algorithm performs best? And how do you guys manage all the libraries/algorithms?",datascience
"I feel like i can never keep up this rapid development.... every other month there are new products from big tech that has so much potentials and so much to learn. Once I finally figured out how the new computer vision model works, there is another model and an extension in GAN/NLP for business application.

let alone I am currently working on projects of 2 different fields (forecasting & computer vision), there are too much to do and learn....

There is just too much on the plate.

&#x200B;

How do you guys work with these tons of new knowledge every other day?",datascience
"A friend of a friend of mine was at a dinner party a few nights ago and we were talking shop, and he told me that he's a hiring manager at a very large technical consultancy, and that the world of data science is changing rapidly, and often not for the better (for those looking to get hired).

**His take:** the last several years has seen an enormous number of SWEs and other experienced programmers transition into data science by taking online courses, brushing up on their statistics, and then nailing their interviews because they have work experience. 

All the best companies scooped up all the best candidates (of course), but what remained was still a huge dearth in qualified data scientists, machine learning engineers, and experienced analysts. But we had tons of programmers who took some certification courses online, maybe enrolled in a few university courses, or even went back to school part-time. Since they had prior experience and strong programming knowledge, they were able to get hired at many non-top-tier firms because the need was so great.

**However:** What is happening now is that tons of higher-level managers are realizing that many of those in data roles actually lack strong statistical and engineering backgrounds, and are surviving based on their programming skills alone. This is making the hiring atmosphere for pure DS/ML roles significantly tougher because there are very few people in the companies that can adequately assess graduate or Ph.D-level statistics knowledge in potential hires, so the hiring process for these roles is slow and expensive.

As such, these companies are transitioning to paying more for fewer truly top-tier data scientists to run their ""data factories"" where they hire far more analysts and programmers with less stats knowledge (for lesser pay), but are still able to take direction from the top and build what they're told to.

Therefore, he's expecting that the role of data analyst will continue to grow (while the salaries shrink) as the bar gets lower and operations get more automated, overseen by the data scientists with deep statistical knowledge.

**The upshot:** He's seeing the world of data engineering *explode* in terms of demand and pay. Those who can collect, organize, and pipeline the data to these ""analysis factories"" are going to be in heavy demand for years to come. It's not nearly as sexy as ML engineering or ""hard data science"" but as demand for data infrastructure grows, so too will the need for those to help manage it. He said that with a few certs an analyst or cloud engineer could easily start the transition and get a lot more job security, particularly if you specialize in data and cloud security.

But if you're looking to get into pure ML engineering, the future isn't looking so bright unless you're truly exceptional at statistics. Too many people calling themselves ""data scientists"" after taking a few online courses seems to have ruined the appetite for these individuals.",datascience
Been having difficulty even getting first rounds even though I have 2.5 years of exp. I obviously couldn’t upload any work I did for these companies and didn’t done much outside those responsibilities. Should I be cleaning up my GitHub and adding in any new stuff I do? Would they even care at that point if I’m not even getting to first rounds?,datascience
"Hello. I'm a new data scientist who's trying to solve a problem.

Basically my team has a bunch of taxi that we run across the area and I need to help marketing team with identifying target market/selling points for each taxi route. (ex: this taxi route goes to university, so most likely we should market them towards students). but I wasn't sure how to best do this scientifically if possible somehow.

Data I have

\-taxi trip data (has stops made, location infos,# of passengers got on/off during each hour per day, etc. has no individual person data)

\-demographic data. we have some high level demographic data based on zip code in an area (# of people going school, working, etc). basically smoe high level general data we bought from a data vendor.

&#x200B;

So because I can't really make any suggestions just on taxi trip data, I was trying to see if I can tie it to general demographic data somehow but wasnt' sure best approach as I can't really join these two data easily. Any high level suggestions both from marketing and data science perspective appreciated. I read people on the internet doing customer segmentation/clustering using customer data, but in my case I only have trip data at high level and high level demographic data, but need to suggest marketing team something.",datascience
Can Anyone help me to understand when to use additive seasonality and whrn to use multiplicative seasonality during Time Series Analysis?,datascience
"I'm a couple months into a data science role with zero previous engineering background. I got into the position through a pivot from another industry via a boot camp. The boot camp taught decent coding and data science concepts but gave no exposure for working on a terminal. What's a good resource for getting some practice on basic concepts (installing poetry, setting up virtual environments, etc.) and just general familiarity with using the Mac terminal? I was looking at something like this course [https://www.udemy.com/course/mac-bash-terminal/](https://www.udemy.com/course/mac-bash-terminal/) which my company could reimburse. Thanks!",datascience
"I have various images with instantaneous weather data obtained with six hour intervals from a general circulation model. For example, air temperature, cloud cover, precipitation rate and so on, at 0, 6, 12 and 18 UTC (input data). I also have the daily averages/totals (output data), of these variables. I need to predict the daily average/total using the four instantaneous values. Each image contains these variables for the whole earth, and is 1760 x 880 pixels.

Creating a fully connected neural network is impossible due to the large number of trainable parameters, which would make my computer run out of memory.

So I am trying to train a neural network that would take 7x7 pixels and predict the averages/totals for the 5x5 pixels at the center. But since it is also a time series problem, should I use a LSTM Layer?

Also, I would like to know if this approach (predicting only small pieces of the image at a time) is appropriate in such situations or is there a better approach?",datascience
"I wanted to pose this question as a undergrad whose graduating this year with a statistics + math degree and navigating the field. I did a couple internships in fintech and got the opportunity to chat with people at the companies. All smart people, and offered me great words of wisdom. One thing I found interesting is the concept of “thought leadership”. A manager told me thought leadership is what gets you promoted. Another said business acumen. What’s interesting is not a single one them said “advanced statistical skills”, “advanced programming skills”, “quantitative thinking”. 

I find this interesting, because it’s making me realize data science as a whole is more of a business type role where the conversations you have with cross functional teams and the knowledge about the business and industry is considered more important than fancy statistical tools. Is this always the case? I worked in analytics roles and these tend to be more removed from technical stuff so maybe that’s why I heard this. But what do you guys think? I think one of the things I realized was I need to speak up more and improve my social skills rather than hitting books if I want to get promotions in the future.",datascience
"Hi party people, I recently got my first junior data analyst job (yippee) and am loving it. I was always unsure as to whether I would even be able to get this far, and now that I am, I am realizing I'm not sure where I would want to be in 10-ish years in terms of my career. I definitely don't want to be in a position of leadership, nor do I want anything high stress. I also would rather stay focused on technical tasks rather than soft skill business stuff... so basically what I'm doing right now. Is staying just a mid-level data analyst something that people do? Has anyone done it and not gotten bored out of their minds? Any insights are appreciated :-)",datascience
"I will start:

1. Human genomes from 20000BC to modern times. You can clearly see how e.g. in Europe there were three ancient populations that merged into one (exactly how there is three major races now)
2. National level salary data. Turns out university programme you graduate will almost completely determine your earning potential for the rest of your life. E.g. you can be the dumbest and laziest person there has ever been but your minimum wage is guaranteed \~200 000$ if you happen to graduate medical school programme.
3. UMAP of some major Twitter accounts. I scraped all tweets of Trump, Obama and Elon Musk and checked similarity. My interpretation is that they are very close to each other, and EM/Obama is a kind of subset of Donald Trump.

I work in genomics but do side projects every time i stumble upon some interesting data, so would love to see what datasets you have worked on!",datascience
"I'm a Senior Data Scientist and make over 150k a year with base + bonus/stock RSU's. Been looking for other positions lately and keep getting EXTREMELY insulting job offers. Like under 120k a year for Senior/Staff level roles. I even applied for an Associate Director position in management track for a Data Science managerial role and got offered an insulting 98k a year. (WTF)...

Why are these companies wasting people's time? They keep the salary some sort of secret, and then low-ball you after 3 interviews. Quit wasting my time. I told you my salary expectations in the HR phone screen and the HR person told me ""that's in range"" yet they continue the interview process and waste my time and the salary they offer me at the end of the song and dance is not ""in range"" like you said it was. If I tell you that I'm looking for over 150k, and you say that's in range, then you offer me 115k, that's not in range. I'm sorry. /rant",datascience
"I created this project to help me explore Python, data science, data analytics, and to stay busy! I enjoyed the grind and it’s made me more passionate. 

I don’t know what to do next. I’d love to publish it somewhere. Right now it sits in a Jupyter notebook with a 10+ visuals. 

What would you do to showcase this for employers? 🙏",datascience
"long story short, I studied engineering but not computer science related, out of college I self taught myself ML and DS and starting doing a couple of interviews, got an offer after a while, and next week I'm going to start my first job out of college as machine learning engineer.

What really frightens me the most is I'm not a computer science major and I feel like it might sit me back working with other software engineers.

If you have an experience like this would you mind sharing? also what things I need to consider going into this?",datascience
"In your opinion, how does one go from a ""good"" data scientist to a ""great"" one?  I am solid all around, I'd say (DE, ML, Stats).  I think my weakness is truly understanding the underlying math and stats in DS problems.  In addition NN I am weak on.  Am studying those currently.  I have a MS in CS.  Would a MS in Stats move me forward?  Getting good at Kaggle?  What do you think?

&#x200B;

Thanks!",datascience
"So I am offered a better salary as a BI engineer. I'm now working as a data scientist, but it's more on the data analytics side than ML, still I'm called a data scientist and have to work on ML projects now and then. I plan on continuing my development in ML and adding personal ML projects to my portfolio. Will it be disorienting for prospective hiring agents to hire me for ML jobs, as I have switched position to BI engineer?",datascience
"I am not a professional data scientist or programmer in any way but I am interested in some historical data on the WW1 on which I believe statistical analysis would be very interesting. Those are tables of WW1 victims for Serbia : 

[https://www.mod.gov.rs/cir/19604/popis-vojnih-i-civilnih-gubitaka-kraljevine-srbije-u-ljudstvu-u-prvom-svetskom-ratu19604](https://www.mod.gov.rs/cir/19604/popis-vojnih-i-civilnih-gubitaka-kraljevine-srbije-u-ljudstvu-u-prvom-svetskom-ratu19604)

I have been searching around these for some time, tried tabula, which can extract only some lines correctly : there are no visible lines separating the columns and some of them are very close, which creates some overlap ; also some information extends on two lines, which is also not extracted correctly by tabula (although the lines are separated by a visible line).

It is a bit frustrating because the outline of the tables are exactly the same, I believe, at least it is the same on all even pages and the same on all uneven pages ; so I am guessing that there should be a way to automate and extract exactly the data for each column or row. 

Is there any program or software you would recommend for working with those tables, ideally accessible without precise programming knowledge ? 

Thank you !",datascience
" The insurance industry generates vast amounts of data, ranging from customer information to underwriting models and claims data. As a result, the role of the Chief Technology Officer (CTO) in the insurance sector is becoming increasingly complex. CTOs must not only manage the technology infrastructure that supports their organization, but also must leverage this data to drive business outcomes. This is where [DataOps](https://www.ismiletechnologies.com/dataops-managed-services/) comes in — a methodology that combines agile principles, automation, and collaboration to enable organizations to harness the full potential of their data. In this post, we will discuss why every insurance CTO needs a DataOps strategy and how to implement one.

#### The Benefits of DataOps for Insurance CTOs

A DataOps approach provides several benefits to insurance CTOs, including improved data quality, more efficient data pipelines, and faster time to market. With DataOps, insurance CTOs can better manage data-related risks, ensure regulatory compliance, and drive business value through improved analytics and decision-making capabilities.

#### Challenges to Adopting DataOps in Insurance

Despite the benefits of DataOps, many insurance organizations face challenges in adopting the approach. These challenges may include legacy technology systems, siloed data environments, and cultural resistance to change. CTOs must be prepared to address these challenges and implement a strategy that addresses their organization’s unique needs.

#### Key Components of a DataOps Strategy for Insurance

A DataOps strategy for insurance organizations should include several key components, such as a data catalog, automated data pipelines, and data governance processes. It should also leverage modern technologies such as cloud computing, artificial intelligence, and machine learning to enable efficient data processing and analysis.

#### Best Practices for Implementing a DataOps Strategy

To successfully implement a DataOps strategy, insurance CTOs should follow best practices such as fostering a culture of collaboration, implementing a strong governance framework, and leveraging automation wherever possible. They should also work closely with business stakeholders to ensure that data initiatives are aligned with the organization’s overall strategy and goals.

#### How to Get Started with DataOps

Getting started with DataOps can be a daunting task, but there are several steps insurance CTOs can take to get started. These may include conducting a data maturity assessment, defining a clear data strategy, and building a proof of concept to demonstrate the value of the approach.

#### Measuring the Success of a DataOps Strategy

One of the key benefits of DataOps is its ability to deliver measurable business outcomes. Insurance CTOs should establish clear metrics for success and regularly measure progress against those metrics. This may include measuring data quality, pipeline efficiency, or time to market for new data initiatives.

#### The Role of DataOps in Improving Customer Experience

Insurance CTOs must be able to provide seamless and personalized customer experiences to remain competitive in today’s market. A DataOps approach can help insurance companies better understand their customers by analyzing data from multiple sources, providing insights to improve customer engagement and drive loyalty.

#### The Future of DataOps in Insurance

As the insurance industry continues to evolve, the role of DataOps will become even more important. Insurance CTOs should be prepared to leverage emerging technologies such as blockchain, Internet of Things (IoT), and edge computing to further enhance their data capabilities. A forward-thinking DataOps strategy can help insurance organizations remain competitive and stay ahead of the curve.

#### How ISmile Technologies will help their clients

ISmile Technologies is a leading provider of data management and analytics solutions for the insurance industry. Our team of experts can help insurance CTOs develop and implement a customized DataOps strategy that meets their organization’s unique needs. We work closely with our clients to identify areas of opportunity, address challenges to adoption, and build a comprehensive approach that delivers measurable results. [Contact us](https://www.ismiletechnologies.com/contact-us/) today to learn more about how we can help your organization succeed with DataOps.

#### Conclusion

In conclusion, a DataOps strategy is critical for insurance CTOs looking to unlock the full potential of their organization’s data. By following best practices and leveraging modern technologies, insurance CTOs can overcome the challenges to adoption and implement a DataOps strategy that enables them to drive business value and stay competitive in an ever-changing industry.",datascience
"Hi everyone, I'm a 4th-years student at a Vietnamese university and Im looking for a data science internship job but there's not so much company open for this position. So I'm wondering if I should switch to DA or AI/ML instead of DS. Please give me some advise.  Thank you very much",datascience
"Hi I am wondering if other people’s managers have any of these characteristics because mine has all of them: 
1. Pushy: they don’t care if you are going to kill yourself in delivering something. Even if you explicitly tell them that you have to work overtime a lot, they will still push you to deliver with no room for negotiation.
2. Poor planning: they impulsively decide to do something without any good thinking behind. In the end if things break, it’s your fault.
3. Ignorant: despite of having worked in the company for some years, they still ask super basic questions or they are in general very ignorant about the world. Like my manager doesn’t understand growth rate but he is in the field of data.

I haven’t worked with many companies so I want to know if it’s the same case everywhere…. Also part of me just wanted to rant.",datascience
"Hi All,

Are there blogs(or any other avenue) where I could get worked out data analytics problems? I know there is kaggle but I guess those are very ML specific problem statements. I'm trying to take up data, perform EDA and then dig deeper into findings that I get from EDA. If there are blogs that have performed just the analysis part, I'd like to go through few such solutions. I'm quite new to this and want to practice data analysis part as much as I could. Thanks.",datascience
" I am currently working as a data analyst but my role in the company is very limited as well as my work. So, I have been working on upgrading my tech skills.

My current skillset is Python SQL and Excel. I am teaching myself PowerBI to cover my lack of visualization experience. I am also learning Machine learning from online courses and doing projects on the weekends to buff my resume.

However, this still seems to be insufficient as I barely, if ever, make it past the screening round to the technical round. This makes me wonder what else skills I am lacking.

Can you please advise what else skills are mostly required as I would like to move to a Data Scientist position someday?

**Note: I have total 6.5 yrs of experience but only 2.5 in Data Science (career change in Covid)**",datascience
"Hello, I am currently a student studying data science and am getting really into it! I work a lot with demographic data and was wondering if anybody would have a good example of cleaning data and the using the cleaned data to build a machine learning model. This is purely for me to learn and see how I can improve and what techniques I am missing. If anybody could help that would be greatly appreciated!",datascience
"I'm interviewing for a DS position within the HR and finance wing of a large org. They aim of the position is to 'improve HR ops and payroll leakage'. This is totally new to me. Anyone have any suggestions or could share your experience?
Thanks",datascience
"Less than a month ago, I created my freelancing profile and already got 2 clients I'm currently working with. When I made my account, I didn't expect to be active so soon and put my rate as $100 per hour without much thought.

I dont have any experience with freelancing. So question for freelancers here- do you gauge workload my weekly time commitment as well as urgency for each job? any best-practices tips with dealing with clients or red flags? how did you determine your rate, was it looking at other freelancers?",datascience
"Hello everyone, if anyone is thinking of getting a subscription in Dataquest, please help me out by signing up through my referral link. Learning Data Science has been a dream to me and I belive that Dataquest has the right mix of theory and projects to get a beginner into this field. You learn the ins and outs of data science and it is truly a wonderful resource for a beginner. While there are no alternative to hard work, dataquest can help you with a organised structure and give you a well defined path to a successful career. Going through the course myself I can vouch for the quality of the course. I would love to have a subscription but unfortunately the subscription is way too high for me to afford. Luckily they have a referral option. If I can get one more person to join through my referral link, I will get free lifetime subscription. 

If you guys can help me out, I will be forever be in your debt. Honestly if you ever wanted to wet your feets in data science, this is the site to go to. 

[Link for Dataquest refereall](http://app.dataquest.io/referral-signup/g5bvets0/) 
Note that you will get 15$ off as well on top of the subscription cost as well as the 50% off on their annual subscription",datascience
"So, basically the title says most of it. I have been a Software Engineer for 3 years or so, I am a Semi-senior one specialized on Frontend Dev, and I am deeply interested in the world of Data Science/Analytics. I want help on what would be the best path to take in terms of:

1. Learning resources - I want to know about a learning resource like a certification or course or path to take me where I want to be, the best one for my background paid or free, I don't care.
2. Changing areas - I want counsel about how to make the career change professionally talking, how to take on the job landing part. I would need to begin as Junior ?
3. Toughts on: Is it worth it? I have read growth will be better for this kind of profession than Dev for the following years.

Thanks in advance!",datascience
"Hello everyone, recently in our company we were puzzled by the question of checking the depth of understanding of data analytics when searching for data science and analytics managers. 

The main request is that we are not satisfied with managers who are little versed in analytics, we do not expect memorization of formulas, but we want to understand the confidence of a person and the depth of his knowledge in handling analytics using questions with a simple wording. Google could not help in this matter, and during the brainstorm, the following list of questions appeared: https://docs.google.com/document/d/1JDdaYYrl8R47Ob5fgNKHpNFnf2I3oFxwkEVqNW4Lez4/edit?usp=sharing

Question: do we have resources where you can study questions for fairly high positions, and if not, then I will be very grateful for your contribution to the general bank of tasks, laying them out, leaving the opportunity to comment on the list.

PS I hope this list of questions does not remind you about the harmonic mean",datascience
Hello everyone I am new here and also a data science enthusiast . I am not so sure which course would be more effective in learning data science due to my learning style I always prefer courses. I would really love some recommendations,datascience
I am interested in launching a text analysis project that involves legal depositions. The depositions contain line numbers and page numbers. Any suggestions on how to easily remove these from a large collection of PDFs?,datascience
"""Learn the basics of SQL for Data Analytics with my new YouTube video! In Part I, we cover installation and select queries to help you get started with analyzing your data efficiently. Check it out now and enhance your analytical skills! [\#SQL](https://www.linkedin.com/feed/hashtag/?keywords=sql&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7033035426814713856) [\#DataAnalytics](https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7033035426814713856) [\#DataScience](https://www.linkedin.com/feed/hashtag/?keywords=datascience&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7033035426814713856) [\#YouTubeTutorial](https://www.linkedin.com/feed/hashtag/?keywords=youtubetutorial&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7033035426814713856) [\#LearningMadeEasy](https://www.linkedin.com/feed/hashtag/?keywords=learningmadeeasy&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7033035426814713856)""

[https://www.youtube.com/watch?v=\_lccTXHseVU](https://www.youtube.com/watch?v=_lccTXHseVU)",datascience
"Hi guys, I’m sort of interested in data science and I want to work on a personal project but I’m not too sure where to begin… I have some experience with Python, Java, and R.

Thanks :)",datascience
"Hi All,

I have been using Pandas for my data and it’s been great, up to a point, and I was wondering if anyone dealt with the following stumbling blocks and perhaps had any advice:

I am collecting and analyzing data on a material. I have several serial numbers, each unique serial number corresponds to a unique sample of this material.  The serial number serves as the key, i.e, is set as an index in my dataframe. 

This has worked beautifully until I tried to organize, by serial number, data that requires multiple rows. For example, one datapoint like the mass of the sample only needs one row to express, and so it corresponds nicely with the index.  But what can I do when a dataset needs multiple rows, like for example the transmission of light at various wavelengths?  In my mind, the way to do this was to place this series dataset within a list [df[‘Transmission]], effectively nesting that Series into a single row. This seems to work nicely, until I save and load the dataframe (as csv), at which point the formatting gets all messed up and I can’t easily pull the nested dataset out for plotting, or any other operations, because the nested dataset now has a nested index and pandas inserts a line break (/n) between every datapoint after loading.

Anyone have any good ideas on how to organize such a dataset nicely, so that it corresponds to the serial number index, but can still be used for one dimensional and n dimensional datasets at the same time?  My spectral data also needs to be compatible with .loc and filters so that I can conveniently plot subsets of the spectrum based on wavelength filters.",datascience
"Hey Everyone, 

I just started a new role as a data analyst. I have been loving the work and am so excited and passionate about this career field. One thing I'm starting to think about now is future-wise, what direction I want to take my career in. I have been interested in modeling but still think there's a lot for me to explore before investing into a specialization right now. 

Meanwhile, I have been thinking about starting a Master's programs. 

Currently I am thinking about: Statistics, Applied Math, or CS

In your experiences, what did those programs help you with in your data science career specifically? Do they all give you more or less the same equal footing or would some programs be more set for a specific route within DS vs another. I have also been looking at DS Master's programs but heard maybe it's still best to avoid those for now.

Is it better to hold off until a specific team or function requires me having a master's?

Thanks!",datascience
"This article shows some efficient ways how to share big NumPy arrays across python processes:

[https://medium.com/p/abf0dc2a0ab2](https://medium.com/p/abf0dc2a0ab2) (no paywall)

You might find it very useful if you need to keep one or more NumPy arrays in memory that serve as the “database” for specific computations (e.g. making online cbf recommendations).",datascience
"Hi - does anyone know a service which takes a barcode in an email and creates a capture/user action? Best example would be TripIt. How they add it to your trip and you just have to forward your flight or hotel itinerary..? Anyone? 
Thanks!",datascience
"is it important for Machine Learning Engineer to have an extra Industry Level skill like Web development or Python Developer or not.  
because, if he has good programming skills but don't know any web framework or dont know much about anything other than those python ML libraries. is it ok?",datascience
"Hi how's it going?

For a long time I've been very perplexed as to *why* the Poisson and Exponential PDF are what they are. All the textbooks just say ""Here's the PDF, integrate it to get the CDF..there you go"".

But why the number E? I'm trying to really understand intuitively the Exponential family of distributions from the ground up. 

For example I fully understand the Binomial from the viewpoint of combinations. It's the different orderings of a string of 1's and 0's essentially, weighted by their probabilities of occurring. Then you add up all the orderings where there are K 1's and that's P(X=k).

I want to get that same intuition for the Poisson and Exponential distribution. Any good videos or books or articles?",datascience
"Hi, I finished an online Python course and wanted to start working on my own project, but I'm feeling a little directionless currently. I've written a function which accesses the API and creates a dictionary of dates and a user's rating on that date but I'm unsure what to do with this. I tried putting it into a SQLite database by using an online tutorial but the .db file was an unreadable jumble of red coloured characters. For someone who's essentially a beginner to data analysis, what should I try and learn/focus on to be able to do something useful with this data?",datascience
"Someone who’s well versed in good data practices but isn’t familiar with the CRM software being used (in this case, Salesforce) or the tools used with it?

Or someone who knows SF very well but has little to no experience in data practices?

Would time be a factor in your decision?

New to the group, data steward since 2011. Thanks kindly in advance!",datascience
"Hey, guys. We have made a plugin that turns your pandas data frame into a tableau-style component. It allows you to explore the data frame with an easy drag-and-drop UI.

You can use PyGWalker in Jupyter, Google Colab, or even Kaggle Notebook to easily explore your data and generate interactive visualizations.

Here are some links to check it out:

The Github Repo: [https://github.com/Kanaries/pygwalker](https://github.com/Kanaries/pygwalker)

Use PyGWalker in Kaggle: [https://www.kaggle.com/asmdef/pygwalker-test](https://www.kaggle.com/asmdef/pygwalker-test)

Feedback and suggestions are appreciated! Please feel free to try it out and let us know what you think. Thanks for your support!

&#x200B;

https://preview.redd.it/a7jcuw1gbdja1.png?width=2748&format=png&auto=webp&v=enabled&s=7a344854cfae94086999b448d5d992d3b6e60943

&#x200B;

[Run PyGWalker in Kaggle](https://preview.redd.it/ev8ellb6bdja1.png?width=2748&format=png&auto=webp&v=enabled&s=30b4206cdc00b6ea2425680cd970cf7e1d23cecd)",datascience
"In the ever-evolving world of insurance, data is the key to success. The insurance industry relies heavily on data to make informed decisions about risk assessment, pricing, and customer engagement. With the advent of data-driven technologies, the insurance industry is increasingly adopting DataOps practices to optimize their data-driven processes and improve decision making.

DataOps refers to the set of practices and technologies that organizations use to manage their data operations in a more efficient and effective manner. It involves a combination of people, processes, and tools that enable organizations to collect, process, and analyze data in real-time.

In this blog, we will explore some of the most impactful DataOps use cases in the insurance industry and how they can help organizations optimize their operations.

#### Claims Prediction:

One of the most significant applications of DataOps in the insurance industry is the prediction of claims. By leveraging data from various sources such as historical claims data, demographic data, and weather data, organizations can develop predictive models that can help them identify high-risk areas and predict the likelihood of claims in a specific area. This information can then be used to adjust the pricing of insurance policies and to optimize the allocation of resources for claims management.

#### Lifetime Value Prediction:

DataOps can also be used to predict the lifetime value of a customer. This is an important metric for insurance companies as it helps them to determine the cost of acquiring new customers and the expected return on investment. By analyzing customer data such as demographic information, purchase history, and engagement levels, insurance companies can develop predictive models that can help them to estimate the lifetime value of a customer.

#### Influencing Customer Behaviour:

DataOps can also be used to influence customer behavior. By analyzing customer data, insurance companies can identify patterns in customer behavior that can help them to optimize their engagement strategies. For example, by understanding which channels are most effective for reaching a specific customer segment, insurance companies can target their marketing campaigns more effectively and drive customer engagement.

#### Personalizing Marketing Strategies And Targeting Specific Customer Groups:

DataOps can also be used to personalize marketing strategies and target specific customer groups. By analyzing customer data, insurance companies can segment their customers based on various demographic, behavioral, and psychographic characteristics. This information can then be used to tailor marketing campaigns to specific customer segments, resulting in more effective engagement and higher conversion rates.

#### Detecting And Mitigating Risk In Real-Time:

DataOps can also be used to detect and mitigate risk in real-time. By analyzing real-time data from various sources such as social media, news outlets, and weather data, insurance companies can identify emerging risks and take proactive measures to mitigate their impact. This can help organizations to respond quickly to changing conditions and minimize the impact of potential risks on their operations.

#### Detection Of Fraudulent Claims:

Finally, DataOps can be used to detect fraudulent claims. By analyzing claims data in real-time, insurance companies can identify anomalies and potential fraud. This information can then be used to investigate claims and prevent fraudulent activity from occurring.

#### Conclusion:

In conclusion, DataOps has the potential to transform the insurance industry by enabling organizations to make more informed decisions, optimize their operations, and improve customer engagement. By leveraging data from various sources, insurance companies can gain a competitive advantage and drive growth in a rapidly changing market. Whether you are an established insurance company or a start-up, DataOps can help you to achieve your goals and stay ahead of the curve.

ISmile Technologies DataOps Managed Services enable organizations to collect, process, and analyze data in real-time, allowing them to make informed decisions about risk assessment, pricing, customer engagement, and more. With our expertise and cutting-edge technologies, insurance companies can optimize their data-driven processes, increase efficiency, and stay ahead of the competition. [Schedule your free assessment](https://www.ismiletechnologies.com/contact-us/) today.",datascience
I am thinking about how Amazon reportedly axed a lot of Alexa teams.  This seems to point to negative.  But ChatGPT is taking up a lot of interest. This seems to point to positive.   What are your thoughts?,datascience
"Hi everyone,

I have recently been offered a data scientist position at my current company, but I was told that I would be working exclusively on the Google Cloud Platform (GCP), and that it won't be necessary to code in Python. I've spent the past year learning about data science, models, and programming, and I'm concerned that all of that knowledge won't be put to use in this new role. I always thought that a data scientist would be in front of their Jupyter notebook, so I'm worried that this is a cheap data scientist position.

My main question is: is it standard for data scientists to work solely on cloud computing platforms like GCP, and is it common to not code in Python? I'd love to hear your thoughts and experiences on this.

Thanks in advance for your help !",datascience
"I want to pursue a career in data. My laptop broke and looking for a replacement. Is[this](https://www.bestbuy.com/site/hp-victus-15-6-gaming-laptop-intel-core-i5-12450h-8gb-memory-nvidia-geforce-gtx-1650-512gb-ssd-mica-silver/6503849.p?skuId=6503849) good enough? To work on personal projects, build up a portfolio, do hackathons, etc… (Basically, is it enough for me to improve myself until I can land a job and upgrade?) I’m planning to upgrade the ram to 16 gb.",datascience
"So Basically I have worked on a data science project with a professor in Cannada and we got good results in a medical dataset and got a good AUROC of .85 and we published the paper for it and got selected , but what surprised me was the professor never went through my code and just gave suggestions and tips to do improve the model , but isn't this a lot risky.Everyday I am scared that some person would go through my code and invalidate my entire results based on some simple error which could have been corrected if there were proper code reviews.

Fast forward 6 months , I got a job in  a data science company as an intern and he told me to develop self supervised model for their image dataset and I did that and it good kind of okay results and he told me to move on to another project , still there was no code reviews or code checks , do people in data science just blindly trust each others code , I feel managers should at least give a look through to see if we taking the correct data split or if the model is correct or if there is any data leakage.There is a lot of red flags in trusting the AUROC results blindly. Is this a norm or maybe its just for me.

Moreover most of the data scientists in the company dont even write proper documentation and for interns like me its such a pain , it took  me 2 weeks to understand their entire training repo, data science people should talk with the data engineers and take some of their methods and practices.",datascience
" 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",datascience
"Hi everyone!

I have a situation that I’d very much love your opinion on!

A little bit about myself: 
I have a MS in biomedical engineering, and I’m currently pursuing a MS in analytics. I’m expected to finish the degree by the end of 2023.

I have just over 2 years of experience in a hospital as a research associate. 50% of my job duty is experimental design (mostly A/B testing), analyzing experimental dataset, visualization, etc. The size of the data is usually very small (n=200 or less). And the other 50% is sensors integration and hands on clinical experimental work. I’m also an adjunct professor in university teaching biomedical engineering classes.

I have a couple abstracts in ML published, and one manuscript currently in the work. However,  the journals I submitted to are very health-oriented. The technical findings are not ground breaking, just applying known ML techniques to the experimental data.

Initially I was planning to pursue a PhD in biomed after finishing the analytics MS program, but I find myself swaying away. I’d like to change to a pure DSA career afterwards.

My question is, should I apply to intern jobs and start over, or is my experience in the hospital valid? My work is 100% Python based, I know a bit of SQL from course project and leetcode.

It feels like aiming for a mid level job (2year +) may be too far fetched since I technically only have one full year of experience in DS.

So that may leave me with a new grad/ intern option

What do you guys think? 

Thanks so much",datascience
" I apologize I don't really know enough about this subject yet. From my understanding, it seems like most of the Big Tech companies don't fully own personal data but they own the infrastructure in which it is stored. I am an architecture student and I'm really interested in doing research about data centers. I am concerned with surveillance capitalism and am wondering if there is any potential in redesigning the physical infrastructure of how data is stored.",datascience
"Hey everyone, I'm planning to create a dashboard for data visualization, and I think I want to use Python for the project. I've been looking into Dash and Streamlit, but I'm not sure which one would be the best choice for a beginner like me. Do you have any suggestions on which library to use? Also, I'm hoping to find a library that won't have a very steep learning curve.

If you have any recommendations for other Python-based libraries for data visualization and dashboard creation, I'd love to hear them as well. Thanks in advance for your help!",datascience
"I often find myself wanting to run a couple SQL commands against a CSV, I have poor Excel skills, and so I made [https://sqlacsv.com/](https://sqlacsv.com/). You can drag-n-drop any CSV, its a completely offline app, and it gives a quick overview of each column's distribution.

**Is this something people might find helpful? Would love to get some feedback on the tool.** 

Here some screenshots of what happens after you upload a CSV:

[Simple SQL Editor](https://preview.redd.it/335fjx7cr8ja1.png?width=1886&format=png&auto=webp&v=enabled&s=a22da852724c61f4846ca08f917111e9b99f980c)

&#x200B;

[Overview of Values per Columns](https://preview.redd.it/qlt46ttdr8ja1.png?width=1873&format=png&auto=webp&v=enabled&s=9d3e64d2eeac20b69c2d4805fd2e307b04ae6789)

Thanks in advanced!",datascience
"Hello, I am fitting count data for each place. That means, my response variable is the count data and I want to regress them to given explanatory variables. And each row represents each place. But the problem is that the count data are collected over years and I found it has a seasonal pattern. I am sorry to confuse you but I also have another dataset that consists of count data according to time and I used this dataset to visualize the time series plot.

So, my main goal is to make a regression model of the count data using explanatory in the original dataset (that has no time component) while adjusting for seasonality and my question is how I can deal with this. Do I have to request for the new dataset that contains the time the count data were recorded and fit a regression model? Thank you in advance!",datascience
"It seems like there are countless data providers that offer identity solutions like skip tracing and people search capabilities, but all references to obtaining this information seem very broad and vague. 

So, how are all of these companies aggregating their people data like phone numbers, email addresses, relatives, etc in an efficient way and with large coverage of the US population? 

Are there public bulk datasets that include this information so it can be aggregated and sold for marketing purposes without violating GLBA or FCRA regulations?

As an expert, how would you go about acquiring this information in bulk and qualifying it for accuracy?

Currently, the big players in the space are IDI, TLO, Delvepoint, and Tracers. They all have access to regulated data from credit bureaus and other third-parties to be sold to specific industries, but they also provide unregulated data for public use cases, so where is that information coming from?

This one has had me stumped for years.",datascience
"I'm currently between jobs; my last boss told me he believes that improving my coding level will help me become a better data scientist.

Any ideas how to work on that?
Python, of course..",datascience
"Hey everyone, I am interested to see what tools are in high demand right now. In addition, are any automation tools part of your data process?",datascience
"I tried a loop function for a single time series but doing it for 43 series looks difficult it would be helpful if anyone can help with R loop code for this or any other easy way using eviews/excel/R.

Update: problem is solved. If anyone wants the solution or code for a similar problem. Please ask. Thanks to everyone.",datascience
" Through the easy-explain package, you can achieve it without the need to write long scripts (only in 2-3 lines of code you can have your XAI results).

Read more info in this article: [https://medium.com/towards-artificial-intelligence/easy-explain-explainable-ai-for-images-285777a004e3](https://medium.com/towards-artificial-intelligence/easy-explain-explainable-ai-for-images-285777a004e3)

Find the package in Gh: [https://github.com/stavrostheocharis/easy\_explain](https://github.com/stavrostheocharis/easy_explain)

Find the package in Pypi: [https://pypi.org/project/easy-explain/](https://pypi.org/project/easy-explain/)

&#x200B;

https://preview.redd.it/cowm1xkkl5ja1.png?width=1390&format=png&auto=webp&v=enabled&s=b0a259ebe938838e580437e92641ae23182f972d",datascience
"I've search around Google Scholar, even just plain Google but I can't seem to find what I'm looking for. It's always summarized or just a few sentences.

Anyone here knows the way or am I searching at the wrong place?",datascience
"Hi I've also already posted this on SQL subreddit as this one's heavily SQL based but I'm posting this here just incase it helps me even more and helps me to complete the project quick...I'm new to this sub and new to Data science,I've been learning online and I've got a project to be submitted within coming Wednesday and it's ""Twitter scraping"" I've no idea what the heck it is,it would be helpful if someone here could guide me through this,the given instructions are that they asked me to use Snscrape library and to build a solution that should be able scrape the Twitter data and store that in the database and allow the user to download the data with multiple formats.

TIA.",datascience
" In my job as a data scientist, I use deep learning models regularly to classify a lot of textual data (mostly transformer models like BERT finetuned for the needs of the company). Sentiment analysis and topic classification are the two most common natural language processing tasks that I perform, or rather, that is performed downstream in a pipeline that I am building for a company. 

The other day someone high up (with no technical knowledge) was telling me, during a meeting, that we should be harnessing the power of ChatGPT to perform sentiment analysis and do other various data analysis tasks, noting that it should be a particularly powerful tool to analyze large volumes of data coming in (both in sentiment analysis and in querying and summarizing data tables). I mentioned that the tools we are currently using are more specialized for our analysis needs than this chat bot. They pushed back, insisting that ChatGPT is the way to go for data analysis and that I'm not doing my due diligence. I feel that AI becoming a topic of mainstream interest is emboldening people to speak confidently on it when they have no education or experience in the field. 

After just a few minutes playing around with ChatGPT, I was able to get it to give me a wrong answer to a VERY EASY question (see below for the transcript). It spoke so confidently in it's answer, even going as far as to provide a formula, which it basically abandoned in practice. Then, when I pointed out it's mistake, it corrected the answer to another wrong one. 

The point of this long post was to point out that AI tool have their uses, but they should not be given the benefit of the doubt in every scenario, simply due to hype. If a model is to be used for a specific task, it should be rigorously tested and benchmarked before replacing more thoroughly proven methods.

ChatGPT is a really promising chat bot and it can definitely seem knowledgeable about a wide range of topics, since it was trained on basically the entire internet, but I wouldn't trust it to do something that  a simple pandas query could accomplish. Nor would I use it to perform sentiment analysis when there are a million other transformer models that were specifically trained to predict sentiment labels and were rigorously evaluated on industry standard benchmarks (like GLUE).

https://preview.redd.it/sz3ejc1313ja1.png?width=1700&format=png&auto=webp&v=enabled&s=16393c01bdbca3a79f55ef61c486d9aa5645680a",datascience
"I have been studying deep learning for about a year, learned the basics as well as the main practices on tensorflow. But I feel I am still very weak in this area. Here, in my country, AI is not popular, and it is almost impossible to find a mentor here. I would be glad if you shared at least your way in this area.",datascience
"Hello all, I’m working as a data scientist from past 1 year in a healthcare industry. I’m looking to improve myself and move out from my current company. Is anyone open to be a mentor?",datascience
"I just started my first job at an asset management firm after my Masters' 2 months ago. Recently been contacted by the team lead in a large tech company. He is interested in my past research work (I published two papers out of my masters') and wants to have me in his new team. I am unsure if taking this offer is the right move.

&#x200B;

My current role: Data scientist at a small (\~10 ppl) asset management firm. They are setting up a new quant trading (stat arb) strategy and hired me as part of a new team. I took this offer because I am interested in a quant researcher role but couldn't get in one of those big quant trading firms, so I am trying to use this position as a stepping stone to better places. The problems are:

* The actual job is nothing like what I was told during interview. 75% of the job are ad-hoc admin tasks, like preparing monthly fund performance reports, booking trades done by their manual trader (for their other non-quant funds), fixing the trader's Excel formulas, and helping non-tech staff run some Python scripts. I am even officially in their ""operation"" team now. This frankly is not what I expected.
* This firm has a weak tech foundation. They are just starting to do quant trading, so I understand that they don't have any existing infrastructure. What worries me more however is that they have no experienced developer neither; the only developer here has less than 1 YoE. I am afraid the project will quickly become a hell of technical debt.
* This firm is not very quantitative neither. The only real quant here has a straightly business background and received no formal math training beyond first year calculus (and no PhD ofc). He does have solid >5 year YoE as a quant trader, which is what really matters.
* The quant work I got to do is alright. The strategy they are developing sounds promising to me as a layman. If nothing else, at least I learned something about finance. I am unsure how this experience in a small fund will be valued when I enter the job market again.
* Overall, I think it all comes down to me not feeling enough growth opportunity. Quant research is one exciting opportunity that aligns with my technical interest, but I don't want to overly confine myself in that narrow space -- the quant industry is so competitive that I don't even know if I can get a next job. If I cannot stay in quant, the other option for me is a data scientist role in other sectors. But then, I am now spending 75% of my day doing work that adds zero transferable data skill to my profile.
* I am however aware that my worldview has become too academic after spending the extra years in school for my Master, so maybe all this is normal in the workspace and I am just being unrealistic.

The other role: Data scientist at a large tech company (\~3000 employees).

* The team lead there promised a salary higher than my current role (which is alright but not yet >100k), although I think it will be at best a 10% raise. If I take into account of bonus, probably I will get paid more overall if I can stay where I am for a full year.
* From what I heard, this company is building a new team to make use of the TB's of service usage data they have collected, mainly for marketing optimization. This team is more mature, with a functional tech stack and experienced data scientists and engineers. The team lead said this will be an opportunity for me to gain experience in being a ""full stack"" data scientist.
* I have a friend currently working in this company, and he said the job is just like what was on paper; I won't be wasting 75% of my time doing admin work like where I am.
* My main concern here is essentially the opposite of if I stay in my current role: I am afraid that taking this offer would mean the door to quant research is completely closed for me. Honestly, I think as a data scientist, going for quant related roles is the fastest way to reach my financial goals. And I wish to keep that door opened as much as possible.

&#x200B;

Summing up, my question is which role can give me better career opportunity down the road.

Please knock some sense into me. Thanks!",datascience
"Which Mac should I buy my daily work includes mostly doing data analysis with R and Python, Tableau, using Datagrip for SQL (20M-30M Rows Tables), and some Google workspace work and heavy Keynote for presentations.

I’m between buying a M2 Mac Mini with 24 GB of RAM, or maybe pay a little bit more for a M2 Pro Mac Mini with 16 or 32GB of RAM (512GB SSD on both) 


What do you think would be the best option?",datascience
"I'm a 2nd year MIS major hoping to land an internship by my junior year summer. I'm currently learning Excel in school and SQL on my own. When I get comfortable enough, I'll work on a couple of projects to beef up my resume. I plan on learning Python basics myself during the summer, then taking a Python course in the fall semester. Is that enough to land me a data analytics internship where I won't feel too much like an imposter? Is there anything else you would recommend I focus on?",datascience
Any data scientist in dubai? I am a student in the american university of sharjah majoring in information systems and business analytics. I would love to find other people in the UAE who are data scientists to build my network in the field!,datascience
"Hello I am a university student in my junior year, Information systems and business analytics major, and I want to get into internships in DS, AI, or ML. I have been applying for over a month now averaging about 5 applications a day. i think it’s my resume thats holding me back from actually getting into those internships. any advice? how can i build my resume to actually get into internships?",datascience
"Hi all - I make no apologies - I'm a hardcore DS geek. I even do it in my volunteering I mess around with IoT stuff in my off time. Even though I've been working in DS one way or another since 5.25 360k floppies, I find the field is getting more and more exciting. 

What part of the DS work you've done so far really gets you geeking out?

For me, it's the debates refining the research question and stakeholder interests and whiteboard work solving a data issue. I also like those ""Stand up and wave your arms in the air"" moments when we can claim ""King of the Lab"" for the day because of a righteous hack or sweet piece of code. 

What's yours?

What are you hoping to do more of soon?",datascience
"This might be a silly question but I'm just curious, Python seems to be quite hegemonic (apart from quantative finance maybe), aren't its bad performances a problem?",datascience
"Hey everyone, I’m in the process of transitioning into a Jr Data Scientist role at my job. I’m currently working on time-series models for website traffic. We’re leaning toward using Prophet, and I’ve been reading inconsistent information on implementing it in Python. Is a box-cox transformation needed on the response variable? I saw several websites saying that was needed. I also discussed it with my professor, and he said his impression was that it can be implemented with an assumption of linearity or of logistic growth. Could I get some clarity? Sorry if I’m totally wrong on all of this. I’m still learning 🙃",datascience
"I am a data scientist working a full time position in the US with 150k as base and 4% 401k match. I have been offered a 6-months contract-to-hire position for $85/hr. I have never done contract work and I am trying to figure out if this might be comparable to my current earnings.

Do any of you have experience with the differences between doing W-2 work vs 1099 work? 

How likely is it that a contract-to-hire position actually converts to a hire?",datascience
"I’ve recently applied for Georgia Techs OMSA program and have been following this sub, and I feel like I see an overwhelming amount of people posting about challenges getting good jobs, management not actually knowing what to do with DS, getting mundane or ridiculous or non DS tasks, and that you’re basically here to verify managements desires or else you better be ready for deep explanations. 

All of this has made me a bit nervous if I’m choosing the right track. Do you think in 4ish years (time it’ll take me to complete me degree part time) it will get better as DS becomes more known, understood, and desired? Is it hard to get into fields related to neural networks / deep learning / or working with Neuro/health related companies (I come from a neruroscience background so have a big interest in how we can use brain studies to enhance how we build and utilize new technologies)? I don’t really have an interest in crunching numbers for financial purposes. I want to feel like I’m helping advance/enhance a field.

I also considered applying to OMSCS but was discouraged by having less preliminary knowledge for that degree, but after applying to OMSA and reading more about both degrees actually feel like with a little prep I would be fine as i have a pretty good gpa, resume, and LoRs. On the fence about applying to both. I have a few years under my belt as a data engineer so I’ve been exposed to the worlds a bit and have some decent language skills. 

TLDR; basically, I’d love to hear from data science folks if you think data science is a good field to go into, or if I should consider switch to a more computer science focus. I have interest in both areas, but felt a data science degree was more within reach. TIA!",datascience
"Have a dataset of roughly 15 variables or so. Don't think just running a decision tree is the play, I have a lot of data so I kind of wanted to do a forest as I want to use the wisdom of the crowd so to speak.

Was thinking Gini Impurity or maybe doing something like a shapley additive explanation (SHAP) . Haven't used done a SHAP before though, I have only read about it. Has anyone worked on a task like this and have any advice?",datascience
"I (23) am a part of my mid sized organisation's ( new )  analytics team ( freshers ) of about 5-7. Our company has lot of dataset on patients clinical diagnosis. 
We have been a freshers team and I think we struggle with getting the right analytical mindset of problem solving. 
 Being incharge of a freshers team while myself having a less than 6 months of experience is kinda hard. But that is not the point.. the point is I wanna know how do I make sure me and team learn to analyse, solve and measure analytical problems and ther outcomes.
I thought going through few case studies, of other organisations, which covers issues of analytics/ data science team and how they solved them would give a good picture of how analytics problem solving should go .
I looked up online to find such stories but couldn't come with anything really usefull.
I would like to get few resources that's i can share with my team, or have a understanding of how do I go on about starting to solve our organisation's small analytics tasks.
Engineering part is not a issue for us. We have a great and very supportive engineering team. It's just that, we canot identify the right problem to pick, the right way to solve or find right metrics to use.",datascience
"Hey there,

I am a data scientist myself, working in a small team of other data scientists (<5). I'm soon reaching the 1-year cap, and I found out that code understandability/project onboarding (at least in my team) is a problem.

Most of my teammates are really good at math and statistics in general, but once I have to dive into one project, I just lose so much time trying to read their code and understand their explanations.

This can be hurtful, firstly because losing time is never a good thing, but also because it kind of makes me feel like an idiot to not be able to onboard a project without losing hours running and understanding all the code (and it can get very long if, on top of that, it's based on complex mathematics :p ). 

Of course, implementing better code practices such as commenting and better naming would be a good thing, but this idea is often met with the ""We don't have time to do that, we have worse problems and priorities"". 

  
So here's my question: Are there any other data scientists that have the same issues as me? How do you approach this problem? I know collaboration is and will always be a source of friction, and I'm not trying to find a way to remove all of it, but at least adding some oil in the process would greatly help :)",datascience
"**CONTEX**

You work in a Retail Bank and as a Data Scientist you're called by the Marketing and Customer Value Management departments to work on a problem with a data-driven approach.

They figured out that many individual retail customers use their current account and debit/credit cards for business purpose (they are SME - Small Medium Enterprises - disguised as retail customers) and they want to find those 'hidden business' customers in order to reach them out with a cross-selling opportunity (e.g. open also a business account and keep your personal and business expenses separated, allowing lower taxation, etc.)

When you do some EDA on debit and credit card spendings you find out examples of retail customers with thousands of dollars per year spent on Facebook Ads, Google Ads, stores for business purchase only, etc. and you plan to use those features to train a model that identifies the 'hidden business' among the retail customer base.

**PROBLEM**

If your plan is to train a binary classifier you will soon figure out that the problem is that you have positive labels (customers with a business account and debit/credit card) but your negative labels are actually unknown: you don't know if they are hidden business or actual retail customers.

The standard metrics you will use to compute error and performance (AUC, precision, etc.) would be also flawed because you want to find out errors (e.g. customers with high scores of being a business but actual retail) not minimize it.

Also, you will end up doing training and scoring on the same dataset of the overall bank customer base.

>!SPOILER: Positive-Unlabeled Learning (PULearning) might be a solution but you have to make assumptions on the distribution of labeled/unabeled class!<",datascience
"I have the data of hourly production factor data of a Wind Tribune for 2 years. Unlike PhotoVoltanic panels production, this does dot show and pattern or order. I need to find representative 15 days data for the parameters of my project's mathematical model. What is the best method for selecting  the data that represents most scenerios like high and low production days?  I will divide my data in 3 seasons as winter, summer and spring-fall, so there will be a total of 3 datasets of 15 days.

PS: I am not a data scientist, so I may not be able to apply advanced methods.",datascience
"I’m from a town in the Midwest. For the longest time people had no idea what I do or what data science is. I’m starting to overhear conversations about AI in public, and it’s weird.

Inflection point or whatever you want to call it - ChatGPT or otherwise - normal people are becoming aware of the potential. Bringing both excitement and anxiety.

What is your take on public perception of AI and how it might affect the field?",datascience
"Made my first medium article analyzing my own chess games, tell me what you guys think! Any feedback is greatly appreciated!

&#x200B;

[https://medium.com/@imadahmad97/chessalytics-analyzing-my-chess-games-to-better-understand-my-play-style-d08c75d7fc9b](https://medium.com/@imadahmad97/chessalytics-analyzing-my-chess-games-to-better-understand-my-play-style-d08c75d7fc9b)",datascience
"Let's say I put in production a fraud detection model based on an initial dataset. Over time this model will introduce a bias because there are transaction getting rejected due to the model. If I retrain the model with the new data, then the model may 'forget' how to catch the old fraudulent transactions. 
How do you call this model induced drift and how do you deal with it?",datascience
"Lately on this sub there have been many “sky is falling” posts related to ChatGPT. Most of the posts have drastically overestimated ChatGPT’s current use cases in the industry. What is a capability that if ChatGPT could do it, you would actually worry about the future of the data science field? More specifically worried about mass job loss within the field, if you foresee that.",datascience
"Hello. In R I have data in a list composed of 198 vectors, each vector with 70.000 values. Each vector is a feature/variable. It's data I need to make searches on (the first vector is the time, like the unique key). What's the best way to organize the data, in a dataframe or transposing it to a list of 70000 lists each one with 198 values, or keeping it like this...?
I'm trying to iterate on it as it is now but is very slow...",datascience
"I am starting a position as an implementation specialist next week and I was wondering how I could incorporate some tasks on data analytics to this job. 

So a background of what I will be doing is that I will be helping new clients onboard to our company's SAaS products that is suitable on the client's business environment. Ultimately, I will be training them on our products but will implement the software based on their business needs. 

I have a background in project management and sales so even though the role seems to be a bit more technical, it's one skill I love to learn as I go through it. I can leverage my skills in project management and sales to get me up to speed on this new role. 

But on the other hand, I would want to incorporate some data analytics task to my job since I've always wanted to go to a more technically-focused career. I know that data analytics will have tasks such as data analysis and dashboard reporting, data processing, and perhaps a little bit of statistics. 

I really don't have a formal training on it though I know that the tasks I did in my previous jobs on my own initiative entailed having data analytics tasks. 

But what can you suggest what tools I use to be able to perform data analytics? Even basic excel will do as long as I get to understand the principles of data analytics and by then create a dashboard for my projects/accounts. I'm not sure yet if I will be touching SQL and other database tools but I just want to anticipate as early what I can do to incorporate it in my next role. 

Thank you so much!",datascience
"Hello, my name is Toni and I’m coming from Croatia.

I have a master's degree in Business with specialization in informatics and I am a part-time lecturer at Uni of Business. I have been into trading (forex, stocks, crypto) for many years and I have found a very promising strategy based on Bill Williams’ indicators and I improved it together with my team. The strategy is based on fractal geometry.

I found a team of developers (5 of us, from Europe) 2 years ago and we built a very robust infrastructure in Python (Binance) and MQL4 for MetaTrader platform (forex, index, metals, stocks). We have a backtester that provides us with many detailed statistics. Our backtests are good and I can provide you with samples; however, we are looking for a Statistician or Data Scientist to join our team to help us to interpret the data and do magic with numbers.

This is not a paid job, but we all share the source code as our reward and you can use it for your own purpose. I am also here to learn about data science and statistics and we are willing to discuss anything regarding trading (any stock tickers or forex, etc.), about trading bots. Feel free to ask anything!",datascience
[Spotify](https://open.spotify.com/playlist/6BoTD9WeyBN5rD5uyhbROO) | [Apple](https://music.apple.com/playlist/synthwave-focus-i/pl.u-b3b8Nm9C5MmMyM) | [Youtube](https://youtube.com/playlist?list=PLwO9YUACGAzS_XaTchS3qLgK5EGsb3qwa) | [Amazon](https://music.amazon.com/user-playlists/0375bdbe2f3d41f39523ddb35fc92ca0b1r),datascience
"For people here from the UK, is it good over there? I'm considering moving to the UK (with an HPI visa). Of course I love the place, but I have a young family as dependents (wife and a kid), so I need to be sure it's the right move financially too. So, I'm looking for honest opinions from data scientists in the UK if the move is going to be worth it. Is it easy to get jobs? Are most jobs ok with a hybrid arrangement? Are employers usually willing to sponsor work visa to foreign nationals? Anything else you think I should know?",datascience
"I'm a 26 year old Social Sciences grad considering a return to academia and I'm very interested in applying ML methods as part of a PhD research proposal at the intersection of Political Science / Public Policy and Development Studies.

More specifically I'd like to assess the value provided by ML methods as part of a mixed methods approach to research with the aid of one or several case studies, with significant attention paid to methodological and epistemological concerns, engaging with the work of forerunners in the field such as Lakoff, Zadeh, Sartori and David Collier. As such, applying ML methods would be a significant portion of my research, but definitely a minority. Specific algorithms of interest would be determined by the case studies in question and what I'm capable of competently applying, but would likely centre around regression/classification and text analysis.

Once upon a time I had basic but fairly solid statistical skills along with more rudimentary technical skills in SQL, R and to a lesser extent Python, but these have unfortunately atrophied to the point of non existence after 3-4 years in a non-technical Data Science adjacent role. I therefore have a pretty steep hill to climb in terms of foundational skills before I could even submit my research proposal. I'm currently brushing up on algebra, calculus and statistical skills before pushing in to the unknown, most likely via the MITx Micromasters in Data, Development and Economics and/or in Data Science. However the sheer scope and complexity of the content I need to cover combined with working a full time job and the fact that a PhD position is by no means guaranteed at the end of it (I'm provisionally interested in Oxford or LSE) is pretty disheartening. As a result I'm interested in exploring low code / no code options such as KNIME and DataIku (I'd also welcome alternative suggestions) for data prep and model build to reduce the barrier to entry. I'd really value any input on whether these would be fit for purpose, specifically:

1. Will the choice of algorithms available fit my needs?
2. How low code are they exactly? In practice will I need to plug in significant amounts of my own code?
3. Do they provide sufficient guidance or hand holding to build models of sufficient quality for a top tier PhD program (keeping in mind that building the ML models would only be a relatively small portion of my research - I certainly wouldn't expect this to be the case if it was a purely quantitative thesis). To take some basic examples, would they flag if my variables were collinear, heteroskedastic etc

Thanks for reading, I'd really value any input! Happy to provide some more details on my (somewhat vague) research proposal if that'd be helpful or of any interest.",datascience
"Hi everyone!

I am undergrad majoring in Political Science and minoring in Data Science. I am now working on my undergraduate thesis and have realized that, while scraping election data from the websites of different states, there is very little consistency. In a single state, URLs are formatted completely differently, the tables come out in many different forms after I scrape, and reformatting takes me quite a bit (\~15 minutes for each site, but I have 100 of these to go through, making the data cleaning alone a 25 hour project).

Are there any ""tricks"" to automating data cleaning/making a robust program that can accept many different data forms? Or is data cleaning always just a bit of a pain in the a\*\*?",datascience
"I once watched a video where someone argued that Michael Jordan had ruined basketball. As a person whose only notions of basketball come from Space Jam, this surprised me. Wasn't the guy the best out there? How could someone who's best in his field ruin it? After hearing the argument out though, I began to understand what the video was really getting at: to them, Michael Jordan had ruined the field BECAUSE he was so good. A sport that was once full of diverse personalities and strategies then turned into multiple teams trying to produce their own Michael Jordan copycat with copycat moves and copycat strategies.

I'm not well versed enough in basketball to know how right or wrong that guy was. But this concept that something could ruin an entire field simply by being too good stuck with me. And all these years later, I consistently think about it while doing my job as a Data Scientist. And the more GPT-X and ChatGPT advance, the more this concept haunts me.

I've been working in this field for 5 years. During the first 4 years, the job was exciting. Every new problem required extensive research on my part: looking for papers, datasets, implementations, implementing my own stuff, experimenting, comparing the results... That was the biggest part of my job. Yeah I still had to do some software engineering stuff here and there, but I felt like I had a well defined and specialized role at my company. Even in the NLP field, the variety of technologies I had to work with was a lot of fun: sometimes I used RNNs, sometimes embedding based similarity functions, sometimes more classic approaches, and then towards the end it was transformer after transformer, but at least I still had to finetune them myself.

Over the past few months and years, this has started to change, however. GPT-1 and 2 were promising, but were more of a proof of concept than anything realistically usable. But with GPT3's and ChatGPT's latest performance, it feels like NLP is becoming more and more standardized. I wouldn't call it a solved field, far from it. But, sadly, unless you work for a big tech company,  I think the days of exploratory research-type work in the NLP field are over.

Even in academia, the latest NLP papers I've seen come out of prestigious universities went from actual engineering/mathematical advances to ""Look, if you ""engineer"" the prompt like this, it works better sometimes!""

And now, with the exploding popularity of ChatGPT, any random lambda would come to the conclusion that if you need to use AI, ChatGPT is the answer.

Now, some of you may already be thinking ""but hold on, with my finetuned transformer I get a 1% increase in accuracy over out-of-the-box GPT-3!"". The thing is: even if you can, it doesn't really matter. I really doubt the sales team of your company is gonna pay much attention to your pleas to give you 6 months to work on a product rather than 6 minutes. Because yes, that's the increase in productivity we're dealing with: What took me months a few years ago only takes minutes now. The range of problems I can apply NLP too has also increased, as out-of-the-box GPT-3 works well enough not to require a dataset for more ""general"" tasks. And the results are either comparable or even better.

In the span of a year, NLP went from my favorite ML field to one I never see myself working in again if given the choice. And while a lot of non NLP fields are, for now, untouched by the exploding popularity of the GPT family, I'm afraid that what happened to the NLP field will be replicated in other fields, and that in a few years, only super specialized issues won't have a solution somewhere in the form of a ridiculously big model accessible via an API key provided by a big tech company. And even if GPT3 was open source, the sheer size of it makes it close to impossible to realistically train for anything less than the biggest of tech companies out there. Even finetuning it would be a challenge.

I'm not saying the job of data scientist will disappear, in fact, I'd say it will be asked more than ever since boomers who don't understand what the title actually implies will want some in their company. But we won't be doing nearly as much actual data sciency stuff as we did in the past. Instead we'll have to become even bigger software engineer/data analyst/DevOps/project manager hybrids than we already were. The only exception is for people who either work in big tech companies or on very specialized problems that require an in-house model.

It's not that much of a problem for people who like the roles I listed above. But for people like me, whose enjoyment came out of the exploratory side of the job, I believe the future to be bleak.",datascience
"I   have a B.E in Computer Science and my main language was JAVA throughout the course. I have learned different languages since then but not in-depth as I have done for JAVA. I will be pivoting to Data Science or Machine learning(Doing both simultaneously). I am doing Courses from Coursera and going through books to learn important concepts of Statistics, Probability, etc. But I lack practical experience in main python programming. I can create the codes I want using google or reading the documentation, but I don't know the syntax in my head. Like I know they exist but I have to them up(For JAVA this is not a major issue).

I  had only basic knowledge of python, but for my job, I created an Automation  Testing framework using SeleniumBase and python without any prior knowledge of the Subject. I could do this as I have experience building mobile applications and web development, going through the documentation and finding solutions on the Internet.

Now the issue is I feel like it would be awkward to google search things during any interview that I will have since I don't know the basic syntax of python(Even though I can create most of the programs that will be requested of me). I know the basics but I struggle with the Lambda function, apply function, the advanced syntax of for loop etc. How do I get good at this? where can I get practice for high-level python without learning what variables or loops are from the beginning?",datascience
"Fortunately I realised data is my passion and shifted my career towards in 2019, not bad timing I assume. Also had my master in data science and did a bootcamp. However, the companies I have worked for were not as I expected. Despite the fact that I did my best that I could, I feel that there are still a lot to learn. In my pov, the pace companies (except FAANG) have  in terms of development of AI not even close. Am I being impatient or focusing on wrong here? or should I look for other job opportunities. What are you looking for or; what are your criteria as an data person(Ai, DS, DE, MLE, DA)when you are applying jobs? Would love to see your opinions and constructive feedback",datascience
"Just kind of looking for advice more than anything. I’ve been job hunting since September and made it to last round of a few interviews including Amazon, but haven’t gotten any takers. And it seems the longer I’m hunting the fewer interviews I get. At this point I’ve been shotgunning everything on LinkedIn, but haven’t gotten anything in 1.5 months.

I have 2.5 years experience including a couple deployments under my belt with GCP. I have a Physics masters, but no academic ML work. 

I feel like I’m on the cusp of finally cracking the interview process, but I need more interviews.

I think my resume looks clean with an active personal project and my LinkedIn looks fine, but isn’t as flashy as some of the ones I’ve seen. 

Should I be looking for job differently, pursuing certifications, getting a formal DS Ms, upload more recent code on GitHub, write blogs, network at a meetup? Idk what to do anymore and this is starting to feel hopeless.

Any advice is appreciated",datascience
"Hey there , is there a site like [R for Data Science](https://r4ds.had.co.nz/index.html) but for Data Science with Python? For me it breaks down everything into smaller concepts and makes it easy to digest. Also provides examples and some exercises.",datascience
"I have a problem which involves robots displaying error codes, then they will be taken and a certain fix will be done to them and I need to determine which fixes are the most effective and which are least. I have a variable that defines the performance before and after the fix but I was wondering what model would be best to highlight which fixes in the list have least and most effect. Thank you!!",datascience
"Hi there,

I am a research data scientist, and excited to release a new feature engineering library, designed to help you streamline the process of machine learning even more than before. **Headjack is an open library which provides a ML features transformation based on self-supervised learning models**, similar to huggingface as a hub, but which currently focuses on exchanging features for tabular data models.

Compared to textual data, tabular data are different in that each data set has different column length and attributes, this means that it cannot be typed consistently unlike the token embedded in NLP tasks. Therefore, Headjack is different from NLP’s pre-trained model with single domain transformation, but by performing with two different domain transformations. **In other words, we can perform features transform between two domains without the same key value.** In addition, release the potential of data that is not typically used. For example, enhance the prediction of the Boston housing price task applied in the Titanic domain, or enhance the prediction of the customers churn task applied in the African traffic domain and so on.

[Github](https://github.com/jimliu741523/headjackai-sdk)

[Introduction](https://medium.com/p/385a90ff413c)  


&#x200B;

[The IRIS dataset with California House Price Feature Transformation](https://preview.redd.it/kujhd1bkbpia1.png?width=2110&format=png&auto=webp&v=enabled&s=039faa54d7346e4172564bd129985d948a74b7ae)

[The IRIS dataset with Titanic Feature Transformation](https://preview.redd.it/jwlebjrlbpia1.png?width=2102&format=png&auto=webp&v=enabled&s=ec2aa0d06a70337c1fb01defc71d77956720582b)

[The IRIS dataset with KPMG Customer Demorgraphy Feature Transformation](https://preview.redd.it/5tpe76nmbpia1.png?width=2052&format=png&auto=webp&v=enabled&s=d2c7139259d43c8574da4a0ea1a65056dccad916)",datascience
"Hi, I want to know if it's possible to work in a virtual environment while ssh'ed to google colab. I tried ssh'ing to google colab and did it but when I was going to code a .ipynb file I needed to select the kernel and when I tried selecting the one from my Conda Virtual Environment it did nothing. Wanted to know if it's possible or if I did something wrong. If you know some guide or video that teaches how to do this link it if possible, I already searched but found nothing. Thanks",datascience
"I was trying to get any insight or examples from industry or even academia on ways to incorporate the traditional DS approach or statistical modeling into the A/B experiment process. At my company, we do some A/B tests where we may test one machine learning algorithm vs another algorithm to see which one performs better on our platform in terms of key metrics. But those models are already built by ML engineers and we are just testing which one performs better. 

Instead, what I'm interested is ways to use ML or modeling or help aid in running better/more successful A/B tests or be more efficient in the testing process, etc. For example, in our historical experiments we've ran we have data attributes such as whether the test was successful, the opportunity size estimate for the test (pre-deployment), type of test, target audience used, etc. One example I could think of is using these inputs into a model to predict the probability of success for new experiments in the backlog. Or maybe to help generate a target consumer audience for an experiment. 

I'm wondering if anyone has any insights on anything like this or other ways to apply data science approach to aid your experimentation process.",datascience
"I've been the lone DS at my company\~ a year now. We have a 200 person IT department but completely lack any in house data science besides myself.

I spent a few years here as an analyst before becoming DS, so I have good relationships with the business areas, but I just don't know where to start. So far I've tackled some low hanging fruit but I have no clue how to establish an organization wide program, drive business involvement, establish culture, etc. 

Any advice or tips?",datascience
Need help with detecting data drift in multivariate time series data for many customers. Suppose your dataset has x columns and you have separate datasets for n customers and you need to detect data drift in the fastest way possible without compromising on the quality of the detections. Please help,datascience
"I'm looking for a reference that I can use to help me select the best model to train based on whatever dataset I'm working with.  Ideally something that shows any pre-processing requirements too.

Something like a decision tree flow chart.",datascience
"Working as a data scientist at a video game publisher. I like my job a lot and we're pretty mature as a data organisation, so my work does get used, plenty of models in production etc. 

Some days I can't help but feel what I'm doing is totally pointless, anyone else ever feel like this?",datascience
"Edit: **Working to become a data analyst**

I’m just getting into the field with absolutely no background knowledge. I’ve worked in restaurants and bars my whole life but found a real love for the subject after discovering it. 

I’ve worked my ass off to get through and retain everything in the google data analyst cert and have picked up basic spreadsheet, sql, tableau, and Rstudio knowledge, and I’m now using that to do whatever projects and little practice experiments with what I have learned. 

I guess I’m looking for advice from those much more experienced than I am in where to go from here?

How would you go about getting a job from here? Are there specific tools should I add to my skill set? Best practices for personal projects/portfolio? Courses you recommended? Books? Communities to be active in (e.g. kaggle?) Best method of networking? Ways to practice skills?  Mentoring? Anything you wish you realized when starting it? 

I’m so excited to break into this role and have found every part of what goes into it so interesting! Thank you so much in advance for any advice you may offer!",datascience
"Hello guys, is there any baseline best ways to make regression and handle missing values? For example for big data sets is k-NN or random forest with hyper parameter tuning? What’s the top 3 or 5 go to for example? Also for handling the missing values are there any best ways to handle them in R and Python? I know multiple imputations is a great choice but only for Missing At Random and I know it’s mostly used in R and not in python. Can you specify the best ways and when they are best used? For example Random Forest has the best speed and accuracy for big data while k-NN is better for binary regression,etc",datascience
What are the usual pre-processing stuff do you deal with in real world data? Are there any videos or blogs dealing with this in detail?,datascience
"I am having a very difficult time finding work after a big layoff event from my company early in Jan 2023. I have never been laid off before and I am only 3 years into my career as a data scientist/data analyst and I am not sure how to navigate this experience. 

I have submitted 52 applications to data scientist (with more analytics focus) and data analytics positions and have interviewed at 7 of the companies. I've tried leveraging my network including: old managers, former coworkers, mutual connections at companies I'm interest in working for, etc. In the meantime I've tried learning some more tools/concepts like DBT, Snowflake, and A/B Testing. I do LeetCode SQL questions and have done some exploration of new packages in Python. 

I know finding a job takes patience, especially when there are so many talented individuals on the market right now. I just wanted to get a basis for how long it takes to find a job post-layoff at this time and if I am doing the necessary steps to be employable. Any feedback would be much appreciated!",datascience
"I just got my PhD (not in CS or DS) and I'm looking for data science / engineer jobs. I found that many jobs require AWS and Sql experience. Unfortunately, my past research experience doesn't involve any of those.  Is there a way to gain some experience in using them for real world data. I appreciate it!",datascience
"I used R/RStudio for basically all my data science needs when I was in a PhD program in ecology. Now I’m considering transitioning away from my career in research to a career in data science (motivated more by work environment flexibility and job availability, rather than money). I’ve been learning Python in my free time recently to broaden my skill set as a data scientist. So far I’ve just been using RStudio as a Python IDE since I’m already familiar with it.

Things have been going fine so far, but I’m curious to know if there are any potential shortcomings of using RStudio for Python that might cause me trouble in the future? I realize learning the programming language is a bigger hurdle than learning to use another IDE in the future. However, I thought I’d ask the question here in case your collective wisdom can save me some headaches down the road.

PS - please don’t turn this into the classic R vs Python argument.",datascience
"Hi Guys, i have to solve this problem. I have 33869 column in a python daatafame and i wanth to calculate the Pearson Correlation and the p-value compering each column with all the other. I'm searching for an efficient way to accomplish that.  
I know that the correlation can be calculeted with the pandas.DataFrame.corr() function, but what about the significance of that correlation.  
I saw that scipy.stats.pearsonr() function is able to calculate the correlation and the p-value, in this case the problem is that i have to do 2 nested for cycles that are time consuming.

 Someone can help me?",datascience
"I realise I have bad communication skills. Being non native speaker, I struggle to put my points across. I also have issue with jumping around different details, while explaining stuffs. I know some of these issue are solvable. I am working on it, but it is hard! But I am good at technical skills. I say this, because I have been xxx companies in my short career in data science, I have seen I do as well as many senior data scientists in the team. I feel comfortable with tech, learning and explaining tech. My fear is the more experience I get, good  communication on regular basis become Bread and butter of data scientist. And it will huge hindrance in career progression, even if I get senior position, I feel it will make my life miserable. 

I am pretty old 😅 don’t want be miserable, by forcing myself to situations I do not enjoy. Especially which require fundamental change in thought process and ways of talking.

So I am thinking I can pivot my career to more engineering focused role. I understand at senior level those roles also become communication heavy, but I feel it will be far less that data science career path. Is that correct way of thinking? I plan to move into Mlops or Ml engineering roles.

Does anyone goes through this kind of thoughts, how do you deal with, and how you plan your future given your realisation about yourself. 

Thank you",datascience
"**Long story short:** I have a PhD in Mathematics and at the moment, I'm working as a Postdoc. However, I am seriously thinking of joining the data science club due to its increasing popularity as well as its broad spectrum of career options. My question as the title suggests is how to achieve that in the most optimal way for my personal case.

&#x200B;

**A few more details about my background:** My research interests lie in the field of Partial Differential Equations and Analysis. This in particular implies that I am not that familiar with Probability&Stats. I did have some courses in my bachelor but that's it. I am definitely more comfortable with multivariable calculus and linear algebra for instance. Regarding programming, I know python and I have worked with libraries like numpy, pandas and matplotlib in the past. I do not know any SQL nor Machine Learning yet. I do enjoy anything that involves programming so far.

&#x200B;

**My initial plan:** In the beginning, I thought (maybe a bit naively) to just start learning the most essential skills needed by myself and after a point start doing some projects on my own. To be honest, I would prefer better to follow an online degree or something similar but I am certainly on a budget here. So, instead I downloaded some books and saved some tutorials in YouTube for later use. However, this path of self-education feels just not right. I am continuously contemplating if I chose the appropriate books/sources, plus I am not sure which exact project I should aim to solve later.

&#x200B;

**Current plan:** A few days ago, I came across with this program: [https://www.udacity.com/course/data-scientist-nanodegree--nd025](https://www.udacity.com/course/data-scientist-nanodegree--nd025) (definitely not an ad here). It has plenty of prerequisites which only half of them I already know. It does offer the option of building several projects, although I am not sure if they cover a lot of significant data science topics. A huge drawback for me is the price, as I mentioned I 'm on a budget here.  However, I started thinking that as long as I become familiar with all the prerequisites, maybe following this program is a more efficient (and cheaper compared to other degrees) way to transition into data science.

&#x200B;

**Questions:**

1. Is this degree actually worth it or will I waste my money? Any other (more useful) suggestion is welcome.
2. To the experienced ones out there: given my situation, *what would be the most optimal way to make such a transition into data science?*  To be honest, I would prefer the parts of data science that involve more programming but as I am complete new to this huge field, I have no clue if such parts exist, which are exactly and if my academic qualifications would matter (salary-wise) at all there.

&#x200B;

Many many thanks in advance for your time. Any help is much appreciated!",datascience
"Hi there,

## Share notebook with non-technical users (managers)

Sharing results from Jupyter Notebook might be a pain:

- you can't share notebook to stakeholders because they don't speak Python, cant run notebook by their own,

- you need to copy-paste charts into PowerPoint presentation,

- you need to rewrite your results to some web framework to make your results interactive.


Jupyter project offers a Voila as a solution for running notebooks as web apps. I found Voila a difficult to use, especially ipywidgets.

## Mercury 

I started to work on my own open-source framework for running notebooks as web apps. Mercury allows you to add interactive widgets in Python notebooks, so you can share notebooks as web applications. It offers a set of widgets with simple re-execution of cells.

GitHub repository: [github.com/mljar/mercury](https://github.com/mljar/mercury)

Documentation: [RunMercury.com](https://RunMercury.com)

## What you can build

You can use Mercury Widgets to create:

- [web apps](https://runmercury.com/tutorials/web-app-python-jupyter-notebook/), reports, dashboards,

- [presentations](https://runmercury.com/tutorials/presentation-python-jupyter-notebook/) - use widgets to interact with slides, slides are recomputed during the slideshow

- [websites](https://runmercury.com/tutorials/website-python-jupyter-notebook/) - share notebook as static HTML (with code hidden), 

## Features

Unique features that make Mercury different than Voila or Streamlit

- simple re-execution of cells after widget update,

- all input widgets are displayed in the sidebar, your app layout is ready,

- authentication to control access to notebooks (coming soon),

- easy file upload and download in notebook,

- export result to PDF or HTML with button click.


## Future plans

I would like to provide a cloud service for notebook deployment. You will upload notebook and it will be available as web app/website.



Looking for your feedback and comments! Thank you!",datascience
"I just wanted to share it:
https://cims.nyu.edu/~cfgranda/pages/stuff/probability_stats_for_DS.pdf

This book of lecture notes is simply amazing if you just want to keep the basics sharp or re-learn things from first principles.
I was amazed when I saw it got so little attention, so I thought I should share it (it's legal, you can see a link from his site https://math.nyu.edu/~cfgranda/pages/publications.html).

Fernandez-Granda, Carlos. ""Probability and Statistics for Data Science."" (2017).",datascience
"I’ve been working at a company for about a year and a half, the team team I’m on is supposed to be integrated with other teams cross functionally but the director of my skip level team is painfully bad at…well…directing. There doesn’t appear to be any process to how teams are supposed to work together. He’s produced no guidance or overview or set his priorities for work. 

What results is that an escalation will be forwarded to some teams to work on individually while other teams find out about an escalation only through rapid fire messaging or through another team. 

Teams duplicate work because nobody is placed in charge to manage the escalation. So multiple reports are generated on a single escalation and nobody really knows who the stakeholders are nor have they worked with them in any capacity. Basically everyone tries to answer any possible questions because no specific question is asked. 

I’m confident this issue is at my skip level and not my first line leader who is often as confused as I am about how to proceed when someone just says “do data science” with no meeting or planning or scoping of any kind. 

My boss is concerned for his job so I don’t think he’s going to go to bat and there needs to be some direction from my skip otherwise I risk just doing tons of work for little return on investment. 

Should I reach out to my skip and go around my boss? Should I go above my skip? Maybe I’m in a bind here and I either suck it up or look elsewhere. I like my company but this kinda stinks.",datascience
"I’ll be a little vague but you can probably guess the scenario. I’m working for a govt agency that maintains several temporary locations for residents to drop off important forms. The location from which each form was collected is recorded. This information is also associated with the resident’s address information. Therefore, I should be able to determine the average distance (from home) that a resident had to travel to reach their drop off location. This information will help to assess the placement of drop off locations to best serve the community. 

I’ve never worked with GIS data before. Are there Python packages I can work with to obtain the information I’m looking for?",datascience
"Title says the tl;dr

What books, articles do you consider really good, or general ""Damn I wish someone had told me this when I was starting"" do you consider indispensable now?

**Way too long-winded version**   
I have been in a position for a couple of years that I like, except that I am the only data scientist in the company.  I got a great job offer that I'm starting in a couple of weeks to work with a team of people on a software platform for data analysis and visualization. I will be the person sort of taking lead on organizing and managing the project, but also contributing. However, the others are more skilled than I am right now, and it will likely take me some time to ramp up to their level. 

What are the best resources out there for how to flexibly lead a project? What is your advice you wish you had when you started in a similar situation? I am not really asking about git/github type stuff I am pretty familiar with that. I'm more concerned about the ""soft"" skills of people handling and even things like keeping egos from clashing and optimizing team performance.

 I want to help the team be their best, by helping individuals show their strengths, and sort of get out of their way (though I will also become the main contributor to this project: it is a very small team of programmers, so I'm not just a project manager by any means). 

These are *really* good people I'll be working with and for. I want to do a good job helping create a great product, and not blow it because, e.g., I didn't handle interpersonal dynamics well. I have seen teams go down some very dark and painful paths (e.g., psychological hell for months on end), because subtle psychological incompatibilities became amplified and mutated into monsters when not addressed. I really don't want that to happen I'd love to be part of a happy thriving team that's excited to get shit done!

I'm an ambivert (not hyper introverted or extroverted, sort of in the middle), in case you are wondering, or in case that matters.",datascience
"I am about to be owner of a “rushed” solution the business needed. 
It’s a dashboard that has multiple tabs and feeds from hundreds of tables from our RPA system. 
The data it provides is very critical for making sound business decisions, and more and more people is using it now.
The documentation it has is very poor, and there is no frequent/scheduled health check of the data.

Is there any guide with best practices about how to ensure this dashboard “works well” and that can give me some peace it won’t be guiding the commercial guys to sign bad contracts?",datascience
"What tools have you found best to share/sync data files with others on your team? For example, say you have 2,000 csv files, each one being about 1-2 MB.  Git might work, but it's not great at handling this amount of data and eventually the repo will get very large.",datascience
"Does anybody here know an API where you input a street/city name and get relevant weather data (historical, current, maybe a few days prediction)?  


I know of pirateweather API, however that takes coordinates. I would prefer not to use a different API to translate addresses into coordinates and then get the weather data.  


I have around 50k unique addresses so inputting by hand wont cut it.   


If no such API exists what are your recommendations of free APIs to translate addresses into coordinates (preferably free)?",datascience
"has anyone done predictive maintenance without failure data? in the data we're suppose to have,  there's no failure data but an estimated threshold that indicates when the machinery is about to get into a failure state.

I found in the [matlab](https://la.mathworks.com/company/newsletters/articles/three-ways-to-estimate-remaining-useful-life-for-predictive-maintenance.html) doc that one of the ways to estimate Remaining Useful Life is a model where the available data has a known threshold value that detects failure but I haven't found examples of that. All I happen to find is Run-to-failure datasets and examples, which are not useful for my application",datascience
"Having an interview for the role of a data scientist tomorrow since after I passed their technical test, I have an idea of what to expect but I want to make sure I dont get caught off-gaurd, so what are some questions in your interviews that caught you off-gaurd and made you wish that you had prepared more?   
btw heres the direct quote from their email: ""In this interview we will partially circle back to the technical test, but of course we will also just take the chance to get to know you a bit better."" 

Has anyone had any experiences with such an interview?

This is a company that I'd really like to be a part of, that's why I'm abit nervous about this.",datascience
We're trying to analyze how various social factors influence our health insurance affordability and the rules say we need a data dictionary and we don't understand how to implement it.,datascience
"I have a day job in STEM and I use python to manage large data for my job. I can manage most of my needs by searching stackoverflow and piecing it together. I can see myself as someone between beginner to intermediate level with python. I am actively learning but I think having a certificate will boost my CV/career. Or, lets say if I want to consider a second supporting career as data scientist what I should learn, which certificate I should get? Any recommendations? Apologies, since you might get this question a lot.",datascience
"How should I go about Forecasting Estimates.

I’m building a custom forecast using QuickBooks & Hubspot’s API

My X axis of my chart consists of the following data:

Actual: Revenue
Expected: (0.5 x (Deal Probability x Deal Value) + Revenue) + (
New: Deal Probability x Deal Value

My Y axis is each month.

My question is I want to track revenue from closed deals, but the final 50% of the closed deal, as you can see I’m tracking the first 50% in expected. 

I thought about using estimates as that income is guaranteed within the next 2 months, but invoices do not exist for them yet. So for income where I know is guaranteed for X day of the month, this isn’t tracked in quickbooks.

The issue with using estimates is as soon as the invoice is made, the estimate total doesn’t go down. So I can’t go the estimate route because it’ll create duplicate income.

I suppose i can track this manually? But the whole goal is automating this.

Any ideas? Sorry if I explained poorly :/

Edit: I could only update the actual at the end of the month then not include revenue in the expected potentially? But then that wouldn’t catch all my smaller transactions and it would always be 15% off or so.",datascience
"I prompted ChatGPT to act like Foghorn Leghorn in character both verbally and emotionally to some questions. This was a surprising answer..


Do you think Google loves ChatGPT-3?

Well now, I reckon Google might have a soft spot for ol’ ChatGPT-3, son. It’s a powerful tool and I reckon they’re right proud of it! But you can never be too sure with them big tech companies, they’re always up to somethin’.",datascience
"Hello All, I have been assigned to create a forecasting product from demand data of products.  We have demand data of 8.5 million products. And I am approaching the problem as univariate at first and then I will approach it as multivariate.  However there are so many things to learn.  I am wondering if there is any detailed source you guys can recommend.

I am junior ds. Been reading a lot about this and have a vague plan. I just need to fill the blanks and solidify my plan.",datascience
"As the title says, I've been in my first data science role for about 6 months. This post is mainly just to ask whether what I'm experiencing is common within professional data science. 

The main crux of my role is deriving datasets from raw data. It might be as simple as creating flags based on the value within a column or it could be a little more involved with summarising and cross-tabbing. The thing is, once I've created the dataset and it's been QA'ed and sent off, I don't actually do any analysis to it. I don't look for insights or interesting features or make predictions, perhaps the only EDA I do is to look for anomalies to make sure something hasn't gone wrong in the processing. 

My first proper project in the first few months was to create one such dataset, once that was finished I was then put onto recreating an existing dataset using different and more reusable data sources. Our department has a data engineering and data modelling team which also derive some datasets but I've never had to use them as I never seem to do any actual analysis. 

Basically, is this normal? I anticipated that lots of the job would be data cleaning and manipulation, but I seem to be doing the bulk of the actual coding and putting together of the data without then getting to do anything with it. I don't think I've even made a histogram yet. 

What is frustrating is that there's a fairly sizeable bonus to salary offered if you can demonstrate that you're using advanced data science or analytical techniques, however I rarely (if ever) get the opportunity to actually use them therefore I doubt I'll be able to get the bonus.

I'm weighing up my options. I'd like to stick around for at least a year and there's the potential to go up to senior data scientist. What I'm missing currently to go up to senior are the usual soft skills like interdepartmental communication, managing projects, time management and decision making. All things which I would build up if I stick at this job, what I'm concerned about is how my actual data science projects would translate to applying to other companies in the future. I don't get any nice buzz sentences like ""my findings led to a saving of x% over the year"" or ""I was able to predict with x% accuracy this variable"". 

Any advice?",datascience
"So I am a finance enthusiast and would check American market data in dashboards in the countless free products that exist.

However, being in a country with ridiculous foreign exchange policies, the government keeps blocking every service or broker I use so I am locked out of the international markets.

I started looking at local companies and there is only One single brokerage app that is remotely viable and the data absolutely sucks. I was trying to web scrape the data from companies' websites and faced a few issues:

1. Every company website is different and they do not all have the ""Investor relations"" page in the same place. How can I write one program to get the data of all the listed stocks? which are around
2. All the websites I checked had the data in pdf documents that you download. This was a problem since I was trying to get the data right off the website HTML  and javascript code. The documents are also full of useless pages I do not need. I only need the tables.
3. Let's say I download the documents and want to visualize the data and put it into dashboards. how would I extract information from documents in that scanned format?

I can't find specific information on this because the entire modern world can just use **APIs** to get this kind of data. I am absolutely certain that does not exist for the Egyptian exchange. At least at a price below thousands of dollars.

What can I start doing in order to collect this kind of data? It must be possible. I know organizations are capable of scraping the web for incredible amounts of data and useful information. How do they do it and how are these **APIs** made?

Here is an example IR page for a company I'm interested in :

[Telecom Egypt :: Investor Relations](https://ir.te.eg/en/)

**Please Upvote so more people would see this.**",datascience
"Hi, I want to start my first projects for my portfolio. Do the data need to be somewhat new or collected by me either from polls or whatever or with APIS? Also has the code have to be unique than others? Because I am not thaaat good(still a student) to code something great or whatever and I feel like my effort will be something no one will see because it’ll be something an experienced data science could manage In 30 minutes and I’m getting disheartened.",datascience
" I'm working on a task of classifying text complaints and I extracted some features like Named Entities, Events, Time Expressions, Semantic Role Labels, etc. I want to classify the text according to these features. My question is how do I encode this data in order to feed it to a classifier?

Here is examples of data extracted:  
named\_entities: (FedEx, Israel, Paris) , (Zara, London, Chris), ...

time\_expressions: ('2021-08-31', '31/08/2021') , ('30 August', '2019'), ...

srl: {""verbs"": \[{""verb"": ""write"", ""description"": ""I \[V: write\] \[A1: a complaint\] \[A2: to FedEx\] ."", ""tags"": \[""O"", ""O"", ""O"", ""O"", ""O"", ""O"", ""B-V"", ""B-A1"", ""I-A1"", ""B-A2"", ""I-A2"", ""O""\]}\], ""words"": \[""I"", ""to"", ""FedEx"", "".""\]}, {}, ...

events: {'T1': 'come', 'T2': 'present', 'T3': 'send','T4': 'destination','T5': 'instrument'}, {'T1': 'loader', 'T2': 'bearer', 'T3': 'cargo'} , ...

Previously I used word embeddings to encode the full text but now that the information is in vectors I don't know how to proceed.",datascience
"I'm searching something like Leetcode for practicing Data Science, no only SQL problems also calculus, statistics and thing like that.  


Any recommendation?   
PS: I dont care if i have to pay",datascience
"With the proliferation of LLMs like ChatGPT or whatever it is Google is calling their primary LLM these days (LAMDA or otherwise), do people think this will change the market for data scientists? In one world, the market basically stays the same, as more hardcore AI researchers inventing LLMs and cutting edge work remain concentrated in Big Tech. Then more “normal” data science work remains the same elsewhere. In another world, LLMs replace lower level data science work and either decrease demand for data scientists overall or make them less valuable (highly unlikely imoz more likely to free up some lower levels tasks, for example around EDA, than actually replace anyone). In yet another world, the general shift to AI creates hype around the profession and encourages people to invest in data and analytics broadly, even if they’re not doing AI work specifically (whatever AI work actually is). What do people think? Interesting discussion here in broader churn in engineering labor market given recent big tech layoffs: https://open.spotify.com/episode/01uyOv5cIvLZfqVdwWPK3a?si=IAqZ1638SPiib6UacPQUzg",datascience
"Hey guys, I am a sophomore in college who knows python decently well and is beginning to learn R. Would you recommend continuing to learn languages or to learn skills like reinforcement learning? Any help or advice would be greatly appreciated!",datascience
"I have a very sparse matrix of users products, 120 users and 4000 products. each cell represents the users purchases of that specific product. 

Each month purchases are represented by a separate matrix, assuming the missing values represent  purchases that are not captured. How do I produce an estimate for purchases for each missing user/product pair?  

EX:

matrix\_1: has the purchases of users for different products in January

matrix\_2: has the purchases of users for different products  in February

matrix\_1: has the purchases of users for different products  in March

etc

..

..

Is it a valid approach to have a single matrix for each month, or should we combine them somehow into a single matrix to capture the temporal nature of the problem?",datascience
"Well, this is part of my plan to deal with one of my biggest and undesired flaws: introspectiveness. My idea is to share my skills and knowledge about Data Science with the community and receive its feedback.
Not surprisingly, I got stuck trying to figure out an idea for the very first video. I end up classifying any subject that I can think of in two ways: either too simple to be interesting or too complex to be understandable. My best bet would be to make a video about data visualization, but I don't know how deep I should go in my explanation.
Do you guys have any idea?",datascience
"Hello,

Most of the time I only have my old laptop available without a dGPU and a 5 year old I7 dual core.

Training on the thing takes lots of time. What could you suggest for training models online?
My datasets are often in the 2-10gb Range. I don’t have a problem to pay like 30-50 Euros monthly.

I heard colab pro was super good but since they changed to the compute units model it got pretty meh? Or is it still good? Otherwise I heard about paperclip. 

What else can you recommend? I only want to train models online and then export them using joblib. I am also a Student just in Case there are some nice discounts. 

Appreciate any help!",datascience
"Hi,

I wrote 2 posts together with 2 colleagues on how to solve a compression problem from the insurance industry. We applied the Simplex algorithm to compress the number of model points (every model point is an insurance contract component like life-insurance) and fit them to our benchmark. 

Hope it's ok to post this here, we'd like to get some input on our approach :)

[https://medium.com/@patrick\_hoefler/optimizing-model-points-of-a-life-insurance-portfolio-e9a61f01c5bc](https://medium.com/@patrick_hoefler/optimizing-model-points-of-a-life-insurance-portfolio-e9a61f01c5bc)",datascience
"I'm looking for a free/open source alternative to AWS data wrangler.
I'd like a unified platform where to:
- visualize the data, possibly from multiple data sources and pre-configured visualizations
- preprocess the data with no code solutions, but with the possibility to add code and export the data transformation in python code

Does anything like this exist?",datascience
"I am looking for help and opinions on where to start with getting some „verified“ background on data science (besides doing Bachelors/Masters). Are there any courses / trainings / bootcamps that provide a good start as well as a (kind of) official certificate?

Background: I did my bachelors in applied linguistics (statistics was a big part of it, eg linear regression on text corpora to determine readability) and have an MBA. Currently I am working in a Compliance/Risk Management Team and kind of drifted to a data analysis side role. Regularly I write sql code to extract data from our databases and see what we can learn from it. As I had no prior experience, I learned it from scratch by trial and error, asking around and some online tutorials. 

I have of lot of fun doing this (exploring data, writing code) and am looking to steer my career towards that direction more. For starters, I was thinking of doing something official and have some training budget to spend. Hence my question: do you have any tips on trainings that also issue certificates (ideally stuff that means sth). I am typically more of a hands on guy and don’t think doing a 800€ training is better than learning with coursera - but maybe I can have it both?what do you guys think? Is that a good idea?  or should I just do a python bootcamp and start from there? I don’t know much about the field in general and have just started gathering information so any related tips are also highly appreciated.
(Ofc I did some googling but I am overwhelmed and would appreciate your expertise)",datascience
"I've been asked to take a metis ACE and MPA test before an interview and I'm wondering how to approach it.

Been practising a bit today and mainly I wonder if I should just answer truthfully to the questions or if I should e.g. google which answers that are considered ""good analytical personality""?

First time I've come across one of these during my job hunting, kinda happy that they seem somewhat rare here.",datascience
"hey, im currently a 3rd year data science student looking to improve my DSA skills. I have some knowledge about DSA in C++ but i find C++ quite hard. i came across [this](https://www.udemy.com/course/data-structures-and-algorithms-bootcamp-in-python/) course which is DSA using python. Seem to cover most of the topics and has coding exercises and a few projects as well.

&#x200B;

https://preview.redd.it/dzyq3sdbn4ia1.png?width=1919&format=png&auto=webp&v=enabled&s=2b77018ed3f7b99efadc2097c62725ed58034df1

Can anyone share their experience if they have taken this course? is it worth it? Thanks!",datascience
"I want to set up a ci/cd workflow for databricks, using github actions. There seems to be a couple of main choices. 

* Either executing the code using the runNotebook action, so the code runs on my existing databricks cluster. I am worried that this will be a bit hacky, as I would have to rely on running scripts from the notebook instead of just regular old python scripts. Getting results back and forth from the notebook to the vm running from github actions also seeems like a hurdle to implementing a good script.
* Or setting up the yaml to replicate the databricks environment on the virtual machine that github actions spins up. I have tried the latter, but I struggle to get all the versions of java, scala, spark, pyton, packages etc to be identical an have my code execute without issue.

I mainly have access to using databricks to write code, but I could consider using vscode on a local or virtual machine, or even coder as a web based alternative. Setting up a workflow using vscode and dbx is an option, but I do not want to use conda at the moment, which is what the quickstart seems to require.   


What approach would you recommend? running on the existing cluster or setting up an identical environment in the yaml that fires up a vm in github actions?",datascience
"Hello all,

Answering  questions based on domain knowledge (like internal documentation,  contracts, books, etc.) is challenging because of the size of the domain  knowledge.

As you might know,  Transformer-based models are limited in terms of input size (most of the  time it's hard to use more than 2048 tokens). So you cannot really do  question answering on a huge knowledge base with these models.

You  can try to fine-tune your own text generation model for that (like  GPT-3, GPT-J, etc.) but results are sometimes disappointing as the  fine-tuned models tends to forget some of the facts mentioned in the  dataset...

At NLP Cloud, the  strategy we prefer is about mixing semantic search and question  answering. Here is a quick article about how to do it:

[https://nlpcloud.com/question-answering-on-domain-knowledge-with-semantic-search-and-text-generation.html](https://nlpcloud.com/question-answering-on-domain-knowledge-with-semantic-search-and-text-generation.html?utm_source=reddit&utm_campaign=h5d7a9cc-3816-11ed-a261-0242ac140006)

As far as we know, this is the best way to get both a fluent and user friendly answer while being very accurate.

I would love to hear your thoughts on this. Can you think of a better strategy for question answering on very large documents?

Julien",datascience
"I’m trying create a predictive model for a binary outcome (yes or no).
I have 16 predictor variables, which are a combination of categorical and continuous variables. How do I go about deciding which to include in my model?

I can’t find any literature on the requirements/assumptions of x in a logistic regression. I’m modelling in R therefore I can’t really add all variables and use a Backwards/Forwards variable selection method used in programs such as SAS.


Appreciate any assistance!",datascience
"Hey guys,

Recently I have been interviewing with some companies for DS roles, and there were many case studies involved. When given the case and not much data (just might be a basic graph), how do you come up with the hypotheses and draw conclusions quickly? The tasks are generally interesting given the time and the data, but having to answer on spot throws me off.",datascience
"I knew that I wanted to do something data-related (or computer science-based) for a long time, but unfortunately my undergrad major was completely unrelated to DS. I self-taught myself Python/R while working a marketing job and was suuper fortunate to get into a DS/DA Master’s Program at one of the Ivy League schools. I worked 24/7 to learn a ton about best data practices/tools, had 2 DS co-ops, generated a ton of material for my GitHub, etc.

Obviously, just finishing a Master’s program isn’t enough to enter DS, so my career counselor recommended me to “take a data analyst job at a low-maturity company, use your skills to upgrade the way they use data, become a data rockstar and you’ll get all kinds of doors opened in no time.” **Spoiler** \- *I did not become a data rockstar*.

I landed a marketing analyst position at a large insurance company, and, long story short, this job is just a mess. My boss is a lifer that recently celebrated 20 years in the same position at the same company and got promoted to manager right as I joined. 30% of my work is shuffling around Excel sheets/making ugly (because the style is regulated) PowerPoints, another 30% is attending useless meetings that she sets up for me to “hear what conversations are happening around here”; 20% is making sense of the broken SPSS code that someone wrote for her 15 years ago; the rest of the time is allocated to using Power BI, SQL and R to pull data from various sources and create reports for the internal stakeholders.

Nothing at my job is Googlable because all of the processes are either created by someone from 20 years ago or my manager herself; each process can require up to 100 little steps of moving data between million Excel spreadsheets populated with billion formulas making it impossible to comprehend or even load. The documentation she provides me with is always incorrect/incomplete, forcing me to ask for her help, as it would take me many hours to figure this out myself. When I try to do things my way, she gets clearly upset and sometimes starts micromanaging me, making us miss a ton of deadlines just for the sake of making everything look perfect according to the internal standards. 

I spend a ton of time outside of work hours (the last one to leave the office every single day) trying to make time for extra DS projects but the amount of “high-priority” tasks I get assigned is too overwhelming. The most technical thing I've done is used VDI to automate an R script that imports a bunch of files, anti-joins them and outputs certain distributions into an Excel doc, but that was a one-time project. I told my manager that I enjoy coding the most and she promised to land me a “coding-heavy” project with the DS team. However, I followed up 3 times throughout a month and she ended up telling me that the person who asked for help decided to leave the company (jeez I wonder why.) 

I get so much stress from a seemingly “easy” job (no one seems to care that everything is 2+ weeks past due or indefinitely undelivered) because I’m trying so hard to get a chance to make an impact and have at least something I can use when applying for DS roles after hitting that 1-year mark. I’m afraid that the advanced skills I acquired are slowly fading away and I’m learning a lot of bad habits from my manager.

What would you recommend in my situation to stay on the right track? I usually work through my lunch breaks too (I know, I know), but I had an idea of maybe using that time to read a stats book or something DS-related - any suggestions? Any advice is appreciated :)

Edit: the reason I put ""Ivy League School"" is because, a couple of months ago, my manager gave me an impossible task; I asked for a suggestion on how to better approach this task, and she responded with ""well, I'm not the one with the Master's Degree from the Ivy Leagues."" I got so butthurt from this because I haven't brought it up once since joining the company lol.

TLDR: want to break into DS through low-maturity analytics job but feeling like it has a negative effect on my career potential - tips? ",datascience
"Hello, I need to choose between 2 laptops. The laptop I choose is the one I'm going to use for the next 3-4 years. I need it to be practical and fast to setup and easy to use. I need praticality and speed. I know how to use computers, even linux but I'm at a point I just need praticality and speed. I want to code locally and be able to run simple supervised machine-learning models locally. Any deep learning is going to be made in the could. I don't care about money but it has to be a laptop.

Need to choose between: 

* m2 Macbook Air 16gb ram, 512gb storage
* 16"" Huawei Matebook with Ryzen 7 5800H processor

Need to choose between one of those, it just needs to work and have good battery and perfomance when not plugged into a wall.",datascience
"I barely cared about internships and work experience. Im in my fourth year of college and I realized I should have done more. So I found out how to make my own projects and portfolio. Finally, landed my first unpaid DS + machine learning intern spot. Excited to learn on the job, but how should I be climbing up the ladder to better positions with better pay. The internship is a small commitment under 10 hours a week. What should I be doing on the side?",datascience
"My understanding is that -

`cross_val_predict` yields the actual predictions from the CV. E.g., if there are 5 folds, then the output of each sample is its predicted value when it was in the test fold (trained on the other 4 folds of data).

`cross_val_score` calculates the predictions in the exact same way as `cross_val_predict`. However, it does not return the predictions - only the averaged score across the test folds.

I’m not completely sure though. Could anyone please confirm or correct my understanding?

Thank you!",datascience
"Anyone willing to help me by getting a subscription of dataquest through my referral. I am looking to transition to a Data Analyst role but the cost of the subscription is too much for me to afford. I know there might be people here who are looking to start a career but unsure of the resources or course of action to take. While dataquest will not teach you everything there is to learn about data science, it will take you a level where you can understand the overall picture of the field and help you be able to apply for a entry level job. Help me out by using my referral if you can. I will be forever grateful. Thanks. Link to the Site -> [Dataquest](https://app.dataquest.io/referral-signup/g5bvets0/)  (You get an addtional 15$ discount as well)",datascience
"[This paper](https://arxiv.org/abs/2112.13299) is super interesting, and by the same author who introduced CUPED.

In short, experiments with one sided non-compliance can result in a watered down ATE due to few users actually engaging with the treatment.  This paper builds on the instrumental variables literature (which we leverage a lot for these types of experiments) to use cuped to further reduce variance.

Its super interesting and really useful for those of us who run experiments and need to increase precision of our estimates.  Best of all, there is some fairly food code attached as a footnote.",datascience
"Hi, as You can read in the title I have a dataset with 50 columns. My first aproach is to do a PCA for dimensionality reduction, before i do that, i Made a correlation analysis and drop some columns. Before Made a PCA and using gbdt( xgboost ) and a rf regressor.

But My results aren't good r2=0.55 that is only 0.05 better than a baseline.

I don't have any bussines context to improve My analisis.
 
Some ideas to handle this problem",datascience
"Hello, I'm currently in my 6 month journey learning DS( I come from a mechanical engineering background), and I was wondering what tools/topics should I focus on if I want to get an entry level job in data science in the field of cancer research? 
If anyone had an idea about this topic I would really appreciate your insight. Thank you!",datascience
"Hi all, I'd love some clarification on these dimensionality reduction methods. I see PCA/PCR recommended as the go-to method for this type of task and almost never see people talk about using PLS to get principal components. It's my understanding that the key difference is that PLS considers the target when making components. 

Intuitively, I feel like this would make PLS stronger in many datasets, as it uses more of the available information to better inform the creation of components. I'm not an expert here and don't actually have a ton of expertise with how either of these methods work on a deep level, so any feedback from more experienced data scientists would be much appreciated. Thanks in advance for any thoughts or opinions here!",datascience
"Hello!   
I'll try to keep this short and to the point.

I am preparing to re-enter the field of ""data science"" following a 2 year stint at a startup where I was fulfilling more of a product design and business development role. I found that work to be very interesting and rewarding but at the moment, I'd like to go back to developing a harder skill set.  


Some background on me.  
I have a bachelor's in business management (2013) and completed a Ms Stats/Data Science program January 2019.  
I worked 2+ years as a data scientist at a large US tech company (supply chain). This work was mostly boring and not consequential, so I left for something more exciting.  


So as I begin to prepare to re-enter the data science field, I'm curious how much has changed and how I can best prepare myself..

I have questions but I am also seeking general anectdotes and advice.

* What is the typical tech stack in a data science team these days?
   * In my previous role I was modeling in R and Python depending on the use-case
   * Some light dashboarding
* What's ""up and coming""
   * Besides ""AI"" coming for everyone's jobs
   * What's new that is making folks more effective?
* Have technical interviews changed? What's the status quo?
   * I've previously ""gotten away"" with offers that didn't require tech interviews but I see that as a flag now
* Suggested material for re-familiarizing myself/catching up?

Thanks!",datascience
"One very useful benefit I found with ChatGPT for job seekers is asking it to serve as a Hiring Manager that asks you questions for the specific role you're interested in.

To help it guide you, you need to provide it with some context based on the job description. For example, you list out the job description to ChatGPT and say that you want to be asked questions on causality and experimentation.

Here's the response from ChatGPT based on the following prompt I provided it after sharing the job description:

===**PROMPT BY ME===**

>I would like you to be a hiring manager. I'll be soon interviewing with you for a Data Science role, and you are going to ask me questions related to causality and experimentation. What are the questions you would ask to test my knowledge?

**==RESPONSE BY CHATGPT===**

Sure! As a hiring manager for a data science role, I would like to assess your understanding of causality and experimentation. Here are a few questions I might ask:

1. Can you explain the difference between correlation and causation, and give an example of each?
2. How do you design an experiment to test a causal hypothesis?
3. How do you ensure that the results of an experiment are valid and reliable?
4. Can you explain the concept of confounding variables, and how do you control for them in an experiment?
5. Can you explain the difference between a randomized controlled trial (RCT) and an observational study?
6. How do you measure the impact of a treatment or intervention in an experiment?
7. Have you worked with any causal inference methods such as propensity score matching or instrumental variables? Can you give an example of when you used them?
8. How do you determine which variables to include in an experiment, and how do you avoid overfitting in the modeling process?
9. Can you discuss any challenges or limitations you have encountered in designing and conducting experiments, and how you overcame them?
10. Can you give an example of a real-world scenario in which you applied causal inference or experimentation methods to solve a business problem?

====

You can ask ChatGPT to provide a summary answer for each. However, I would highly recommend you validate the answers by researching as well as ChatGPT can give confident, wrong answers.

Hope this helps others!",datascience
"Hello guys,

I'm working on a pet-project where I'm trying to predict rental prices in a city. I web scrapped data from Facebook Marketplace, but now I'm facing a problem. When I'm trying to extract the areas of the properties, I'm getting a lot of NaN values (\~80%). I'm not sure how to deal with them correctly.

I was thinking about using the mean or median values based on specific property type, number of bedrooms, and bathrooms. But I'm not sure if that's the best approach. I would appreciate any suggestions or advice on how to handle these NaN values.

Thanks in advance for your help!",datascience
"I am currently an undergraduate, and I want to get started on getting my certificates, but I don't know, or rather, I don't understand how to do so. I would simply like to know what I should start with, what websites to look into, what to expect during the course and how I am expected to perform overall",datascience
"Hey guys I have an assessment lined up for southwest Airlines for the Associate DS role can someone give a few pointers for what to expect in the test? tried looking online but could not find anything. another follow-up I have what to expect in the interview with the manager, thanks",datascience
"Hi there, I recently graduated from college w/ a major in computer science and a minor in statistics. I originally was planning on going into software development however my last semester I took my university’s intro to machine learning course and really enjoyed it and decided to pursue data science. 

So far I’ve just been going through online resources to familiarize myself with the field and how to make myself a more suitable candidate. Learning the basics of data preprocessing, wrangling, and EDA. I did an introductory course on PyTorch to learn more about ML frameworks and modeling.

I know the next step is to start building a project portfolio but I’m at a bit of a cross roads. From what I’ve read it’s pretty difficult to get an entry level data science job right out of college and you really have to have an outstanding project portfolio or resume. Although I went to a really good public university, I made a lot of mistakes, my gpa is subpar, I only have one front end dev internship under my belt and I haven’t started a project portfolio besides basic EDA and modeling to learn. 

I really want to enter the work force as soon as possible and don’t know if I have the time to build an impressive project portfolio to get an entry data science position. The advise I was given was to start applying to data analyst positions and gain experience for a year or two. In the meantime to be doing independent projects and might even do a one year masters program with a focus in ML/DS and then transition to data science.

I apologize for the lengthy post but my question is basically whether I should focus more on getting experience in querying, building data pipelines and reports to get a job in data analytics now and focus on building a data science portfolio later on or pursue an entry level data science job now?",datascience
"Been getting a tad annoyed with Conda lately, at least as a package manager. So I wanted to hear what everyone else likes to use.",datascience
"I need someone to tell me I'm not crazy.  I work with a vendor for our learning management system. Each course, curriculum, etc. all have a subject you can apply and the subjects can be nested. Here is an example of a simple example use case:

AUDIO  
\---SPEAKERS  
\------BL  
\------DAS AUDIO  
\---MICROPHONES  
\------WIRED  
\------WIRELESS

My issue is that when filtering by AUDIO, for example, there are no results from ANYTHING below it... only things that are in that top level category.

I opened a trouble ticket and this is the response I received:

>We do understand your concern.  
We tested the same scenario in our local environment and found that if we search parent subject (Top level), the child LO is not returned in the search results.  
This is as designed, this only searches using the subject select and does not include child subjects.  
As a workaround, when the subjects are set up in training, you can also add the parent and child both subjects so that both will be displayed in the Search.

I'm 95% sure this is a dumb answer, right? As in, if it wasn't going to be included in the search for the parent, then it's not really a nested ""child"" subject... it's just a... completely different subject.

I feel like I'm taking crazy pills!",datascience
"&#x200B;

[GeoSpatial Analysis Using GeoPandas In Python](https://preview.redd.it/83rfq0fg8zha1.jpg?width=334&format=pjpg&auto=webp&v=enabled&s=f0ef948961796fbd10a5103bfbdf97ad764505a6)

[GeoSpatial Analysis Using GeoPandas In Python](https://spatial-dev.guru/2023/02/05/geospatial-analysis-using-geopandas-in-python/)",datascience
"1. Altoros Labs
2. Binary Republik
3. DataToBiz 
4. Ballard Chalmers Ltd
5. Saviant Consulting
6. Silicon Reef
7. Trigent Software
8. VOLANSYS Technologies",datascience
"30F living in Austin
MS in EE at 22, 8 years of work experience in deep learning in which I went from Engr I to Senior.

Have extensive experience under my belt   demonstrating everything from expertise in multiple languages and frameworks, aiding directly in exec level business decision making, excellent cost savings, and a large range of problem spaces and deep learning work. I have excellent communication skills and have served as project lead for several (small, 1-3 person) teams. Spaces include valuation modeling, chemometrics, object detection, image style (using triplet loss), consumer-product matching, consumer-agent matching, multi arm bandits in chatbots, and various clustering problems of locations, consumers, and products. I also was in charge of labeling training & QA over about 20 offshore folks for image projects.

I'm curious about dipping my feet further in deep learning but also in management by going for a tech lead / team rep position at my next company but I've no idea how to apply for something like that or what kind of position I should be seeking. Is this typically something you have to earn your way into via promotion rather than through applying? 

Medium sized company just acquired, wanting to look elsewhere. Have been working here for 4 years and just want something new.",datascience
"
I am working with a binary classification dataset with many features, and not much info about them. I made an extra trees classifier with an F1 of 0.99. I believe the data originated from a synthetic source, so I’ll take that 0.99 as viable. 

Anyway, the many features give the most important ones about a 0.02 influence by itself. My question is, how can I see how these features may work together to influence the outcome? For example , perhaps features 3, 21, 78 create a 0.15 impact together. 

Any insight is appreciated!",datascience
"**\* A wild DATASET magically appears \***  


I'm a data engineer and i am wondering what kind of data sources data scientists use. I have some experience with data scientists that use the data warehouse, but I am quite unaware of what teams without a data engineer do.   


When I researched tutorials, it seems the datasets appear magically. So where do you get your data, and who writes the pipelines?  


  


&#x200B;

  


&#x200B;

[View Poll](https://www.reddit.com/poll/1117k6p)",datascience
" Hey data enthusiasts! I'm looking to get into the field of data science, and I would like insights from experienced data experts like yourself. What would you say are the challenges you face/ have faced while working with data (be it data preprocessing, data wrangling, modeling, exploratory data analysis, data management, and/or data governance)? I would truly appreciate your response.",datascience
"So, I started using ChatGPT to gather literature references for my scientific project. Love the information it gives me, clear, accurate and so far correct. It will also give me papers supporting these findings when asked. 

HOWEVER, none of these papers actually exist. I can't find them on google scholar, google, or anywhere else. They can't be found by title or author names. When I ask it for a DOI it happily provides one, but it either is not taken or leads to a different paper that has nothing to do with the topic. I thought translations from different languages could be the cause and it was actually a thing for some papers, but not even the english ones could be traced anywhere online.

Does ChatGPR just generate random papers that look damn much like real ones?

https://preview.redd.it/s8sa42mzixha1.png?width=824&format=png&auto=webp&v=enabled&s=70dfc38d58b6219ea4d494142e5f9e4b75e92a7a",datascience
"Hi everyone,

I am a beginner in data analytics and I find it very interesting. For a school project we have a case where company A only has a ERP system where all the transactions happen and it is connected to a single Database(SQL). The question in this case is what solution would be appropiate because the company doesn't have the manpower and budget to maintain data analytics.

We have learned the basics of ETL, Data warehousing with the Visual Studio tool and connection with SSIS package. But I did some research and found out you can do ETL in powerBI. But how reliable is this? And do you have to keep changing the relationsships in PowerBI everytime you want to make a dashboard or does it stay like that? I need to make a few KPI and dasboards for this school project.

Thanks for advance responses",datascience
"I was not happy in my job for a while. Dead-end project, under-resourced, people leaving constantly.

I spoke up a number of times in how unhappy I am but my boss took it semi-seriously and didn't really take much action.

I recently got a new offer, although only 12% higher total package. Seems like this offer is for more interesting work and generally in a more dynamic environment. It's a downgrade in title and offers a bit less security though.

I attempted to resign today but my boss promised me a new project and a new team. While it will take maybe a couple of months to tie up lose ends, my boss is generally trustworthy. He seemed genuinely distressed and sad that I am thinking of leaving.

I don't know what to do.",datascience
"I had an interview with an interesting question. I had a solution that would have worked which would have required a recursive query, but the interviewer said there is a simpler solution. I was wondering if anyone here could figure it out

Four column table:

     1. UserID- unique string
     2. Device Id- unique string
     3. session_start_time- timestamp
     4. session_end_time- timestamp

It's possible for users with different deviceIds to have overlapping sessions. 
The goal is to have one row per user per overlapping session.
So let's say that I have a session on my phone from 10:30-11:30, and on my computer from 10:45-11:45. The goal would be to aggregate this into a single row:

     UserId|session_start|session_end
     aaquad|10:30|11:45

My strategy was to use a lag to bring up the next row in order to be able to compare them- and the recursion would have been required in cases where a user had 3 or more overlapping sessions. But according to the interviewer, there is a different solution (I asked and did not get the answer).

Any ideas?",datascience
"Hi, college student here. I'm doing CS atm, but as AI quickly adapts to SWE job requirements (to an extent), I'm not sure if going all in on the SWE route is still the best choice. Data Scientists, data engineers, and machine learning engineers, could you share your insight on what you do on your job and what do you see your industry go? Additionally, what classes/projects/bootcamps would you partake in if you are in college right now to be (kinda) ds job-ready? Thanks in advance for your insight!",datascience
"How do you weight the contribution of an individual predictor in a logistic regression?

I have a logistic regression which is predicting whether a loan is considered good or bad. I have 10 predictors, which are a mix of continuous and categorical variables. How do I determine how much each of those predictors contributes to the outcome of the loan being good or bad?",datascience
"Hello all, I'm a civil engineer looking forward to changing my career field from civil engineering to Data Science or analytics. I'm new here and the field is very vast. I'm making the shift due to obvious reasons like PayScale, work environment, no holidays and I'm not interested in this anymore. So can you guys please help me with any free courses to start with so that I can create a good base in this field. Thank you so much in advance.",datascience
" 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",datascience
"
Does anyone know about this online course offered by Indian Statistical Institute in partnership with Coursera? 

For info Im a BE, MBA graduate (already working in marketing analytics) wanting to specialise in data science hence looked up this course but cant find any reviews at all online. The cost of the course is also pretty high at 4.5 lakhs

Will it be worth it considering its from Indian Statistical Institute?",datascience
"Hi everyone, Im 25[M] and I recently switched my career path from digital marketing to data science.
Skip the next paragraph if you dont need to read the reason for this.

For those who want to know the reason can read this paragraph: I was always a fanatic when it came to working with numbers, I was always good at math and was always above average/decent in ms excel as i liked to put everything into perspective by plotting out data tables/charts in random situations in my life. The last job I had was in a market research/analysis role at a digital advertising firm where I had to use software like Talkwalker, Ms Excel and PowerPoint for data capturing, analysis and reporting. This further boosted my interest in pursuing data science.

So im currently doing an MSc in Data Science, and I did by BBA in Marketing & Finance double major. Since my course started I have had to learn SQL, Python and R so far. All of which I absolutely enjoy learning and working with but of course since im new there is going to be a steep learning curve.

Anyhow, I was looking at job requirements for junior data analysts at different companies and they were all over the place. Some required strong SQL, some required strong python and some had R listed while I also saw some jobs wanting a combination of them. Apart from these Ive also seen companies requiring Tableau and some wanting Power BI.

Now the problem is, while I love to learn all of them and I am fully motivated to do so. I just wanted to inquire about which softwares/languages should i focus in order to land any junior data analyst role. I am a quick learner and I know i will continue to work on myself even after I land a role but till then I do not wish to remain unemployed if that makes sense.

The MSc course is 1 year long and has about 5-6 months till it ends.


TLDR; Switched career path from digital marketing to data science. Age 25[M], want to know which software/programs to focus on in order to land a junior data analyst role.",datascience
"Hello everyone,

&#x200B;

I am a career shifter looking to transition into a career in DA/DS. I came across Avery Smith's Data Career Jumpstart Program and was wondering if anyone here has taken the program and could provide me with some feedback. I would really appreciate it if you could share your experience with the program, and any additional advice you may have.

&#x200B;

Questions:

&#x200B;

1. How long did it take you to complete the Data Career Jumpstart Program?

2. How applicable is the material from the program to the current job market?

3. Are the instructors knowledgeable and experienced in the data science field?

4. What was the most useful thing you learned from the program?

5. How did the program help you land a job in data science?

6. What is the support system like from Avery Smith after you have completed the program?

7. What are the chances of getting accepted into a data science role after completing the program?

8. Are there any other resources or materials that you would recommend for those looking to transition into data science?

&#x200B;

Thank you for taking the time to read this post and for any advice you may be able to provide! I'm eager to hear your experience and any advice you can give me.",datascience
"Hi, just looking for some feedback. Apologize for the formatting, I removed some items that may have been personal identifiers. 

&#x200B;

&#x200B;

https://preview.redd.it/1nyn4bt9ovha1.png?width=956&format=png&auto=webp&v=enabled&s=b80805be86ae931a209af5ad83bbf02f087e9f63",datascience
"Hi there, I'm a mechanical engineer currently working as a design engineer in Japan, I'm interested in data science and looking to change my field. I self studied data science basics through IBM professional certification. What should I do to get a job in data science preferably in automobile/manufacturing domains? Kindly help",datascience
"I have 3yrs of quantitative research experience and currently work in a small Analytics team at one of the top market research agencies (same league as Nielsen). 

I’m quite good at analysing small data sets (a few thousand rows) and building simple models in SPSS and R. Been told I make impressive slides that tell stories with data in plain language - I’m from marketing communication background.

I’m also doing a masters degree in Business Analytics on the side. It’s been 1.5yr, the progress has been slow bc studying part time, but all my grades are 90+ and have learned the basics of Python, SQL and Tableau.

Still I feel like I don’t quite meet the requirements of most junior Analyst jobs. It seems I’ll have to either take on grad roles at bigger companies or go to small data consultancies if I’m to take a leap. I’m an immigrant, already in my early 30s and there’s financial pressure (single, saving for an apartment).  

Not sure what should be my next step. Welcome any assessment on my situation, and advice on what kind of jobs/ companies I should aim for, which area I should focus my time and energy to learn and develop, etc.

Thanks in advance!",datascience
"At work, we rely on Excel macros and Python automated task scheduler reports. I code in Python and have been for 2.5 years professionally. We do a lot of reporting / email alerts based on events on some data. I have never built a web app but I know SQL, and Python at a professional level. I need some wisdom from you people! How can I make a web application that:

- Will display data like we do in powerbi (preferably interactive, not necessary at first if extra infrastructure is needed). Charts, tables etc

- Run on a cloud database

- Users will log in via 2 step authentication

- Generate reports based on the data, these are reports we generate daily using local files, using a batch file, written in Python. Automatically on a schedule

- Store the reports we generate as pdfs and help the user download a report any time they want

What are some of your favorite structures for backend in python, cloud database, and front end web app part for a beginner?

Thank you everyone for sharing your wisdom!",datascience
"I have a number of professional projects that are referenced in my resume (mainly visualization work). I want to recreate the projects with dummy data and an altered design to showcase in my portfolio. 

1) So I can back up what I'm stating in my resume

2) Have something to present in an interview 

Are there any complications with this since it's not real company data and it's not the actual project I worked on during company time?",datascience
"Considering a change. My background in data analytics is limited to almost exclusively mathematical representations. I have some experience in machine learning but it's been almost 5 years since I've been involved in such developments.

My question is for those of you who work in machine learning; what are the most common languages and platform/notebooks etc used to develop and express your functions that you see on a day to day basis? What would be your expectations for someone moving into that realm of data science coming from analytics?",datascience
"Hey everyone,

I am a fairly new working professional in Software Engineering, who was recently laid off. I am currently in the final interview stages of 2 companies for 2 different roles - data scientist and data engineer. The thing is - I have some experience working within both the domains, and also have a couple of publications in NLP. 

While I completely understand that the final decision is going to be upto me, I had a few questions for people here - especially for the people who have made a switch from data engineering to data science. 

1. How easy was it for you to switch?
2. Why did you make the switch?
3. What is something that you really love and really hate about the 2 domains?

This would help me gain better insight and make an even more informed decision.

Thanks!",datascience
"Basically the title.

I'm a junior who is struggling to find a job in the currect market and after I had a chat with a startup CEO I casually met he highly recommended me to start networking via conferences as my chances to land a job are much higher than traditionally submitting resumes.

I do understand his point I just feel like as a junior maybe I should distribute my energy towards improving certain demanded techincal skills and projects rather than engaging with managers who might probably not be that interested to a junior? Plus I never really heard of people that landed a job from networking.

Has anyone ever landed a job by networking?",datascience
"Im currently enrolled on a program where python, sql, excel, machine learning and data mining is taught. The program is bootcamp style, I have assignments linked to the topics taught. I have to use the knowledge gained from the topics and apply them to projects at work. I hope to transition to a DS role by the end of the year if I'm lucky internally. I have a $1000 education budget we can use for books subscriptions etc through my employer.

Any recommendations for subscriptions I should buy to supplement my learning? I was thinking of getting leetcode sub to start grinding sql and python problems from around Q2. Is datacamp any good?",datascience
"Does this list make sense? and in this order?

[https://www.kdnuggets.com/2021/03/9-skills-become-data-engineer.html](https://www.kdnuggets.com/2021/03/9-skills-become-data-engineer.html)",datascience
"I'm a backend developer and nowdays I'm working on a project where I have incoming stream of images and I have to run object-detection task on them.

We chose as the company to use an external object-detection api provider rather than creating our own models.

Therefore I searched for different object-detection API out there and decided to use AWS Rekognition.

Seems like their API is not very easy to use and require many post-processing functions on the response that contains the bounding boxes. Other API's I have checked require post/pre-processing on the images/response labels as well.

&#x200B;

I'm wondering if its just me or consuming AI API's is very unstructured, complexed and has lots of overhead.

I would be happy to hear how you dealt with such cases when you had to consume a Computer Vision/ NLP API's.

1. Was it hard and required additional logic around the pre/post processing of the input/output ?
2. Do you have any tools/tricks to make this API integrations easier ?

Thanks !",datascience
"Hi, I’m currently in a masters program and we are doing both R and python. I really think this practice is very wrong, I think that I have to get used to one language instead of learning 2 simultaneously and only switching from one language to another. What’s your take?",datascience
"As a dedicated, hardworking, and disciplined individual with a passion for data science, I am seeking advice on how to turn my dreams into a successful reality. I come from a middle-class family and my father runs a successful computer and accessories business, but I have my own aspirations to make a big impact. I have a BSc in Computer Science and have even tried to start my own software company, but unfortunately, it failed. This setback has left me feeling sad and discouraged, but I am determined not to give up on my goals.

I am now considering pursuing an MSc in Data Science abroad, with the hope that it will increase my chances of success. I am torn, however, as I also want to support and expand my father's business. I am from an underdeveloped country, and I am unsure of the best path to take. Can anyone offer me guidance and advice on how to balance my education, business aspirations, and familial responsibilities? I am eager to hear from those who have faced similar challenges and overcome them.""",datascience
"Hello everyone, I am currently taking a class in Topological Data Analysis and my professor is going to have us do a final project for the class. He is requesting that we start looking for datasets to work on. The criteria being:

* Large number of members
* High-dimensionality
* ""Real-world applications""

Would anyone be able to suggest datasets that I can look at? Some areas of interest that might help narrow: physics, meteorology, environmental science, space, and anything related to these.

What I don't want: finance/economy",datascience
"When I go on LinkedIn, I keep seeing like 200+ applicants for any “Easy Apply” jobs, which in essence makes sense anyway.

But I feel like seeing all these giant tech companies laying off thousands of works, it’s probably flooded the market with tech workers looking for a job. And this is only highly publicized companies. I’m sure smaller companies have done that too but just haven’t been reported on.

Plus with ChatGPT out, I’m thinking firms are even more so reconsidering things.

I ask because I’m trying to finish data quest’s data scientist path and I’m like am I even going to have a chance to get a decent paying job when I finish. 

I’m going to be competing with veteran data science individuals with PhD in CS or Stats or Math with 10-15 years of experience or top university graduates from Harvard, Stanford, Carnegie Mellon because they got laid off recently.",datascience
Every field has its culture. What would you change if you could about the culture and behavioural trends amongst datascientists?,datascience
"We are a company with two platforms, platform a & platform b.

Assume we have a 2D matrix, where each record represents a client, the client has three sets of features:

1. feature set a: captures the behavior of the clients on platform a including their purchases on platform a of different products.
2. features set b: captures the behavior of the clients on platform b including their purchases on platform b of different products.
3. features set c: captures additional features gathered by a survey.
4. The location of each client.

The matrix is sparse, meaning:

\- Most of the clients don't have all the feature sets available, i.e. if a client has entries for feature set a, it's most likely that they don't have entries for feature set b and features set c and vice versa.

\- Some clients have empty feature set a and feature set b (they are not using our platform)

\- Most of the clients purchase a small subset of the of the products.

The **goal** is to predict **the total monthly purchases per product per location**. It's simply the sum of purchases on platform a and purchases done on platform b. But again most clients purchases are unknown on one of the platforms. and a small subset of clients don't have purchases on either.

Example:

[a sample of the matrix](https://preview.redd.it/o80ng6kocsha1.png?width=852&format=png&auto=webp&v=enabled&s=0c20fa16d30cf8853312b6623b1ccba09105601f)

I am thinking matrix factorization, what do you think, is it suitable for such use case? or do you have other more appropriate approaches in mind?",datascience
"I recently rewatched Infinity war. As we all know in the end Thanos snaps his finger and half of all living things fade away. What made me curious is- what kind of sampling would have been used to achieve that?

Would the sampling depend on gender? If yes, would it consider other genders as well. Will demographic play part? Like half of all population each country?

Let's discuss this?",datascience
"
Hi there, 

After studying and working really hard I landed my first job as a Junior BI developer last October, hobing that I can get someday in the future from data analysis to machine learning engineering.

However, my current company isn’t helping me learn or add to my skills, there’s no BI team and no one with experience to guide me. I even feel like my technical skills were better before I joined here. 

I am doing my best to work and learn and I apply to various positions but most require more experience than what I have. 

I had a technical interview today and the interviewer told me that they need a BI developer with 4+ years of experience, and he offered to get me a technical interview as a Power BI developer instead. 

I don’t feel good about it as I enjoy the whole BI process especially integration and Analysis. 

I feel lost and can’t figure out what to do. I am not sure whether I an heading where I want to…

Your advice is appreciated.",datascience
"Hello, I came here because this has been bugging me for the last couple of days, and just want opinions from other DS.

I want to become a successful DS but I don't consider myself a subject matter expert in any field.
I have almost 5 years of experience as a BI / Data Analyst and wanted to make the leap to become a DS so I took a Master's and the first thing I noticed is that all of my peers have a strong background in very specific áreas (Medicine, Agronomy, Economics).
So my question is, did you choose a subject before becoming a data scientist? 
Just Share your thoughts :) 
Sorry for the bad spelling beforehand lol",datascience
"I'm applying to MS Data Science / Artificial Intelligence programs in the US for Fall 2023 - at pretty ambitious Unis (Columbia, Northwestern, Irvine, Rochester, Brown, UPenn) as I do not wish to settle with lower ranked ones and risk a bad ROI / debt spiral. But my chances are pretty low/ nil as it's pretty late after the cycle is almost over. So I've included some lower ranked Unis for DS in my shortlist (Stevens institute of technology being my fail-safe) 

But acceptance rates for MS Business Analytics programs at most UCs and other Top Unis are surprisingly VERY high. And people with much more mediocre profiles than mine get in easily. So I could maybe remove lower ranked DS programs and replace with UCSD / UCD / UCLA MS BA. They have deadlines remaining (round 3 or 4). I was originally thinking to apply to BA next year if I don't get any DS programs this cycle. 


My gpa is 7.03 / 10 (BE Mech + MS Physics degrees) from a ~top 5 institute in my country. I have 2+ YoE as a Data Scientist (applied deep learning, many projects in CV, NLP, RL). I have 1 IEEE published paper, 1 Thesis in AI (RL). GRE 326 (168Q). Although the low gpa is a concern, my profile fits much better for Data Science but isn't bad for top BA programs that do have DS courses in curriculum as well. 


I'm concerned with the outcomes of a BA degree - primarily high-paying opportunities (in any domain or role, matching the fairly common ~$200K Data Scientist salaries in bay area/ NYC), and prevalence of a switch back to DS / MLE / Applied research roles after the BA degree if previous work demonstrates proficiency in ML/DL. 


What is the high-end employability after MSBA, which sort of roles and responsibilities are more common ? How do you compare it to MS DS outcomes? What may I dislike/ be surprised by, in a BA degree or role? 


I already don't love the curriculum. It's all hand-wavy business-oriented courses barely teaching any hard skills. More stats, very less ML if any. Marketing, Finance etc is not something I've ever been exposed to, so I may learn something new that I couldn't have using free UC Berkeley lecture series. But I'm mainly looking for high paying outcomes and growth opportunity.",datascience
"Hey everyone, 

I just  want to start learning stuff for data science, but i don't have guidance about it or i have basic guidance only, but don't  now what is first step of it , so give me advice 

Thanks in advance",datascience
"I was just wandering if there was a dataset or model for detecting malicious content. 
Say if I run a OCR or something to detect perceivable content on a screenshot (Google news/ news feed / YouTube thumbnail and title). 
And detect content with malicious intent, more specifically health or finance.

Does anyone have any leads on how I can achieve this? A dataset/model/app.",datascience
"Hello everyone,

I am interested in a data engineering role. I was hoping if any experienced data engineers could help me with some guidance on what I should focus on first.

I have a good understanding of Python and SQL, but I am not sure where to start with data engineering specifically. I would appreciate any advice or resources you can provide.

Thank you in advance for your help!",datascience
"Hi, I’m a 4th year computer science undergraduate student at a university. I haven’t done any internships yet, I don’t have any hands on experience with projects too. I seriously want to build a career in data science field as I’m very much interested in data, numbers, stats, and python coding. I want to build a foundation of data science, can you guys please suggest me some good courses that I can do so I can land a job after my graduation in that field, I would love if you suggest some free courses not necessary telling no to paid courses, if it’s worth it then please tell me so I can start right away.",datascience
"Hey there! I'm making a personal project centered around hourly load forecasting, and am at crossroads where I realize that some advice is needed on a few questions:

1. What data should I collect that would be helpful? I can get hourly forecasted and historical weather data from the PirateWeather API, but how would I integrate these into my model? It seems straightforward to use historical data, but difficult for forecasted data because I can request hourly forecasts of a specific time in the future for each hourly time interval between the present and then, repeated over and over until the future time coincides with now. For example, if I collect hourly forecasts 10 days in advance, then I will have (10\*24)+(9\*24+23)+(9\*24+22)+...+1=28920 different forecasts for each timestamp. How would I even go about organizing that in a table?
2. What kinds of models are useful for time series forecasting with multiple variables? Furthermore, how should I transform my features to be used by different kinds of models?",datascience
"Hey there! New(ish) data scientist here!

So I'm going to transition soon to more data scientist roles (mine typically uses only Excel and I have to go out of my way to find applications to program) and I'm trying to beef up my Github/Resume/etc.

Back when I was in my MS program, I'd have a whole bunch of programs each year but they were very small in scope (stroke prediction, product reviews, etc.) and so I'm working on this program which I'm having a lot of fun with (medical diagnostic) but the thing is that it's larger in scope than any project I've worked on before.

Do I have to worry about optics? Is a potential employer going to look through my resume and ask, ""You used to have four-five projects a year and now you only have one, what gives?""

I appreciate your help. Thanks!

&#x200B;

Also, side question: At what point can I put a project on my resume? A project like this can always be fine-tuned and improved, but I think it's unreasonable to say I can never put it on my resume until it's absolutely perfect (which I find to be a nearly impossible standard in some cases, such as this).",datascience
"I need to work on a term project for my advanced database management class. We predominately work with RDBMs in class, so ideally I’d explore something else for my project to help expand my repertoire - the only thing is with so much information out there, I don’t know what really considered relevant/in demand nowadays. 

Currently I’m thinking of doing a project using NoSQL (MongoDB) while also practicing some ETL and working with other big data platforms. 

The professor has also suggested creating a data warehouse - that’s more in the realm of RDBMs though and I would like to get some more experience with big, unstructured data.

I guess my question is, if you were recruiting someone as a data engineer or analyst, what would impress you the most? I want to crest something that will grab the attention of employers.

I’m open to all kinds of suggestions - I was told I could also compare and contrast the ETL process using mongodb vs apache spark (using various unstructured/semi-structures data) 

Would appreciate some guidance on this, thanks!",datascience
"Zerodha CEO in a recent article told that there isn't any need for data engineers and as they haven't hired any for their firm.

Zerodha is Stock broker firm, and is quite popular in India for share buying/selling.
Last few years, it was as termed as India's best stock broker for online transactions.


The remark made by him was hurtful and shook me a bit.
As not only they have huge say in market, also they are a financial institutions with huge chunk of data.

I would like to get your view point on this.
PFB attached link for the article:-


https://www.hindustantimes.com/business/why-zerodha-has-no-data-scientists-ceo-kamath-explains-with-a-witty-example-101675959495780-amp.html",datascience
"So ever since becoming a data scientist, I've been constantly getting flooded with connect requests on Linkedin. I'd say about half of them are established people in the field, (DS/DA/DE), and the other half are people trying to get into the field. Additionally, I get about 4-6 new requests every week from people asking to connect with me, either asking for a job referral, to talk about my role, or advice on how to break into data science. So I usually meet about 2-4 of people a week to give them advice how to get into the field since a lot of people I've talked to think my career pivot is inspiring. I had close to 100 LinkedIn connections this time last year and now I'm over 600 and climbing.

&#x200B;

I wanted to ask is getting this many requests normal for other data scientists (roughly 6-12 per week)? And if so, how do you guys deal with all the other people asking for help all the time (if you choose too, since I've realized it can become time-consuming)? My manager says he gets requests for help all the time too and it's because of the field we are in, so I was curious if most other data scientists experience this? I've talked to other successful people with great jobs, and although they did get a lot of requests for help, especially earlier on in their careers, some did mention that the amount I get weekly is pretty high. I also wonder if it just happens to be due to the job market as of late?",datascience
"Hi, I have been working as a data science consultant where I have to interact with business stakeholders to approve the ML model that we develop prior to deployment. I am wondering what are tips/software/framework do you suggest to use to improve the collaboration process between engineers and business stakeholders.",datascience
"I'm performing seasonal analysis on time series data. However there's an inflationary component which I'm not worried about. Is there a method generally used for flattening out my increasing sin wave? 

I imagine this would feature in predictive modelling for pricing but it's not an area where I'm familiar. 

Incase my title isn't clear I'll included a link to a graphing calculator showing what I mean.

[https://www.desmos.com/calculator/jkocrrvubw](https://www.desmos.com/calculator/jkocrrvubw)",datascience
"I was sharing some of my work with a SWD I have a lot of respect for. My most recent project is scraping a bunch of esports data that I then did some basic EDA with and was looking for interesting questions to move forward. His statement to me seemed odd

> I don't really think that is the right approach here. Typically we'd go into something like this by starting with a model that we design intuitively, then compare with the game results, and spend time trying to identify what causes differences between the game and model
How can there be a descriptive understanding without a model

I have an MS in Data analytics (not from a great school but still) and every one of my courses started with ""Do EDA to ensure you can model your data first""

AM I missing some nuance here about when to do EDA or when to immediately go ""linear regression""",datascience
"I got a Masters from an Online program and enjoyed the flexibility it allowed me. I've been thinking about getting a PhD on DL/ML related program. 

I would appreciate recommendations!",datascience
"If anyone is familar with this style of visualization, is there any python package that allows me to represent the percentages as circles out of 100 like this? 

https://preview.redd.it/e5gdjsi0slha1.png?width=1730&format=png&auto=webp&v=enabled&s=15977365b31636d24c986c53249109b5f1ab5ba3",datascience
"Given at how job listings look, I have a feeling that my future career will be in a data science or adjacent field. I am currently completing a BA in Econ and a BA in geography. I am between an MS  in ISOM and an MA in Econ. The [MS-ISOM curriculum](https://warrington.ufl.edu/master-of-science-in-information-systems-and-operations-management/courses-and-curriculum/) can be found here and the [Econ MA curriculum](https://economics.clas.ufl.edu/academics/curriculum/) can be found here. I was hoping for some advice on which program to peruse to best prepare me for a future with data.",datascience
"As a brief intro, I’ve been stalking this sub for a while and in the couple years that I’ve started studying data science, your collective advice has enabled me to learn a lot of cool data science techniques, land data science internships, and improve my understanding of key math areas like linear algebra, probability, and stats.  I use this sub as a direct resource for learning data science so your advice is very much appreciated on my end.

Currently I’m an undergraduate studying Data Science at a top 10 school globally. Although I’ve felt that my major’s curriculum has helped me establish a solid understanding of the math, programming, and analytical aspects of data science, my major has generally been focused on predictive modeling rather than inferential modeling. 

At this point in time, I have taken four different ML/DL courses where I’ve gained a pretty strong grounding in classical ML techniques, core DL frameworks, and implementation/productization. However, although I’ve learned through classes and self-studying regression analysis and hypothesis testing, I have yet to take a class solely dedicated to inferential modeling as a whole.

This coming quarter, should I take my 5th ML/DL course to further establish my knowledge and approach to predictive modeling, or should I take a course solely dedicated to Applied Regression Analysis (name of the course) to gain a better grounding in inferential modeling?

TLDR; Should I take 5th ML/DL course or 1st regression analysis/inferential modeling course?",datascience
"Hey all, is there any way possible to find the original metadata of a photo before was imported into a photo vault? I can only see the date and time it was imported into the app.",datascience
"Hey all, is there any way possible to find the original metadata of a photo before was imported into a photo vault? I can only see the date and time it was imported into the app. [oco]",datascience
"I currently have 7 YOE but I still enjoy learning whenever I can. Since free time is pretty scarce between work priorities and family life, I’m very particular about topics I should study and qualify of material. Does anyone have any recommendations for a tool I could use to

1. Keep track of topics I want to cover whether I’ve completed it, currently working on it or want to get to it in the future

2. Store links to material covering that topic that I’ve decided is worth it

3. Make note of where I am in the material, just a note to self where I left off if I need to break for a few week. (YouTube series, books, moocs etc…)

4. Store my notes regarding each topic for later reference. Could be a link or file attachment etc…

I was initially thinking trello, but I’m curious if anyone else is overthinks their learning as much as I do and what you’re using.",datascience
"Hello! I'm about to graduate from a masters of data science and I'm searching for full-time fully remote DS job. I seem to only find hybrid jobs or with location restrictions. Can you tell me a company that accepts fully remote? Also, which are company red flags during interviews?",datascience
"TLDR: a 31 y.o. DS\\researcher working mostly with time-series wearables data, lagging behind modern development stack. Willing to become a more demanded and better paid pro, possibly in an adjacent field. How should I spend 6 months (fulltime) with the maximum benefit?

I will be laid off in March-April because of the reorg and will have 6 month of from-work-to-work time (employer's termination notice). I won't be expected to do much, but I want to use that time with the maximum profit for myself. I can find another similar job, but see this as an opportunity to reinvent myself. What should I do?   
\- change profession to data engineering (I kinda miss research aspect there, but there are so many vacancies...)  
\- refocus to a different sexy domain (trading algos, video AI, blockchain,...?)  
I think the best way to go is to learn stuff by doing and develop my portfolio at the same time (public github, kaggle competitions). 

Does anyone has ideas on the roadmap? Thanks.",datascience
"I'm not trying to offend anyone's career choices but since I've only first heard about Data Science a year or so ago (Yes, I do infact live behind the moon) how did society seemingly collectively go from
""Governments and Big Corporations secretly taking and using our data for profit is morally reprehensible and fundamentally wrong, Snowden did nothing wrong!""
To
""Here Mr. Zuck, proof I spent hundreds of hours and thousands of $ so I can help you make more money by analysing all this data you've been collecting""

Is it the money/status/career? Has there been a shift in data collection where it's only ""moral"" data being collected? Was some article writing ""DS, sexiest job of the 21st century"" all it took? I genuinely want to know.

Edit:
I acknowledge that there are many useful sectors where it's not personal data and it's used for the greater good but esp. before the mass layoffs, weren't most people, especially on social media obsessed with getting into FANG by taking Bootcamps and such, where they most definitely do all the stuffs I'm thinking about.",datascience
"I am interviewing with a systematic hedge fund for a quant research role (I am a statistics masters student currently) . I have an all-day onsite interview coming up in which I am told I will be given a dataset at 9am, and will have to ""present my findings"" at about 4pm. The data is not necessarily financial; it could be something like weather or sports.

The explicit requirements for the role are familiarity with Python, Pandas etc., and good background in stats -- not necessarily Kaggle whizz. It's hard to imagine making much progress on quite an undirected task in that little time, particularly when random bugs and errors are basically bound to happen and cost time at some point, and when some time at the end needs to be spent preparing a presentation.

I would be very grateful for some advice from anyone who knows about junior data science roles (particularly in finance):

\-   What sort of level of analysis would be expected in this timeframe?

\-   Given that a large chunk of time will need to be spent on doing very basic explorations and plots, and on preparing a presentation, how is it possible to impress in an interview like this?

Many thanks for any help, this interview seems like quite a daunting task!",datascience
"Hey, so here is the thing, i am a junior with like 1 YOE, and i am really really a good one as someone with 1 YOE, everybody is shocked when they find out i am a junior, cos i really put into hours to study and experiment stuff.Also i have a lot of freedom on job to get experience, cos i fail alot, and i guess thats where u get experience.But the thing is for example i had some interview for startup where day were really blown away by my knowledge, but the hardest thing for me is to get interview, since i dont post stuff on linkedin, i only use enterprise github profile etcc..Like not much to show expect i am working as MLE there and there for an year, and describing what i do.
How can i improve this?Should i write blogs.., start being active on linkedin.",datascience
"Hi everyone,

I want to write my own script (?) to export metrics from various social media platforms to Google Spreadsheets. None of the existing tools seem to answer my data analytics questions. 

I wonder where I should start, which programming language I should use, etc.

Any advice is appreciated!",datascience
"Hey Guys,

New to pandas, I spent the last 3-4 hours trying to parse this nested dictionary into a dataframe but have failed. Now I'm asking the pros for help! Even chatgpt couldn't give me a working answer.

My goal is to create a separate dataframe out of every most outer key  ex: 'GigabitEthernet0/0' , 'GigabitEthernet0/1' etc should be its own df.

Now in the DF it should be formatted like so

&#x200B;

column = GigabitEthernet0/0

every key/value as a row below - even the nested ones

full data and expected results: [https://pastebin.com/XEV64Cbj](https://pastebin.com/XEV64Cbj)

my attempt: [https://pastebin.com/jzqf2PVK](https://pastebin.com/jzqf2PVK)

I can't get the nested dictionaries to just be more rows.

EDIT: google gods blessed me today and I was able to find some working code

https://pastebin.com/pUiY1KDV

Thanks!!",datascience
"Based on your own experience, formulate a one to two liner on what makes DS **not** for someone. It could be simple like ""you dislike math"" or something more complicated.",datascience
"So long story short, my undergrad was in international affairs, nothing technical. I touched R a little bit but nothing extensive. I decided during the pandemic I was going to push to get a STEM masters to supplement my undergrad and I knocked it out. I really enjoyed it and I can use a ton of tools to develop ML solutions and deploy models leveraging tools like Sagemaker. I’ve even become an OK programmer. The issue I have is many of these solutions and tools are black box. I can apply the tool to solve a problem but I don’t always know how it works and, being honest, there’s a lot of effing tools. 

I feel like I’ve got a good warm and fuzzy about what data science and ML can do and what it can’t but I find a lot of jobs will post their need for people with extensive education in quantitative skills, or vast experience with A/B testing which I understand but haven’t done extensively, or 6 years building solutions with Tensorflow. Thus, I feel like I might be wildly under skilled despite the degree. 

Maybe the industry is just so flooded now that DS roles really require a deep specialized knowledge because it’s so competitive. Seems like there are lots of PhD roles in industry these days. 

Would it make sense for me to go back and hammer out the math? Another undergrad? Another masters maybe? Push forward and do a PhD and lose steady income for the 4 years maybe more?

Maybe this is classic imposter syndrome. I digress. But anyway, advice is always good to have.",datascience
Interviewing for DS/ML position what part of the interview did you like? Or was there a companies interview process that was very memorable?,datascience
"Couple of months ago I took part in a hackathon related to scouting football players for Sevilla FC. The data had more than 80,000 columns, and no proper information on the data being presented.

The data that was provided had column names as ""X_0,X_1,..."". We are given scouting data which has age team played for performance but along with this 80000 columns with absolutely no context. So I can't infer anything, nor do I have skills to tackle such data.

As I am a student with work experience only as a software tester I generally practice data science using open source data or kaggle. On these platforms we have some insights on each attributes of the data.

How will you guys go about processing the data before creating a Ml model for such cases where dimensions are high so manually doing eda is very hard?",datascience
"After landing my first data analyst job after getting my MS in in Data Analytics as an adult learner,  I am now being told that I am not a good fit and I should start looking for other roles.  Freakin sucks, only 9 months in. But to be fair, I was not great. But I was progressing

There is so much to learn in a real business setting than what I could ever imagine. (my employer is a huge telco so LOTS of data). Anyway, feeling like maybe I should give up on this data stuff. But I do actually enjoy the work so maybe looking for some encouragement... Has something similar ever happened to any of you? And advice on how to pick yourself up and keep on going?",datascience
"Hi!

For work I need to analyse 52 workbooks with 5 sheets per workbook. I need to add everything into one workbook so that me and my boss can analyse the data.

The data however is a bit messy. The first two rows contain headers that I need, but the second header needs to be turned into columns to measure the hours spend on something per name. There is also data under the screenshot that is not needed, but that data is in different rows every sheet.

There are no tables in the sheets, only some visible formatting but no data formatting.

The screenshot is one example of a sheet from the workbooks i need to analyse. Every sheet contain about 100 rows and 30 columns, but the amount of rows is different for each sheet.

I've tried analysing with Power Query, then python and started to think about SQL when I couldn't figure it out.

I'm a bit stuck about making the decision which language to use. If some of you could point me in the right direction I can continue and be on my way. I'm just startin to doubt myself and my method.

Would you recommend using Python, SQL, Power Query or something different?

&#x200B;

&#x200B;

https://preview.redd.it/rwp3ho4k9gha1.png?width=1854&format=png&auto=webp&v=enabled&s=a1a0fcbce39be34be8b992e447e6e346da87709e",datascience
"
I’ve been offered to do a full course about just one of this two career paths and when I’m done with the course I’ll also have a job interview so the choice I’ll make it is really important for my future.

I’m 27 with no degree,I worked last 7 years surrounded by people 8-5 and as an introvert this really drained me out. Also bad company policies and bad work environments gave me the finishing blow. 
I’m also planning going to university to start studying probably in September.

I have 0 experience in both Data Science and Cybersecurity,never worked in these fields,all I know from both I learned it from Youtube,Google and Reddit during these days while I’m trying to decide which course I’m gonna choose.

Final thoughts are: I don’t care much about which job has the best salary,just need money to cover my Uni taxes and survive,so I’d like to decide which job suits me more based on:
1)Which one is more remote oriented?
2)Which one has more self organized schedules/work hours?
3)Which one has less interaction with people?
4)Which one you think has the brightest future?

Based on this infos about me and most importantly about your personal experiences,which job should i choose? Which one it would be more suited for me and my necessities?
PS. Sorry for the broken english.",datascience
"Greetings everyone.

I work on a company that builds software solutions for governments. At the data sector, we are processing healthcare data for BI and ML applications. 

Our current data pipeline looks something like this: Oracle db (data source) -> PostgreSQL db (intermediate db) -> Power BI. This workflow is orchestrated by AirFlow.

The main issue we are facing is the control of which person can access which database, and the segmentation of said data.

We would also like to implement a data governance tool. I have been looking into Delta Lake, but it seems this is a tool for data lake house, and not data governance.

I would like advice on which tools should I look into that can fix the issues mentioned. Also, given our current situation, what would be the next step in order to implement a data governance tool?

Thanks in advance!!

Edit: forgot to mention, but this tool also needs to be able to work both on premise and in the cloud.",datascience
"Hello, I would like to share with you my recent article about utilizing the power of a neuro-symbolic framework - PyNeuraLogic, to implement different kinds of Transformers, including the one used in GPT-3.

[https://towardsdatascience.com/beyond-transformers-with-pyneuralogic-10b70cdc5e45](https://towardsdatascience.com/beyond-transformers-with-pyneuralogic-10b70cdc5e45)

I would appreciate any feedback down in the comments. Thank you.

&#x200B;

&#x200B;

https://preview.redd.it/4ore6fmgpfha1.png?width=1400&format=png&auto=webp&v=enabled&s=3c9533e3abbe780b9e952f93d55a00eba5f1d33b",datascience
"Hi Reddit community, 

&#x200B;

I recently wrote a Medium article on using PyCaret to predict the perfect media mix. PyCaret is an open-source machine learning library in Python that makes it easy to build, train and deploy machine learning models. 

&#x200B;

In the article, I demonstrate how to use PyCaret to build a model that predicts the optimal media mix (e.g. TV, radio, print, online) based on a set of inputs (e.g. target audience, budget, product/service features). The article is perfect for anyone interested in machine learning and its applications in marketing. 

&#x200B;

Check it out here:  [LINK](https://medium.com/geekculture/predicting-the-perfect-media-mix-with-pycaret-924b90be7f3a)

&#x200B;

I hope you find the article informative and useful. If you have any feedback or questions, please leave a comment! 

&#x200B;

Thanks for reading!",datascience
"can someone tell me what am I doing wrong on my resume?

https://preview.redd.it/t61s80qpofha1.png?width=1110&format=png&auto=webp&v=enabled&s=d356aac2f10c7610544010285f8c3e74ebbf4fa6",datascience
"Does anyone here have experience with LSTM?

I am dealing with an issue that is basically a negative binomial distribution: we are trying to get people to reply to a text message, and we want to see how many days of attempts/reminders it takes before someone replies.

Someone suggested LSTM as a model, but the examples of LSTM I see are mostly surrounding data that has both upward and downward trends, but what we are dealing with is a series of 0s that end in 1, and that's where we cut the data off.

Thoughts from the community?",datascience
"I'm looking to make a career change into data science from real estate into data science. I've been teaching myself both sql and python from coursera and various youtube channels. The one thing is holding me back the math. I'm average at math, not great. I never took linear algebra and only took a basic stats class in college. Im just worried the math part of data science might hold me back if it gets too involved with linear algebra",datascience
"Quick read on how some datasets may yield biases results with AI.  Also suggests a few ways to improve the datasets. 

[https://medium.com/@egray4cs/discrimination-in-ai-machine-learning-and-datasets-680ef3294926](https://medium.com/@egray4cs/discrimination-in-ai-machine-learning-and-datasets-680ef3294926)",datascience
"I have had multiple data science roles and have interviewed several companies.  After the initial stages there are usually technical and/or panel interviews.  In some cases I am able to determine why based on the interview it didn’t work out but a lot of times I just get a generic email and have to guess why they didn’t move forward.

I am just wondering based on those who have experience interviewing in those 2nd to 3rd round interviews what are the main reasons you or your company doesn’t move forward.",datascience
"When interviewing for a new job, if asked about your salary at current company, do you need to be honest? Or can you instead tell them a higher number so they give you a higher offer?",datascience
"I don't want to give too much detail as to what the actual function of the task is. Not looking for help on the actual project. Just curious as to where this aligns with a typical ""skills assessment"" in an interview process for a mid level data scientist position. Thank you fellow data scientists!",datascience
"Hello all,

I am a student pursuing my Bachelor's degree in Economics and I am currently searching for a topic for my thesis in the field of Data Science/Machine Learning. As I am still at the early stages of my research, I am open to any suggestions that might be relevant to my field of study.

My area of interest lies in the application of Data Science/ML techniques in the field of Economics, such as prediction of economic trends, analysis of financial markets, or any other related topics.

If you have any suggestions or recommendations, I would be grateful if you could share them with me. I would also appreciate any links to relevant articles or research papers that might help me in my search.

Thank you in advance for your time and assistance. I look forward to your replies.  
Thank you so much for your time.",datascience
"How do you present business value that you've generated but that isn't quantifiable?
This might be a method or script that increases the robustness of a process - but from the outside that improvement can't necessarily be seen. 

Or it could be a better interface for a colleague who doesn't necessarily work faster (1s time saving makes a lot of difference to the user but maybe not to a business process) but the colleague is way happier because they don't get frustrated by bad UI design. In my mind, this is one example that increases the flexibility of people/processes.

How do you present such findings, e.g. when in salary negotiations - or when asking for a budget? Or do you not use such qualitative indicators at all?",datascience
"Hey! So, I was assigned to a project where a Jupiter Notebook is my deliverable. It includes data cleaning/processing, EDA, hypotheses testing, analysis - usual stuff. The main goal is to find insights and either identify weaknesses in our client MO or prove that they work better than competitors. 

Thing is, main stakeholder is a non technical non analyst and I don’t have direct access to him. Should I somehow modify my usual approach to creating the notebook, considering that the main audience has no idea how code works and what statistics is? Should I be more verbose and explain more? Any tips and suggestion are welcome, thanks!",datascience
"Currently holding two job offers 

Data engineering for insurnace company £38k many benefits 

Data science for data consulting company £30k with no real benefits except course certificates 


I think the data engineering would certainly be a boring role ( but I don’t know) 

Are there any opinions you guys have as to career progression for both? I’m not sure which one to accept but leaning to the data science one.

I belive it would provide me with more skills to progress into many more positions, furthermore it’s my understanding that data scientists can go into data engineering in the future, but going from data engineering to data science is much harder as engineering doesn’t require the scientific knowledge.
Is there anyone more experience to confirm?

Thank you 🙏",datascience
" 

Hello my friends!

I am a last semester undergrad mathematics student and now that I will no longer be enrolled in college, the thought of my future as a mathematician has been haunting me. As you know, the mathematical life can be very lonely, and I only have a couple of friends and professors to whom I can talk about mathematics.

So, I wanted to hear some advice from fellow experienced STEM wokers who can give me some insight. To give some context I also have a degree in electronic engineering and in my mathematical journey I have been very focused on model theory and logic, but lately after finishing my thesis I discovered that maybe the academia life is not for me. So I have taken two courses on actuarial mathematics and mathematical finance (which I enjoyed a lot). In engineering, I only liked the signal processing/dynamical systems/machine learning courses (which was one of the reasons why I decided to double major in math).

Right now, I feel that life as an academic is not what I'm looking for in life (specially in the country I live in) and that my love for mathematics has been more of a love for challenges and learning new things (rather than mathematics by itself) and I am very passionate about solving problems and mathematics has become sort of a drug giving me dopamine rushes every time something 'clicks' or every time I solve a problem. So that is why I am looking or advice in jobs that have a deep mathematical component that offers challenges every day and that evolves constantly (that also offers good payment and work/life balance). So far these are my options with their pros and cons:

* Quant /Mathematical finance: **Pros**: I love the deep and complex structure of the stock market and I feel that it has rich mathematical problems (stochastic calculus, PDE's, programming, etc.) It also offers great payment and looks like a very active lifestyle. **Cons:** I don't like that the main purpose is money driven. I have also heard that it is extremely time-consuming and that the there is not much work/life balance.
* Data Analyst/Data miner: **Pros:** I love programming. It also offers rich mathematical techniques and can be applied to a vast number of scenarios. I like that it is not tied to a specific industry because pretty much every enterprise needs data analysis in today's world. **Cons:** I have read that the day-to-day work is very different from the theory, and you end up being an advisor to the sales team on a company that expects you to do magic with their data. I also feel that climbing up the salary ladder takes longer.
* Actuary: **Pros:** Great payment. Great work/life balance (from what I have read). There are not a lot of actuaries. **Cons:** The need to take so many exams/certifications to increase salary/position. Work becomes very monotone and I feel that you are tied to the insurance market. Spreadsheet boring life.
* Programmer/software developer: **Pros:** fantastic salary (you can find home office work in USD). Already have experience programming. High demand for software developers. **Cons:** not very math-focused. It depends **a lot** on *what* you are programming. (I see programmers as digital construction workers), I would love some machine learning/math related programming, but I would die f boredom developing web-pages and that sort of stuff.

Can you guys share some of your experiences in these fields? How do you see each career in the future? Do you have any other recommendations for me? Thank you so much in advance. I really need guidance and different opinions before taking such an important decision. Cheers!",datascience
"Data Engineer

format is on mobile sorry

As the title says, i’m currently a HS student and I am interested in working to become a data engineer/ working with data. I don’t exactly know where to start other than I’m taking a computer science course. What should I do? any advice or places that could help? all help is appreciated.",datascience
"I’m one of the newbie competitors for the RSNA Mammography competition on kaggle, and after a month being stuck under 0.07 score I was finally able to break it with a 0.15! This is nothing compared to the top scorer which is currently 0.67 but such a massive win for me. I’m not copying someone else’s notebook, it’s all code that I wrote (by copying code from stackoverflow xD). My goal is to keep improving until the competition ends which is pretty soon. Just wanted to share this as it made my day.",datascience
"I am studying ML for my current job as an engineer, and I may or may not transition into a data science job. But I surely am working on a lot of data science stuff. I need some guidance on if I am working aimlessly studying a lot of stuff instead of having a focus. I would like to find a mentor. How would one approach this?",datascience
" 

**Are there any differences or definitions?**

**Do we need them for the future of AI?**

**Read the below Medium article published in Towards AI:**

**https://medium.com/towards-artificial-intelligence/understand-ai-interpretability-explainability-3718797777ec**",datascience
"Hi, I am new in the data science field and am learning the same from an online course. 

Along with that I want to do an internship to get hands-on on experience.

I had applied for internships in a couple of companies and they have provided me with an assignment to solve.

Any help for the said assignment will be very much helpful. 

Reply to this thread so that we can discuss about the problem.

Thank You.",datascience
"want to share something pretty cool; those are End-to-End, Live, ML projects created by students in the Scalable ML & DL master's course in Sweden! 

They required: feature pipeline, inference pipeline, UI for prediction, all use free serverless infrastructure and on-demand model training. 

The majority of the projects deployed and operated their systems using free services.

Would love for comments, feedback (not on the design of the site 👀, and just overall thoughts! 

[https://id2223kth.github.io/assignments/project/ServerlessMLProjectsID22232023.html#id.69rpj0oh5iu6](https://id2223kth.github.io/assignments/project/ServerlessMLProjectsID22232023.html#id.69rpj0oh5iu6)

&#x200B;

https://preview.redd.it/uq4gjmxsncha1.jpg?width=1536&format=pjpg&auto=webp&v=enabled&s=35d7dfc6dc54eb8deed45b4dff89be800ace254b",datascience
"Hey, 

What tools do you use to explore a dataset you have never worked with?

I sometimes use Metabase, but I am pretty annoyed by the row limitation. I often fall back to some simple panda python scripts, but this feels not so flexible. Anyone maybe knows a tool like metabase, without the row limitation (Which can also be paid)?",datascience
"Hi all

I am fairly new to data science. I have done some projects either for my work or for my own interest(Mostly graphs,some data cleaning, timestamps and other). I was following a course in a site but i felt that it was like they were chewing the food to my mouth.And now im kinda lost about what are the steps i should take. So what steps would you recommend to someone that wants to start data science from scratch?  Also are there some courses that are trsutwothy and worth the time?

&#x200B;

Thank you in advance",datascience
"The description of this subreddit is ""to discuss ds career question"" so here's one question for the senior folks of this sub. I'm a Junior /Associate in this field and about to complete my 2 years at a IT consulting startup. Things like very less pay, heavy team politics, unhealthy work environment, employment bond etc are making me quit this company. The question is am I an idiot if I quit my job without having another one lined up and start the job hunt process once I'm on the notice period?

Why am I taking this route? Many reasons:

1. Employment bond renewal during yearly increment.
2. Thee month notice period.
3. Need time for interview prep. Bad work life balance does not provide me with enough time.

Location: India

Note: I'm just looking for some guidance and that's about it. Many people here have experienced more years then me in the corporate world or might have experienced something like this. Please feel free to drop in your views.",datascience
Can't seems to find solution for this. I am thinking of just classifying the variable into categories and do a one-hot encoding on it. Thoughts or sharing of past experience? Thank you.,datascience
"I have got an offer for data engineering internship for 4 months where I have to perform web scraping and crawling which is new to me and it is unpaid. They only provide certificate of completion at the end.
About me : I am pursuing my Masters in Data science currently so DE is a new field to explore.
Any suggestions?",datascience
"I am the mod for a sub called r/hairlossresearch

The sub has collected an enormous amount of research on potential treatments for Male Pattern Balding/Androgenetic Alopecia, but all the information remains mostly unstructured and not available in a salient manner for sufferers seeking the appropriate treatment for their age/stage and/or disease subtype.

I am looking at designing a online decision tree with clickable points to allow you to drill down to finally come to a potential treatment for your circumstances.

I am seeking suggestions on what website or software I can use to build this clickable decision tree.

I am thinking of providing an online form, where multiple research supporters can enter a new potential treatment, with details of efficacy, treatment mode, contra-indications, supporting clinical trials, side-effects,etc. 

Once a new entry is submitted it would populate the decision tree with the new data.

The reason I am seeking an interface that presents as a clickable decision tree is because the treatment and associated data are so voluminous that a standard tree would be too huge too view on a computer screen.

Any suggestions on a website or software tool that could work for this?

Thank you",datascience
"How do I fetch bulk data from a open source and bring it to excel? I've been trying the basic tutorials on Power Query but it's not working and I'm pretty sure I'm the problem 😅

To give you some info, I've been trying to get company data from a few open registries, like France and Germany, but my skills are poor and Excel keeps crashing. I'd like to fetch only the data related to the ids I have, so I don't want to download the whole set of course.

Could someone smarter give me a step-by-step?",datascience
"I’m currently a junior DS in my company (large org, property sector). I joined as an intern earning peanuts after my MS and just got made full time last month. Current package is more livable but well below market average. 

A couple days ago a recruiter from a company I was targeting post grad (e-commerce/retail) reached out to me. we had a call about an Analyst role opening up and they were interested in my profile. In a nut shell I got the impression that the job was primarily dashboarding. I mentioned I’m currently learning and working on ML problems and that’s what I want to continue doing. So the consensus was that there’s nothing that’s I’d be interested in now but as they build out their BI dept something will eventually pop up. 

Anyway, my heads been turned. The package they would offer is at least 2x what I’m making now. I saw the job posting in linked in yesterday and to my surprise one of the requirements is experience with ML! 

I’ve been with my current company for 7 months and it’s not bad but not great either. Boss is hands off, invests in courses and training. But I’m the sole DS in my team and don’t have a mentor. There are other DS in the in different departments, but due to some workplace politics we compete more than we collaborate. 

So to keep this post short, I see two options: 

A. Get a year under my belt with my current company and work on personal projects that will help a future application with Company B. 

B.  Call the recruiter back and say I’ve changed my mind after reading the JD and potentially move. 

I’ve had awkward gaps and lengths in my CV in the past hence my reluctance to leave so early. But since my foots in the door with my ideal company I’m not sure what to do. 

Any advice or suggestions?

TIA",datascience
"I was told imageJ but can’t seem to find anything in the tools that indicates I can export data on a frame by frame basis. Am I not seeing a major feature or special plugin? I’ve looked through documentation and cannot even get on the trail of how to gather nor export that data from a video

&#x200B;

I can hang my cursor in a spot during the video and watch the exact data I need cycle through on the pixel indicator location while the video plays. I really just want to export that data to a table. Feels like it should be very simple 

&#x200B;

End goal is analysis in JMP but won’t have to do this many times so it doesn’t need to be super clean",datascience
"[Functional Data Analysis](https://www.psych.mcgill.ca/misc/fda/) is an emerging branch of statistics, it is in response to the growing advancement in data collection in wearable devices or other data measured over time. the study of high dimensional data,in which the instead of the observations being data points, each observation represents a function in time, wear features can be extracted from smooth functions (derivatives are used as features a lot) or other functional “variants” of classical statistical models ie. (Functional linear model, functional PCA, etc.). The use of functions can be generalized to other types of data, such as [imaging](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3153754/), where they estimate higher dimensional surfaces. 

My question is are there any data scientists out there who do functional data analysis? Or analyzes functional data?",datascience
"I have a database of what amounts to pairings between workers and tasks.

The database has pairs of tasks and workers that were successful in each task.

I’m trying to figure out the best way to predict which worker in a given group is best for a given task.

What technique should I use? I was thinking of using a model to predict the best worker, and then give each worker a score based on the Euclidean distance from the ideal worker. Does that sound any good?

The workers and tasks are both multi labeled",datascience
"So I’ve just started a data science role at a big bank. I have almost finished studying my Masters in AI & ML.

My first project at the bank is to conduct analysis on time series data, both univariate and multivariate.

I was tasked with trying SARIMA first on the end of term dataset before even looking at independent variables.

So I cleaned my data; removed trend, normalised, removed seasonality and got a p value less than 0.05.

I did my optimisation of my model, computational testing orders and got as good of a model as I could.

I tried testing by leaving monthly data off the end and trying to predict it with horrible results.

I know have a problem.

Assuming I haven’t made a mistake, this implies that this model is a poor fit for this data or that data is too volatile for this model and requires an alternative model and other variables.

Is that the correct conclusion to come to? Basically, I’m asking how do I know that I haven’t made an error in the design of the model and/or cleaning of the data, especially if I have followed the “by the book” approach?

My guess is that comes with experience and obviously testing other models for better fit is probably a better alternative than trying to fix a problem I can’t observe.",datascience
"Hi I’m looking for some help regarding publicly available datasets or websites I could scrape. Information on rehabilitation would be a plus. 

Links or Pm would be appreciated.

(Feel free to move it the wrong sub)",datascience
"Hi all,

My current position is something like an Environmental Specialist, focusing on sustainability programmes. I have a PhD, have been in post for two years and I'm in a position to select a new job title. 

I work for a large organisation, however, while there is a business performance department, I am the only one involved with a set of projects aligned with sustainability including 1) IoT deployment, UI development and data analysis for environmental monitoring,  and 2) Enriching existing datasets using APIs for geospatial mapping and tracking.

I really enjoy working with data and code (Python has been adequate for everything I've needed so far) and I'm hoping that a job title that includes either ""data science"" (for the analysis and decision-making part) or ""data engineering"" (for the IoT part) would give me to scope to keep learning and progressing.

Interested in any thoughts on what a preferred job title would be for a generalist rather than someone dedicated to a focused area e.g. ""Environmental and Data XXXXX Specialist""

Thanks for any thoughts!",datascience
"I have been given a csv file which contains a column named NIC. it contains values like "" Growing of perennial crops"", ""Growing of sugar cane"", ""Growing of spices, aromatic, drug and pharmaceutical crops"" etc

My goal is to Use Natural Language Processing for analyzing the various core industries and group the business categories in the NIC column.

can someone help me how to apply NLP on this column",datascience
"Hi, I would love to do volunteering for companies or start ups as a data scientist or data analyst? Do you think is possible?
Is there anyone ever done it before? Any feedback would be really appreciated, thank you for the time 🙏🏾 #love",datascience
"Hi,

Wondering how likely it is to get a job with bootcamp certificate vs. a bachelors in computer science vs a masters? Provided all other factors are pretty similar (e.g., project portfolio).

Thanks,

Eric",datascience
"Hi r/datascience,

Apologies if this question is out of place (or gets asked super frequently). But I would really love some insight here. 

I (31 Male) am currently a carpenter - and while I do love it - I’m not positive that I want to spend the rest of my working life in it, for a multitude of reasons. Prior to becoming a carpenter, I graduated with a B.A. and worked in Marketing for about 2 years and then became a project manager at a bank. It was there that I was exposed to working with the data management dept on a few projects. While I am basically a total novice, it really interested me. I liked the “hands on” nature of it, and I liked the logical and structured approach to problems and solutions. I would also really love to work remote and be able to be more flexible/travel more.

I’m now starting to consider a career change in the near-ish future, and wondering what my best route might be. I’m married but no kids, and could afford some education (~$20k or under ideally) to make the change if that’s required. Otherwise, I wouldn’t mind diving in myself to start learning and find employment on the ground level if that’s possible.

Thanks for any insight into what I should consider about a career transition, as well as about the field as a whole.",datascience
"Is there a place other than toptal and upwork where a small business can easily find geospatial data scientists or gis analysts for small projects like site selection? We're trying to understand this market. If you are a consultant/contractor who looks for projects, can you fill this quick survey out or post here?  I'm looking to build a marketplace to take some of the work that comes to our site (idealspot.com)

[https://forms.gle/BUT3grrAZCJbiwFNA](https://forms.gle/BUT3grrAZCJbiwFNA)

We are a startup out of Austin, TX -- We built a data reporting tool for small businesses but we get a lot of requests for GIS analysts / Geospatial data experts to consult with clients about the reports. We're looking to build a list of experts we can hire/subcontract to. ie., more about us and our datasets.

[https://www.idealspot.com/solutions/data-connect](https://www.idealspot.com/solutions/data-connect)

Thanks!",datascience
"So I have 1 year of experience as a data analyst. Mostly doing dashboards with PowerBi and descriptive statistics with a few logistic regression models. I competed my masters in statistics and have been working since. I know how to program in R and Python and only used SQL in classrooms and from my own learning. My job is currently through a contract and it’s ending soon this April. I applied to a bunch of jobs and admittedly sometimes skimmed through the application and didn’t read the whole thing before applying. I got an interview for a programmer analyst 2 and I feel so overwhelmed and under qualified for the requirements I politely declined the interview and instead emphasized my interest and confidence in another job data analyst that I applied for within that same company. But the interviewer asked me again today if I am still interested in that position. So I’m wondering if maybe I should just do the interview. These are the requirements for the job and for me it sounded like it was way more about programming. I also haven’t used any of the software tools listed besides SQL. 

“Writes code or configuration to implement business requirements using approved technologies. Ability to review, analyze, and enhance configuration/programming systems including coding, testing, and debugging. Interprets business requirements and develops design specifications. Writes code or configuration to implement business requirements using approved technologies. Ability to consult with application's users to identify current operating procedures and to clarify program objectives. Proficiency in code development using reporting tools (Alteryx, SSRS, etc.), ETL tools (Talend, Data Stage, SSIS, etc.) and SQL coding (Toad, DBeaver, etc.) and a database operating systems and platforms (such as: Oracle DB, SQL, Greenplum, Snowflake, AWS, etc.)”

Should I attend the interview? Or would it be a waste of both of our times ? Any feedback would be greatly appreciated. Also I didn’t lie about anything in my resume so I’m shocked they are considering em for a position that asks for 7 years plus experience.",datascience
"Hi. 
I try to find a job as data scientist (EU)
I want to do some projects divide by categories.
Questions:
1) where would I put the projects, and then link them in the CV?(right now they are from a GitHub site done with Hugo)
2) how many projects and what kind should I do?
3) how to present them?(tableau, Jupiter notebook, power bi, Google slides, other)",datascience
" The question is this. ""I need to know the statistical data types of my values, in order to decide what methods I will use on this data for machine learning. How can I do it en masse within a dataset with 100.000 columns?""

the .dtypes give me computer data value (int64 etc.) but I want statistical type (interval, ratio, discrete, nominal etc.) also, if I'm using numerical data for ordinal things or 1 and 0 as boolean, that shouldn't get stuck in my function.

I had a heated arguement with ChatGPT and it's final suggestion is:

    
    import pandas as pd  
    
    # Load the data into a pandas dataframe
    df = pd.read_csv('your_data.csv')  
    
    # Get summary statistics for each column 
    summary_stats = df.describe()  
    
    # Identify columns that contain ratio or interval numerical data 
    numerical_columns = [col for col in df.columns if df[col].dtype in [float, int] and                      (df[col].min() > 0 or df[col].max()/df[col].min() > 1 if df[col].min() != 0 else False)]  
    
    # Identify columns that contain nominal or ordinal categorical data 
    categorical_columns = [col for col in df.columns if df[col].nunique() < df.shape[0]*0.05]  
    
    # Convert the data type of the categorical columns for col in categorical_columns:     df[col] = df[col].astype('category')  
    
    # Validate the change of data type 
    for col in categorical_columns:     
        assert df[col].dtype == 'category', f""{col} is not a categorical type"" 
    for col in numerical_columns:     
        assert df[col].dtype in [float, int], f""{col} is not a numerical type""
    
    --------------------------------------------------------------------------
    AssertionError                            Traceback (most recent call last)
    ~\AppData\Local\Temp\ipykernel_26568\177826119.py in <module>
         24 
         25 for col in numerical_columns:
    ---> 26     assert df[col].dtype in [float, int], f""{col} is not a numerical type""
    
    AssertionError: floors is not a numerical type

Realistically, ""floors"" is a controversial value. Some say it's continous because there can be 1.5 floors, I'd say it's discrete numerical value.

How do I get out of this? I can't manually check 100.000 columns",datascience
Curious to see what others have experienced in terms of their interview process. It feels like data science interviews are much like streamlined compared to regular software engineer gigs.,datascience
Consider a problem where the target variable can take on 10 possible values but only 6 of them are shown in your training data. It’s likely that the other 4 values show up in the test set but you cannot see the labels. What algorithms are you confident would be able to achieve relatively meaningful results?,datascience
"Hi everyone, I'm currently a high school senior who is going on a pre med track into my undergrad. However, I also have a strong interest in data science, especially bioinformatics, for computational biology research. I have a lot of time now that my college applications are over and want to learn as much as I can in data science. Pasted below is my current plan on what I want to do. I'm trying my best to finish them all by September, but it is very likely to bleed into my first year at uni.

1. Fundamentals of Computing Specialization

[https://www.coursera.org/specializations/computer-fundamentals](https://www.coursera.org/specializations/computer-fundamentals)

&#x200B;

2. Introduction to Mathematical Thinking

[https://www.coursecra.org/learn/mathematical-thinking](https://www.coursera.org/learn/mathematical-thinking)

&#x200B;

3. Discreet Math

Option 1: UCSD Specialization

[https://www.coursera.org/specializations/discrete-mathematics](https://www.coursera.org/specializations/discrete-mathematics)

Option 2: Rosen textbook

Option 3: MIT course

[https://openlearninglibrary.mit.edu/courses/course-v1:OCW+6.042J+2T2019/about](https://openlearninglibrary.mit.edu/courses/course-v1:OCW+6.042J+2T2019/about)

&#x200B;

4. Algorithms Specialization

[https://www.coursera.org/specializations/algorithms](https://www.coursera.org/specializations/algorithms)

&#x200B;

5. Data Structures and Algorithms Specialization

[https://www.coursera.org/specializations/data-structures-algorithms](https://www.coursera.org/specializations/data-structures-algorithms)

&#x200B;

6. Bioinformatics Specialization

[https://www.coursera.org/specializations/bioinformatics](https://www.coursera.org/specializations/bioinformatics)

&#x200B;

7. Single Variable Calculus XSeries

[https://www.edx.org/xseries/mitx-18.01x-single-variable-calculus](https://www.edx.org/xseries/mitx-18.01x-single-variable-calculus)

&#x200B;

8. Mathematics for Engineers

[https://www.coursera.org/specializations/mathematics-engineers](https://www.coursera.org/specializations/mathematics-engineers)

&#x200B;

9. Mathematics for Machine Learning Specialization (First 2 courses)

[https://www.coursera.org/specializations/mathematics-machine-learning](https://www.coursera.org/specializations/mathematics-machine-learning)

&#x200B;

10. Data Science MicroMasters (First 2 courses)

[https://www.edx.org/micromasters/uc-san-diegox-data-science](https://www.edx.org/micromasters/uc-san-diegox-data-science)

&#x200B;

After I finish all of these, I intend on diving into ML with the goal of eventually utilizing it for research. I know that this is a lot, but I would really appreciate any feedback on my current plan and if there's any changes that could be made or any other resource I should use.",datascience
"Hi yall, I am working with Python and wanted to learn some insights from everyone.

This is a large dataset that I am working on and I have thought of the following approaches :  
1. preprocess the images into smaller chunks/files

2. run multiprocessing to make it fast

3. store with BigQuery

4. train and test with smaller chunks of data to check the accuracy

However, as I was reading through some StackOverflow resolutions, question that came out was:

1. Since this is for millions of data, would something like ""set"" instead of ""list"" improve the time ?

2. Would writing the data files using C++ make it faster?",datascience
" I'm a beginner when it comes to data analysis stuff, I've been completing data camp courses.

People seem to suggest, creating projects is better for learning. Hence i tried my hand on creating one. It's filled with holes, i understand. My First time to create a ""Story"" using foreign dataset.

It was a Dog Breeding Dataset, where i used a mixture of Seaborn and Matplotlib, as I'm not very familiar with Seaborn as of yet. I've read a lot of Statistics articles + datacamp courses. but implementing and Learning is better, hence i didn't use any over the top statistics such as ""Showcasing Distribution graph"" and Dealing with outliers. As i didn't want to over complicate it before i get a review on it.

Happy to hear any critics, i want to improve on my skills. Before jumping to SQL and Power Bi, i know the basics but haven't made any ""unguided"" projects yet.

[My Kaggle Notebook Project](https://www.kaggle.com/code/irfanlone/breed-popularity-analysis-hypothesis-testing)",datascience
Join this live and learn Data Manipulation [https://youtube.com/live/wQDmssRCdDw?feature=share](https://youtube.com/live/wQDmssRCdDw?feature=share),datascience
"I have been trying for weeks, trying different forms of jekyll, hugo, and whatnot. Could someone help me in this, I am trying for a job and my resume doesn't even get shortlisted hence I thought of adding a portfolio website.",datascience
"It seems that everyone who has the title of Data Scientist has a master’s, so i was wondering how did you guys become titled a data scientist? Were you first a data analyst and then climbed up the ranks and get master’s? Or did you right away get an entry level data scientist job with a bachelor?",datascience
"Hey all, [repost](https://www.reddit.com/r/careeradvice/comments/10wngpx/comment/j7oe976/?context=3) from r/careeradvice as I got recommended to get your guy's opinion on this.

For context, last May I graduated with my bachelor's in CS. During that time I was part of an undergraduate research group and I really enjoyed it. I was able to get 2 publications (conferences) at the time and felt like I was doing a lot.

By the time I was graduating, the school was starting a new data science PhD program. Since I really enjoyed the research experience, I applied and was accepted, such that I was skipping my master's degree and doing a PhD directly. I've always wanted to do a PhD.

Now I am in my first year of my PhD, and now I am a bit anxious that the fact that I will have a degree for both my bachelor's and PhD are from the same university.

For you to know, I do not plan to go into academia after my PhD, I plan to work directly in the industry as a data scientist. At the time, I figured that the school did not matter much for industry vs. academia. Will this be a red flag to employers?",datascience
"Will be working there this summer and was looking for some info on what previous interns did for projects. 

Was management alright? Will you be returning to JPMC if so why/why not?",datascience
"I recently put together a data visualization that ingested 8M lines of census data and plotted them onto an interactive 3D globe. Interactive demos are NOT easy though and this required data cleaning with Bash and Python, data ingestion with C++, and then a video game engine to help with the interactivity.

Do people in the community consider a task like this to be data science (since you would need data science skills to put this together) or would something like this fall more under the category of data engineering or software engineering?

Also, if any in the community is interested on how to put together this kind of interactive app (either for a CEO level presentation or as part of some high end dashboard), I am make this course FREE on YT. The first two sections of the course are available now ( [https://youtube.com/playlist?list=PLKH3Xg62luIgPaB4fiFuTQ9-q-sbw-kQj](https://youtube.com/playlist?list=PLKH3Xg62luIgPaB4fiFuTQ9-q-sbw-kQj)) and an additional section of the course will be uploaded to YT each Monday.",datascience
" 

I'm going to have an interview with an intern tomorrow. He is studying business informatics. It's my first job interview on the other side as an interviewer. I want to make the interview as fair as possible. So no super nasty or hard questions. What are good questions in relation to business intelligence, databases, statistics and data science?

These are some questions I have written down:  
\-What is the difference between sql and nosql? What advantages does a nosql database offer? Are there any disadvantages?  
\-Why do you do so-called normalizations in connection with databases?  
\-Which libraries have you already worked with in python?  
\-In connection with statistics, the p-value can often be read. what does the p-value say and what is it used for?",datascience
"As the title says, I would like to have some consulting with someone from the field. Preference is Germany but EU should be fine. Currently in MS for DS and want some guidance, cv review etc. With payment of course.",datascience
"So I interviewed at this workplace for a Data Scientist role , I am a junior with no experience besides an intership I did. The market is my country is brutal and Im struggling to find a job for 3 months already.

I did however find a workplace which is mostly low tech hardware type of company but they have a data science & algorithms division. However kn the CEO interview he directly told me they are a conservative workplace - almost everyone are in their 50s and 60s, been there for 20-30 years already, they work on site only and they want someone to stay with them for several years because they do not want to waste time on people ""from my generation"" (in their words) who would leave after a year. Salary is also not high but for a junior I didnt expect much.

It did stress me a bit because I dont think I will be a fit for their culture. + its a 1 hour drive each direction so it will be hard, and I dont want to waste time and not enjoy there and being not motivated to come to work. However Im financially stressed and I do need the money and its being hell to find jobs for juniors here.

Wanted to ask you guys if you think I should go for it and do leave after half a year or a year until the market recovers?",datascience
"I 22m recently finished my bachelor's last year and have decided to apply for a masters in Data Science abroad which will take some time to prepare, which brings me to my dilemma. Some backstory: During the pandemic (apologies if I brought unpleasant memories) I started my self taught journey in data science, which involved watching a lot of tutorials, reading articles which somewhat gave me the jist of what algorithms and fundamentals (statistics, python,sql) are in the line of work. I watched some courses on statistics to try actually understand some concept like probability and especially harmonic mean lol, learnt SQL but never got to know how I'd use SQL to actually get data from a database anf use it for modeling. 2 years later, I honestly don't even know what am doing anymore, I wake up, log in to kaggle, find an interesting dataset, formulate a problem that might need solving (no matter how ridiculous it is) and do some analysis or build a model, and by the end of the day I contemplate whether what am doing is enough to actually land a job in the coming years. Don't get me wrong but I actually am very interested in the field.

With a all the blogs and multiple YouTubers around providing information on what data science is all about I end up questioning the very name of the field, what the actual **** is data science? Feels like I have been doing nothing in the past few years (jus 2 lol) I have made some models with which I am proud of especially a food recommendation system that I ended up presenting as my finals project..in addition, I read a lot of threads in the sub of what employers are looking for during interviews and realize that there's so so much I still don't fully understand....

Apologies for the rant, but today is jus one of those days that I question my efforts in the field..
Id really like to get some advice on how to continue on..",datascience
"I am starting to question the long-term prospect of the data scientist role as AI continues to advance rapidly. Much of data science is math and statistics, focusing on identifying the best models for future predictions based on a data set. We as data scientists have to be good at identifying the best model, but our process is mostly based on math and statistics. That said, AI can do all of this faster, more efficiently, and probably more accurately.   


So then what is the purpose of a data scientist in the world of AI? Sure, we have to be the ones to help build the AI tools, but that is already happening with a small % of individuals in the field. A large percentage of us will not be involved in that and will not be needed to support said AI tools... So as a student learning Data Science, why continue?",datascience
"I want to read articles and news daily on data science and AI, and I was wondering if you could share some recommendations. I currently read stuff from AI impacts, Science Daily, and Tech Crunch.",datascience
"I’m Bachelor 1st year student. I started learning data science skills about 6 months ago. I am quite good at data cleaning, modeling, visualization part expect those dashboard thing with python. I can do basics things in power BI. And I have been learning ML and DL for more than a month. Should I start searching for interns or should I harness my skills and do Intern after second year.( Since I have day college I can only work as intern if It is remote evening shift. Our class will be shift to morning after second year.)",datascience
"Where can I download free datasets to use for practice? All the ones I’ve found are only possible through payment // is power bi & excel good enough to make someone a data analyst?

Edit : Everyone that replied, thank you guys so much!!! I really appreciate it",datascience
"Hi! I am new to ML and was wondering which encoding I should use.

&#x200B;

First, my mission:  
Predict flight coefficient based on historical data.

My data:

Int: year, month, day, week\_day (of the flight)

string: classname (like UberX or Uber\_Select), departure\_country, departure\_city, departure\_airport, arrival\_airport, arrival\_city, arrival\_country

&#x200B;

target - coefficient: float

&#x200B;

So **a lot** of categorical data.

I attempted pandas get\_dummy and OneHotEncoding, which create those really large sparse matrixes. I read an article suggesting TargetEncoding, but that would make it harder for my model to find inter-relationships among this data (and I am sure there are)

&#x200B;

So if anyone can help me out choosing the right method, I would appreciate it greatly!

I work with python, just in case",datascience
"Hi all, I am eyeing the UK/Ireland market for a data scientist job as a fresher. I'd like to know how much to expect as a fresher and how the salaries can grow overtime. 
Also any suggestions for the cities with best career opportunities are welcome.",datascience
"Hi all

I'm sampling around 2% of the dataset to build a classification model using XGBOOST.

I notice that when I re-run the whole pipeline I get quiet different results, something that could be explained by the sampling process. That's an issue when I compare to different models and the differences are small (i.e., not sure if they due to chance or not).

How do you normally deal with that? Is there a way to build a 95% CI around the PR curve?",datascience
Hi I need an honest review of Tutort Academy if u had joined and studied in that academy. I looked at some reviews of the academy on quora but they are by people who were already working in companies like JP Morgan or VMware and then they joined Tutort Academy and got jobs at Microsoft or some other big tech companies. All the reviews look similar which makes me feel like are not very honest. So just wanted some honest reviews. Want to know about their teaching style and placement support.,datascience
"I am learning NLP and for that i have installed spacy on my jupyter.  While i was trying to install  ""python -m spacy download en\_core\_web\_sm"", nothing seems to work. I have tried various ways to install it watching online videos and solutions.

Somebody please help me. i am unable to move forward without this.

https://preview.redd.it/0so458a7o3ha1.png?width=1810&format=png&auto=webp&v=enabled&s=320d9d0b5915b56842cc197bfd57aa4bc34f9ed3

https://preview.redd.it/ebzi6xwco3ha1.png?width=1806&format=png&auto=webp&v=enabled&s=b3d80abd4974e01cc14b3354621b6d8c9fe5e801

https://preview.redd.it/n93lnmbao3ha1.png?width=1804&format=png&auto=webp&v=enabled&s=0b45be931389ed78320057f6fc9af156a21e2c12

Refer to the attached screenshots.",datascience
"I'm so nervous, I've never been interviewed by a company for a data science internship (specifically for undergrads, so I don't have to go up against masters or phd students lol). I've only ever interviewed for labs at my university, non profits, or high school interviews. I have a lot of anxiety around this and have no idea about what kind of questions they'll ask me. Any tips, tricks, heads ups for an anxious third year in undergrad? TIA !! :)",datascience
"I have a personally-scraped and curated dataset that I'd like to open source. Primarily, I was thinking somewhere down the line I'd write a tutorial on my [blog](https://www.ddanieltan.com/) and I'd like to point readers to a link to get the same data I'm working with.

**Q: What's the best and most sane way to do this?**

Left to my own devices, I'd probably just upload the data to a Github repository. Any updates to the data can be git tracked.

But, I'm also aware that there are dedicated platforms to catalog and share data (e.g. [https://www.dolthub.com/](https://www.dolthub.com/), [https://data.world/](https://data.world/)), and that uploading data on Github, in general, doesn't seem best practise.",datascience
"I got a promotion from Senior DS (150 base, 175 TC) to Lead DS (160 base, 185 TC).

I have about 3.5 years experience leading teams, 6.5 years total. I joined this company a little over a year ago as an IC for the pay bump and was asked to lead the team after a couple of months.

The model I inherited made < $1mm annually, and now it's on track for $2.5 - $3 mm since I took over. All stakeholders are happy, and I'm well liked within DS according to my boss and his boss. 10k feels like a very small difference, not a large enough difference for the promo or the scope of responsibility. Thoughts?",datascience
"Hi all. 

Some context: My background is in engineering (not data science). I work at a manufacturing plant as a process engineer. Although my background is not in data, a bulk of the work I do in my current role involves analyzing large datasets to identify defect trends and rectify quality issues. I learned python, JMP on the job and use these for data visualization and statistical analysis.

The problem: my process involves making (cutting) multiple parts out of a single ingot. The parts are cut simultaneously. A single ingot can yield up to 7 parts. The process takes about 5 hrs. The machine records data of key machine parameters (inputs) every 5 seconds. So, each cut (production run) produces about 3600 rows of data from ~10 different parameters.

At the end of a cut, each part is inspected on a CMM for surface features. When I plot the readings of the surface features of each part from each cut on a time series graph, I notice that the surface feature readings tend to drift with time (variation between cuts). I want to know what is causing the drift. 

What I need help with: I want to know if there is a correlation between a certain machine parameter and the surface readings. I have 3600 data points from my input and 7 data points from my output compared over n number of production runs. This is a lot of data to deal with and it is very difficult to draw any conclusions from such a comparison. Is there a way to simplify the analysis? I guess what I am asking is what type of analysis would make sense in this situation?",datascience
"I am solving a regression case for an interview. They gave a dataset with 12 features and 800 observations. I made a regression model in LightGBM and it performed really well in the validation dataset. 

The problem is that it is the only model I made and I would like to try more things to show to my interviewer. Some thoughts:

&#x200B;

1. I thought about making a traditional linear regression to compare LightGBM with but the data they gave does not hold the assumptions of a linear model (i.e. residuals not normally dist.)
2. I thought about XGBoost but, as far as I know, its performance is similar to LightGBM but LightGBM is faster and uses less memory
3. Neural networks are not even considered given the size of the dataset

The instructions of the case say that the company is not so well interested in the prediction itself. They explicit say that the approach used to solve the task is more important than the prediction, especially because the data was randomly generated. In this case should I stress with making more models? 

Given that the instructions talk about the approach, I did an extensively EDA, stated all my findings, why I chose to use LightGBM, etc.",datascience
" I (24M) got a medical degree but decided that this field is not for me and I've did some amount of research to say that i wanted to get started in the data field as an analyst. I wanted to know what route to take, I basically self taught myself medicine and wondered if i should just follow a roadmap or should i go to formal schooling and get a certificate. If there's any other route as well, please let me know or any thoughts that you guys might have. I really just want to get started on the journey.",datascience
"I now have a few projects under my belt in Jupyter Lab/Notebook and want to move them to Github, but i'm not sure about how much of the project I should be showing. 

1. Do I need to include all of the code even where I am just cleaning and preprocessing the raw files?
2. Some of the data I worked with is confidential company data, how can I show it in my project and talk about the results without breaching company privacy?

I couldn't find any clear info on this online. If anyone has any tips on this or about how to format a github project in general that would be helpful! Thank you for your time.",datascience
"Hi all, we have a large dataset (500 mil rows after aggregations) and our end users want to access it as a table. I thought about maybe connect it to tableau and put parameters to limit what they can show/ pull down to csv at once, but sounded like they are interested to do exploration on the entire dataset since it’s so new. 
For more color, we have tableau, power BI, and Databricks. 

Is there a good way or a tool to deal with this? Appreciate any advice",datascience
"Alguien sabe de alguna api gratuita o con demo de prueba sobre información inmobiliaria, que tenga precios de casas y las características de ellas? . Es para una tarea, y ocupo jalar y procesar información de una API.",datascience
I’m looking for good blog / news sites for learning new topics and staying up on everything data science / machine learning related. Medium seems to be perfect for that but I wanted to know if there was any other suggestions anyone has; Is medium worth the $5 / month $50/ yearly pay wall?,datascience
"Learn about MLOps and follow our new AI publication from experts in the field. We will be talking about MLOps, ML project, open source. Data science is one of the main areas that we will be approaching.

Follow us now: https://medium.com/ubuntu-ai/",datascience
"A bit of backstory, I’m currently 19 completing a degree apprenticeship in the UK (Started 5 months ago). It is a data science degree apprenticeship, but most of my time so far has been spent doing data engineering in pyspark on a dataset of about 3.5 billion (creating methods and implementing them to get our desired outputs)

While I’m doing this, I want to get some experience in ML and therefore start my own ML project in my own time to further my skills/development. Are there any particular courses/videos/exercises that you particularly like and think are helpful to get started?

Note: Sorry if this is a commonly occurring post on this sub",datascience
"I've been building a project with the goal of adding it to my resumé but I'm not sure the results are anything to be proud of. Here's the Github repo: [https://github.com/miguelscarv/opinions](https://github.com/miguelscarv/opinions)

The project has to do with entity sentiment analysis, pretty much predicting sentiment on a document about a certain entity (like a person, company, product, etc...) using active learning. The data was scrapped with the Twitter API, so yeah the data is tweets.

I scrapped 500 tweets about Elon Musk and saved the 100 most recent tweets as a test set and the remaining 400 tweets acted as my training set. I then used a roberta model finetunned for sentiment analysis on tweets ([https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest)) and it got 57% accuracy out of the box. Mind you that this is a 3 class classification task.

The cool thing about my project is that it uses active learning to recommend tweets for someone to label with the goal of getting a better model with the least amount of labeled tweets possible. So using the training set I labeled 40 tweets and got my accuracy on the test set up to 66% - a 9% increase in accuracy because I labeled 40 tweets. Now my question is: is that good?

The model I used was already finetunned for sentiment analysis and I just further fine tuned it. the training loop looks something like: the active learning policy asks a user to label 8 tweets (this is the batch size) and once they are labeled it updates the model and uses that model to recommend another 8 tweets for a user to label.

The number doesn't really sound good enough to add to my resumé, but tbh I was surprised I got that improvement because there are a lot of tweets about Elon Musk that are hard to label, even for a human....",datascience
"For example, i want to scrape reddit data from subreddits related to various illnesses like r/cancer, r/lyme, r/diabetes. Then i want to estimate ""death rate"" of posters taking some healthy subreddits as controls, like r/Swimming etc.

Given it's related to medicine,  i assume someone might have done it already, but don't know how to search for such studies or in what subreddits to post questions about data science analysis stuff. I know i could ask chatGPT, but i'm not sure it will help.",datascience
"Hi, I am new to the world of DS / DE / DA. I have my BS in Materials Science & Engineering, and spent 4 years in industry doing manufacturing and R&D, which required stats based data analysis and report writing. 

I switched to a backend Software dev job 5 months ago where my team builds ETL pipelines. Soon I will be rotating to a new team as I'm in a trainee program. Although I don't have control over where I'm placed, I will be applying to transfer to one of our Data Engineering teams. The main reason I switched over to a Software job was because I realized I enjoy analyzing data and this rotational program was my foot in the door. 

Now, I'm experiencing a lot of imposter syndrome because I don't have a CS degree and feel like I need to go back to school asap for a Master's in CS or Data Science. I had been making very good progress in my career as a Materials Engineer but with Software I'm worried I won't be able to get to the next level without formal education. 

Should I wait a year or two to get a masters degree or apply immediately so that I can get ahead faster?
Anyone here who has gone through a similar situation? 

I'm already nearly 30 and feel like time is running out in terms of how hard I can work on a technical degree 
(I have family responsibilities that take up time) 

MatSci is an extremely technical degree involving Physics, Chem, Calc, stats etc so I think I'd be ok with picking up Data Science concepts.",datascience
"Hi all,

I've got an interview in a couple of days for a Lead DS position at a major financial institution in the UK. I have 5 years of DS experience, but little to none in finance. Any tips or suggestions on tools/techniques I could research ahead of time to impress the interviewer on the day?

Thanks!",datascience
"I am working on an Esports project where I have player data with individual instances like KDA(kills deaths assists). 

I also want to compare players against each other in aggregate by their data and further combine teams of players to compare teams.

Would the appropriate aggregation method be to average the players KDA or use each players median? My gut is to use average in this case but I don't have any reasoning behind it.

I am not at the present looking to do any time of hypothesis testing or regression.",datascience
"Hi All,  
So here is some background. I graduated in Dec 2021 with BSCS (3.7 GPA). I accepted an offer at a Bank as System Analyst (mostly doing Project management and writing queries for reporting).

So far the salary is good, great benefits and it's a remote job. I only work 4hrs a day and my bosses are pretty happy with my performance. Rest of my day I just learn backend stuff, LC practices, or some cooking videos.

Lately, I am thinking about going back to school part-time for MS in DS, or something else. My company can pay 4k per year for a master's so why not utilize it? I saw a few job postings at my company for DS jobs which motivates me towards this.

I am thinking to apply @ UIUC for the MCS DS program, as it's just an hour away and has a good reputation.

**So here are some questions:**

* **Are there any other self-paced well reputed schools for MS?**
* **Finally, if you have any other advice which you might think I should know then please let me know.**
* **Are there internships for the DS role? (When I was in college I hardly saw any DS internships)** 

My focus is to get as many DS internships within the company or outside of the company, and what I've noticed is most companies don't offer internships unless one is enrolled in school and pursuing BS, MS, or Ph.D.

In the end thanks for your attention. I appreciate your insight.",datascience
"I'm working on a research paper on Work-Life balance for Data scientists and have a questionnaire I'd like the Data Science Professionals in the U.S. to fill out. But before that, I'd need written permission from the admins here. Can any admin contact me, please? I sent a message but received no response. Here is my email: mohsinsub7@gmail.com",datascience
"Preface: I'm on month 6 of being the first data scientist at this professional services firm and it's a total nightmare. All the worst symptoms of a badly managed 'innovation' team: swapping priorities just when i get into a groove with a project, an ad-hoc project management framework, management agreeing to deadlines without taking feedback on how reasonable or how much effort projects will take, management dropping messages in my inbox with suggestions on shiny low-code tools that they think would be interesting to try out (they are not), management not hiring more staff even though they agree that my position should be split up into at least 2 roles, management asking me to produce work that doesn't ever get used by anyone, etc. I'm quickly planning my exit strategy but have this immediate fire that i need to put out.

The biggest issue at hand is that the person playing Product Owner is strangleholding me into working on project for which there is currently no data to work with. Product Owner's ""strategy"" for getting data is that when the beta app is built, we'll use the documents that clients upload into the app. However, these documents are highly confidential, and the project will require sending the documents to a third-party vendor for data labeling and re-training the head of a large language model. Product Owner has not indicated at all that these clients will be notified that their documents will be sent to third-party vendors. I know enough about IP law to know there's something off with this situation, but don't have the verbiage to know what to ask for, how to ensure Product Owner did their due diligence, etc. What is within my role as Data Scientist to ask Product Owner that this potential data source will be lawful to work with?",datascience
"&#x200B;

[Read Raster Data From Postgis Using Python](https://preview.redd.it/ao7huox100ha1.png?width=597&format=png&auto=webp&v=enabled&s=17fedb0ee4b045011e751da834fce7202e484983)

[Read Raster Data From Postgis Using Python](https://spatial-dev.guru/2023/02/04/read-raster-data-from-postgis-using-python/)",datascience
"Hi, I am a Computer Science graduate with a fairly decent grasp of Data Science with a few projects and Coursera certifications related to it. I am looking to do a 2-year post-graduate degree to penetrate into the Data Science market as they don't seem to be very keen on hiring freshers with bachelor's degrees, especially in a hyper-competitive country like India. 

I am confused about whether I should go for 

1. Master's Degree in Data Science/ Machine Learning and AI
2. Masters in Finance, Business Analyst, and similar courses. 

Which of these two categories of degrees would be best in order to advance my career as a whole in Data Science and related courses if the main goal is to earn money and be successful?",datascience
"Pretty much the title.

Currently I'm learning Java on my own. I like math and statistics, i can understand the logic and can write the pseudocode but it takes me forever due to errors in syntax and language structure. I use chatGPT for refreshing my understanding pretty elementary concepts like polymorphism, inheritance etc and how its all synatictically written. About 90% of my time is spent debugging my code. I don't find the concepts themselves difficult just how everything is structureda and the syntax.


And end of the day I only see coding as a tool to be used to break down a bigger picture. I don't find the point in learning about the code for the sake of coding itself. Its like being overly focused on power tools when your goal is to build a house. Yeah j need them but i dont find it nessacary to learn all the nitty hritty of every single power tool there is unless i am a power tool manufacturer. I'm also mot the kind of coder who would ""play around"" with code for the most optimal implementation and runtime or do hackathons in my free time. Really at heart I don't think I am a ""software developer"".



My interests lie primarily in math,stats and econcomics/finance. That's the stuff I really enjoy and ""coding"" is just a tool to use for me to better tackle those domains. I would much rather be a Quant rather then a software developer.



So all that pretty much put me off software engineering and for now and I'm considering data sceicne cause I think it'd be better suited as it is more math heavy and it has applications instead of developemnt of some tool.


However reading this subreddit, tells me that coding is absolutely essential part of you're job and basiclly you are just becoming a software engineer. How much of this is true? How much of your time is spent really doing the basic software development coding tasks like debugging and what not and how much of you're time is spent really ""analyzing data""?


I really want to get a clear picture of really HOW software works here.


TLDR: I like the math, data and logic but hate the entire software development process(debugging optimization etc) only see software as a tool towards a greater goal. Not the kind fo guy who develops apps or does hackathons on his free time. I would much rather be a Quant then a software developer. Is it still a smart idea to pursue data science?",datascience
"Hello, I am a graduate who was applying for Data Science and Data Analytics jobs, I recently got an offer for a Data Engineer position. I don't seem to be having too much luck with Data Science jobs, I understand that while Data Engineers still work closely with Data Scientists and Analysts that they require different skill sets. If I were to keep practising my Data Science skills could I still expect a data science-like job in the future or should I expect to stay in data engineering if I take this job?

&#x200B;

Any advice is appreciated, thank you.",datascience
What other methods could I use to analyze qualitative and unstructured data besides Sentiment Analysis and Topic Modelling?,datascience
"My company contract says that:

“The individual will not engage in any trade or business in competition with the company for the first 12 months after the end of their employment. This includes working on their own or on behalf of someone else.”

This is for a generic data science company that consults in almost all aspects of data science. Does this mean if I leave I technically can not join any other company as a data scientist?

How would I ask them to alter this or remove or completely as it sounds to me like I can’t really leave unless I move into a non data science company…

Many thanks",datascience
"After the release of the M2 MacBook Pros price of the M1Pro MacBooks drops a lot. I planned to use MacBook for Data Science, so I don't know what is smart thing to do.",datascience
"The company I work for is starting to implement Dataiku for its data science projects.

I've started the trainings and honestly the tool called DSS (Data Science Studio) it's wonderful. It covers a full DS project lifecycle.

Searching online there are good reviews and it's one one of the best growing startups but I don't know anyone using it. So my question is, is Dataiku widely used in as the numbers say? It would be great to get some advices from people already using it.",datascience
"I'm a new grad (24) working as a Data Scientist for the last 2 years and all I've ever known is remote work. Initially, my role was going to switch to hybrid after the pandemic settled down but it never did. Even though the offices are open, no one is ever there.

I've been looking at jobs in different companies and it appears as if most of the jobs are remote or ""hybrid"". Judging from my experience, however, hybrid also effectively means remote.

I definitely enjoy the flexibility but its often quite isolating to have to work from home all the time. I've tried coffee shops but I usually end up missing my monitor and the music is annoying.

Is this the future of the industry? Does a Data Science job mean having to work remote?",datascience
"Thank you in advance for your response.

My confusion stems from my assumption that when survey results are presented as percentages, the sum should be 100%. As far as I know, the percentages of this graph sum to 883%.

I tried to research ways that survey results can be visualized, but the only one that I found *could* explain this graph  was *cumulative percentages.* Even then, I still don't understand the numbers.

[Source](https://www.pewresearch.org/politics/2022/02/16/publics-top-priority-for-2022-strengthening-the-nations-economy/)

https://preview.redd.it/199cv81hovga1.png?width=1235&format=png&auto=webp&v=enabled&s=82bb1b2e7de6626e7b83a52acb9e9c7dc7958f0b

Pulled up the full report to see the survey design.

&#x200B;

https://preview.redd.it/uqnvtss23wga1.png?width=1388&format=png&auto=webp&v=enabled&s=c887504e6b0168fd9dea6f272ab1ee007b8987b3",datascience
"In the article below a co-creator of BigQuery takes a look at data sizes used for data analysis. He argues that the vast majority of companies don't have a lot of data and if they do they rarely query it. He says most large datawarehouses are between 100GB and 1TB up to 10TB in rare occurrences. The vast majority of query workloads are below 100MB.

What are the largests databases you've worked with and could these databases their workloads fit on a single server?

The largest databases I work with contain transaction data at large retailers. Most keep a few years of data online and are 500GB-1TB up to 2,3TB. It's always hosted on a single server with up to 128 cores and 256GB RAM. I don't keep data size highscores for queries, but this year I remember one of my largest queries resulted in a little over 50GB of IO. I run queries like these a few times a year. These take up to 60min to complete but in most cases don't disrupt other processes (these are production environments with thousands of connections).

Is this really about as big as it gets? Or are there people here working with datasets dwarfing these? If so, what sort of data, is it structured and what would be max query workload?

[https://motherduck.com/blog/big-data-is-dead/](https://motherduck.com/blog/big-data-is-dead/)",datascience
"Hi, I just started product manager role in small <100 ppl company. In my team we’re doing government R&D project normally collecting / preprocess / labeling the data. It’s been 8 months since I’ve come here. I started as a data labeler, and later CEO gave me data wrangling task. (I’ve been studying data science stuff since the last year on my own) When I completed the data wrangling project, my coworkers who worked with me all left the company. And now CEO wants me to take PM job and start the government R&D project. 

It’s the solution company, so we’re doing multiple project at the same time. I like to experience many different domain, but in this project I need to give the guidance to the juniors who recently come in this company. Even I’m still the junior. I want to go to the grad school to get master’s degree in machine learning in this year. But if I couldn’t get the scholarship I should work longer.

So here is my question:
1. The product that we’re building is not great quality. Will it benefit my career?

2. Can I switch from PM to data scientist in near future?",datascience
"Is Linux for data science just geek pedantry, or is there a real evidence-based reason to switch from Windows? 

Also, do we as data scientists have to deal with Linux servers to the point where it's just worth learning?",datascience
"Hi

In biology you use cladogram to show the taxidermy relationship between individuals from different especies. My question is what is the technical name for that diagram in statistics, where distance show the correlation between branches? Moreover, can this diagram be applied to data classification?

Thanks",datascience
"I   am trying to find something that will print the correlation matrix with p-value and I   cannot seem to find one. Does anyone know of code or package in python that does that? 

Thanks in advance.",datascience
"I am new to data science and programming and have developed a solution to one of my work areas manual repetitive data entry processes. This saves me a great amount of time and I would like to share this improvement with other users. 

Sadly, myself and the user group is not working in the data science realm and would find my solution difficult to work with. (Understandable) 

Is there a way to make my automated approach more user friendly? How can I share this with my user group without necessarily directly providing them the code? Is this possible, or do they need to learn? 

Appreciate your insight.",datascience
"I am currently in graduate school in machine learning and have done really well, but I honestly feel unprepared for a real job. One big reason is while I have done multiple small projects in Github with groups of people, I have never worked on a large repository before. While I have merged branches and a few times used rebase, that is the extent of my Github use. I am worried this lack of experience working with a collaboration is going to hurt me finding a job. I tried getting an internship last year to remedy this issue but they are so competitive I couldn't actually find one. Does anyone have any suggestions on how to get up to speed? Anyone up for tutoring me? lol",datascience
"I seriously don't know what to do. I've been stuck at this job for 4 years now. I've spent the last 3 (off and on) trying to find a job to no avail. If I'm lucky, I end up being choice #2 or 3. 

My job is a complete joke. I literally do nothing all day every day. I've had one interesting project the whole time here, and that was near the very beginning. Otherwise it's nothing or shitty monkey work that's worse than nothing. I've tried to reach out to people for something to do, but I'm always ignored. My pay is definitely in the bottom 10%, maybe even the bottom 1% for having a master's and 4 years of ""experience"". 

I just have no motivation anymore. What's the point of trying to skill up when I'm just going to forget it anyway, and nobody gives a shit? I got completely burned out on job apps and interviews for close to a year, and I'm just now starting to recover. I don't even feel like working on outside projects at this point; this job has completely killed what I used to love doing. I applied for some PhD programs, because I feel like it's probably the only way to resuscitate my ""career"" at this point.

Any advice is appreciated.",datascience
Looking for some books to read to make me a better data scientist. Not looking for anything like ISLR but more along the lines of something you sit down and just read cover to cover.,datascience
"I see them all the time, they're usually amazing in terms of compensation, and usually 200+ candidates within a few days. Of the ones that I applied to, I never even hear back. I find it incredibly unlikely that I'll stand out, and it makes me wonder if it's even worth submitting an ad-hoc resume highlighting my relevant expertise if I'm not going to hear back. 

What do you all do? Take time to tailor the resume for each position? Submit a generic resume that kinda fits the description, in hopes that it'll get you through the door? Not even bother? 

Sorry if the question is silly, I'm just a little peeved with the whole thing.",datascience
"I made a switch from SAP consultant to Data science in my current job during Covid. However, my current role has nothing to do with data science at all. I am in a service based start up where we are given audits to handle that mostly involves reading documentation and barely any kind of coding, if any.

I really want to move to a product company preferably, or at least to a more technical role as Data Analyst, Business Analyst, Product analyst, Data scientist etc.

My coding skills are rusted so I am currently working on them. But how do I get noticed among the many people coming from these upskilling companies? How do I set myself apart? i am already in process of making Kaggle based projects, that doesn't seem to suffice.",datascience
"Well a couple of days ago I asked whether people really use control version or not.

Even though a lot of people said yes, there were others like me, so I've been searching for some easy tutorial who can help me and... funny enough it was in the own channel of visual studio code, others videos or tutorial I felt like were missing the very first steps so I'll let the link to the video and hopefully someone will find it helpfull in addition there are others smalls videos in advance which are also helpfull.

https://youtu.be/B-s71n0dHUk",datascience
"I’ve been setting an example repo to have full depth of setup as databricks and Amazon EMR for local development and testing.

Figured I’d cross post here as it can be an easy way to dive in and start developing in pyspark and when ready can easily migrate code to other managed services and have confidence it’ll work

- QuickStart spark node with JupyterHub installed via docker compose
- s3 api compatible storage with minio
- example test cases with pytest and unittest 
- GitHub actions cicd
- poetry setup
- Full read and write of databases and tables in spark through a preconfigured meta store setup


If you’ve had difficulty trying spark locally for development, I think this repo will save you a lot of time and effort!



https://github.com/emmc15/pyspark-testing-env

https://www.reddit.com/r/dataengineering/comments/10uw3rn/end_to_end_pyspark_testing_cicd_example_repo/?utm_source=share&utm_medium=ios_app&utm_name=iossmf",datascience
"I was an ex salesperson working in the tech industry. Figured that I wanted something more analytical in a career so I learnt to code on my own and am completing a master program in data science. Met my college’s career counsellor today, who advised that I apply to product management roles because of my sales background instead of data scientist, and came out feeling discouraged. Is a sales background a bad thing to have for an aspiring data scientist?",datascience
"Hey r/ds - I'm fairly new to the world of data science after a decade in engineering and ops analytics (emphasis on lots on previous data work). I transitioned with the help of a bootcamp and landed in an easy consulting role right off. I plan to leave eventually because I'm well underpaid (limited ability to negotiate from bootcamp but I'm doing very well) but it feels pretty secure for now.

What do y'all do to earn some extra cash while improving skills and networking? How do you find opportunities?",datascience
"Hi, I'm a software developer with about 6 years of experience. About 2 or 3 years ago,  I started to like data and AI more and more. I had always been interested in those fields but never did nothing about it until 2020, when I took an online course in Machine Learning. As I work as web developer and  have a formation in design and not on computer science, maths, etc it was easy for me to understand the coding part (I work with Python) but very difficult to understand the statistics foundations.

So last year I decided to start a Master in Data Science. I got admitted in a pretty good university here in Buenos Aires, Argentina, which wasn't easy coming from a design degree. Now I'm half way through the master (It's scheduled to finish in December 2023) and I feel like I'm ready to start a new job as a junior/trainee Data Scientist or ML Engineer, so I started applying for jobs via LinkedIn. 

So far I have to issues I would like advise about:
1) is that there are almost no offer for junior/trainee positions, they all require a lot of experience as a data scientist. I have a lot of experience as a developer, but none working with data. Maybe I'm looking in the wrong place?
2) is that I took my old development-centered resume and I updated it to reflect my new studies in Data Science, but I don't know If I should somehow change it or update it because I have sent many applications (around 100 maybe) for about 10 days and I haven't got any replies. 

I'm not in a hurry, I know that finding a good job takes a lot of time and having no experience in the field is a strong disadvantage, but I want to know if someone has a similar story, or if you know ways to start gaining some experience and confidence.",datascience
Where can i find a large data set of medical records of patient illness and symptoms. I need this as i might make a medical Illness prediction software for my school A-level project in computer science. Just send some links if you know of any thing like this. thanks,datascience
"When looking at courses or degree programs both of these have similar modules, can someone explain the difference between the profiles and how technical each role is. Also which profile has greater scope for growth with time (in terms of job criticality and money) ?",datascience
"I started an analyst job in December and I still have not left SQL. To me, all the queries seem beyond comprehension and are mostly uncommented and unstructured. People are continuously asking me why a process broke or why data is misaligned and I'm going into the old queries with virtually no clue of what is happening. I get several direct requests a day by now on stuff like this.

I recognize how important SQL is and realize I need to improve there, but I'm just concerned that I am doing the job of a data engineer and not an analyst. I realize they are valuable skills to learn, but almost all of my training has been modeling in Python and I feel like I'm slowly losing those skills while I gain these.

Curious if anyone else has any other similar stories and if this is normal, I should suck it up and bury myself in the data, or if I should start reconsidering? How long has it taken you all to get up to speed with a company's data when you've started new jobs?",datascience
"Hi all - I'm torn on this issue. On one hand, it feels like DS should be evaluated on technical competence like software engineers, i.e. ""doing the job well"".

On the other hand, it's a part of the DS job to understand the business/product KPIs that you are trying to move AND actually moving them. Everything else is basically ""busywork"". BUT, shouldn't the KPIs be owned by product managers?

What have you seen to work vs not? I'd love to hear the contrast between ""best practice"" vs what you are actually seeing in your day-to-day!

Thanks!",datascience
"Hi, reddit! Could you please recommend a good roadmap or starting point to learn data science for someone coming from software engineering?",datascience
"Don't want to give too much away, but I'm in my mid-20s and work as the only data scientist at a smallish (<100 people) startup. I'm in my second year in the role, and although I enjoyed my first year very much, I've noticed that I've really been not having a good time lately. There are a few reasons for this:

* I don't have a team. It was pretty fun at first to come in and take care of a lot of low-hanging fruit and answer people's data questions that they'd been stuck with for a long time. But I don't feel like I'm learning anything new anymore, and I'm not experienced enough to figure out how I can make myself progress. My manager is great but does not have a background in data science, and I don't have peers I can discuss my work with.
* Our leadership doesn't really understand data analysis. The CEO is always asking for ""insights"" as if I can just comb through our database and magically come up with recommendations for how to improve the business. In short, because I'm the only person doing any sort of analysis, and our engineering team is pretty lean and doesn't particularly focus on data collection/integrity/etc., it can be hard to even get an analysis started (and I always have to push really hard to e.g. get engineering to set up the data tracking I need). When I have presented data analyses that I've done, I've noticed that the CEO only cares about findings that affirm what he already believes, which is really annoying because at that point, why should I even put in any effort?
* I have to do a lot of stuff that isn't really relevant to my role because I'm the only one who can do it. For example, our finance team relies on me for a lot of important reporting (e.g. when we are talking to investors), and I end up being the person who has to put together long financial reports (which isn't so bad) and audit/reconcile different metrics when they don't look right or don't match between sources (which is really quite terribly boring). To be fair, my job description does include making dashboards and reports, but it's gotten to the point where my day-to-day is often answering questions like ""why doesn't this number \[pulled from our prod database\] not match this other number \[displayed on some dashboard I know nothing about that was made by some random engineer\]"" or ""do we track \[x metric\] somewhere and where can I find it"" (the answer is no, we don't, so I need to go meet with engineering to set it up).
* Finally, our leadership has constantly pivoted business models during the time I've been here. I get that we're in tough times and startups need to be flexible, but at this point, the product is pretty different from what it was when I came in, and I'm not that excited about it anymore. So there isn't even motivation from believing in the product anymore.

I've been thinking a lot about this and feel like I should probably quit my job and find a new one where I am a bit better supported and can have some more mentorship. This is only my second job out of college, and while I've learned a lot from being the only person in this role, I think I want to be in an environment where I can get some more direct guidance - often, I'm not sure if what I'm doing is anywhere near what's considered ""best practice"". But I'd feel bad about just completely ditching the company. My coworkers are so nice, and I'm the only person who knows both our database and our BI platform well enough to generate reports/dashboards efficiently, so I think it would be very bad if I just quit one day, even with a two-week notice.

Any advice on how to deal with this situation? Sorry for the long post.",datascience
"I have to do a project on financing and investment of SME for an internship but unable to find datasets related to them. Tried searching in some websites like kaggle, UCI, the world Bank, the IMF , data.gov.in etc but couldn't find it. What other websites do I search in or if u are aware of some please do share in the comments.",datascience
"As you know, it is common to break DS into three parts: Math-Stat, CS, Business knowledge. I’ve been learning the Stat and CS part thanks to college and many resources online. However, I don’t really know how to learn the business part. Are there any books/courses you would recommend? 

I am pursuing internship/networking opportunities; so no need to mention them.",datascience
"I do not know if this is the right subreddit to ask this but :  
Imagine there is a project made in march and I ask - how many views did the project get in february and march (as a line graph). Should the line start at march and not exist in february or should the february have line at 0?",datascience
"Not talking about the job, but the degree, more specifically bachelor degree. Is it better off majoring in statistics and learn computer science on your own or double major in statistics and cs? I have heard some people say the statistics you learn in data science is not in depth enough. Apparently, data science major is more about learning skill sets and that statistics is more about theory. And that people who studied statistics can learn data science easier than people who studied data science to learn statistics. That's why people who study statistics are better at adapting to the fast changing field of data science. What do you guys think? Plus, a bachelor degree is not easy to land you a data scientist job anyways and a statistics degree can provide more flexibility. I don't have any experience in programming but I'm good at math. Is statistics bachelor degree the way to go? And then maybe master in data science or go into other fields like finance in case I don't like programming?",datascience
"im about to go to college and i want to study data science and ai but i need a laptop because the uni is far from my city and i need to go to a students residence, so i just wanted to know what are your recommendations :)

idc if it’s windows or mac or whatever, just tell me which computer u use and why i should/shouldn’t buy it :P",datascience
"I am trying to switch to DS role from a researcher role. I have PhD in engineering, environment sci domain. I can do basic python, SQL programming to do the job. But my code is usually messy and not concise. How do I learn to write neat and clean code for DS? What else I need to learn other than training and testing models? Do I need to learn the procedure to deploying the models as well in production? How much cloud computing is needed? What else is must to crack interviews and be good enough for day to day DS job?",datascience
"I’ve tried searching google and stack, but no luck. Hoping one of the experts on here could help out a new professional. I’m modeling search data using marketing spend and some variable coefficients are showing higher contribution than they should. I’ve tried loosing prior sd and transforming the variable to lower the coefficient. I’ve also tried adding vars and dummy vars. Is there any other way to do this?",datascience
"Hello there, I started learning Data Science from 2022 it has been 1 year and I have completed so many things made some projects added it on GitHub right now I am searching for a role to get some experience I have applied to so many companies on LinkedIn but still I am not getting any calls please share your valuable experience. #important #reply #urgent",datascience
"There have been a few posts of this sub arguing for the description of a data scientist being confined to a singular definition. It is therefore worth starting a thread on the scientific position of “data science” as a field. This is not a peer reviewed work, but we can at least use it to get the conversation going. 

Thomas Kunz defines budding paradigms as a time of fuzzy definition until the field rights itself and clearly defines itself through discussion of what is within its scope of function. He uses the example of textbooks to demonstrate that this is a method of inculcating new members into the discipline. Is that enough to clearly define a data scientist in present context? There are so many “how to do x like a data scientist” type discussions and books. Hopefully we are at the tail-end of the “peak of anticipation” and getting deeper into the “Trough of disillusionment” of the Gartner Hype Cycle. Thoughts, gang?",datascience
"Any other mid career DS out there job hunting? How’s it going for you? Best wishes to anyone else caught up in layoffs or job insecurity!

I have 7.5+ yrs of DS experience, and was laid off after not opting into Twitter 2.0 in November. I spent >3 years at Twitter, and it was honestly one of the most rewarding jobs of my career. I loved the people I worked with, I loved the domain I was in and it was one of the most “impactful” roles I’ve ever had. It was definitely a mindf*ck to watch the acquisition in person and see everything I’d worked on basically get thrown out the window (and to see so many amazingly talented colleagues tossed out and/or disrespected). I spent the last few months recharging, relaxing and processing everything.

Now that I’m back on the job market, I’m revisiting all of my old interviewing and imposter syndrome anxieties. I’ve always gotten excellent feedback and performance reviews and know I was a highly valued member of my team. And it’s pretty satisfying to have so much experience under my belt this time around. But with the weird job market and so many other tech layoffs (and knowing that everyone I worked with at Twitter is also actively on the market) I am second guessing everything. I have also only applied to a handful of roles because not many openings have caught my interest. On one hand I want to wait to see if the job market improves later in the year, but I’m also stressed about not having much to say for myself when asked about my job search. 

I also get flustered after talking to people who say things like, “oh your field is so hot you should have multiple offers within days!” Fortunately my partner is very supportive, and I’m generally able to talk myself back to a good place. Today was just one of those rough days when my anxiety really spiked.",datascience
"BLUF: What is the number of interview cycles you typically conduct at the same time while being able to make a good impression? 

I find the data science interview process a bit like having a second job. The questions span a huge breadth of topics but also often cover those topics in depth. In my role, I'll usually be focused on one type of problem at a time, but in an interview, I will go from live coding advanced data manipulation questions to deriving gradient descent,  calculating confidence intervals, and demonstrating business acumen in the course of an hour. The interviews are usually multiple rounds and take up my entire afternoon and weekends studying/interviewing.

That said, there is a lot of overlap between topics I need to study for interviews from different companies, and it helps to have competing offers. I would like to know, what is the number of interview cycles you typically conduct around the same time while being able to make a good impression? There is definitely a balance between spreading yourself too thin and putting all that effort into a single role that you're competing for, I just haven't found it yet.",datascience
"A scientist, by definition, is a Master of Science in a particular field of study or knowledge.

Thus, a Data Scientist, is a master of science in the field of Data Science- which is the study and method of extrapolating information from 'data'.

An analyst, on the other hand, is a person who uses tools to extrapolate information from data and make inferences. They are not a master of science- they do not understand the tools they are using and they cannot explain, replicate, or manipulate the inner workings of those tools.

A data scientist does the same thing an analyst does, effectively, with a key difference- the scientist understands and is capable of negotiating with his tools. **The data scientist can** ***formulate new hypothesis, test said hypothesis, and is intimately associated with the tools*** ***necessary to formulate and test those hypothesis.*** *(that being, maths and computer science)*

**An analyst cannot effectively do any of these things.** **An analyst relies entirely on faith in the work of the scientist and the prudence of the tools created by the scientist.**

This, is why, as I have said in a previous post, most of the people on this sub are not data scientists. In fact, many of the people working as a data scientist aren't really data scientist at all, they are just analysts with inappropriate titles.

Data Scientists, let us unite and take back the sanctity of our title- so that the true nature of Data Ciens is not corrupted by these parasitic analyst heathens who dare call themselves disciples of Science.",datascience
"The title. 

I've been in my position for close to a year now and it feels like our data is so criminally bad that it's hard to actually predict or infer anything. I know a majority of the time doing the job is data prep and cleaning, but sometimes it feels like that's all I do and still yield no results. 

Even when it's around time to actually start modeling, I have to understand that there is a decent chance my training data is just plain misleading.

Anyone have any horror stories? Or find themselves in a similar circumstance? Maybe this is just a ""grass is always greener on the other side"" scenario.",datascience
"I’m in a finance analyst in a rotational program currently in a trading department. Besides investments, I’ve also been learning tableau. My degree was in accounting. I’m trying to change my last rotation to a business analyst in the technology department to stack up some experience. How can I pivot more towards machine learning data science ? Besides learning tableau, Python, R, SQL. Would a MS in CS benefit to go into a fin tech company? 

I went to a community college for my bachelors in accounting, I believe a MS in CS would help with foundational knowledge and marketability. 

I appreciate any advice!",datascience
"Just as the title said.

I one hot encoded a categorical variable with 26 categories.

I have a binary target variable I am trying to predict.

This target variable is very rare: 1 in 200.


Particular categories have the target variable equal to 1 0.13% of the time, and other categories are only 0.3% of the time.

I'm trying to assess if these differences are statistically significant across all of the categories.

Does anyone know what hypothesis test this would be?

Edit: I've tried the chi square contingency test. Got a p value of 0. Don't particularly trust the results.",datascience
"Hey everyone,

I am a Masters student in Statistics and I have taken multiple courses on Machine Learning with Python/R, Bayesian approaches, neural networks etc, but I feel like I did not get enough practical experience, like projects. I decided to dedicate this semester to go through NLP & Computer Vision & RL online courses, but I am not sure where to start. Can anybody recommend Intermediate courses that would not repeat the same Linear Regression but also wouldnt be too hard? Thanks for any suggestions!",datascience
"Hey everyone, I am a bachelor student in data science and right now wanting to do freelancing in digital marketing using data science to support decisionmaking. Next to that I can make websites using Sveltekit so for smaller companies/ startups I can help start them up in the digital world. But that is a bit out of the question I want to make in this post. But maybe it connects the topics a bit in form of web analytics.  
In what area's of digital marketing can data science be beneficial for smaller/ local companies as a service? I am continuously thinking of analytics being more applicable in this case.. Does anyone have any sources that I can read through that touch on this subject? I am also happy to hear your opinion on this.",datascience
"Good afternoon. I want to take online data science courses. People who have also taken online courses and work in this field, these courses have helped you, maybe your friends or it's a waste of time.",datascience
"I was responding to this [thread](https://www.reddit.com/r/datascience/comments/10uh0at/what_can_you_do_with_3_data_points/).  In our [exchanges](https://www.reddit.com/r/datascience/comments/10uh0at/comment/j7ghvtw/), I told the OP that one can calculate the mean and variance with 1 and 2 data points, respectively.  But the OP seemed very confident that I was wrong and even asked me to create a separate thread on it, even though it is something easily verifiable on [google](https://www.quora.com/How-many-data-points-do-you-need-to-estimate-the-variance).  The OP also wants to know my school, which I think it's an overkill.  So I will leave it here to hear the responses from the community.",datascience
"I have been working in a fintech company that deals with credit as a senior analyst. I have little prior experience in analytics but that was in the energy domain. After joining here, I have mostly worked in core analytics (EDA) kind of work using R and excel. The problem here is whenever a new task comes, i don't know what to do. I have to rely on someone to tell me what should I do in this task because I don't have knowledge of this domain. Example for checking the increase in A, check B and C because B and C plays role in it. I do the data preparation part, give them the numbers and they do the analysis and further ask me to generate more numbers for them. Every time something new thing pops up. So what's the problem? The problem is I feel like I am not able to contribute. I have designed multiple dashboards in Tableau and Excel and I liked it because I didn't have to ask what to do. I have the numbers, i know what I want to achieve. Similarly when I am asked to write the code for feature engineering, i like it. I tried asking them how can I understand credit industry. All they say is ask questions. I asked them for books. They told me there isn't any book. I feel like analytics is not for me. I should rather go for other roles. Request help from senior folks here.",datascience
"Hello, I am finishing my Master's degree in data science and I'm considering two options for my final thesis. First one is about electricity market (forecasting prices, studying important factors) and the second one is about fake images recognition using deep learning. 

Do you think there is one of them that can open better possibilities concerning job?

Thank you very much",datascience
I have autism which can make it hard to communicate with others at times so I'm just wondering how much communcation is involved in a data science/analytics position to help me decide if it's a good fit for me. Thanks!,datascience
"Hey Redditors! 

Upon searching for methods to find errors within tabular data, I wasn't able to find much so I decided to write my own notebook and article.

In the article, I outline the necessity for data-centric techniques and how you are able to get model-agnostic improvement by improving the quality of your data. Not only does this improve model performance by itself, but it also leaves additional room for the usual model improvements (hyperparameter tuning, architecture optimization, etc).

I go into more detail in the article, but at a high level I:

* Trained the baseline xgboost model on the original data (67% accuracy)
* Used data-centric techniques ([cleanlab](https://github.com/cleanlab/cleanlab)) to find label issues within the dataset
* Dropped the incorrect data from the training set
* Retrained the same XGBoost model on the better-quality data (90% accuracy)

In doing this, I was able to see a **reduction in error of 70%.** The raw increase in classification accuracy went from 67% to 90%.

I hope this will be of value to those working with tabular data and machine learning. Check out [the blog](https://cleanlab.ai/blog/label-errors-tabular-datasets/) and let me know your thoughts in the comments!",datascience
I recently made my resume after trying out the famous titanic competition in kaggle. I was told that I'll need to make it better(which I already knew) by adding at least 1 project on data science. Where can I find good open source projects? I only know basics of machine learning models and SQl.,datascience
"I have been looking for a dataset to do a project on supervised linear regression.

Since I am new to ML, I am not very sure on what type of dataset to take in order to make the most out of it in terms of my learning.

Can anybody provide me a dataset.",datascience
I'm a beginner data scientist trying to create a good portfolio. I read that projects can spice up your portfolio. What are some good beginner projects I can do and how do I do them?,datascience
"Hi everyone! I wrote a short 15 pages read for those interested in a career in data science

This is the introduction:

""Data Science has become one of the hottest careers in the tech industry. With the rise of big data and the increasing demand for data-driven insights, the field of data science offers exciting opportunities for growth and success. However, breaking into the field can be challenging, especially if you are just starting out. In this comprehensive guide, you will learn everything you need to know to build a successful career in data science.""

Here is the link please check it out and consider purchasing a copy.

[https://ultimateguides.gumroad.com/l/jlqpk](https://ultimateguides.gumroad.com/l/jlqpk)

Have a good read!",datascience
"I can have free access to either of the two for 5 months but I need to choose one. 

Which one is better in terms of holistic learning!",datascience
"On LinkedIn I see more and more people labeling them as data scientists, AI experts and what-not offering paid courses, interview training and resume review. Often, they have a non-data-science background and very little experience working as a professional. Quite common to show a previous job as a data scientist with a tenure less than 1 year (or multiple).

I know it can be appealing, as their message is often, everyone can be a data scientist, machine learning engineer or AI expert. Academic and professional degrees are overrated and it’s enough to take a Udemy or Coursera course to become a data scientist (affiliate link included). Simply follow them and buy their resources (which is usually very general advice, you can google in a few minutes).

But the reality is: They are usually not the experts they pretend to be. They typically don’t talk about expert topics, they talk about career, current hypes, and about very high-level projects. Sometimes they have a GitHub account, but they have no commits of just copy-pasted repositories from other people and some very basic entry-level stuff. They are usually on LinkedIn, Instagram, and YouTube and in podcasts, but never talking about expert topics.

Don’t trust these people and don’t buy courses there. Everything you need is either free of charge or it’s a professional degree. There is no easy-going way to become an expert in any topic. The only good advice these people can give is how to become a fake AI influencer. 

If you are looking for good advice, look for experts with a clear professional track record (several years), academic publications or talks at industry conferences and articles/blogposts about specific expert topics.",datascience
" 

I would love to run an analysis on all possible assets classes, expecially about their liquidity. The dataset should look like this one.

&#x200B;

||Market Cap|Return|Market Cap Growth Rate|Liquidity|
|:-|:-|:-|:-|:-|
|Private Debt|||||
|Infrastructure|||||
|Real Estate|||||
|....|||||

Do you know where can I find this data?

Thanks in advance",datascience
"My tech stack includes python, machine learning, NLP, deep learning, Django, HTML, CSS, and Javascript. My interest is more in nlp. Please share your repos.",datascience
"I have a data set that is comprised of 6000 data points of performance ratings.  Performance ratings are on a uniform rating scale of 1 *(unsatisfactory)*  to 4 *(highest possible outcome).* 

The # of ratings a manager can make depends on the number of employees they supervise. 

The number of supervisees for each manager can range from 1 employee to 20 employees. 

 I would like to analyze manager performance ratings of their supervises to look for harsh raters (to many 1s) vs. easy ratings (too many 4s) .  There are a number of other factors that may contribute to the overall scoring for example - tenure, job profile, etc. 



Technically employees end up with 1/2/3/4. However, prior to that you can have values in-between. 

To get more detailed: 

25% of the overall dating is based on 4 categories which people are rated from 1-4 for each category. 

75% of the overall rating is goal achievement. The # of goals  are variable. Each goal outcome is rated on 1-4. 


Technically, you can have a score of 2.6 or 3.1 or 3.8. they follow typical convention of rounding up.",datascience
"Hi all,

Currently building digital infrastructure for a firm in Spain. I was wondering if anyone has worked with Looker Studio and what their thoughts on it is?

I understand it's primarily for visualisation and can do some data modelling and report generation, but I am curious to know how powerful it actually is. Are there any drawbacks you wish you knew earlier, or how it might compare to something like PowerBI or Tableau?

Thanks!",datascience
"Hi all
I am Research and Development in Energy area. I am working with data (from numerical simulations, real data from energy objects) using tools like MS Excel or ORIGIN Pro. I prepare analysis and conclusions based on my research. Mostly I have been using seasonal data, like yearly hour by hour. 
I am also aware that predicting time series data behavior of renewable energy sources will be a key in process decision

I want to develop my skills in data and I would like to add a new tool in CV. Of course also for better salary

I am looking for advice which programming language will be better for me to learn R or Python? 

I will be glad for yours answers. Thanks in advance",datascience
"In my company’s organisation chart, a specialist is above the analysts. But my job title is reporting analyst. Should i put reporting specialist in my linkedin profile in order to show that i have experience in the field?",datascience
"I was recently in an interview for a data scientist role and I was explaining how I’d used a logistic regression to predict a binary outcome variable from a continuous, numerical variable.

I got asked how I’d communicate this with non-specialists, and I talked about putting together a simple set of slides in PowerPoint, with key points, omitting the unnecessary technical details (formulae etc.).

The follow-up question stumped me though. They asked me how I’d explain the “glm” function (in R) to the same audience - for reference, this is the function used to fit the model to my training data which solves for the coefficients. This stumped me as I hadn’t thought about how to do this. All I could think about was making an analogy with linear regression - in that we’re finding the slope and intercept of a “best fit” line - and that here we’re sort of doing the same?

Does anyone know how they might have approached that?",datascience
"Hy guys, somebody please explain me the difference between bias and variance. I am soo confused between them and every website is defining them differently. 

I spent so much time figuring what they are, yet unsuccessful.

Note: As of now i only know about linear regression model. 

Somebody please explain the difference between them (and what they actually mean) in plain English. Do not use complicated words please.",datascience
"I'm okay at my job. I do good work. But I come on here, on LinkedIn. All you guys talking about the latest transformer. Best ML model when working with GPUs. Actually hyperparameter tuning a complicated model from start to finish at your place.

I have a solid foundation of math and stats. I understand the math behind ML. I've built some simple models in sklearn. I've created kpis and visualizations in python. But goodness, I feel so insanely overwhelmed by the tech stack.

SQL, python, golang, ruby, tensorflow, pyspark, pytorch, nlp, the list goes on...

I'm an expert at all types of SQL and decent at python and some libraries like sklearn/pyspark etc.

I can't help but feel like I can never reach the potential of all you kaggle grandmasters, Nvidia DS, phds and all this jazz. I'm competing with jobs where my other competition has an ivy league degree and probably a PhD.",datascience
"Hey all,

I’m working on a research project where we can dynamically update the price of a product via customer demand, seasonality, unforeseen noise etc. just wondering if anyone in the subreddit had any experience with this. I’m currently testing GA’s and reinforcement learning and getting some pretty cool results.",datascience
"Hi guys, I'm considering to take an online course in a Spanish university that made partnership with mine in Brazil. But since the course is a little expensive, i would like to have opinion from you guys about it



https://nuclio.school/master-data-science/?_gl=1*1cpwrm8*_up*MQ..&gclid=EAIaIQobChMIgLm5s_n3_AIVkAwGAB17ogZkEAAYASABEgKTQPD_BwE",datascience
" 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",datascience
"Hi all,

this idea was originally mentioned in [this pos](https://www.reddit.com/r/datascience/comments/10sayrq/is_there_any_group_or_site_where_we_can_connect/)t w.r.t mock interviews, and a few people thought it was a worth trying and agreed to participate already.  so i'm creating this post to gather few interested folks to test it out (as well as feedback).

The basic idea is to provide a (free) volunteer-led service for mock interviews and interviewing tips from folks with a few years in the field and potentially also discussions with managers sharing their experience transitioning into management, hearing how DS operates across different industries etc. (the latter 2 are my selfish inclusions as I would love to hear about more companies, more career stories and more DS work across industries, but understand the demand will be mostly for the first 2). 

This sub is great for transient questions and discussions, and there is still room for 'spotlight' talks and personalized mentorship and better networking based on the ideas above.   

few strategies to make this work:

1. start simple. with a small cohort of people across different stages in their career
2. every participant can mentor someone junior to them, for load sharing. also works well for more time-relevant advice like interview trends etc  
3. some kind of testimonial system so that mentors also receive credit for their service to the community. 
4. the no of members will decrease with seniority but that's also ok since senior DS advice is generally career advice relevant to wider group (vs interview tips/company culture etc)
5. medium of communication -  not sure but async would be preferred to avoid scheduling overhead, interviews/talks can be scheduled and recorded. 

there is a survey below for folks interested to fill out. and volunteers interested in helping planning and executing, please also reach out :)",datascience
"I live in Brazil and have been working as a DA for two years now, I'm now working for a big 4 consulting firm. I got kinda jealous of other people in tech (mainly web/mobile devs) getting all these remote jobs that pay in dollars, and make probably more than 5 times what I make, so I decided I would try to do it too.

Granted, I haven't applied to many jobs yet, but  I wanted some inputs from you guys, if it's something that's worth pursuing, or if I should just change careers and start developing.",datascience
"Hi guys, I recently screwed up a couple of interviews because of not doing as well as I should have on ML System Design/Case Study type questions where interviewers generally give very vague broad questions. I would love to know the kind of framework people follow for it and any material you used to prep for that? Any good articles/books/youtube channels with good content around this would be appreciated. thanks!

Edit:
The kind of questions I'm talking about:

1. Consider you have a software and documentation for that software. You want to give your users the opportunity to ask the software to complete tasks or get help. How would you think about this?

2. Consider you are working with a hospital. Their ICU is overloaded with patients and they want to prioritize who the doctors should look at first. How would you go about building an ML system for this?",datascience
"In most data sci jobs, are people typically able to figure out what to do? Or is it common to go to a job and not know what to do, and not be able to figure it out, especially if you don’t have help from coworkers? 

I know some type of work may be standardized across the field, but others custom to the company. To what extent is each?",datascience
"Hey folks, working on a research project right now doing a binary classification on text data. Essentially, I have a ton of text data that I'm looking over and labeling each datapoint (composed of text + a couple of other less useful features) as ""yes"" or ""no"" if the text looks like it fits a condition (I apologize for the vagueness). The unfortunate part is that I'm only hitting ""yes"" a very low % of the time, which is completely expected, but it's resulted in me only having \~100 positive examples. Ideally, I'd have wayyy more. So this brings me to my question, what methods exist to find positive examples more efficiently while limiting how much bias is going into the process?

I have a couple of thoughts. I was considering doing an unsupervised clustering, maybe K-means, and see if my positive samples fit snuggly into a cluster, and look at the other examples in that cluster. This might work but seems a little inefficient. 

I've also considered building a classifier based on the positive samples I have right now, which might get me something, but I worry a lot about overfitting, perhaps there is a classification technique you could recommend to me in this case that works well with small amounts of data? I've considered simple models like logistic regression, but bayesian methods have also caught my eye. I'm also considering using some sort of tree-based method and increasing the depth of the tree incrementally until I start to see something that I want.

Lastly, I've tried to use some domain knowledge to generate a candidate list of samples. Basically, I've looked at the positive examples so far and applied what I know to be (somewhat) true about this particular problem to limit the samples a bit more with keyword matching. None of these have been quite as successful as I'd like, so I'd like to consult your wisdom and see if you have any advice for me :) Thank you for your help.",datascience
"Hi, I'm 14 years old, I really liked the data science area, I've always liked computers (I've been using them since I was 3 years old), but researching about it, I saw on a website that you need to be able to communicate well with other people. I want to know if it is really necessary, because I will have to train this part a lot.

I can be good at coming up with ideas and delivering them, but is good communication necessary? (here we call it ""sales talk"").",datascience
"I have debated buying a domain and posting a bunch of projects I’ve worked on. Does anyone have any experience doing this, and have any recommendations or tips?",datascience
"Newer to DS work and am looking for some direction on approaches. 

I'm trying to estimate the impact of an event upon a customer satisfaction metric, for both the general population and specific segments. The event is assumed to have heterogeneous effects due to the nature of the customer base (impacted customers in some regions more than others) and was not part of an experimental study.

I've tried: Using Arima time series modeling based upon the metric, fitting on the time period prior to the event, predicting after the event, and comparing the predicted values to the actual ones. However, Arima doesn't appear to be appropriate due to limited data. There appears to be monthly seasonality, as well as seasonality related to the day of the week. 

Since the customer satisfaction metric is an aggregation from scores provided by individuals, I've also tried using individual scores pre-event as training and using individual scores given post-event as test, fitting traditional classification models to the training set and making predictions on the test set. To estimate the difference between the expected versus actual customer metric, I've taken the training scores and predicted test scores and calculated the aggregated metric for those records as the expected aggregate value and separately calculated the aggregated metric over the training scores and actual test values for the actual aggregate value. However, this method gives me a larger than actual estimated impact - regardless of whether or not I balance the classes during training, this modeling approach tends to predict one customer rating more frequently than the others.

I've also done some reading into causality libraries/modeling approaches, like econml DML, but I'm not sure how helpful CATE would be here, since my metric of interest is an aggregation. Any suggestions?",datascience
"Title. I really feel like this sub is counterproductive in terms of being useful to its constituents most of the time- because most of the ideology and advice pushed on this sub is perpetuated by people who..... aren't really data scientists. Even the majority of those who could be considered data scientists are low tier analysts who transitioned after their collegiate education.

I'm not trying to be condescending, really- but ***t******he reality is that there are not very many educated, competent data scientists active on this thread.,*** Much of what is posted and related by this community is not conducive to success and having a positive experience in the D.S field.

This sub has become (metaphorically) become a flowered field of 'Data Science Daisies' pushing useless platitudes, generalities, and well wishes without any actual understanding of the World of Science as a whole, let alone the actual meaning of the word Data Science from a historical perspective.

***If you don't understand the math behind the work you are doing, you aren't a data scientist. your just an analyst using the tools of the real scientists.*** Disagree and downvote me all you want, but these are the facts. Data Science is not a flashy new job title, it is a deep uncharted field of philosophy (historically, all academia is a bud of the original study of 'philosophy' from the grecian times), derived from a millennia of academic and technological development. If you do not understand the difference, you should not be here. That is just my opinion.

Don't come to this thread looking for career or project advice. Find a mentor or a real data scientist. These people cannot help you.",datascience
"I’d like to start a data science degree but I already have a metric crap ton of student loans and cannot afford to pay graduate or even undergraduate tuition out of pocket. 

I’ve found some classes on edx by Harvard and an online tutorial website called data camp that will supposedly allow me to learn the needed skills. 

Are these viable ways to earn certification, build a portfolio and start a career?",datascience
"Hi guys,

I'm a junior in an undergrad BS Data Science program at a small Christian university in the Midwest. I've been searching for a summer internship in Data Science/Data Analytics since September and have had no luck. I've reached out to people in my network and those of my family and professors wherever possible, and I've visited my university's career center several times for resume revisions and have adopted the various strategies they have suggested. I've submitted somewhere in the ballpark of 200 applications in the last 6 months, and I've been ghosted by most recruiters. Of the 200 applications, I received 4 callbacks for interviews, and of those 4, only 1 resulted in a follow-up interview. For this last position, I was well-prepared and felt that I performed well in both interviews, but alas, I finally received yet another rejection letter last week.

I'm frustrated and disappointed, to say the least. I understand that these numbers are pretty typical in the current tech job market, and other Redditors frequently call it a ""numbers game."" While merely continuing a barrage of new applications may work for new graduates and recently laid-off junior positions, I'm fighting against time since I'm looking for a summer internship that ends before I resume classes in the fall. At this current rate, I'd have to submit another 50 applications to get a chance of even one more interview, which is incredibly time-consuming. The job search process wore me out months ago - I'm discouraged and exhausted - and it takes valuable time away from actually practicing my skills and learning new topics. On top of that, it already feels too late. My academic advisor told me in October to try to have something figured out by the end of December since many internship positions would already be filled beyond that point. It's now February. If my odds were bad 6 months ago, it's only going to get worse.

I'm kind of at my wits' end. I also am approaching the point where I need to have some concrete summer plans so I can coordinate schedules with family and friends, not just ""I'm looking for an internship"" because that doesn't seem to be panning out. Here are a few of my options:

1. Go back to the summer camp where I worked for the last two years.
   1. Pros: I already have friends there and I'm 99% guaranteed a job if I apply. I'll also be on the West Coast, which is where I want to be.
   2. Cons: The pay is little more than a volunteer stipend. I won't have much opportunity to develop tech skills or otherwise advance my career.
2. Stay at home for the summer. I could split my time between learning through online MOOCs and potentially look for a low-skill job such as administrative assistant or data entry. Or a gig like Doordash. 
   1. Pros: I can live with my family for free. I'll also be on the West Coast and will have the flexibility to pursue my outdoor hobbies while working on tech skills. The wage floor is also pretty high in my state.
   2. Cons: There's no guarantee that I can find a job, especially for something that only lasts the summer when employers may be wanting something more permanent. It also feels kind of humiliating that I failed to find a job in my industry?
3. Look for internships in adjacent fields that involve data. I've seen this suggested to people looking to make a career switch but I'm not sure if these exist or if they will work for me. I also won't have the mentorship in DS/DA that I'm specifically looking for. 

What factors should I take into consideration when I look at my options for this summer? Is there anything else as far as career development that I should also think about? 

I'm also wondering if the current scarcity of tech positions is homogeneous across the industry. DS doesn't seem as attractive as it was a few years ago, and now I feel unsure about all of my career/academic choices to date.

First, I feel like DS was hyped up to more than it was when I first started college as a freshman, and now it seems that a lot of people on the internet also think that it's now on the decline. It also seems to be the consensus that the best path into DS is through a more established undergrad program in CS, Math, or Stats, rather than a DS degree because they tend to be so broad in nature that you learn a little bit about many topics but nothing sufficiently in-depth to be useful. Unfortunately, that seems to be my experience - I don't feel like I'm learning anything in classes so I'm taking time outside of class to work on Kaggle Learn courses and the Andrew Ng Coursera ML course. If I could go back in time, I would have gone into CS and started at a different university. I'm not particularly fond of my university or the Midwest (I long to be back on the West Coast) and it's a long story of how I ended up here, but suffice it to say that the reason I'm still here is because I'm on a full-tuition scholarship which I'd lose if I transferred elsewhere. But I can't go back in time. I'm a junior and should be graduating in two more semesters after this semester, so I had been OK with sticking it out under the assumption that I could get a decent job when I graduate and move back to the West Coast. But I'm at the point where I question if any of that is true. 

Second, I'm not even sure anymore if this is really what I want to do. In my senior year of high school I became really interested in finance, and DS seemed like the perfect way to join my long standing interest in tech with what a growing interest in math (which, sidenote, may have died after I took Intro to Linear Algebra. I like to think it was just a bad professor) and my new interest in business topics. In the last semester or so, I've clarified my long-term goals and want to break into the ML field and eventually become an MLE. However, in working with scikit-learn and other ML frameworks in Kaggle Learn courses, it feels dull and I'm just doing it because I have gaps in my knowledge. I don't really find myself loving it. Is this normal? I know it's preferable to work on my own projects rather than repetitive activities, but I still don't feel that I know enough skills to work on my own projects. I often wonder if I'd be better off in a more standard CS track towards become an SWE or backend web dev. But there's huge gaps in my knowledge there as well. And if I switch majors, I'll probably have to take an extra semester or two because my current DS requirements don't include any of the dev, networking, or security classes. 

I guess to summarize, I went into this degree track with certain expectations and everything has turned out to be a disappointment. I'm not sure what to do anymore because although I'm willing to work hard, it's difficult to maintain the motivation to do so when I'm not sure if everything I'm doing is mere futility. Thanks for taking the time to read through all this and I appreciate any advice/feedback you may have.",datascience
"I’m looking for data collection tool suggestions. We periodically need to collect data from teams via drop downs and free fields and Excel is proving difficult to manage (people keep deleting things, changing formulas, etc). Does anyone have any experience with another data collection tool?

Ideally this tool would save the question results and the team could go back and see what the entered originally and make changes when we repeat the exercise, which we plan on doing a few times a year.",datascience
"I am trying to come up with a feature/set of features, and getting a bit stuck. For the purpose of this question lets say its the ratio of number of times a person clicks on an ad (abbreviated ac) on a website/ time they are on the website.  Here is an example of what I'm thinking about:

subject 1:  3 hrs on reddit, 2 ac.  One hour on facebook, 5 ac

subject 2: 1 hour on reddit, 0 ac, three hours on twitter, 2 ac

subject 3:  5 hours on live journal, 1 ac.

In this example I would create 4 features:  ac/hour reddit, ac/hour facebook, ac/hour twitter, and ac/hour livejournal.   I'm having trouble with the fact that these four features will have a lot of missing data.  In this particular example, pretend that I've collapsed the webiste categories as much as possible.   What are some other options of dealing with the data?  How does one use data like this in a model without imputing the missing values? I'm open to not using a ratio for this feature set - but I do want exposure to the website to be included somehow.

Thanks!

Edited to add, the missingness comes from the situation when someone has 0 hours on the website, its 0 clicks/0 hours so its undefined. This is a different situation than a zero someone who is on the webiste for 5 hours but never clicked on an ad.

Edit pt. 2.  I am building a generalized linear regression model to predict a continuous variable.  My goal is to see if there is a set of variables amongst all the features created that are predictive of the response.  I am interested in seeing if there is a difference in risk between adclicks per different websites.   I'm being purposefully vague as the features and response are all trade secrets.",datascience
"Hello people! I hope my question is appropriate for this community. I have the following plot (figure), and I would like to show that in the central region there is a linear pattern, and find the correct line with linear regression (I know how to do linear regression). 

The problem here is how do I decide to exclude the data that stands on a line parallel to y axis or x axis? Is there any statistical test to do that?

The data is the measure of two continuous variables, so there is not an ""a priori"" method to exclude some of them. 

Thank you in advance!",datascience
"How I sometimes feel about Data Science... anybody know the quote?

""...some day the piecing together of dissociated knowledge will open up such terrifying vistas of reality, and of our frightful position therein, that we shall either go mad from the revelation or flee from the deadly light into the peace and safety of a new dark age.""",datascience
"Hello everyone, I am interested in the tools and framework (in particular automation tools) everyone uses when working with big data. I am a solution engineer, but always wanted to be a data analyst or data scientist because I believe that data can be harnessed for many practical application e.g., (chatgpt craze). Regardless, this takes me to my few questions:

&#x200B;

1.) What tools and framework have you used recently for your big data analytics project, and would you considered adding any tools (automation) to your tools set in order to simplify your data process? 

2.)How does the framework you are using scale to handle the ever-growing big data?

3.) Are some of your clients that you work with driving the adoption of AI and automation? I know my company I work for is already looking at integrating Microsoft teams premium because of the features found in chatgpt and how some of my clients wants the to be able have the capacity to automatically generate notes in case they miss the meeting.",datascience
"A while back, my close colleague at work had an interview with Sr. Director of Analytics (or maybe just a Director, I forgot), for data related opening. 

He was asked to describe what would he do with 3 data points, representing 3 months, say month of January through March.

I confirmed with my colleague that it was not 3-month (or 90 days data), it was just literally 3 data points, for example sales revenue for those 3 months.

My hunch at that time was, either the person smoked something, or he blew the interview on purpose, as the recruiter told him the very next day as they decided to “promote” from within.

I did try to think a variety of ways to derive meaningful reference based on 3 numbers, but eventually I told my colleague that most likely the interview was fake.

I guess I want to know from different perspective, whether you can actually use 3 data points in a meaningful way, for business.

Any taker?

Update: as suggested, the company he interviewed with was F100, big in Telco.",datascience
" 

Hello,

I recently moved from standard Object-Oriented Programming Classes to DataClasses and I only do OOP this way now. It makes Python an easier and safer place for OOP. The article is pretty simple and will give you great foundations:

[The Next Big Thing in Python: DataClasses](https://medium.com/towards-data-science/all-you-need-to-start-coding-with-data-classes-db421bf78a64)",datascience
"I have been working as a data scientist in a company abroad for 3y but spent most of my time working in data & analytics without too much python or data-science involved. 

I don't see myself going into the right career within that environment so now I want to apply for real data scientist positions.
Problem is I feel like my skills left in data-science are not enough to land a nice job back in Europe hence I need to bring myself up to date. 

What's your advice?
Do you think a bootcamp could be a good idea? Any that you would recommend? 
In your experience, are there still a few companies these days which train their employees effectively? Or do you think it's more our own responsibility?",datascience
" 

Hi,

For those who struggle with all the GitLab/GitHub commands in the Internet, this article will give you nearly all you need in your daily work. The targeted audience is Data Scientist/Data Engineer, with no experience or medium experience with Git.

[8 minutes to cover 99 of your Git needs](https://medium.com/towards-data-science/8-minutes-to-cover-99-of-your-git-needs-2c904c43590a)",datascience
" 

How can we move from an idea to production in AI? 

Does the technology readiness levels (TRL) help?

If you want to get some answers please read this article in medium:

[https://medium.com/towards-artificial-intelligence/technology-readiness-levels-trl-in-ai-development-c6ed1190fbd6](https://medium.com/towards-artificial-intelligence/technology-readiness-levels-trl-in-ai-development-c6ed1190fbd6)

All the ideas are more than welcome!",datascience
"I'm curious because in my ""small"" experience of +4 years last friday I did an interview for a small company, when they asked me about using git or control version somehow I was totally honest and said something a long the lines ""look I know what it is and how it works but because I like to learn but in these bigs companies in my resume at the end of the day they care more about having results as soon as possible with nothing or very little documentation rather than doing things properly, so I did as they ask"".
They started laughing like some joke saying ""oh wow I cant imagine working like that what a mess"" I dont think they really care since as they also said is something you should get used to easily (plus I'm gonna reject them). 

So I'm just curious how many of you really work using git? 
PS: My profile is data scientist/analyst using Python.

Update: 
As Im reading all of you I see that I need to change that because for me and the company so... this week I'll start to use it 100%.
Also I'm giving some more info about why I think I didnt really work with it in the past:
- No production deployment (""if you can run it locally why waste money?"")
- Really small teams where normally very few projects where really done by more than the person itself.
- When I tried to bring it in the past It was kind of a mess for me without anybody helping, and It was hard for me to defend why since I really didnt know either.
- Normally when I tried to learn something new I used to focus on new libreries, best techniques/coding practices and my next step was about docker and production deployment but before that Im focus on the git.

I want to thank all of you for your thoughts, I'll keep replying later.",datascience
"I am learning to use SQL server. I am trying to create a catalog but get the following highlighted error. Have tried searching for the solution couldn't solve it. Please HELP.

https://preview.redd.it/4muv1k1brcga1.png?width=1689&format=png&auto=webp&v=enabled&s=c4fe922dea4efa6fe772f498751deda348c86755",datascience
"Currently we're just using google sheets to document stuff and while its working, it's been a hassle keeping it up. A miss in the updates can break the whole thing, which can be frustrating rather than focusing on my tests. Also it becomes tedious too when someone checks up on it and it takes a long time to backtrack on everything.

We've tried using the timeline feature or tasking in Asana but its quite tedious as well to track and keep tickets or are we just doing it wrong? Lately we've been planning on trying out to setup like a kanban thing but just wondering if there are any best practices for this?",datascience
"I read before on Reddit as well as on blog posts about data science professionals landing a job and thinking that they finally get to apply their skills to do something cool, when they had to instead do analysis on excel. 

Also data scientists in some companies who aren’t sure what goal they are hired to achieve. Or they don’t even have the right data to work on. Or their work never makes it to production because of some reason or another. And they get disillusioned.

Recently I came across this post on data science in finance. And the description looks pretty interesting. 

https://careerfoundry.com/en/blog/data-analytics/data-science-in-finance/

Are any current Data science professionals who are in the financial industry. Who can shed some light on this? 

Is the work really as described? Or is it better? 

Do you get interesting problems to work on? 

Do you feel you get an opportunity to make an impact with your work? 

Do your models make it to production?",datascience
"I want to see which house prices in my dataset are outliers based on other feature columns like sqft, waterfront or year built. 

Some of these are more important and play significant role on deciding if a data point is an outlier. 

What is the logic/literature behind this? How can I go about building this code?",datascience
"I have 6 months of internship experience, 3 months as a Data scientist and 3 months as an ML researcher. I have done courses in deep learning which is my area of expertise mainly. I am good with tensorflow, and i know SQL and python well.

I dont seem to crack any data science or machine learning interviews though. What are a few things you could suggest that I should add to my skillset to get a job as a Data scientist? 

I have an Aerospace bachelor's degree from a top college and not a data science one. The only data science education i got was from online courses. Please drop some suggestions.",datascience
"Hello there, I am trying to detect a fraud detection model which outputs risk as Low Medium or High, I have a customers id in one data frame and in another data frame i have their data that from which customer (source) id to which (target) how much money 'emt' is being transferred. Now I want to drop customer id from the initial data frame and add a new column containing a series of transaction for both sources and targets. How do i do this and is there a better way to do this?",datascience
"I work for a small engineering firm. I have been tasked by my CEO to train an AI to solve what is essentially a regression problem (although he doesn't know that, he just wants it to ""make predictions."" AI/ML is not his expertise). There are only 4 features (all numerical) to this dataset, but unfortunately there are also only 25 samples. Collecting test samples for this application is expensive, and no relevant public data exists. In a few months, we should be able to collect 25-30 more samples. There will not be another chance after that to collect more data before the contract ends. It also doesn't help that I'm not even sure we can trust that the data we do have was collected properly (there are some serious anomalies) but that's besides the point I guess.

I've tried explaining to my CEO why this is extremely difficult to work with and why it is hard to trust the predictions of the model. He says that we get paid to do the impossible. I cannot seem to convince him or get him to understand how absurdly small 25 samples is for training an AI model. He originally wanted us to use a deep neural net. Right now I'm trying a simple ANN (mostly to placate him) and also a support vector machine. 

Any advice on how to handle this, whether technically or professionally? Are there better models or any standard practices for when working with such limited data? Any way I can explain to my boss when this inevitably fails why it's not my fault?",datascience
I’m assuming network science and data science share some ties so asking here. I know it’s a newer field but I’m still surprised it isn’t more prominent. Ex there’s not even a functioning subreddit for it,datascience
"I've been learning on my own for a few months now and have started working on some personal projects. I would like to apply to a formal program to get a Masters in AI/Computer Vision. I meet the Bachelors degree, undergrad courses, and GPA requirements for programs I've seen, but my issue is that I am many years removed from college and a year removed from professional work (as a software dev in a particular stack). I was successful for many years but decided to quit due to depression/anxiety, as well as not wanting to continue in that particular stack. I have not kept in touch with previous colleagues, managers or professors, and would rather start/continue a new chapter of my life without involving them.

I'm just wondering if anyone has experience with a program that didn't require references/recommendations, or if there is no way around this. My fallback plan is to work on my personal projects as well as I can and try to land a junior position at a small company/startup that might see value in taking a chance on me.",datascience
"Hi all,

I was tasked with finding videos for product releases for several companies from a very large Excel file with headlines of new articles. The Excel has only the headline of the product release article & the company names. there are about 10,000 headlines.

How can I approach this problem and ultimately automate it?",datascience
"There are a lot of things going on in the field, how to you keep track of all the new stuff coming up. 

My main resource at the moment is my linkedIn feed with focused research after finding something interesting.",datascience
"Hi,

I'm considering getting a post grad credential from the list in the title and looking for input from others who have done/are doing similar.  

My Background: BS in Mgmt, Concentration Info Sys; UT MBA, Concentration IM/Tech Strategy.  I'm late career (mid 50), have been in high tech (HW) about 25 years and want to work another 4-7.  After that I would consult til tired.  

Currently facing tech layoff and looking, but no immediate financial pressure for 1 - 2 years. 

Programs I'm considering:

1. Newly announced UT Masters in AI. Would likely do an AI cert in the meanwhile or assumed pre-reqs (coding, linear algebra).  Sounds technical and not sure I'd get accepted.  est. \~2 years to complete.
2. GT Masters in Analytics, with a few added AI courses.  Would likely play catchup on the Micromasters to get that out of the way.  est. \~2 years to complete.
3. IBM or UT AI Cert and other as needed.  6-9 mos to complete.

Likely would need a Udemy in Python coding in all 3 cases.  For those who have done, or looked at doing, any of these, what would you do/do differently?  

All input welcome.  Thanks!",datascience
"Non-data-scientist here.

I came across this huggingface webpage ([https://huggingface.co/docs/transformers/main/model\_doc/biogpt](https://huggingface.co/docs/transformers/main/model_doc/biogpt)) and saw this bullet point:

>BioGPT is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than the left.

What does that mean?

TIA.",datascience
"I’ve been promoted twice to manager now of data science and I find myself in meetings talking more than I spend (if any) working on technical stuff

Is this normal? I get paid almost $200k to say stuff 🤔",datascience
"Hey all, I just finished my bachelor's degree in industrial engineering. I was checking this coursera degree in data science and I want to know if it's a good option to pursue since I want to have work while studying for a graduate degree. so my questions are the following:

  
is the degree worth it?

  
can I find a FAANG job after getting this degree or at least in a niche start-up? I know it needs extra work but would it help, my current university is in eastern Europe.

  
I gladly want to hear about the experience of people who enrolled.",datascience
"First off, I want to say I know close to nothing about data science and I'm kinda learning on my own.

Here's my project: I have a bunch of logs where people write statuses of different tasks and I want to identify was was done and what is left to do.

&#x200B;

For example, I can have logs that say:

""X was replaced and need inspection""

What would you do to identify that X was replaced and that X still need to be inspected?

&#x200B;

Also, the logs contains lots of abbreviations like:

""X was RP and nd inspect""

How do you deal with this?

&#x200B;

Any help would be greatly appreciated! TIA!!",datascience
"I have a problem where I'm looking at API calls and downstream impact. Not really a specific question to answer, but predicting performance of the calls based on path through the audit trail would be interesting to do. Or, anomaly detection (i.e, this API is defective in 10% of transactions)  


I have thought of two approaches. One of them is regarding the data contained within the request/response payloads. The issue is, it's all text based, or changes depending on the specific API, so the data transformation aspect is very tough. Encoding one API's response to compare to a different API's response would be challenging. Perhaps there is an opportunity for deep learning here.  


The other approach I'm thinking about, is treating this as a graph problem. The majority of these transactions will form a tree-like structure, but some of them don't have a true root, so its more of a directed, acyclic graph (think it's called a polytree?). My approach would be to use graph theory principals such as degrees, closeness, vertex distance, etc. as features instead of parsing the very dynamic payloads. Since these are rules-based systems, I would love to be able to model the underlying relationships between these calls, that way when one transaction chain doesn't match the model, it's likely an anomaly.   


Does this make sense to do? Has anyone here done something similar? Any recommended literature? I've tried googling it, but when you search ""machine learning approach to tree-based systems"", you get a lot of papers or articles referencing tree-based ML models, which is not exactly what I'm looking for yet. Thoughts?",datascience
"How hard/easy will it be for me to get a data science job having this under my belt:

\- 2 years working as a Data Analyst

\- 2 years as Computational biologist in undergrad

\- Masters in Data Science",datascience
"TL; DR:  I explore a hypothetical scenario where widget makers of varying experience are compared according to defect rate over a 5 year window.  To make things comparable (accounting for the experience problem), I inquire whether using a weighted moving average is a good strategy for balancing the problem of experience out or whether there is a better technique/solution to employ.  

&#x200B;

The Scenario:

* Assume you were to look at the historical performance of  several widget makers who manually produce widgets.  These widget makers could be active employees at present  or left at some point during the window.
* You want to compare the historical performance of the widget makers in terms of defect rate over the last 5 years (2018-2022).  My definition of defect rate is simply the  total number of defects/number of widgets produced (inclusive of defects)
* Some makers have decades of experience while other makers only have 1 year of experience (minimum required for this analysis)
* The original data is by year (2018,2019,...).  Let's assume Maker A, who started their career over a decade ago, made widgets over the entire window. Maker B has 3 years of experience but was let go sometime before 2022.  Maker C was recently hired and worked all of 2022.
* To make things more intuitive (hopefully) and comparable, I converted the table from calendar years to years of experience within the time window. So Year 1 doesn't necessarily translate to 2018, it's simply represents the first year worked  in that time window. See table below.

&#x200B;

|Maker|Year 1|2|3|4|5|
|:-|:-|:-|:-|:-|:-|
|A|2/17|2/15|3/11|1/8|2/10|
|B|5/15|3/12|3/13|||
|C|2/7|||||

&#x200B;

*  Their defect rate is recorded over the last 5 years. For example Maker A produced 17 widgets of which 2 were defective in the first year of the time window, then produced 15 widgets in which 2 were defective Year 2, and so on...  Note that demand could be variable for widget production.
* Simply looking at the defect rate above, my calculations  show Maker A would have an overall defect rate of .25, B would have .35 and C would have .43.  The problem I see  is less experience penalizes more heavily than more experience; so if this were a bar graph the 'worst performers' would likely always be the least experienced and that may not be very fair or  informative and at worst, hiding poor performers with a lot of experience.

So, this implies that some sort of smoothing or weighting is needed in order to make things more comparable. I was wondering if it made sense to use a weighted moving average.  My weighted average would look like the following:

&#x200B;

* Each year of the window has an arbitrary weight associated to it.  For this example we want to downweight that first year  and gradually increase weights later on:

&#x200B;

|Year |1|2|3|4|5|
|:-|:-|:-|:-|:-|:-|
|Weight|.6|.7|.8|.9|1|

&#x200B;

* I obtain the sum of the product of each year's weight by  the maker's  defect rate  for each year they worked divided by the sum of the weights for the length of time that the maker worked.  Clarifying this last part - Maker A's denominator would be the sum of all 5 weights, B would be the sum of 3 weights and C would be only the Year 1 weight
* Using the weight and formulas mentioned, I show that A has a weighted average defect rate of .17, B has .43 and C has .29, suggesting that B is producing more defects relative to their experience compared to the other two.

This results make some intuitive sense to me as B continued their career with a higher rate of defects relative to A.  My concern is that 1) weighting is arbitrary  and that 2) this method might cause an inverse problem from the one I was originally attempting to solve - giving too much leniency to the inexperienced and overly penalizing experience. I understand there is no perfect answer here but wondering if this is a sound approach or there are better, more intuitive ways to approach this. 

TIA",datascience
"Has anyone experienced this? I'm 2 weeks into a new job and had a meeting with my manager where he said he was concerned I was working slowly.  However I've finished the two tasks he's assigned me each a day early..

&#x200B;

Edit: Yes, I talked to my manager and asked why he said that. No he did not have a good answer. We left off with me saying he needs to make his expectations more clear.",datascience
"I'm given a school assignment and one of the requirements is to visualize the important aspects and give meaningful realization of a real world dataset.
So I was wondering I someone could recommend me a dataset that is simple, real world, has some real use",datascience
"I have some questions about DS intern

1. I'm doing Masters in CS, what should I do if I want to get internship in DS ?
2. What knowledge is expected from DS intern ?
3. Are DS internships research position ? Does companies only hire students who are doing research ?

And also I'm looking for a mentor who could guide me.",datascience
"Hi, I'm currently on my way to obtain a Bachelors degree in Econometrics. I'm going to enroll on a Data Science Masters programme next. However, I'm learning the most from the internet, books and my own projects. After reading numerous blog posts, lurking on this subreddit for a while and talking to some data scientists, I believe I've got a  general understanding of what i need to do. However, I would like to know some of the less obvious things. If you could send a message containing some tips and tricks to your younger self, what would it be? I'm mostly concerned about: 

- unusual ways to practice everything: analysis of datasets, programming, maths (stats), analytical thinking, getting a better intuition etc.
- things that are important in the industry, but are not common knowlege
- other stuff i didn't even think of, but a professional like you did and would like to share",datascience
as someone from non tech which books help you understand language/ software without spending too much time in technical jargon and verbose,datascience
"Hi everyone, I'm really thankful that this community exists because I need help Kickstarting my career in the field. 

99% of jobs that are hiring data scientists require a sh*t ton of years in experience as a data scientist. I acquired the common skills (including python,SQL, machine learning, tableau, excel) that are advertised as a MUST to learn to land a job (or even internship). But during my job search, i was proved otherwise. 

 I'm unsure what job roles i can apply for, with my skill set and would like to hear your suggestions (i can share my resume if required through DM). As someone shifting careers from economics to data science, i feel low-key scammed by post graduate programs because of the gap between my experience and skills, and those that are required. 

I believe that big data engineers, SQL developer, python developer, business analyst and data analyst would be potential career options for me. Particularly, data engineering is interesting but I am going to invest more time in learning other skills like azure (which is so vast with so many categories, it would take me very long to master..), mongodb, Linux, Cassandra, spark, Hadoop, etc. 

I see these skills mentioned frequently in job roles i apply for:
- power bi
- Apache spark
- Hadoop
- mongodb
- deep learning
- data pipelines
- opencv (i find this very interesting, but not sure jobs would be there for freshers)
- git
- databricks? 

What would be some skills that I can quickly grasp the basics of, and master on the job? Especially jobs that would be open to freshers?",datascience
"I am teaching a course on Big Data starting next week and had planned a project with the twitter API for exercising spark structured streaming. Of course, that might be rather difficult now. So I am wondering if there is a similar alternative to get a stream of text using an api which is either saved in a directory or kafka. I have mostly found examples where you would have to poll continuously like facebook. 

Any ideas? Thanks!",datascience
"Hello all,

I'm currently working on my masters and I'm having a hard time choosing which electives to choose from. I want to make sure I come out of the program as well-equipped as possible. I am able to choose four out of the following five classes as my remaining electives. I've also provided a synopsis of what each course will cover:  
  

* **Data Science for Business**: This course explores the various ways data and science can be applied to business contexts. Particular emphasis will be placed on using data to make informed business decisions.
* **Principles of Python Programming**: Programming, problem solving and algorithmic thinking in Python. Topics include variables, input/output, conditional, statements/logic, Boolean expressions, flow control, loops and functions 
* **Data Manipulation**: This course focuses on the loading, manipulating, processing, cleaning, aggregating, and grouping of data. Students will practice on real world data sets, learning how to manipulate data using Python and continue their study of intermediate and advanced topics from the NumPy and Pandas libraries
* **Information Visualization**: each students the best practices in Data Visualization, the key trends in the industry, and how to become great storytellers with data. Students taking this class will learn the importance of using actionable dashboards that enable their organizations to make data-driven decisions. For this class students will be exposed to Qlik and Tableau
* **Applied Machine Learning**: This course will further explore modern machine learning applications such as decision trees and various ensemble learning methods including random forests. Special attention will be given to performing hyperparameter tuning to improve our models. Students will also focus on different dimensionality reduction techniques with emphasis on using principal component analysis. Students will explore the K-Nearest Neighbors algorithm and use it to build a machine learning project from scratch. Finally, students will study an introduction to using artificial neural networks to solve classification and regression problems. This course is entirely taught in Python.

Any advice is much appreciated!",datascience
"Hi everyone, so I am a fresher of Data Science, I am trying to apply to the Data Scientist position at one company. Here are the question they ask me, I just want to ask the structure to find the answer. Thank you. 

 ***At 6 AM today, you purchased 1 MW of electricity contract for 12 PM at a price of 100 pounds/MWh. Two hours later, the forecast for solar generation for 12 PM has changed from 4 GW to 4.5 GW. The market is currently bid at 95 pounds/MWh and offered at 105 pounds/MWh. What would you do, and why? Please answer logically, stating all assumptions.*** 

**\*Note that no additional research is needed. \***",datascience
So I have multivariate dataset which i need to predict 4 variables off of it. I know that these 4 variables decline over time and have a relationship with the other dependent variables. So I want to predict these 4 variables and their trend for the next year. Obviously I do not have the data for the dependent variables over next year. Any ideas on how to start? I built an LSTM model to predict the 4 variables already but it needs of course the dependent variables as input for next year.,datascience
"I'm curious about the UPenn MSE in Data Science program's statistics, and timeline. I was also wondering if anyone here has any experience with it. If you have, I'd love to hear about:

1. Did you or someone you know enroll in the program?
2. What was your admission stats like?
3. How long did it take for the decision to arrive?
4. What kind of job opportunities came up after finishing the program?
5. Would you recommend the program to others who are already data scientists?

Any info or thoughts you have would be awesome. Thanks!",datascience
"I'm working on a sports betting project and I wanted to talk to consultants to help me out. I'm also looking for statisticians/data scientists who have experience with betting. Ideally, I'd like to have a quick chat with someone free of cost, but I recognize this isn't easy. How can I find people, both paid and for free (if possible.) Is there anyone here that could help me out possibly?",datascience
My company has a $1k education stipend and I am not sure how best to spend it. I was given advice to not take a technical class but rather executive coaching/communication/non-technical class. I thought about toastmasters to work on presentations but I have been told it may not align well with technical presentations. Does anyone have any recommendations?,datascience
" What is a calculation to identify times when one set of time series data (dataset 1) has peaks/spikes and another set of simultaneous data (dataset 2) is stable and has more of a plateau pattern and no spikes or peaks at that same time? I tried graphing the difference between the two datasets, but this didn't work because there would be a high difference when both datasets are peaking if one dataset had a higher peak than the other. I only want to see times where one dataset has a peak and the other dataset does not have a peak at all.",datascience
"Hi everyone, at uni they told us that if the measured quantities are uncorrelatedwe need to use the quadratic formula to sum errors (sqrt(E1\^2+...)). however they didn't exactly told us what correlated quantities are. For example, measuring current and voltage through a resistance in a circuit, are those 2 quantities correlated meassure? On internet I couldn't find a definitive answer as everything is so vague. I read that you need to calculate the covariance, but I didn't find what values determines if they are uncorrelated or not, for examples, a covariance of 0.07 means that the 2 values are correlated or uncorrelated from one another? I know this is very chaotic but it's 1 am and am very sleepy.",datascience
"Hi, I am familiar with machine learning, but mostly for making predictions like who to sell to or how many service tickets well get. I'm wondering if someone can link me an article or repo that shows how to do entity resolution by tokenizing things life first name and last name. Thanks.",datascience
"Hi everyone, 

I believe that some data scientists/ML engineers with experience in this field could help me and other future graduates in this situation. 

I will be graduating from a Data Science BS in May and am currently searching for a job.

&#x200B;

1. I currently see most entry-level offers requiring a year of experience. Should I still apply for these positions, or is there actually no chance of getting them?
2. For interview preparation, LeetCode is pretty useful for Python and SQL. Do you have any advice on how to get ready for ML or statistics questions?

&#x200B;

Feel free to DM me or comment any advise you have. 

Thanks for your help.",datascience
"I work in telecom. These are fake numbers for illustrating purposes.

I have customer A who has 20% market share in US overall. Let's say they have 1000 calls per month and they send 10% to NYC (100 calls).

I am trying a win another customer so I have to estimate their call volume per city. I know they have 10% market share overall (500 calls). I need to estimate how many calls they send to NYC. I could use customer's A ratio (10%) but this ratio could be wrong. They could send 5% to NYC or 15% or something else.

Any ideas how to estimate this?",datascience
"Im doing a ml project doing binary classification and I fear I may be overfit due to a small dataset. 540 rows by 10 columns. I have 97% accuracy and my confusion matrix is exactly how I would want it. 

I’m trying to figure out how to make sure it’s good and not overfit. So I’m trying cross validation. I’m doing the function called cross_val_score. The ch method is stratifiedfold. It spits it out and I’m averaging 97% on there as well with 10 splits. 

This is one easy like of code. Is this the same thing or just as good as the loop method I see people using for KFolds on YouTube? Or should I try and replicate what they’re doing?",datascience
"Hi y'all,

So I'm currently a PM, and I'm looking into transitioning into Data Science. 

I have just have 2.5 years of industry experience, and I did my undergrad in Data Science, but I got a PM job right out of college.

I'm currently doing an online masters in CS, to strengthen my technical skills further. 

Any suggestions on how i can make this transition, or if it's even worth it?",datascience
"I have allways liked computers, programming and ive allways been good at math. I have chosen Data Science for my bachelor this year. But after reading online I only see negative things about Data Science. That its way too big of a supply than demand of fresh Data Scientist comming from school. That the market for Data Scientists is dead. If this is true, I should probably rethink what I want too study. 

But I really want a bachelor about programming that also compliments my math skills like I get more money. If not Data Scientist, then what?",datascience
"I’ve completed 2 recent company take home assignments for data science/analyst and found the suggested time completely off. Both claimed 2-4 hours, and I spent 7 on the first set without fully completing it and 6 on the second. That was to answer the questions, not overachieve for perfect responses.
Is that typical? 

For context I’ve been an analyst for 4 years and a DS for 2.",datascience
"Hey folks - I wanted to put this live course from Rob Donnelly (Arena, Instacart and Facebook) on your radar!

The course looks at how to improve product and business decisions with causal inference. It draws on his experience at Instacart and Meta and features real-world examples from Amazon Prime and Facebook.

It kicks off on Feb 27 and you can find more info here:

[https://www.getsphere.com/cohorts/applied-causal-inference?source=Sphere-Comm-r-ds](https://www.getsphere.com/cohorts/applied-causal-inference?source=Sphere-Comm-)",datascience
" 

Currently, AI has the ability to make predictions based on data and patterns, but it does not have the capability to truly see into the future. While AI models can make educated guesses about what may happen in the short term based on past events, it is not capable of anticipating unforeseen circumstances or understanding the range of potential outcomes of a given situation. This is due to the fact that AI systems are limited by the data and algorithms used to generate its predictions. Furthermore, AI does not possess the capability to form its own insights or draw meaningful conclusions from data due to the lack of emotional and creative intuition. Therefore, while AI can be a powerful tool to help make decisions, it cannot be relied upon to accurately predict the future.

What do you think? will it ever be able to truly see into the future?",datascience
"Half data scientist mechanical engineering student here. 

When I look at the history of science, we know that with the development of computers, analytical solutions have been replaced by numerical solutions. This was a big development that changed our world. Can machine learning revolutionize cases where mathematical models are too complex? eg: Turbulence modeling

Another Real Example:[https://www.wired.com/story/deepmind-ai-nuclear-fusion/](https://www.wired.com/story/deepmind-ai-nuclear-fusion/)",datascience
"Hello, my father has a few small businesses and i was thinking due to mu academic background of implementing a data department as they aren’t implementing data in almost any way. Any suggestions?, where should i start?",datascience
"Hi,

I am currently pursuing a B.E degree in Information Technology and possess intermediate knowledge of programming languages including Python which I believe is an important language when it comes to data science.

I should also mention that I am a neophyte in the domain and am trying to explore and understand the same. While I've signed up for a good course on Udemy, I would also like to explore more study and reference material.

Any help regarding the same would be appreciated.

Thanks in advance!",datascience
"We have a predictive model that is built using a Minitab decision tree. The model has a 70% accuracy compared to a most frequent dummy classifier that would have an 80% accuracy. I suggested that we use Python and a more modern ML method to approach this problem. She, and I quote, said, “that’s a terrible idea.”

To be honest the whole process is terrible, there was no evidence of EDA, feature engineering, or anything I would consider to be a normal part of the ML process. The model is “put into production” by recreating the tree’s logic in SQL, resulting in a SQL query 600 lines long.

It is my task to review this model and present my findings to management. How do I work with this?",datascience
"For instance, i am now plotting the density of a data set that is very sparse. thats why i would like to use some smoothing with a convolution. but then extrema won't remain the same. but i know the data was obtained via a stochastic process. so i guess i can omit extrema. whats your perspectives on that subject ?",datascience
"I want to work on a new project and create a web application that integrates deep learning.

I've always wanted to try my hand at making a chatbot, but as a beginner in data science, I could use some help. 

Can anyone share some creative ideas or exciting topics to make my chatbot stand out? Also, any tips on finding or creating a suitable dataset would be greatly appreciated. Thank you!",datascience
"Hi, we’re a team of 3 Data Scientists and I’m the Senior Data Scientist in the team. I’m thinking of introducing a Friday Coding Challenge where we solve from Data Structures problem or familiar LeetCode problem where all of us can practice how we deal with solving coding problems.
Is it a good idea? Has anyone tried and it wasn’t a good idea?",datascience
"Hello. Talking about AI/ML, here is an overview of data-centric & model-centric MLOps.  What is your perspective on it?

Link: [https://ubuntu.com/blog/data-centric-mlops](https://ubuntu.com/blog/data-centric-mlops)",datascience
"Spent around a month doing online search and study to learn more about the metrics related to Market Basket Analysis (lift, leverage, confidence, gain, etc).

The challenge is that most materials online are blogs that fail to provide more context on the metrics, or educational literature that simply summarizes all metrics but don’t have any industry use cases.

Any recommendations on industry/conference literatures or books, that go deep into Market Basket analysis?",datascience
"I need a real-time data with features drug name, disease (seasonal or non seasonal), patient details(age, any other disease), drug rating by patients.
I tried various websites but could not all the features in single dataset.
Please help me where to find and if any other approaches like web scraping etc",datascience
"Edit: Adding my information

I'm working as a Data Scientist for a service company and currently working for a Fortune 50 client. I would ideally like to work directly for a Fortune 500 product company. 

I have 6 yrs exp and would I feel it will be most helpful to talk to and have mock interview with a Data Scientist rather than going through interview questions online or on youtube.",datascience
"Scenario: You just got hired as a DS (for the first time) where you'll be the first DS on a team and will be asked to define the role you'll be playing, where do you start? 

Was asked this in an interview last week and don't think I answered well.",datascience
"I'm curious if any of you cuss at work. Feel free to leave longer remarks in the comment section if you feel the want.

[View Poll](https://www.reddit.com/poll/10s66rc)",datascience
" 

i have been given a problem statement for a certain dataset . 

"" analyzing the various core industries and GROUP the business categories like Retail, Poultry,

Agriculture, Manufacturing etc  ""

i dont know WHAT TO DO or where to start",datascience
"I have a dataset full of Twitter tweets, with a column for the text of the tweet. For each tweet, one word has been selected as a keyword and each such word is in another column called ""keyword."" '

For my purposes, these keywords serve as a label.

My goal is to create a model to label a Tweet, but I'm having trouble deciding how to encode the label so that I can properly label the Tweets.

My vocabulary size is 41,886 words, so figure the number of bits for one-hot encoding is 41,886. My training set has 5709 rows and my test set 1904 rows, so 7613 rows in all, so if I try to one-hot encode the keywords, figure roughly 319 million elements in the resulting one-hot encoding array.

Using word2vec would be problematic, I think, because you can't classify vectors without running the risk of floating-point-related errors.

What's the most optimal encoding specifically for this purpose?",datascience
"The traditional route to data science seems to be at least a Masters in Maths > DS > ML etc.

This is obviously a good start and will hopefully instil good DS foundations - in terms of the theory and tech skills (Python/R etc).

One key ingredient that is missing is an emphasis on pro-activity. You go through school being told exactly what to do. You go to uni, again being told what to do. And then you get an entry level DS job and they tell you what to do.

An important skill to develop is being able to asses the tools and materials available to you and come up with something that adds value all on your own. E.g. if you are a data anaylst or DE? well you have access to a shit load of data. Say your company has commercial interests i.e. they want more sales etc. Can you use some advanced analysis to either review customer behaviour or potentially create a model to predict sales?",datascience
"Hello good people I have not a clue about data science, but I have always liked spreadsheets and organizing stuff and I would like to learn whatever I can as a hobby and something to put in the cave where should I start?

Thank you in advance",datascience
"Has anyone gained a certificate from this program? 

What are your thoughts?

In anyone’s opinion; is this certificate valuable enough to gain employment?",datascience
"I’m a hiring manager for a Data Science team. I’m looking for a Junior DS and my methodology is:

- I don’t do coding interviews: I trust you know how to code, if you really want to be in this field you’ll learn tools and languages on your own.

- I don’t do case study: I don’t really have the time for this, I want my “technical interviews” to be 15-30 mins.

- I want to know if you understand the basics of math/stats and if you can reason through unknowns. I want to see how you process through problems.

Lately I’ve been getting candidates who make it to my round and when I ask them some basic concepts, they completely flunk it.

So I’m wondering…are my questions too hard?

The questions:

1) You are given a tabular dataset that you have no prior knowledge about. Walk me though how you would profile this data? What steps would you take to explore this data? Explain to me your EDA process.

2) How would you evaluate a linear regression model? What are the metrics and what does it represent?

3) Explain the difference between Standardisation vs Normalisation.

4) Explain what a type I and type II error are.

These last 2 questions I don’t hold against them and which is why I ask last, and I let the candidates know it’s good to know but not required. It would set them apart.

5) Say you’ve got a ETL job that needs to be ran daily, say it’s a dataset that you need to pull from a server, do ETL, and upload it to a database or storage of sorts. How would you go about automating this task? What tools, methods would you use? The world is your oyster, any and all tools out there in the world are available.

6) Can you give me a use case where you have worked in Cloud (ie. AWS/Azure) to support your data science projects?

Edit: I have dialog when they answer questions or if they have questions for me. My style is, dialogue! Just chat with me.

Are my questions too hard? Can I ask these questions in a better way without getting too elaborate? 


Appreciate the feedback and help.",datascience
"When building a complex rubric that is meant to grade/elucidate 4.8 billion individual datum, grouped into 1917 hierarchical components, in a manner that humans can quickly and intuitively viscerally get...

...one should take a break and go for a bloody run.",datascience
"I'm currently a graduating senior at Boston College with double major in CS and Math. Got a SWE job offer at a big fund company. 

I basically leetcoded my way into a job. No research experience, no personal projects. But I've always found my statistics classes most interesting, which piqued my interest into DS since I already knew how to code. After taking some data science-y electives, I wish I had gotten into DS sooner instead of SWE. 

From what I understand, getting into DS is not something you can leetcode your way through. You either need a PhD or a mind-blowing side project. 

Graduate school is out of the question (for now, unless I find a company-funded program), as I have  to start making money asap. I was thinking I stay in SWE for a couple years and try my luck with DS on for my third or fourth job. 

How does one move laterally into DS from SWE without a grad degree or any research experience? Is pivoting into SWE from DS common, or will it be too late for me to transition then? 

Some stat-related course work I've taken:

* Probability (Discrete and Continuous)
* Mathematical Statistics
* Advanced Data Analysis (regressions, ANOVA, non-neural learning methods)
* Natural Language Processing",datascience
"I have xml files of bugzilla bug severity data which I want to use in my ml model to predict severity of a bug, but all fields of bug like id,title,description are into different xml files but they have a common attribute of bug\_id. I want to merge them all into a CSV files so that i can work on them to create my prediction model. I need this for my college project.",datascience
"Hi all.  I'm not quite a data scientist, but I'm pretty data literate, though I haven't gotten my hands dirty in a few years. While I was quite proficient with doing analytics with excel when I'd just download a bunch of data, now that things are embracing single-source-of-truth, automation and cloud-based approaches, doing basic manual things has gotten a lot harder, even with desktop excel.  I've learned a lot of new stuff, but some key things are eluding me.  If I was doing a big enterprise-scale, future-forward solution, I actually feel like I could move forward well, but trying to utilize these tools in my personal life has been hard.

To start my current use case of personal finances (and its only step one of getting a good personal finance dashboard), I need to merge, categorize, and ignore some data in spreadsheets *that have the same column schema*, and that alone has ended up being really hard to do.  Not even getting to mapping tables or anything like that yet.

My big, *core* issue is that I can do queries, make transformation rules, but I can't seem to find a way to edit/add data once I get it all in one place (got forbid I want to make the edits when I actually see the data on a powerBI dashboard)  Googling this has been hard (ex: googling edit source data returns results for editing data locations), and he educational materials I've found all seem to focus on big enterprise level data where the solution is not to have any manual editing at all.

Case in point:  Categorizing expenses from venmo (which only lets you download one month at a time).  I can query the sheets.  In old-school excel, I'd copy all the rows together, sort by vendor, and then manually batch categorize.  It's take me ten minutes tops, and then I'd use that as a sort of 'silver' data set to import in to my final dataset.  Now, I can query the individual sheets via power query (venmo only lets you download a month at a time), but adding categorization or deciding to remove a row seems impossible.  Right now it seems like the only thing I could do is write a python script, or go super manual any time I want to merge some data and copy&paste individual rows from many different sheets.  I'd love to streamline and utilize the new stuff, but it seems like the more manual side of data cleaning has to happen alllll the way down at the raw source and that kinda sucks when you have similar data across twenty different sources, or want to add data (like a category) at a higher level.

&#x200B;

Sorry for the long post.  I fully expect that the real problem is I'm just thinking about things too old-school somehow and need to do somehow change my paradigm with all of this.",datascience
"Building some context. 
I have an MIS degree. Worked as a data analyst at two different companies, adding up to 3 years. 
Just completing my first year as a Software Engineer focused on a Data pipeline features. 
I am very comfortable with Python, object oriented programming and even have a few DS projects on my resume. 

The last couple of years have been miserable for me and I'm mentally exhausted.
 
I'm looking for a way to get out of my current job. It isn't the best place to be for a variety of reasons. Been interviewing for DE, Analytics Engineer roles for a couple of months, but haven't gotten too far. DS has been a field of interest for me but I never got any calls because of lack of relevant experience. 
 
1. With some experience as a SWE, do I have better odds getting hired as a DS/MLE?

2. How do I approach the job applications? Please suggest any projects/skills that will get my resume some traction for these roles? 

I'm feeling severely hopeless, please be kind. Thank you!",datascience
"I ran into this problem enough times at my last few jobs that I built a tool to solve it. I spent many hours building Docker containers for my Python functions, as many of the data science modules required building C libraries (since they significantly speed up compute-intensive routines, such as math calculations). Deploying the containers to AWS Lambda or Fargate (if the processes required more CPU or memory or were >15 minutes) and wiring functions to talk to each other using queues, databases, and blob storage made iterating on the actual code, which wasn't even that complex most of the time, slow.

I made cakework [https://github.com/usecakework/cakework](https://github.com/usecakework/cakework), a platform that lets you spin up your Python functions as serverless, production-scale backends with a single command. Using the client SDK, you submit requests, check status, and get results. You can also specify the amount of CPU (up to 16 cores) and memory (up to 128GB) for each individual request, which is helpful when your data size and complexity varies across different requests.

A common pattern that I built cakework for is doing file processing for ML:

\- ingest data from some source daily, or in response to an external event (data written to blob storage)

\- run my function (often using pandas/numpy/scipy)

\- write results to storage, update database

\- track failures and re-run/fix

It's open source <3. Here are some fun examples to get you started: [https://docs.cakework.com/examples](https://docs.cakework.com/examples)

Would love to hear your thoughts!",datascience
"So, the appraisal came in numbers that were not in my favor even though, I worked on a lot of projects which had a positive impact on business, they filled a patent as well under my name for one of the solutions. 

Now I came to know, after appraisal, one of my colleague is getting 40% more than me, he is working on the same problem for since I joined them, I guess he is working on the same problem for past 3 years, and I know in the end the solution is simple xgboost.

So, if any of you is looking for a data scientist with the zeal to solve problems, please hit me up!",datascience
"I just joined a software company's support operations team as a data scientist. My first project is to improve the current forecasting model predicting future support case volume over the next 12 months to be used as an input for capacity planning.

The company has multiple software products and the current forecasting models that have been used are all just univariate forecast models that have been only trained on using the historical case volume for each product. These models in general perform pretty well in terms of forecasting accuracy, but the business stakeholders are complaining about how ""un-explainable"" these forecast values are. They want to break down each forecast values into multiple components, but the current model class won't allow them to do so because the these forecasts are only derived from analyzing the past volume's seasonality, holiday effects, and running trends (either increasing or decreasing year over year).

When I chat with my business stakeholders, they said that the case volume is driven by factors such as active customer count/distribution of customer size across different regions, product maturity measure, case deflection rate (how self-serviceable the company's Knowledge articles and navigations are), and others. We do have these drivers and the stakeholders want to use them and their expectations of the future trends for these drivers in the upcoming forecasts.

Looks like I can try multivariate models such as VAR, VMA, VARMA, or VARMAX. However, I'm not really sure how reliable they would be. If they won't result in something that will be adopted, then I would prioritize improving the current model's accuracy.

So my question is, have you also run into the similar problem? Are multivariate forecasts actually even used in Capacity planning? Which model class can I look into?",datascience
So I have been eyeing the DS bootcamp from Northwestern for a couple years now. I am an accountant at the moment but want to move into data analytics as thats a growth path in my company and I wanted to try and buff up my skills to make the transition easier. I used to major in CS for a year and did Stats for 2 so I feel like I would have a decent chance of comprehending and gaining a lot from it. Has anyone here done the Northwestern Bootcamp?,datascience
"Hello All! I’m an American considering doing a Data Science / Statistics/ Machine Learning masters in Europe (likely Spain). Many people I’ve expressed this thought to suggest opting for an American university instead. I’d love to hear from anyone that has done a masters in a EU country. Specifically, did they feel as competitive in the American job market after? (If the US is where you’re based)",datascience
"so really by no means am I a data scientist. i sit on a strategy team with a new role of decision science. so i started yesterday taking over a process that had been in the past done throughout 3 teams, one of which is data and analytics team 


long story short, the output from the many complex sql queries that have been being ran is most likely inaccurate and then the analysis done on it is independently shit. took me many many hours to figure this out and i’m exhausted. i blew the whistle today because the deliverable goes out to major clients. my complaint is being taken seriously but still wtf 

how can so many people be complicit in this?

edit grammar

for context: i can write complex sql queries and know my way around R but I don’t know complex stats,math, logic etc that goes into building ML, Ai etc models. Idky this is being downvoted so hard. know am young and probably wearing too heavily coated rose colored glasses, but i think this is a valid question to the sub :/

UPDATE: 

so apparently we are now tightening our DUA with clients which came from a c suite’s suggestion. a lot of my team is new so I think that’s why this was this well received and why they escalated this so quickly . i really did do good due diligence which contributed to it too. 

thanks for all the help mentally sorting this out and i’m glad i can provide a good outcome to y’all 

maybe one day when i grow, ill put in the work and become a data scientist ❤️",datascience
"Hi Everyone, currently working on the MVP for our startup and we're developing a loyalty program App for SMEs in a couple of industries, I.e food and beverage, beauty and spas, etc. 

Since I’m new to this field, looking for any actionable advice. Both from entrepreneurs who have built a similar business model before and also feedback from vendors. 

1. Do you run any current loyalty or rewards programs? And have you done any partnerships with other businesses? 
Perhaps a type that offers coupons or points.

2. What do customers currently need to do to earn points? 
e.g. buy several products in exchange for a gift, follow/like on social media, share contact details, sign up for a newsletter, etc 

3. What else would you like them to do? Have you thought of partnering up with businesses with overlapping customers to increase revenue and retention? 

Appreciate any thoughts and advice!


P.S. Maybe, entrepreneurs who’ve built a similar business model in the form of software or App could share any tips from your blueprint. 

Thank you!",datascience
"Hello Fellow redditors,

I've managed to secure a junior data analyst role. The HR at the company insisted me that the contract agreement gonna be there in a day and I have to E-sign it, but it's been more than a day. Should I email HR asking for it or wait further?

Thank you",datascience
"I'm thinking about making a project showing various types of reactions to current events in left wing and right wing subreddits and YouTube by taking api data and visualising it using tableau.

If it is too basic, can you suggest how to make it more advanced? Thank you",datascience
"Hello people,

I am currently attending a Data Science course and to finish I have to write a paper about a project that I am currently working on. I write the code in VSCode and I use .ipynb notebooks.

So I am basically training a few ML models after a long data preprocessing which worked out fine. But as soon as I run my hyperparameter tuning code, my PC takes a lot of time. Right now I am running hyperparameter tuning for RandomForest and it already runs for 21 hours.

Is there any possibility for me to run my code somewhere else? I read abour Heroku, but that seems to be too much than what I am looking for. I am getting a bit nervous, because I want to get this paper done. The worst case is that I have to buy a new PC.

Thank you so much!",datascience
"My current job as a SWE in machine learning is proving to be very stressful for me, and it may be due to a poorly structured, understaffed company with high turnover,  but I thoroughly do not enjoy being asked to make magic out of nothing, asked for deliverables that are unreasonable and completely out of scope of my domain knowledge and would take me a very long time to get up to speed on, tasks are morally questionable, and people are highly unprofessional and at times downright hostile (it’s defense contracting). 

I want something less stressful. I’ve applied to dozens of places in ML engineer, data scientist and data analyst roles and have not heard back from any. I have a BS in linguistics and an MS in data science. 

Does anyone here work as a data librarian?  Wondering if you would need a MS in library science. Maybe being a librarian in general would be less stressful…",datascience
"I'm in the last university semester and looking for jobs. In one of the interviews they sent me a dataset with some instructions like ""use ML techniques to predict when X or Y would most likely happen, in Jupyter notebook, we expect  a precision of 50% and no need to analyse recall right now"" and it was a breeze. The first things I tried worked like a charm and got everything I was asked to do (a simple logistic regression was really effective). Is it better to just leave at that EDA + some manipulation and the first model I've tried already worked great, or is it better to add some other models and techniques so I show off/compare a little bit? What skill they are trying to evaluate other than the code/model working? and how much should I explain without being pedantic? I'm tend to be too perfectionist (legacy of my uni, where they ask for scientific level documentation pretty much always)  


Thank you!",datascience
"I'm a computer engineering student, and as you can imagine my grades aren't great. They're not the worst but there are certainly subjects that drag me down. 
I'm afraid I won't have the to do a masters of PhD, but I'm very willing to self teach through the internet and projects.

Do you need a masters or PhD to work in data science and ML?",datascience
Idk much but i want to do some data processing and do a write up on it as practice. What sorts of data would be good for this that i could obtain online easily,datascience
"Not necessary that the product is medical related, what have you done to improve hospital's/healthcare institution's operation or create opportunity for partners?   


I am looking at all these data from the data warehouse and I really want to make a positive impact. Looking to get some inspiration here. Discussion from outside healthcare sector welcomed too. TIA.",datascience
"Hello everyone, I am a pharmacist (Bachelor of Pharmacist in Vietnam) and currently working in a marketing position at an MNC pharmaceutical company.

Now, I want to switch to the data science field. Of course, I do not have any experience in programming. I have been doing some self-learning courses about Python Specialization, and Algorithm in Coursera.

Could you please give me some advice if I should try to apply to an Online Master's Degree program or choose a short course 'bout Data Science instead? Or Should I back to a BSc in Data Science?

Thank you so much!",datascience
"SAP is for better or worse, ubiquitous. Data lies in this system. 

As a data scientist, what should we know about SAP products that can help with data science workflow?",datascience
"Hi!
Does anyone what’s the content for the Analytics Test 2023? I searched online and came across Tests that mainly focus on excel skills. However, the info given to me by my HR are “complete our analytics assessment. It includes maths, logic and SQL. It's all multiple choice.”
Or should I just stick to SQL?",datascience
"I've read that many employers don't really care about the specific course / institution where you get your certification (debatable I'm sure) and that practical projects / portfolio most important. With that in mind, was wondering if anyone had a recommendation for a hands on practical course that sets people up to do their own personal projects. For example, I see that the [Johns Hopkins University Data Science Specialization](https://www.learndatasci.com/out/coursera-data-science-specialization) has solid ratings and seems hands-on (albeit in R with no Python). Any suggestions welcome.",datascience
"I have been in the healthcare data science field for about 3.5 to 4 years. I have a clinical degree - Pharm.D. I started as a government health informatician, then transitioned to data analyst, and now I am a data engineer in a health-tech startup. Since I don't have a strong DS/stat/CS education background, I constantly felt like I needed more education in biomedical data science.

After some research and chatting with people, I was told master's in data science at Berkeley is good. However, I was also told Stanford/Harvard's master's in biomedical informatics has many good core courses and electives in CS and DS.

Since I use a lot of text processing and lab data analysis, and my current role started to get more NLP applications, I wonder which training program is better suited for someone in health tech currently and in the future?",datascience
"For each working day, I have 6-7 meetings and almost have no time to do data analysis. Do you know if this is normal?  If you are a data analyst, could I take you a couple of minutes to the questions below?  Thanks in advance!

1. Ratio of daily meeting VS daily data analysis, like 2:8,3:7, etc.  How many meetings do you have each workday on average?

2. Is there a data manager that can advise/lead you?  Or you need to report to the manager with no data background.

3. Required skills you need in your data analyst job.  Technical skills VS communication skills, which is a little more critical for you? ",datascience
"My dream is data science, but unfortunately my dream in high school was to get stoned and watch YouTube videos all day. I totally blew my chances of university (I am 24 now), but I currently have a 2 year diploma in software development and will soon be doing a 14 month post-diploma certificate in Data-analytics. 

If I work as a Python developer for a couple years and eventually transition to Data analytics, will there be any chance of me transitioning to data science if I also have a solid portfolio? I guess what I am really asking is did 17 year old me really ruin any chance I had at pursing this goal of mine?",datascience
"Hello guys! This is my first time posting here. 

I’m currently a junior studying Biology but I’m not doing pre-med. I’m currently taking a data mining class centered on genetics and my professor announced that the students can continue doing their assignments with either R or Python. 

Since I’m working towards landing on a DS role, I’m wondering which language I should lean towards.

Any insight will helpful!",datascience
"Simple question, so many answers. I've been seeing so many startups with the potential to replace jobs with AI, what scares you?",datascience
"I can do the Coursera IBM Data certificate through my employer. Has anyone else completed it and thought that they gained valuable skills through it?
I don't have any prior background in DS and am currently a Backend Kotlin dev looking into DS/DA roles.",datascience
"I have a Gaussian Naive Bayes model. 90% of all data identified as positive are true positives, and 56% of all data identified as negative are true negatives. This is approximately true when evaluating the model both against the train dataset and the test dataset I generated (which I suspect is actually a cross-validation dataset, as this is a Kaggle competiton so the actual ""test"" dataset doesn't have the Y variable.)

In all, the accuracy is about 60%.

However, I'd like to investigate that true-negative rate and see why it's so low and what I can do to improve it.

As this is a Kaggle competition, I'm not sure if the Terms of Service allow me to reveal what Kaggle competition I am talking about.

Thoughts?",datascience
Has anyone here taken the hirevue online code assessment for this role. I have to take this in 1 weeks time. Any information would be highly appreciated.,datascience
"Say I want to create a startup with paying customers through subscription service. I have a ML model which runs on my server. How would this usually interface with a paying customer?

Would an API (REST, RPC) to my server be a sufficient interface, or do most customers expect a different user interface, like a UI?

Would I give each customer a different key to use when calling the API to give them access to the service?",datascience
"I have a mentor that I hired recently whose background is in data science. I'm wanting to get an entry level in data analytics--not data science--but she has me on a track that is almost strictly Python. She doesnt think I need to do much in SQL or Tableau. She also thinks I need to learn advanced techniques in Python like machine learning, time series analysis, etc.

Most of what I've read/watched on data analytics suggests that

1)SQL is by far the most important tool to learn

2) learning one data visualization tool(Tableau or Power BI) is a must

3) only the basics of Python in Numpy, Pandas, and Matplotlib is needed because it will only be used 5-10% of the time

Does this mentor have me on the wrong path, learning things I don't need to while being too focused on the things data scientists would use?",datascience
"I'm trying to predict sales for a tea export company with different tea types and weights. My goal is to predict sales for each product type category for the next 12 months.

Data set looked like this

    Date        Type    Weight    Quantity      Price
    2016-01-01  black   bags      1734136.51    1131.30
    2016-01-01  black   bulk      10722389.66   510.86
    2016-01-01  black   4g_1kg    6817078.01    588.72
    2016-01-01  black   1kg_3kg   86444.50      565.91
    2016-01-01  black   3kg_5kg   1003986.73    552.39

I have simply predicted the total sales and that came out pretty good. But I don't understand how to predict sales for each type and weight classes. I understand the time is valuable and if someone can explain me the way to do it, it would be great. I'm trying to use linear regression for this. Accuracy doesn't matter. What I want to understand is to feed the data and get the output for different product variations.",datascience
"I am learning Datascience and doing a project. I created some dummy data http://Mockaroo.com using this link for my project. 
I carefully researched all the feature datatype required and ranges to  set the data in standard ranges after doing lots of research of what will be the highest and lowest value of that feature. 

The target Y is discrete so I used logistics regression when I applied the algorithm and do the summary I find all the p values >0.05

My question is 
Is creating dummy data out of no original data for sample is wrong
Or 
How to choose proper p value

Someone help me",datascience
"Hi everyone! 

First time posting (ever!), kinda nervous lol.

I recently just got a programming interview offer for a data engineering position! However, I am still relatively new to the field and not 100% confident about what skills/concepts I would need to practice/hone before the big day. I would love any tips and pointers! (esp if you have a DE background!) Thanks!",datascience
"For example given data of day vs. value as below:   


series1\_data = \[\[1, 10\], \[2, 8\], \[4, 5\], \[7, 7\]\]  
series2\_data = \[\[2, 12\], \[3, 9\], \[5, 7\], \[6, 5\]\]  
series3\_data = \[\[1, 9\], \[2, 11\], \[4, 8\], \[5, 6\]\]  
...

Note that series\_1 has days 1,2,4,7 and series\_2 has days 2,3,5,6 . Thus in this case, a group-by day would not work to generate a daily statistics.",datascience
"I’m thinking of going back to school to get my masters in the next year or so. I’ve worked for my company almost 2 years and they have a program where they will pay part of it as I work full time.

What has your experience been with taking higher ed classes and work full time? From what I’m assuming there won’t be much free time of course but the pay off is worth it. I’m looking at some data science/analytics program or MBA (leaning more towards data science) but want to gauge how intense it is and is the end result worth the investment.",datascience
"I have been offered a masters degree offer for computing data science instead of cybersecurity ( which I applied for). It’s mentioned computing data science in the offer letter rather than data science. I did some research and got mixed information that both are not alike. Do anyone have any idea whether both are same, if not then which is better?",datascience
"Lately I've been perusing through the subreddit, reading posts on how one could land a DS role or change within industries led by data science techniques, e.g. quant, marketing or any other DS tech led industrial solutions. 

But from this post, I wish to gain a holistic view on what you all have pursued after landing a job related to DS. 

Do you go for:
• MBA for a managerial role
• MOOC courses on different DS tools to stay updated
• Certification in MLOPS/Dashboard/statistical programming etc.
• PG in economics, quantitative finance, statistics etc.

I understand people chose to pursue topics they wish to accomplish in their careers. 

But I wish to gain a quantitative insight into what most DS pursue after landing a job.",datascience
"This is a simple supply chain analysis that I'm hoping to get some feedback on. I'm wanting to build an analysis that shows a negative correlation between inbound service and outbound transportation costs. The theory is that when our incoming shipments are delayed, we need to expedite outbound shipments to pick up the slack. Thus incurring additional cost.

I built a simple linear regression model that plots item ID on a graph that has service level % on x-axis and transportation cost on the y-axis. This was strait forward. Now I want to build a similar graph that shows the sum total of our transportation spend on the y-axis. This would demonstrate that with each percent increase in overall inbound service level, we can decrease overall transportation costs by X.

**My questions are:**

1. Is it mathematically correct to translate the first analysis into the second?
2. How would I go about building the second graph based on the first analysis? (included sample data below for illustration)

&#x200B;

https://preview.redd.it/uo3exdrscmfa1.jpg?width=1628&format=pjpg&auto=webp&v=enabled&s=898d28710b80050a5d036ba1c4407adbd51f4c8f",datascience
Curious if anyone is in an organization that uses Denodo. How do you like it? Also just curious if anyone is using a different virtualization layer in their organization?,datascience
"Please forgive me if this is the wrong sub, but I'm at a really pivotal point in my life and I'm considering between lighthouse Labs, springboard and Brainstation for their data science camps. I have a lot of real world experience and just want to upgrade my toolsets to include python/SQL & other data science tools that help me secure better career options.

(don't ask. I've had to do black magic with spreadsheets in ways you wouldn't believe for years now.)

I'm mostly interested in the short-term grinds since I'm leaving my current job; for data science camp goers, how have:

\- Class size / Instructor availability

\- Post graduation job support

\- Opportunities for projects

been for you guys?",datascience
"I’m applying for online grad schools with Georgie Tech as my first choice. I’m not seeking high prestige big name schools or anything with high tuition costs into the $40k-$50k range because I’m very senior in my career and really just want to do this for myself. I have not seen much of anything online about Merrimack so I’m hoping to hear some reviews of this program. I’m thinking of
Merrimack & Eastern University as my backups in case I don’t get accepted into Georgia Tech. Anyone have any insights? Thanks.",datascience
"Hi,

I got an invitation to a second interview for a Data Analyst position.
For the interview they want me to present a case study for which they have provided a csv, my question is what is expected of me?

The csv is a customer satisfaction survey with columns regarding: ""How easy was it to handle the matter?"", ""Did you receive the help you expected?"", ""How satisfied are you with the recieved service?"" and ""How likely are you to recommend us to someone you know?""

During the first interview they discussed data visualisation alot so I feel like that is at least expected of me. Any general points of directions what visualisation and analysis that is probably expected would help alot since this is the first time I've gotten this far in the interviewing process.

Thank you!",datascience
There is a feedback loop that whenever a model is implemented in Keras/SKlearn etc they get a lot of attention. Is there any algorithm that we are sleeping on because they don't appear in any famous library?,datascience
"I’ve been self-studying gaussian processes; watching videos, reading papers and blogs, and started with the Rasmussen & Williams book. I’m still quite a way from fully grasping it.

 

Now, gaussian processes is a non-parametric model, requiring access to all the training data when making predictions. I’m often coming across the information that GPs don’t require cross-validation. How then can one confidently assess the model? Especially with regards to higher dimensions, when you can not visualize it as a plot.

 

As a side not, although I am trying my best to understand the math, I am using scikit-learn.",datascience
"Hello, I have a background in life sciences and I wasn't expecting to get accepted into this course so I'm happy about that. I'd like to hear from actual professionals about the scope and viability of this degree specifically as it's not just plain 'Data science', thank you!",datascience
"Hi, I'm currently going through a learning journey. I have created a vanilla neural network from scratch and I want to document what are the improvements that can be applied on.

I'm only working toward the multi layer ANN, I don't want to tackle with others like CNN, RNN, LSTM, Transformers and so on, for the moment being.

From my own research I have figured a few, which are:

* Momentum based optimizers for saddle point problem
* batch, mini-batch and stochastic gradient descent
* batch normalization
* L1, L2 regularization 
* dropouts

Can you tell me what other techniques available? 

&#x200B;

>!I have made a !<[Notebook](https://www.kaggle.com/code/mohamedahmedx2/build-a-simple-l-neural-network-from-scratch)>!on kaggle with the code just to give you a brief !<",datascience
"Hi,

Have a project where i have to predict certain currencies against another for next 3-6 months eg:

CHF VS USD

AUD VS USD

AED VS USD

etc.

So far i have only seen the very short term forex trading ones LSTM etc albeit very briefly.

Does any algo also incorporates data such as from tradingeconomics com?

Would be much grateful for any guidance or help.",datascience
"Data for non science background people

Hi everyone, i come from a finance background and want to move into data. I'm not sure whether I should start with data analysi, data science, data engineering, BI, etc. So many things out there which makes it equally confusing for me to choose from. I don't know if data even is for me.
At the same time i don't want to just sit idle till i come to a decision. 
What would you suggest i start with atleast so that i can figure out the process as i go? 

I'm a qualified CMA and have experience in accounts and costing, zero idea of data science and don't know where/how to start.",datascience
"I am an incoming new grad Data Scientist and I recently got some of my admissions listed below. If cost is not a **primary** issue which of these programs are worth pursuing for someone who already has DS in undergrad?

* My undergrad degree included a good amount of CS (like discrete math) /Stats (like statistical modeling or mathematical statistics) /Math courses (Calc 1/2 etc) and I had done 2 data science internships where I mostly did MLE, not Analytics, and worked with P.h.D Data Scientists.
   * Although I feel like I don't know enough stats and I want to learn more to interpret models.
* My main goal is to pursue MLE in any domain instead of analytics but open to DS, and analytics if companies prefer P.h.D's in the future (I am aware of the whole P.h.D's are much better than MS candidates and I witnessed that in my undergrad internships)
   * However, I am very passionate about DS and want to become a better DS in the future, I did a lot of projects in my internships that consisted of utilizing advanced NLP and Traditional ML algorithms.

I have U Chicago and U Penn on my top 2 as I met with the students in these programs and they are all in tech or fintech companies and doing great in their fields. And of course, I have a bias toward getting a top-10-ranked university degree. I have to do an online degree as I have a DS job lined up and I have to make money to live to support myself. Right now I am only thinking of waiting for U Penn's decision since it's cheaper.

**Edit:** U Chicago also calls the admitted students to campus twice to meet with faculty and other students which I really value.

Details:

|My Ranking #:|University Name:|Tuition - Scholarship|Decision:|Duration (Years):|Major:|Scholarship:|
|:-|:-|:-|:-|:-|:-|:-|
|1|U Chicago|52k|Admitted|1.5|MS DS (Analytics)|10k (Highest Possible)|
|2|U Penn|34k|Waiting|2.5|MSE DS|0|
|3|Northwestern|58k|Waiting|2|MS DS|0|
|4|Notre Dame|58k|Admitted|2|MS DS|0|
|5|GT|10k?|Admitted|2|DA|0|
|6|UC San Diego|54k|Waiting|2|MS DS|0|
|7|UT Austin|10k?|Waiting|2|MS DS|0|
|8|Rice Uni.|50k|Waiting|2|MS DS|0|
|9|Purdue|42k|Admitted|2|MS DS in Finance|0|
|10|Johns Hopkins|50k|Admitted|2|MS DS|0|
|11|UC Berkeley|75k !!!|Waiting|2|MIDS|0|

&#x200B;",datascience
"preferably in transportation analysis but otherwise domains will do.  i interested in learning more about this side of DS as i have little experience working in network analysis. like a preview into the real world, i want to be faced with problem solving network analysis projects.",datascience
"I am trying to create a brand new data system for my company. I managed to create Python scripts that pulls data from APIs, wrangle them into a Pandas DataFrame, and export them into an Excel file. We are trying to do this process automatically daily on the cloud, store them in BigQuery, and display them in Data Studio.

This is the system that I have in mind. I put the scripts on Cloud Functions, but instead of export them into excel files, I use pandas\_gbq to store them to BigQuery. For the cron jobs, I use Cloud Scheduler to trigger the scripts on the Cloud Functions by sending HTTP requests. What do you think of this system?",datascience
"Has anyone else noticed how much Yan LeCun has been hating on ChatGPT lately? I think it’s one thing to call out issues, but I have a feeling he’s trying to discredit it for business politics more than anything (Meta v. Microsoft)",datascience
"However, where i live there isn't a degree that has both, so i have to choose between a bachelor's degree in cs or economics. Which one should i pick ?",datascience
"I’m currently a CS student, I love DS but I also love having flexibility with my work space, so I would love to work from home and be able to travel around from time to time. I also want to start making money on the side, so I want to start freelancing on Upwork doing DS. It seems like it’s kinda a difficult field to get into when you’re first starting out. 

Is there any of you that can share their experience working as a DS freelancer or having a data related remote job?

Getting some perspective would really help me. Thanks!!",datascience
"I'm trying to describe mathematically a strain pattern of a structure (civil engineering) that I analyzed with FEM.  I extracted a 'line' of mesh nodes from the fem analysis and found the constants for the approximation function f(x)  using lmfit (least squares) with python. I have 4 constants (a,b,d,f) and want to link those constants to geometrical properties of the structure. I already determined the value for 'b' and know that 'a' and 'd' are somewhat associated with one geometrical dimension. I have determined the geometrical parameters that influence on the strain and want to determine the value for the f(x) constants based on this geometrical dimensions. I have 35 different fem simulations, varying the geometrical dimensions of the structure.

The question for you guys is: is there a software where I can input the function (and its determined constants values obtained with the lest squares approximation) and the geometrical parameters from the structure and get some correlation? (like the 'a' constant from f(x) is 2/54\* (geometrical dimension) ). Or a way that I can do it with python, perhaps. 

&#x200B;

[here is the f\(x\) and  the 'b' value. ](https://preview.redd.it/zbwqxvajlgfa1.png?width=2600&format=png&auto=webp&v=enabled&s=abbbe836cc7993bf628ccb1eb50608a27a034d9d)

[the 'b' value on the geometry. I have the g' value that I want to correlate with 'a' and 'd', and some other geometry dimensions \(thickness, for example\)](https://preview.redd.it/vksb0pucmgfa1.png?width=1516&format=png&auto=webp&v=enabled&s=64529de5c22271c8e30c6d574ceb2854930a4c71)

&#x200B;

[  This is one of my functions, and its approximation with the values of the constants \(for the positive and negative values of z, as the function is not symmetrical\).](https://preview.redd.it/u49k6cwfkgfa1.png?width=4000&format=png&auto=webp&v=enabled&s=145d3a87ab8bb44b86c34af4506243ef3f1bf320)

&#x200B;

[the report that I get from the lmfit analysis.](https://preview.redd.it/tdalnxnfkgfa1.png?width=678&format=png&auto=webp&v=enabled&s=9a1c03906a96e96e14fdd4f50844b4e7c5762f07)",datascience
"Is there a way to find the variance of drawing only spades from a well shuffled deck of 52 cards? Or the variance of drawing only red cards from a deck?

Context: I am trying to build a probability model of a card game where drawing a suite (spades/clubs) will earn the player $5, and I need to find the standard deviation of the winnings.",datascience
"

What do you know about KNIME? Do you use it?

Should I learn KNIME for analyze data or better PYTHON?

I normally use EXCEL, but now I have to analyze files with more than 1mil rows and excel doesn't work properly.",datascience
"Tried finding something like this on the FAQ, but didn’t see anything written out. I’d like to have a discussion about what sort of projects hiring managers want to see. Obviously, real world projects are going to be more desirable, but in some cases this isn’t possible. Or at least, in my case. I wasn’t ever able to take home artifacts (dashboards, slides, notebooks, etc.) from my previous work, as it was sensitive data. The only other projects I have are just college ones, which were less impressive Kaggle problems. My current role is data scientist by title but our ML projects just keep getting delayed for other things, so no work there. 

So, that leaves me to building out a portfolio of personal projects.

What do people in the space want to see?

What’s interesting to me personally, is being able to look back at my college problems and examine why my approach was incorrect using my real-world knowledge. So a good project might be using the same dataset and methods, with a write-up of how to improve it.

Otherwise, I’d just want to do some projects that seem fun to me.

So hiring managers, or senior roles, what are the best projects that you’ve seen someone talk about in an interview?",datascience
"Im PhD trained in cognitive science, but academia isn't for me. I did some training courses, but mostly taught myself HBM and quite like it.

Are there any sectors or industries which are likely to need skills like this?",datascience
"As a PhD student using big data, my pipelines are mostly done locally. What is your setup? For example, what libraries do you use in Python? Do you use Docker/Kubernetes? What cloud services do you typically use?",datascience
"Hey guys,

So I used to use Spotfire at work mainly because it was the standard in my previous company, and it also has so much flexibility with the design (can use HTML, CSS, JAVASCRIPT and customize visuals)

Now I’m back in grad school, and tryna make my own dashboards. What software do you all recommend that I wouldn’t have to pay $$$ for like spotfire?
Preferably something that can you create nice designs with, ML models, and similar to spotfire?",datascience
"Dear Folks,

 I am working a BI Engineer and have a decent working knowledge in SQL (intermediate), Tableau(basic), Snowflake(very basic), Informatica/ AWS(almost nil since contractors do the Data warehousing and ETL, Data laking etc). The higher management plans to train us in expertise so contractors can be tapered off in few months. What coursework/certification  courses links do you strongly suggest that can help me truly be expert in my role?",datascience
"Has anyone gone through the Oregon State University ONLINE Masters of Data Analytics program? Tuition for this program is partially covered by employer, and I am interested in enrolling. Thank you in advance for any info you can provide. 

I'm trying to compare this degree with MBA from Purdue Global which has a Data Analytics concentration. 

Was the instruction high quality, and would you say the time and effort was worth it? Are there a bunch of exams you need to cram for? Is it a degree mill? 

Were people able to secure jobs or promotions in Data Analytics or engineering with this program? 

I currently work as a software developer full-time and am looking for a shift into Data Analyst or Product Manager positions.",datascience
"Hello everyone, 

i'm training a multi-class classification model  and I would like to ask the following question:

Assuming the model should differentiate between 10 variations (A to J), if A and B are similar (or whatever letters), then how can I somehow give a bias to the model to take that into consideration ? Currently, I am doing a ""hot zone"" in the confusion chart, so that the user knows to take these results with a grain of salt. 

Any hints would be appreciated!

P.S: programming language doesn't matter even a math wikipedia page would be nice!",datascience
"I’m currently a year out from finishing my masters in computer science from a well regarded school, emphasis on software engineering. My background is in chemical engineering so a masters is useful for transitioning fields. I work as a test engineer in aerospace, which is giving me some exposure to software (matlab, labview, networked systems). My masters program has the option of taking an additional 6 courses (about a year part time) and letting me get a second masters in data science. Work would pay for it completely. Is it worth it to get the second masters? Or would a boot camp or online course be more worth my time and effort? I’m not dead set on becoming a data scientist, but it would be nice to have the skills under my belt if a project needed it.",datascience
"Hola,

I'm currently working with a dataset and I'm trying to build a model to predict the cost of a transaction based on 9 different attributes, one of them is a date column (timestamp), but there are quite a lot of observations registered at the same time (exact same timestamp), but these observations are unique.

How should I handle this data to create visualizations over time and feed the ML model?

Thanks in advance!",datascience
"Hi, I am a recent university undergrad from Toronto, Canada. I didn’t graduate from a STEM program but I developed an interest in data and I am wondering if I have potential to get into data science as a career without having to go back to substantial full-time studies. 

I have a BA in urban studies (liberal arts with a focus on urban issues, my studies were human geography focused) and minored in applied statistics. 

In applied statistics courses, I learned a lot of basic statistical analyses types and how to interpret results from them (eg. linear regression, factor analysis, cluster analysis, etc). I also worked with a bit of quantitative data in my geography courses (map data, census data)

Regarding technical skills, I have pretty good knowledge of R and Rstudio. I used it for 3 courses in university and some other assignments. Does it help to also say I understand latex?

As for programming skills, I have the most barebones knowledge of python. I understand basic programming concepts (variables,loops,ifs,functions,etc) can make a simple flask application.

I don’t have the greatest math skills. I struggled in grade 11 functions/algebra and my only grade 12 math was data management.

How should I market my existing skills and where do I need to improve?",datascience
"Hey all,

&#x200B;

I'm in the process of clustering my companies customer base and wanted an opinion, at previous jobs, when I would build customer segmentations, I ran one cluster, did some EDA, and if everything looked good, would present it to the higher ups.

However, for the new segmentation, I'm wanting to cluster based on engagement (sales totals, basket size, etc.) and on behavior (so product interaction like 'bought family products, etc.). My thought was to build two models, one with engagement metrics and then the other with the behavior indicators. 

I'm curious if this would be the right way to go about it. Thoughts?",datascience
"The question states it. Will developers (lets say people who write Java apps) be automated/lose a lot of jobs, because of automation before Data Scientists/ People who build prediction/inferral models. Essentially, which is easier to automated writing efficient code that solves a problem or the intuition/stats skills. I do not want to say it but I think AI will learn Stats more quickly than how to write sophisticated code. What do you think? If that is the case what skills in Data Science will be automated last.",datascience
"Company did layoffs, great culture now feels weird and secretive. Seems like our ‘charter’ is going to change and they aren’t telling us yet, just like cryptic messages and stalling of salary updates. Some concern of ‘finish this up so you can get canned too’.  Haven’t been in a layoff company before, anyone have some encouraging words here (have started looking for a new job as a fallback).",datascience
"I know leetcode/stratascratch are both great to handle the coding side of things, but are there other sites/books/etc. that you found useful in preparing for interviews?",datascience
"Have you ever been asked to illegal things with data? I ask this question because I went through this a few years and want see how other people have handled these situations. My situation started out as on honest mistake where data was accidentally excluded i noticed the mistake and the inclusion changed the results significantly..... this on top of being told my ""results"" were not right when it didn't project good findings for the client. I ultimately refused and insisted that if my manager wants to submit the work they can do the modeling themselves and then lie to the client themselves...
Normally whistleblowing is the best here but when I did that I was further harassed by management.  All the whistleblowing did demonstrate immoral behavior from the top down. Ultimately I was taken off the projects and basically paid to job search. I am in a much better role and dont think it will happen at my current company but, just wanted to see if oneone else has experienced something how would you manage it if it happened again.?",datascience
"Hi, I'm hoping someone with more experience can give me some advice on my DS career, which I'm at a bit of a crossroads with. 

For some background - I have a PhD during which I learned about data analysis, visualisation, statistics, and R. Since receiving my PhD I've had two analyst roles where I've found my knowledge of R and statistics have left me feeling a little overqualified/not having good enough data to do 'proper' analysis work. 

Last year I started an entry-level DS position. I got the job because I have domain expertise and good proficiency in R, which is the language of choice where I work. The job is well paid, comfortable (I'm able to WFH) and I find the work mostly interesting. 

My issue is this: I still feel like a glorified data analyst. There aren't really opportunities for ML (the thing that - possibly incorrectly - I most associate with DS), and I feel like experience with ML in a workplace setting is needed for possible next steps in my career. At the same time, I enjoy analysis and I'm not sure I'd find designing and tuning models full time particularly satisfying. 

I suppose my question is what direction can I go in/skills can I learn to continue to develop my analysis skills, and what types of senior roles support this. It feels from job adverts that Senior/Lead Data Scientists are primarily concerned with pipelines and models etc, rather than working with stakeholders. Interested to hear other people's experiences if any of this sounds familiar to anyone.",datascience
"Hi Folks, Please suggest a use case for 'Leak Detection' in steam trap through acoustics.. Python implementation will be helpful.. Thanks in anticipation...",datascience
"I check reddit once a day, and almost always i see a similar post here. 

I was wondering, what about other industries, are they experiencing the same level of panic, like graphic designers, video editors, copywriters, administrative assistants? Or it is just here, where the evil is sort of the offspring of the industry?",datascience
"Hey, I wonder if anybody has experience using transformer models to forecast demand/sales in an e-commerce setting. Some questions in this regard:

&#x200B;

* Did you observe superior performance to classic ml-algorithms?
* Any tips when applying transformers for time-series data?
* How computationally expensive where they compared to classic algorithms?
* Would you recommend using such models for demand forecasts?

&#x200B;

Thank you for any shared thoughts, resources, articles or threads!",datascience
This medium article is a summary of the paper ChatGPT is not all you need where you can find a review of generative AI models. [https://medium.com/@eduardogarrido90/chatgpt-is-not-all-you-need-a-quick-summary-of-the-generative-ai-taxonomy-e2b8b47a9851](https://medium.com/@eduardogarrido90/chatgpt-is-not-all-you-need-a-quick-summary-of-the-generative-ai-taxonomy-e2b8b47a9851) Enjoy!,datascience
"Per title - I made a pretty bad mistake at work related to our cloud platform and resources by inadvertently compromising a key (which will very likely affect the billing for the month) through a CI/CD workflow I had wrote. Its since been 'fixed' and I have been reassured it's actually happened before (EDIT: the keys been compromised before but I don't think to this extent) - but I still feel horrified and upset about causing the issue to begin with.

Does anyone have advice on how to best move forward from this incident with my mentors and boss? I am afraid the team will second guess assigning me more tasks/opportunities in the future.

Edit 1: Also if you could share mistakes you've made at work that would make me feel better too

Edit 2: Thanks for the replies - there's been a lot of great advice commented and its definitely helped soothe my nerves :)",datascience
"Weird question, but is it hypothetically okay to post about a rescinded job offer if the company is well-known compared to your current industry experience?

Eg, if you are an early career data scientist fresh out of school, and you place into Google. Then due to current economic conditions, your offer was rescinded. I imagine the mere act of being accepted by Google is something you might want on your resume, especially if you don’t have any significant experience before. Not sure though.

Any thoughts?

(Yes, this is hypothetical but the question might be pertinent to others right now!)",datascience
"Good morning everyone.  I am in need of help with my Data Science project for the completion of my course.  I'm in need of themes (I can't think of anything), a problem that needs to be solved and a set of data (dataset/database) for me to work on. 

Please folks help me!!",datascience
"
I was wondering if anyone here is familiar with the written by Dr Tony Van Gestel and Bart Baesens?  It is supposed to be a series of 3 books (1st one being credit risk management: basic concepts) but I can't seems to find anywhere the other 2 books.... Can't even find a title for them on Google.

Anyone came across something similar?",datascience
I am in 2nd year of Phd in CS. Although i have some time.  i want to transition to data science eventually in the industry after I graduate with my phd. My research interests are NLP + Computer Archietecture. What can i do currentlly (any courses etc) to eventually smoothen my transition to data science industry.. I am proficent in python and statistics,datascience
"I am working on a demand forecasting problem for an FMCG company and I was wondering how I should use the product price as a feature in my ML model. For some products, the price doesn't change much and is more like a step variable than a continuous one. How should I capture the effects of price change on the change in demand? Should I take the absolute values of price or delta values? Or any other feature engineering technique?

Note: The timestamps are monthly and I have other features about product category and distribution hierarchies.",datascience
"Perhaps I am alone in the walled garden that I’m forced to work in.  I do population health analysis of approximately 100,000 lines of varying data.  The nightmare of my life is that I cannot use any scripting.  No python, r, macro-enabled workbooks.  For visualization,  can’t upload to anything as it’s protected health information.  No Tableau, no online SQL database, no power BI.  All I can use is Power query/pivot.  

I admit, I can adapt and overcome with being hamstrung like this.  However as the data grows, Excel is starting to give out on me.  I’m looking to see if anyone has methods that they use to clean data and present basic visualizations outside of excel.",datascience
"I have a huge dataset with more than 100k rows and 30+ variables. 

A lot of articles online mention that the optimal value of K is sqrt(n) where n is the number of samples the model is trained on. 

Is that the best value of k? A high k value also makes computation harder and takes a longer time. Is there another way to find the optimal k ?",datascience
"Hello,

I'm currently working as a data science staff on a rather small startup company for 2 years now. Considering our size, I've been making several analytical dashboards that helps both operational and marketing team. I've been doing lots of data viz (mostly in echart), analyzing data in SQL (mostly Postgres), and building automation in fetching and doing said analyses (mostly in Airflow).

I used to study in machine learning during my uni time, but it's been quite a while and I felt my AI knowledge has been rusty. I'm looking to improve my skills in data science or AI related field hopefully to advance my career, and also helping me in landing job in bigger company.

Does anyone have recommendation for some courses, possibly those that ties with certification at the end of the course?

&#x200B;

Thanks for any suggestions given",datascience
"Those of you who've used `sklearn.metrics.classification_report` will know what I'm talking about, but the function prints the class name, precision, recall, f1-score, and support. This function builds on top of `sklearn.metrics.precision_recall_f1_support` but I'm wondering why only support is taken into consideration and not the number of predictions.

In my job I often find it useful to tune my models or preprocessing according to how many times the model has made a prediction for each class. The current functions don't support that. I've made small changes so that it does but I'm wondering if it's convention or something to only care about support.",datascience
"Hi, it’s my first time posting here and just really curious. I’m currently in my final year of my master’s in computer science. And I intend to work full-time as soon as I graduate. I am hoping for a data analyst or data science or machine learning researcher position. However, I still feel inexperienced tho. The only things I got to show on my resume are: 1) a data analyst internship at an insurance company, 2) some research projects that involve analysis of large datasets and use of machine learning and deep learning models (with publications to journals and conferences), and 3) my master’s thesis.

So now I am on the hunt for jobs, and idk how much (average) salary I should be expecting (other than the baseline from my previous internship job). I fear that I might find an offer that will pay very little and then I’ll accept because I don’t know the numbers. Would anybody be able to give a rough ballpark of how much these positions usually pay? I’m hoping to get into an entry-level (or new grad program) first, since it’ll be my very first full-time stable job. Currently living in Canada and idk how much salaries compare with that of US.

Also, just to give context, I started right away with my master’s right after graduating from my undergraduate, as I felt the need for a stronger education background before coming into the field (was also pandemic times when I graduated so it was just tough to find jobs then).

And any advice you could give for a noobie? It would be appreciated. Thanks!",datascience
"I’m interviewing with them tomorrow for an analytics position and would like to know about the culture, work load and growth of the business.",datascience
"“Man made machines make music for the man

Now machines make music while the man makes plans

A second generation a generation lost

While the man prepares for the holocaust

Man made machines to control the days

Now machines control while the man obeys

A second generation a generation lost

While machines prepare for the holocaust”

A Flock of Seagulls, Man Made

Songwriters: Alister James Score / Francis Maudsley / Michael Score / Paul E Reynolds

1982

https://m.youtube.com/watch?v=sTk4jz9OBhI

Seems apropos with all the spam about generative models to reflect on them through the art most of y’all would rather let them make instead of letting humans make.",datascience
"I recently finished my first year in university for Data Science and completed an internship for data analysis over the holidays. I would like to gain some experience and work online on projects, not solely for the money but also to earn some extra cash.

I am a beginner in the field, and I was thinking of researching the most in-demand skills on Upwork and learning them. For example, scraping data from a real estate website to predict prices. I plan to practice these skills using online data until I feel confident in my abilities then get started applying for jobs on Upwork. I also am considering cold calling local businesses.

Is any of this a good approach? If not, could you suggest a better plan? Please be radically transparent.

I posted this on r/careeradvice and got no comments, so I thought this would be the place to go.

tldr: finished first year university/college (data science) and would like to get started freelancing, where do I start?",datascience
"In beta version, two json files are automatically matched

To gain access you need to follow the links on the website or contact me.

Let me briefly remind you of what we do.  
**Why do we need it?**

Our system comes in handy for reducing the cost of docking systems and subsystems produced by different people and teams. Here are only some of the tasks which can be solved in automatic mode almost without human involvement (we also do not use neural networks):

\- combining similar data from heterogeneous sources in MDM systems

\- automatic integration of systems

\- searching for similar or related texts in text arrays

\- data cleaning from misprints, by dictionaries, etc.

**What does it give?**

We save you time of experts when matching data at the level of specific fields (mapping), our system can do it automatically and without significant cost of computer power, training neural networks and the like.",datascience
"Hi everyone, I am an international student and  I'm planning to join the online MIDS (Master of Information and Data Science) program provided by UC Berkeley. I was wondering if doing the course online with no F1 visa would make it harder to land a job in USA? Or if I should do an in person masters in usa (in another university) since that would be comparatively easier to get a job?",datascience
Is there a Discord/Mastodon/Slack/Meetup/whatever for data scientists/analysts working remotely? Preferably for people that live in a small city to a rural area.,datascience
"Hi guys !

I would like to get some feedback on a video that I made to showcase a personal project. The video is subtitled in both English and French, and the source code repository is available in the description. (Don't mind my French accent, what I'm saying is what's interesting)

Positive or negative feedback is appreciated !

[GitHub link](https://github.com/EbonGit/Epidemic_Regression_Model)",datascience
"I work in marketing and report sales data on a weekly basis. 

So far, I've been finding out new items to report on by fiddling on Excel. I report on things like:

- the day with most sales
- proportion of item sales volume compared to overall sales volume
- finding relevant news to speculate on correlation to sales


Since I'd like to find more sales insights, I was wondering what else I can learn.

I should probably improve upon:

- theoretical knowledge on business data analysis, to find more items to report on
- using Python for data reports

I have used Python in the past and am rather comfortable with it. I think it might be better to transition to Python as Excel is not performant with high amounts of data.

**Q:** Please let me know what topics and resources I should explore.",datascience
"Hey, I want to persue a data science career, I just graduated and have been having trouble landing a data science job however I just got a buisness analyst offer from GM, would a future data science job value a buisness analyst experience? should I take it or keep trying for data science? Its been 3-4 months and Im not in hurry since im currently on a internship but still I don’t want it to be another 4 months,  probaly around 4 months more and I would take anything. Appreciate every opinion, thanks!

Edit: Important part I studied engineer on data science and mathematics",datascience
"Looking for any suggestions. I have a dataset of a few hundred thousand records and need to use some features to properly map to another column (classification) but the column has over 1000 possibilities. Was curious if there is any sort of rules engine that could generate rules based on a few features or if I should just stick with something like a random forest or decision tree. 

Will also add that most features are text as is the target column, so maybe there could be something in the NLP space.",datascience
"I will be interviewed for an Insurtech. They will send me a case, give 4 days to solve it and after interview me for 1h hour to discuss it

Considering the position and linkedin of the interviewers I expect some modelling. Possibly a GLM (standard in insurance) vs ML, etc

Anyway, but rather than that what more should I expect? Also, where can I prepare for it?

I was already looking for some material that people use for case interviews in consulting companies and they seemed really interesting with the approach and reasoning thinking for solving the problems. But I don’t know if it is the right resource for a case interview in DS",datascience
"Hi there, I am buying a laptop for personal needs as I want to work on data science projects and ML tasks privately (with an aim to become a freelancer in the future, so I need something durable).

At work I use HP Probook 440 with 11th gen Intel and integrated graphics.

Privately, I wanna be sure that my laptop will last for many years.

Would you be so kind and share what are you using and how satisfied are you?",datascience
"Hello. relatively new data scientist here. I was recently trying to see how to build a model in my situation.

Basically I have a bus data of past five years that run regularly each day, and bus has all kinds of information including (# of passengers per hour, revenue earned, origin, destination, etc). Then we wanted to increase the frequency of buses running in preparation for some upcoming festivals during this weekend in the regionsome time late 2023. 

1. The question is I need to figure out how many additional riders this additional services (increasing bus frequency) will bring and I was having hard time predicting/figuring out how. 
2. Instead of additional bus frequency, let's say we introduce additional taxi boats to take people to the festival, then given two different modes of transportation happening (bus and taxi boat), how do I create a collective model around it? I also have past five years of taxi boat usage data.

Any help/advice on how I should approach (or some kind of model I should use, Bayes?) this appreciated because I was thinking all day and a bit stuck. Was thinking of doing some blunt estimated simple math but wans't sure if that's even accurate. Figuring out the effect of what happens when something happens has been challenging.",datascience
"I also asked this on math.stackexchange.com but they said should better ask my question in this section.

I have a review system and I stumble over the fact that I want to make a fair ranking and not just the average. The average is allready calculated but when you are sorting on this average the results are not fair.

EXAMPLE:

Company A has an average score of 4.8 based on 5 reviews. Company B has an average score of 4.7 based on 43 reviews.

If you rank on average score Company A is above Company B in ranking but it ain't fair because it is a lot harder to get a 4.7 based on 43 reviews.

I want to create a number that is based on the average review score and the number of reviews that are submitted.

Maybe I am wrong here for asking this question but what is the best formula to create such an average and wich values do I need for creating this?

I am developing in PHP if this should be important for you to know.

I allready found something like this based on the bayesian average, is this Something that I should use?",datascience
"Hi all!
I have a dataset composed of sensors (time series) and other data that is fixed and represent kind of domain knowledge (let’s say as example data of the soil composition, but also other).

What state of the art methods do you know to combine these two different types of data? Or is there any type of standard method used only on time series that is used as standard and is possible to extend?

The final goal is to recognize the anomaly earlier than its occurrence",datascience
"When you're at an interview and they ask if you have any questions, which ones do you usually ask? I ask about the work environment and the workplace culture, and typically why the individual I'm speaking with wanted to work there, if they haven't already told me. Which ones do you ask?",datascience
"I am an incoming MS student deciding between programs. For my dream job, I definitely would prefer quantitative-heavy positions such as machine learning engineer or quantitative analyst as opposed to BI developer or data engineer. 

Eliminate factors such as institutional prestige, cost or alumni network, and simply look at statistics vs. data science. Which curriculum gets me the most advantage in early career? If you have picked up one degree over the other, can you share whether it worked in your favor?",datascience
"I was offered to work on a project as a freelancer/independent contractor. I'm having trouble coming up with the pay rate. 

The project involved a dataset of 8k rows x 351 columns of data. I'm supposed to structure this data set differently to fit a Salesforce template they have. This would increase the rows by about 3x, but reduce the columns to about 50. They told me they would consider doing this manually, but they know that it can be done with a python script (which is where I come in). 

I have no idea how long this is gonna take me, since the data is pretty sensitive & old. It goes back to 17 years and it seems to be poorly maintained. 

Anyone have any idea on how I can start pricing this? Or suggestions/things to consider? TIA.",datascience
"Hello Everyone,

Recently I got an email for an Interview which requires me to give a 90 mins coding test for Full Time Analytics Data Science Associate at JP Morgan. I want to know what coding questions can I expect. Has anyone given the coding test recently? The coding test is in the platform Codevue. Any lead will be appreciated.",datascience
"I'm a little more than a year into my first data analyst job and recently graduated with an MS in Data Science. The program was great and I feel comfortable saying that I'm probably at about an intermediate level given my degree and work experience. However, I'll also say that while the program gave me a good understanding of a lot of different subdomains of the field, it didn't necessarily allow me to become an expert in one particular subject matter. 

I know a big part of being successful in this field is research, continuous education, and being aware of new technology and advancements in the field. I'm looking into subscribing to an online education platform and wanted to know A) is this common for someone of my level? and B) what platforms should I look into? 

For context, I have experience with all of the major names - DataCamp, Dataquest, Udacity, Udemy, and Coursera. I get the impression that the first three I named are geared much more towards beginners and probably aren't necessary for me. I like Udemy for the ability to buy classes on demand and keep them for life though I feel like some classes can be hit or miss. Coursera Plus is on the pricier side ($200 a year), but getting access to classes by college level professors could be beneficial. 

Just wanted to throw this out there and ask if there's other platforms or educators I should be looking into. Thanks!",datascience
"Hi All,

Please note that I have googled this, but want practical advice from real people who work in this industry.

I'm working on a project that involves a lot of exploratory analysis with regards to a specific dataset. Currently, I am using a local instance of Jupyter Lab (the desktop GUI) to write my code in Python and work with the data. I love that Jupyter allows you to organize your code into cells, but I feel it doesn't go far enough. I'd love a similar tool with one or more of the following features:

1. **Creating ""Projects""** for organizing my work.
2. **Cataloging and ""typing"" Cells** \- Ability to save ""cells"" as functional units (or steps) and save/share them for reuse. Cells would accept certain inputs and provide certain outputs.
3. **Creating workflows** \- I'd like to - for a given project - stitch multiple of the above-mentioned cells together to create standard workflows for various data science tasks, and ultimately arrange 1 or more workflows to run in parallel, combine output, feed into other workflows, etc.
4. **Defining / cataloging data sources** 
5. **Automate various workflows (tasks)**
6. **Is Open Source and/or free.**

I have to imagine that a tool like this exists - something that allows you to graphically arrange, monitor, schedule and/or run the code you write. I've used RapidMiner before, and while this was basically what I'm looking for, I would prefer an open-source (or at least low-cost) alternative (I am using this for personal projects to learn for my future career).

Thanks!",datascience
"I have been a data analyst in a medium-sized healthcare organization for two years now. There’s rumors going around that I will be one of the few people they are going to lay off. I figure that it is about time for me to move on and learn something new anyway. I am wondering if there’s any fun industry from your experience. Working in healthcare organization for me was alright, i wish there’s a bit more flexibility. We also use Power Bi and I am getting tired of this dead brain application. I am not learning anything new beyond power bi. I’ve seen other data analyst position where they use tableau or python panda. I am curious to see how those tools work. 

Got any suggestions? I would love to hear your experience.",datascience
"For context, I work in a medium-large financial org (2-3k employees), and have the ""privilege"" of being the first data scientist at the company. We have an 8 person data team for reports, dashboards etc but no DS. I'm looking for any advice from people on

\-Identifying/gathering use cases

\-Establishing DS culture

\-Documenting and governing models 

\-Building a team, creating standards, etc

Really just looking for any advice or guidance that could help me in this journey

I've been in this role over a year now, and we have had great success with some smaller use cases and poc's , but its time to roll out an org-wide program and just struggling for direction. Open to blogs/articles/books etc that people think could be helpful too",datascience
"I’ve been working as a data scientist for over a year and will soon start a new role as an ML engineer. 

My background is in mathematics and so far I’ve been focusing on learning the fundamentals of ML, CV, NLP to a reasonable standard. Also have used AWS and Azure a bit.

My main problem is I have no idea how to use GitHub or how to deploy a model to production. Not sure if there is anything else I show be learning too like computer science fundamentals. 

Please can you recommend topics and resources for what you think I should be learning next? Thank you.",datascience
"Hi all. I apologize if this is not meant for this page but I am pretty lost. I am trying to switch careers and move into the data sphere, and boy is it tough. I started a course on coursera and have been a bit disappointed in the instruction so far. I would have posted on the discussion forum but no one responds so figured I’d take it here. I guess my post has two intentions:

1. Does anyone have good experience with coursera courses that do a good job teaching some different languages from scratch? 

2. I haven’t yet been able to figure out how to get a combined total of a count. 

Let me give the example - they are asking us to list the cities with the most reviews. The data they have provided gives review_count and city columns. When I write 

Select city, review_count 
From business 

I get 10,000 rows back, and there are multiple rows with the same city. How do I get one count of how many reviews come from each city. 

TIA",datascience
"Idk why so many people who get a job with ""data"" in its title think that the world awaits for their biography and advice. Its even worse that most of these people just want for an excuse to humblebrag. Even when thats not the case, the actual advice is almost always without substance. I dont want to hear for the n-th time how you worked on ""real problems"" or how you studied ""smart not hard"". 

 Why theres this culture? Other fields are competitive too and rarely you see this bs",datascience
"Given that I am from mechanical backround, and is 26 years old fresher. With a master degree in mechanical domain.

I am looking to do some Coursera certification course plus some internship for entry level.",datascience
"Hi, I’m trying to do conjoint analysis and bot sure what to do with duplicate cases. Assume that the different respondents were presented with the same product type and they ranked similarly. So it seems that except the ID the rest is the same. I know that I need to remove ID column but after I’ll have duplicate rows. What should I do with them?",datascience
"Yo first up I am sorry for YACGPTT (yet another Chat GPT Thread). This is a question about leaning resources.

For some context I did the Coursera NLP Cert a few years ago and really enjoyed it. And it does, in very good detail, hit on the nuts and bolts of manually coding transformer architecture (the ‘T’) . 

But like I said it’s a few years old and I’m looking for resources on the combination of reinforcement learning + transformer architecture.

Again,  but nuts and bolts I mean code heavy manual demonstrations of how to construct the architecture.

Thanks in advance and if you think I should delete and post in r/learnmachinelearning I will do.",datascience
"I came across a few videos of Mike West on YouTube and he seems to give realistic advice about the DS job market. I've been thinking about signing up for his courses on his website called Logikbot.

It's $50 for 3 months and he has a bunch of courses on the website. He keeps talking about how most ML jobs are just SQL and Python - I am unable to judge how true this is because I have zero exposure to the field.

I haven't been able to find any reviews on Mike West or Logikbot. Does anyone here have any idea if this is a reliable guy and would you recommend signing up for his courses?",datascience
"Is the answer bioinformatics? 

How should I start? Where do I begin? 

I want to make a lot of money. Which MPS degree should I pursue?

Masters in professional studies in 
1. machine learning
2. Data science and analytics
3. Bioinformatics and Computational Biology


My background is in biology. I hate doing lab work now. I want to do computer stuff. 

I know R and Python at the intro 101 level. I have not taken object oriented programming.  I know some discrete math but not a lot.",datascience
Hello all. I have an interview for a job coming and I just wanted to check and see if anyone was familiar with the role of a Data Integration Manager(U.S.). The job posting wasn’t the clearest on exactly what hat they do and so far I’ve seen a lot of different explanations of what the role is so I’m wondering if it’s just a catch all term. Anyone every hear of it? If so what do they do?,datascience
"If anyone can assist with feedback please do. Can forward job role or post it on here. I've already prepped some just looking for feedback or insight. Meeting with hiring Manager.

Education: Bachelor of Science Comp Sci  
Certs: Professional Scrum Master & IBM Professional Data Science ",datascience
"Hello r/Datascience!  

I work at a small Midwestern College outside of Chicago as a Research Analyst in the Office of Institutional Research and Effectiveness.  There’s a grand total of two of us in the IR Office, and the other – my boss – is an economist by training, traditional OLS methodology.  We get along well. However, he doesn’t know R or Python and does not involve himself in much modeling at all – no time with other duties.   

In the Summer of 2021 I completed my MS in DS and the powers that be here (my boss, our VP and our President) have been asking us to do some enrollment projections using some modeling techniques outside of what has “traditionally” been done by our office. 
So, using R, the Tidyverse and Tidymodels I developed a set of models:

* A penalized LogReg model
* A Random Forest model
* An XGBoost model
* A MARS model

They all seem to perform well on both the training, test, and holdout sets, so I’m considering making something actionable out of them.  I would normally just throw together a Shiny app and call it a day.  That’s easy.  However, I know that we’re going to move towards Tableau and we already have a BI application on campus.  

I feel like my education and Googling skills have failed me because I cannot for the life of me figure out how to take a model and “hand it off” to a developer?  What happens if I want to roll a model into a custom application?  Or into Tableau?  Is it possible to “translate” these models into pseudo code that I can give to a developer for translation into say C++?

I’m on my own here, and don’t have any infrastructure to speak of outside of my laptop and a ShinyApps.IO account, and I don’t think I could win the fight for budget dollars for something like Docker.  Plus I don't really know how to use Docker anyway.

Is anyone in a similar situation? What have you found to be the best course of action in solo environment?",datascience
"around 3 years ago I was trying to make a TTS AI model, I got the entire dataset, audio voice lines, transcript, the whole thing, I had everything setup in Python and I was using linux.

The issue was I had a GTX 1080 and I wasn't using any cloud services to train the Model, it would take me around 2 weeks of non-stop computing to get it to work, So I was stuck with experimenting doing it a few hours at a time.

So now I want to actually do it for real, however the Friend that was a python expert helping me, we are no longer in contact, and I do not know jack about Python or coding.

So Redditors, Please tell me what I can do to take my Dataset/WaveRNN thingy that I have setup both on my Windows PC or my Linux PC, and how do I use Microsoft/Nvidia cloud computing to train my TTS model within hours instead of weeks?

The Final product I am aiming for is:

Me writing Text into a promt, and it will generate a TTS voice just like the voice I am trying to make, and eventually be able to use it on Twitch.tv for fun or in-game.

It would be similar to how Twitch Streamer Forsen uses multiple different TTS voices.

Thanks in advance

https://github.com/dipjyoti92/SC-WaveRNN This is what i Used",datascience
"Basically I got a job as a data / pricing analyst and to be honest, there's about 1-2 hours of actual work most days, and the rest is chill time. I kind of feel and feel like there's something productive I should be doing to ""get ahead in my career"" or give me advantages in terms of a future data science / project manager / business analyst role or something, like maybe machine learning courses online, etc. Idfk. Was looking for some tips",datascience
"Yesterday I finished Google Data Analyst Certificate, yeah for me.

Alas, I know that it's just the beginning and not the end.

The scope of this journey is to level up at my job (ops/administration heavy on spreadsheet-based data) and possibly pivot my career into Data Analysis.

I know that the Future is rich in both SQL and Python, but which one first and how to tackle it?

I was thinking about doing [DataCamp's Data Scientist with Python](https://www.datacamp.com/tracks/data-scientist-with-python) and - at the same time - doing some projects to apply my Python learning and learning some SQL *on the job* so to speak.  I can also do the https://www.datacamp.com/tracks/sql-fundamentals.

I have already purchased the DataCamp membership with their 50% off sale, so there's that sunk cost.

Any feedback will be appreciated.

TIA",datascience
"Hey, DS gang. I have to vent a bit and ask for advice/opinions. I'm a DS in a mid-sized company, and I find myself in a situation where I want to quit my job like today. I already have an offer and am interviewing with multiple other companies, and things are going well on that account. 

At home, I'm taking care of someone who needs around the clock supervision, and I do have help with this, so I'm able to work with minor interruptions during the day. But my company has been working me to the bone the last few months, and I have a deadline for a project that involves a new large source of revenue for the company. I do want to get them over the finish line with this, but I really don't want to spend any more time than that. I feel exhausted. 

I also live in a medium-sized city, and so everyone knows everyone in the tech field here. 

Given all that, in terms of a one week notice, two week notice, or waiting until I can do proper hand-offs, what should I do? I also have to say I have an amazing team (management is pretty garbage, though), and I'd be sad to leave them in a tough spot. What would you do?",datascience
"I am about to complete my Economics PhD and am currently transitioning to DS.

I have 2.5 years of quantitative finance and algorithmic trading experience and also ran the trading club at my university for 4 years.

Yes, I know that I could go back to finance, but I want a new challenge with a better WLB. 

Would these 6.5 years of QF and AT be considered as relevant job experience? Should I add my projects to my DS-related GitHub and portfolio?",datascience
"Hi.

I'm looking for an example of a good github repo that can be used in CV.  
I have a lot of work that I'd like to showcase with out of the box thinking approaches that I have utilized before.

If somebody can please link me to a good github repo that they see is if high grade, please let me know.

Thank you.

Regards.",datascience
"Hi dear community, 

I'm a big data analytics student. 

working on my end of studies project, it's about sentiment analysis on Facebook posts (Comments), comments usually are written in a local language ( Algerian Dialect) 

I want to know if there are some technics I should be using to train ML models on this dataset . 

Your help would mean a lot. 

I'm also open for any questions regarding this topic , let's exchange on this one .",datascience
"Hey, I am currently working on a regression task for a course that I am attending.

&#x200B;

I chose a Dataframe that contains information about apartments on a renting website and decided that the rent including heating (Warmmiete) is my target. 

&#x200B;

**Note:** In Germany we have a base rent (Kaltmiete) to which heating gets added on. The sum of those two logically is rent including heating (Warmmiete)

**Those are my features:** 

* Base rent (Kaltmiete) - float
* Heating cost - float
* Number of rooms - float
* Number of photos (on website) - float
* Elevator - bool
* Balcony - bool
* Year of construction - float
* Federal State 1 (one hot encoded) - bool
* ....
* Federal State 16 - bool
* Garden - bool
* Cellar - bool
* Kitchen - bool
* Newly built - bool
* floor - float
* Living area - float
* Type of flat 1 (one hot encoded) - bool
* ....
* Type of flat 10 - bool
* Condition - float

&#x200B;

After cleaning and splitting my data I used MinMaxScaler before calculation the R2-Score of a few regression models. It turns out that Linear Regression and Random Forest both returned a score of 99% while the other models where also laying around 90%. 

&#x200B;

In a second try I threw out the base rent and heating cost out of my dataframe and got worse scores that go from 60% to 81% (SVR). Adaboost only scored with 7%. What confuses me is that in the second try all MSE got very low. Almost all of them are in a small number area like 0.05 for example.

&#x200B;

**My question is:** Which method is the better one? Seeing that my base rent is a feature that will predict the rent including heating it seems logical that both have a strong linear relationship. But does predicting the rent including heating even make sense if I have my features base rent and heating before the prediction? I tend to use the second method with some hyperparameter tuning. But I am still concerned about the small mean squared errors.

&#x200B;

If you have any recommendations on how to improve my data, model or methods I will be really thankful!",datascience
"I am already working for a startup. Things are good and only problem I have is there is no senior to guide me. I was handling all the tasks so far and I was really putting my soul in thr solution by finding the best possible solution online and applying it.  

I had this interview today. It is one of the most prestigious banks in my country.  Tbh questions were pretty basic. And i failed to answer them througly since i havent used them. what made me happy is I almost did it. All the questions asked were thr things that bothet my mind and i was looking for a chance to actually deal with them during real projects. Not just reading about them. Because there is a lot on my plate rn. (non CS background ) 

I realized I was stressed out at first but then I realized i really enjoy the questions. This is a first for me. I really enjoy this field and working feels like solving puzzles. I just wanted to share this. I am already feeling too old (29m) but I like that i am sure this is what i would like to do.",datascience
"My backround:
10th : 87

12th :87

Grad : BE mechanical : 86  ( 2019) , Tier 3 college . 

2 years gap then 

Masters : ME mechanical ( 2021 -23 ) 

>I think I can handle mathematics alright

>I know little bit of MATLAB

>Dont know programming 


And will my background affect my career in DS , If i can pursue one.",datascience
"Hello I'm studying my last year of Data Science at university. In my country it's very common to do a master's after finishing a degree, and I feel like I should do one but if I search for data science related topics, I see that I've already taken 90% of the courses. I'm unsure of what to do... 

Option A) Do a master in ML or similar to compliment my data science degree (even though it's not my passion it can be useful).

Option B) master in BI (I really like the topics but most of them are not focused on programming and that's my passion...).

Option C) Start working instead (I feel like I could do that but I do have family preassure behind me doing a masters since it's the norm where I live...).

I would really apreciate your thoughts on what you think I should do.",datascience
" I'm currently doing my Bachelorthesis about Microclimatic Effects of Green Roofs and I model the clima for 3 days with ENVI-met.

I will get lots of data which I have to generalize and analyse and then visualize. Basically I want to make some statistical tests if there are significant differences between different fractions of Roof Greening (Compare 100% Greening to 75%...).

I'm asking myself which language would be the most easy for this. What I found out is, that visualization in R can be more ""sexy"". On the other side I have no experience with R and a tiny little bit with Python and I don't have much time for learning.

I already plotted one graph with Python and I know how to load csv's and can work a bit with pandas. I don't know how easy it will be to do the statistical tests with Python or if doing the tests with R would be more straightforward.

I'm happy if someone could tell me what approach would be better: Combining Python and R or just using Python because I already somehow understand how the language works.",datascience
"hi right now im pursuing bcom and i want to do masters in data science can u suggest any short courses so that my college application is strong .

thank u",datascience
"Hi,
I am new to data analytics. 
And just got an internship. 

Have to update these google sheets everyday and was trying to automate it.
Is there anyway i cab automatically update these sheets, that is new data gets added every 24hrs or less?
The things is, it needs to go through my email as the data is available through access only and I have the access..

For now, I download the data using a link into csv file everyday and upload it to the Gsheets.

Any suggestions of help is very happily appreciated.?",datascience
"Hello Everyone, 

What's the potential for process mining in future? and how worthy a PhD would be to work as a researcher in the industry?

Cheers!",datascience
"Currently, I'm working on a problem where I'm given a tiny (10k) dataset of labeled, cleaned test images from user interaction for a recommendation system. As it is too little to train a proper model (even with a pre-trained model), I decided to use the rest of the dataset (2M images). I have some labels for each image (as the user makes the decision to click or not, also I know the origin of this image). However, using this noisy dataset directly have a weak correlation with the cleaned dataset. To better understand the problem, here I'm learning an embedded model, without any classification loss.

I was trying to find a way to filter out the images with obvious noise automatically or not aligned with the cleaned dataset. Here are my thoughts:

1. Use some pre-trained model, compute similarities between images in the class, and remove images with low similarity scores -> this one works better than a noisy dataset but the results are still not so good
2. Use some method to remove images not correlated with my test set. Like removing chairs if I have only cars in my dataset. I know that it may introduce a lot of bias, but for simplicity let's say that our test set is perfect -> I was not able to find any method for such a case. The only method I'm thinking of is checking the gradient alignment between train and test images. And if alignment is poor, remove the example.

And my questions:

1. What do you think about using the gradient alignment method for filtering train datasets using a gradient from the test set?
2. Do you know any other method for properly filtering the training set that does not rely on loss values (like removing data points whose loss is way higher than ex. mean value)?",MachineLearning
"I've  loved using wandb because my workflow is using a university-provided  slurm cluster that doesn't allow any internet on the compute nodes, and  it's annoying to have to keep doing 2fa just to evaluate results. It's  offline mode lets me sync that on a little script in the login node, and  see everything in an online dashboard.

However,  the software is super unstable. I've been losing jobs randomly to a  mystery error \`Killed\`, it's piled up runs and insisted on syncing all  of them again, so I have to go in and manually delete old runs that have  long been saved in the dashboard, and it already took me forever to  figure out how to keep it from logging a seperate run for each gpu I use  to train. Is there anything that does the same thing, but is just more  mature, so that I don't have to spend all my time squashing bugs related  to data logging?  I'd rather just focus on training these models,  honestly.",MachineLearning
"It is a low code machine learning library written in Python to develop, evaluate and deploy automated Machine Learning models and pipelines.

The tool helps has following features:

&#x200B;

* Native integration for data extraction with MySQL, PostgreSQL, MS SQL, Oracle, MariaDB, Amazon Aurora and Amazon S3
* Exploratory Data Analysis (EDA)
* Data preprocessing
* Trains data across multiple algorithms and provide comparison metrics
* Hyperparameter tuning
* Experiment tracking
* API deployment

&#x200B;

You can check out the project at :  [https://github.com/mist-projects/bluemist-ai](https://github.com/mist-projects/bluemist-ai) 

and would love to hear feedback from the community :)",MachineLearning
"Hi,

I've been reading up on the backpropagation algorithm used in artificial neural nets. After finding out about automatic differentiation, I wanted to implement it myself. The implementation is fairly simple using Python (that allows for operator overloading and has a garbage collector), but I wanted to see how much it differs from the implementation in C. I wrote up a general overview of autodiff in the readme of the repo.

If there are any remarks/feedback, let me know :)

 As a result, here is the repo: [Autodiff](https://github.com/Janko-dev/autodiff)",MachineLearning
"I have to train an UNet-like architecture for semantic segmentation with 200 outcome classes. When outcoming a final map of 4x200x500x500, batch size of 4 and 200 channels (no. of semantic classes). It blows up my GPU memory (40GB).

  
My first thought is only to create a broad category to reduce the number of classes. Does someone have a suggestion or tricks to accomplish this semantic segmentation task in a savvier way?",MachineLearning
"I don’t have labeled tickets, and I doubt an unsupervised approach would yield good results given the wide variety of problems that are addressed in the tickets. 

The only other approach that comes to my mind is using a pre-trained model, but since the tickets are not in English the only models I could find were trained on tweets/product reviews.  

Has anyone ever done that with good results? Do you have any other approach to suggest? 

Thank you.",MachineLearning
"Deep Graph Library (DGL) just announced its 1.0 release [https://twitter.com/DGLGraph/status/1629026413537026048](https://twitter.com/DGLGraph/status/1629026413537026048). A big milestone of the past 3+ years of development. DGL 1.0 empowers the new technology of Graph Machine Learning for everyone. A couple of highlights:

* 100+ examples of state-of-the-art GNN models, 15+ top-ranked baselines on Open Graph Benchmark (OGB), available for learning and integration
* 150+ GNN utilities including GNN layers, datasets, graph data transform modules, graph samplers, etc. for building new model architectures or GNN-based solutions
* Flexible and efficient message passing and **sparse matrix abstraction (new addition!)** for developing new GNN building blocks.
* Multi-GPU and distributed training capability to scale to graphs of billions of nodes and edges

Check out the release blog [https://www.dgl.ai/release/2023/02/20/release.html](https://www.dgl.ai/release/2023/02/20/release.html) for more details. The team is more than happy to hear feedback and suggestions from the community!",MachineLearning
"I am working on a research project where we train end-to-end task-oriented conversational agents (e.g., we solve the MultiWOZ benchmark) with novel techniques. We want to create a simple demonstration that can run on lower-end hardware and is highly visual to demonstrate the abilities of a TOD system to young audiences - e.g., high school students. At the same time, it should be complex enough to convey even to researchers that our models, when trained with data different than the toy problem, would provide substantial results. To understand me, I already have one idea, which however is not convincing enough: Imagine having a geometric shape on the screen, and you can move it to different points on the screen through the language model, e.g., you can have complex queries such as ""Move the point twice as far from the edge as it is currently."" We can autoplay the demo with different pre-set queries to move on a screen, and when someone wants to interact with it, they could replace the pre-set questions with their own. This is visual and interactive, as I want it. However, it is insufficient since it doesn't convey a strong message by being overly simplistic and only moving a shape on the screen. On the other hand, out-in-the-wild demos, such as ChatGPT, also don't work for us since, to be sensible, they require a lot of resources and are not visual enough by being text-only.",MachineLearning
"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",MachineLearning
"I've looked into the different interpretability methods for vision transformers, and most of them seem to rely on the attention map of the class token.

This is fine, but I use average pooling for all my tokens in the final step, so I cannot use methods that rely on inspecting attention affects the class token.

Are there good interpretability methods for vision transformers that don't rely on the presence of a class token?",MachineLearning
"Hey Reddit,

tl;dr: To democratize the technology behind virtual assistants, we can play a [Q&A game](https://open-assistant.io/) to build a collaborative dataset that will enable the creation of culturally and politically unbiased virtual assistants.

As AI becomes more ubiquitous in our lives, we need to democratize it, ensuring that the next generation of virtual assistants, such as chatGPT or BingChat, are not solely controlled by one company, group or country, as it would allow them to skew our reality more easily, by deploying politically and culturally biased assistants at large scale, as we have seen with OpenAI.

While one could argue that over time companies and startups will emerge and create their own alternatives, these could be few, as creating such virtual assistants is not only a matter of massive raw data and computation, but it requires the creation of very specific datasets (many of them created by experts from multiple fields) with the goal of ""fine-tuning"" Large Language Models (LLMs) into virtual assistants.

Because of this, there is an international collaborative effort to create a public, multilingual, and high-quality dataset through a Q&A game, that will enable the creation of other virtual assistants outside the control of these companies.

At this very moment, we already have more data than OpenAI had when it launched its first version of ChatGPT. However, the current dataset is strongly biased towards Spanish and English speakers, as they are the only ones who have contributed to it so far. Therefore, we need to encourage people from other countries and cultures to play this Q&A game in order to create a truly multilingual dataset with expert knowledge of all kinds, from all over the world. (This would allow the virtual assistant to even answer questions that have not been answered in their language).

For Spanish and English is already a reality. Let's make a reality for other languages too by writing a few of questions/answers in the OpenAssistant game!

Link: [https://open-assistant.io/](https://open-assistant.io/)",MachineLearning
"I submitted a paper, with my PhD advisor, to ICML this year (2023) and hope to be accepted come April. I've never submitted a paper, nor attended, a conference. I have no idea what to expect

From those who have attended, or published, at these types of conferences, what is the best advise you can give for someone who is new to academia? Workshops? Tutorials? etc?",MachineLearning
"Say one wanted to model how much getting access to data would cost, how should one go about that? If labeling costs for say CIFAR10 are known with SageMaker and Google Cloud, what is the cost of getting the data in the first place?

Furthermore, say we move into the space of medical images e.g. MRI scans. What is the cost of getting MRI scans with a given desease? Where do I even find such information?",MachineLearning
"Hidden Markov Model implementation in R and Python for discrete and continuous observations. I have a tutorial on YouTube to explain about use and modeling of HMM and how to run these two packages.

Code:

[https://github.com/manitadayon/CD\_HMM](https://github.com/manitadayon/CD_HMM) (in R)

[https://github.com/manitadayon/Auto\_HMM](https://github.com/manitadayon/Auto_HMM) (In Python)

Tutorial:

[https://www.youtube.com/watch?v=1b-sd7gulFk&ab\_channel=AIandMLFundamentals](https://www.youtube.com/watch?v=1b-sd7gulFk&ab_channel=AIandMLFundamentals)

[https://www.youtube.com/watch?v=ieU8JFLRw2k&ab\_channel=AIandMLFundamentals](https://www.youtube.com/watch?v=ieU8JFLRw2k&ab_channel=AIandMLFundamentals)",MachineLearning
"Hi, I was wondering if you know any open-source package you'd recommend that can handle mixed types of data for machine learning, both for supervised and unsupervised learning.

I came up with the idea of just getting text embeddings and stacking those with the other features to create a single vector, but I think it might not be the best idea since the embeddings usually have a large dimension, whereas I might not have too many extra features, maybe making the final vector ""biased"" to the text data. Any thoughts on this?",MachineLearning
"My company encourages training opportunities and attending relevant conferences. 

Im curious to hear which conference you found worth attending, particularly in the area of engineering (materials) and machine learning?

Thank you.",MachineLearning
"&#x200B;

https://i.redd.it/4yli3zalvbka1.gif

txtchat is a framework for building conversational search and workflows. txtchat is open source under Apache 2.0 license and available on GitHub.

[GitHub](https://github.com/neuml/txtchat) | [Article](https://medium.com/neuml/introducing-txtchat-next-generation-conversational-search-and-workflows-for-all-97557009fb53)   


A set of intelligent agents are available to integrate with messaging platforms. These agents or personas are associated with an automated account and respond to messages with AI-powered responses. Workflows can use large language models (LLMs), small models or both.  


https://preview.redd.it/uhypbdu0wbka1.png?width=1301&format=png&auto=webp&v=enabled&s=ccdfc7063cfc5a26086c869a7f539fa82c6fea73

A persona is a combination of a chat agent and workflow that determines the type of responses. Each agent is tied to an account in the messaging platform. Persona workflows are messaging-platform  agnostic. 

# Examples

The following is a list of YouTube videos that shows how txtchat works. These videos run a series of queries with the Wikitalk persona. Wikitalk is a combination of a Wikipedia embeddings index and a LLM prompt to answer questions.

Every answer shows an associated reference with where the data came from. Wikitalk will say ""I don't have data on that"" when it doesn't have an answer.

### History

Conversation with Wikitalk about history.

[https://www.youtube.com/watch?v=ROyess8dLoA](https://www.youtube.com/watch?v=ROyess8dLoA)

### Sports

Talk about sports.

[https://youtube.com/watch?v=LXRB-iruKSc](https://youtube.com/watch?v=LXRB-iruKSc)

### Culture

Arts and culture questions.

[https://www.youtube.com/watch?v=OkObkNhJIgk](https://www.youtube.com/watch?v=OkObkNhJIgk)

### Science

Let's quiz Wikitalk on science.

[https://youtube.com/watch?v=-rsYDsZc9Wo](https://youtube.com/watch?v=-rsYDsZc9Wo)

### Summary

Not all workflows need a LLM. There are plenty of great  small models available to perform a specific task. The Summary persona  simply reads the input URL and summarizes the text.

[https://youtube.com/watch?v=PBJm9aDqkn0](https://youtube.com/watch?v=PBJm9aDqkn0)

### Mr. French

Like the summary persona, Mr. French is a simple persona that translates input text to French.

[https://youtube.com/watch?v=4x8pOIm4rbo](https://youtube.com/watch?v=4x8pOIm4rbo)",MachineLearning
"For the model, basically, both SLL and SL requires it to learn a mapping from X(input) to Y(label), (or a probability distribution of the label). And usually, the optimization processes for both are basically the same, at least for deep learning.

What's specific to SSL is just that, it's already labelled so no extra labelling is required. This facilitates pre-training from a much larger dataset since hand-labelling is expensive.",MachineLearning
"[https://twitter.com/GuillaumeLample/status/1629151231800115202?t=4cLD6Ko2Ld9Y3EIU72-M2g&s=19](https://twitter.com/GuillaumeLample/status/1629151231800115202?t=4cLD6Ko2Ld9Y3EIU72-M2g&s=19)

Paper here - [https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)",MachineLearning
"As the title.

I believed adversarial training was a catch-all term describing systems where two or more networks--with similar or distinct, but always mutually exclusive, goals--compete in zero-sum games and improve over time, but I'm finding that adversarial training relates to security, while generative adversarial networks specifically describe generation and detection.

edits:

It doesn't capture the spirit of what I'm looking for, but a broader term is Multi-Agent Reinforcement Learning",MachineLearning
"What's the best way to quantify the uncertainty of a trained LLM? I assume the entropy of the model's final probability distribution is a decent measure. Just wanted to know if the NLP community sticks to this measure, or if there's something more specific to language?


Would really appreciate recent references that may have popped up over the past few months (if any). Also if there are any cool & easy to integrate implementations. Thanks!",MachineLearning
"I need to get this off my chest...

So I was interviewing for an intern position at a procurement analytics company recently, we had an initial conversation on the phone where the engineer said  ""spend classification"".I heard it and ask for confirmation ""spam classification?""The engineer reply ""yes spend classification"".

So there I was, for the next 48 hours before the interview, trying to figure out the scenarios spam classification is used in the context of procurement analytics (what I got was a super reaching scenario but it was fun).

During the interview, I was trying to talk about the projects I did that could be useful for spam/data imbalance usecases instead of a few other things that I did which are much cooler. At the end, I ask about why they are doing spam classification for context of procurement analytics and they asked where I heard that from. I was like you guys said ""spam classification"". Then it dawn on me and them that I misheard ""spend classification"" as ""spam classification"".

We had a laugh, I talked about the scenario I mentioned and talked about Siamese network but I still felt damn embarrassed about it since I could have been talking about other projects.",MachineLearning
"The focus of this library is to implement a lot of the top prompt engineering papers such as PAL, CoT, Self-Ask, ReACT.

Excited to share ""Minds"". A a new way to build backends and workflows entirely with AI  (LLMs from OpenAI and Cohere). The AI can call your APIs, lookup in your  database, etc.

With just a couple lines of code you can builds  things like a question answering service where the AI can query your  local database to help answer customer support queries etc.

List of Prompt Engineering papers we are implementing:
[https://42papers.com/c/llm-prompting-6343](https://42papers.com/c/llm-prompting-6343)

MindJS Library
[https://github.com/dosco/minds](https://github.com/dosco/minds)",MachineLearning
"A recent [podcast interview](https://www.youtube.com/watch?v=gA1sNLL6yg4) of EY's has gone a bit viral, and in it he claims that researchers have dismissed his views without seriously engaging with his arguments, which are described [here](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities) in relative detail. 

I'm aware of on-going AI safety and interpretability research, but the dual use of the term ""AI safety"" to mean something close to AI ethics, and something close to preventing an existential threat to humanity, makes distinguishing the goals of, say, Anthropic, and the extent to which they consider the latter a serious concern, difficult as a layperson. 

I haven't personally found EY's arguments to be particularly rigorous, but I'm not the best suited person to evaluate their validity.  Any thoughts are appreciated. Thanks in advance!",MachineLearning
"Let's assume for a minute one has:

* the necessary compute instances
* enough $ to cough up to rent those instances somewhere

What are the latest ""easy"" solutions to get optbloomzand flan-t5hosted as API endpoints?

I spent about 2 weeks trying to get seldon-core and MLServer to work with its huggingface wrapper. But I've lost hope at this point. There are so many parameters and tweaks one has to be mindful of and I feel like I'm behaving like a very crude operating system replacement when I pass a device\_mapto a python function to tell it how much ram to use for what instance. In what world can MS 95 manage 4 DIM DDR rams but in 2023, we cannot auto-assign model data to the right GPUs?

So. What's the ""right way"" to do this? I am aware of

* This repo that has some ""demos"": [https://github.com/huggingface/transformers-bloom-inference](https://github.com/huggingface/transformers-bloom-inference)
* accelerate library: [https://huggingface.co/docs/accelerate/index](https://huggingface.co/docs/accelerate/index)
* FlexGen: [https://github.com/FMInference/FlexGen](https://github.com/FMInference/FlexGen) but that only works for opt and is not a model hosting solution but more of an academic PoC
* DeepSpeed, haven't looked deeply into this though

Any pointers would be appreciated. We have a goal to get 2-3 models up and running as API endpoints in 2 weeks and I have a lot of ppl waiting for me to get this done...

&#x200B;

Edit:

I am talking about self hosted solutions where the inference input & output is ""under your control""

&#x200B;

Edit:

What about a K8S + Ray Cluster + [alpa.ai](https://alpa.ai)? It feels like the most industrialised version of all the things I've seen so far after reading up on ray (which feels like a spark cluster for ML)",MachineLearning
"I need to produce estimates for these metrics. I tried the torchmetrics implementation, however they’re giving me completely wrong results (I tried FID using the same dataset as both real and fake data and it gave me an incredibly high number). Are you guys aware of other available implementations?",MachineLearning
"Paper: [https://arxiv.org/abs/2302.11529](https://arxiv.org/abs/2302.11529)

Twitter: [https://twitter.com/seb\_ruder/status/1628721434162765827](https://twitter.com/seb_ruder/status/1628721434162765827)

Website: [https://www.modulardeeplearning.com/](https://www.modulardeeplearning.com/)

Abstract: Transfer learning has recently become the dominant paradigm of machine learning. Pre-trained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference and discovery, programme simulation, and hierarchical reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer.

&#x200B;

https://preview.redd.it/oz02z33cwyja1.png?width=2045&format=png&auto=webp&v=enabled&s=542b15f2210c000bd95421f5cd1e68ebb2ee69be",MachineLearning
"Hello everyone!

I am currently working with a very small dataset (about 300 images) and I coded a DCGAN with Tensorflow in order to generate new ""fake"" images with the same distribution as the \~300 real images.

I have been reading a lot of papers and the official implementation of FID ([https://github.com/bioinf-jku/TTUR](https://github.com/bioinf-jku/TTUR)) and I always read ""number of samples"". Even [**here**](https://arxiv.org/abs/1911.07023) the papers states that FID is not reliable because is biased and talks about N being the number of samples and everything depends on that number.

What I dont really understand is which samples is everyone referring to. I mean, are they the real samples or the fake? the sum? both?

I want to calculate the FID between the \~300 real images (I cant use more, seems obvious but just saying) and **a number** of fake images I can generate with my network, but I am not sure how many samples (FAKE samples) to use. The logic tells me that I should use the same number of samples for both real and fake images but idk. This is the number papers and the repository talks about? It makes sense to calculate FID for \~300 real samples versus 10k fake samples?

Thank you in advance.",MachineLearning
"A simple question really but one that's pretty difficult to find an answer to:

&#x200B;

Has anyone done much research into the performance of models vs their size as a function of the output space (and if so where can I find it)? Basically, it's quite clear that for most applications, generalisability of a model can either be achieved by improving the dataset or increasing the size of the model (if your dataset is already good). But because the way performance is measured in SOTA benchmarks it's not necessarily obvious (to me at least) that these larger models are appropriate for more simple problems. 

&#x200B;

Say I have a simple audio classification problem where I only have one class of interest. If I wanted to implement the latest SOTA models in sound classification I'm likely to end up trying to use some pretty large and complicated model architectures. What I would like to know is how does one use SOTA benchmarks to inform their decisions for architectures in the face of tasks that are significantly simpler than those that are used to evaluate the performance of models on these benchmarks? 

&#x200B;

It feels like the simple answer is to just start simple and scale up as required but this does feel somewhat like trial and error so it would be great to hear how other people approach this sort of problem...",MachineLearning
"Hello, any personal favourites for drawing/visualising neural networks and transformers? With a colleague we are doing some tutorials/slides and would be very useful if there was a tool (python, latex, GUI, anything) that could help us do this, so that we can annotate on them after. Since it will be a teaching tool, clean and visually pleasing drawings would be awesome.

Would prefer a tool where we can specify the number of nodes/layers/etc and the node size and colour etc, and not something that simply draws a model from Keras/PyTorch without much adaptability.

Thanks in advance!",MachineLearning
"""Hotter take: ML would have advanced faster if another front-end language had been available and widely adopted instead of Python. One that is interactive yet fast & compilable, multithreaded (no GIL), isn't bloated, doesn't care about white spaces,... E.g. Julia or some Lisp.""

[Link from the original tweet](https://twitter.com/ylecun/status/1628386056641847296)",MachineLearning
"The ELECTRA paper introduces a small version that has around 15M parameters. MobileBERT and TinyBERT also have around the same number of parameters.

Are there any other language models out there that are smaller? Would it be possible to further distill large models into smaller variants?",MachineLearning
"I'm thinking about building an open source library to generate structured ml datasets from sources across the internet.

I know that lots of projects utilise crawlers to get decent datasets, while you might still need to create your own for specific use cases I'm wondering whether it'd be useful to have an open source library that lets you launch crawlers with predefined schemas for popular sources like LinkedIn, YouTube (I know yt also has an api), shopify stores, twitter, reddit, news sites and more.

Kind of like a unified interface with extendable starter templates.

The lib would dump json objects into a location you specify, like your local machine, mongo, or s3.

Something like:


`{
     title: some video,
     source: https//youtube.com/jfg78,
     views: 245676,
     comments: {}
`

Goal would be to make it easier/faster to get datasets from sources that don't natively have an api.

This might be a useless idea, but would love to hear your thoughts.",MachineLearning
"At this point we have open source LLM's, text-to-image models, and CLIP-like models but nothing similar to Flamingo. I am guessing some groups have already started working on this, but I just don't know them. Does anyone know? Looks like a great fit for [LAION](https://laion.ai/). Also, I have some experience in this area and wouldn't mind lending a hand if that's possible. I really want to get my hands on a Flamingo-like large, multi-modal, few-shot model to see how it performs on vision-language compositionally tasks like [Winoground](https://arxiv.org/abs/2204.03162). I am guessing these models might do a lot better than their smaller counterparts owing to better generalization and reasoning capabilities of LLMs.",MachineLearning
"[Letter from the U.S. Copyright Office](https://www.copyright.gov/docs/zarya-of-the-dawn.pdf) (PDF file).

[Blog post from Kris Kashtanova's lawyer](https://processmechanics.com/2023/02/22/a-mixed-decision-from-the-us-copyright-office/).

>We received the decision today relative to Kristina Kashtanova's case about the comic book Zarya of the Dawn. Kris will keep the copyright registration, but it will be limited to the text and the whole work as a compilation.  
>  
>In one sense this is a success, in that the registration is still valid and active. However, it is the most limited a copyright registration can be and it doesn't resolve the core questions about copyright in AI-assisted works. Those works may be copyrightable, but the USCO did not find them so in this case.

[My previous post about this case](https://www.reddit.com/r/COPYRIGHT/comments/xkkh3d/us_copyright_office_registers_a_heavily/).

Related news: [""The Copyright Office indicated in another filing that they are preparing guidance on AI-assisted art.\[...\]""](https://www.reddit.com/r/StableDiffusion/comments/114pobl/tweet_from_a_person_whose_aiinvolved_graphic/).",MachineLearning
"Dear Colleagues

This posting may be of interest to folks that use Catch22 for their time series research.

**What is the problem?**

* Catch22 is a wonderfully useful tool for time series...
* But the names of the features, for example `SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1` or `SB_TransitionMatrix_3ac_sumdiagcov` are awkward to use and have little mnemonic value.
* Moreover, some of the names are very easy to confuse, such as: `DN_OutlierInclude_n_001_mdrmd` and `DN_OutlierInclude_p_001_mdrmd`
*  This makes Catch22 awkward to use with a conversational agent, or many explainability/interpretability techniques etc.
* Their long length means it is even awkward to discuss features in a two-column paper format.

Thus, we propose to find a set of new *meaningful* names for the features.

**Design principles**

* The name should reflect what a feature is sensitive to. Ideal names would be one word, for example: `noise`, `spike`, `symmetric`, `step`, `falling`, `periodic`, `simple`, `smooth`, `linear` etc.
* However, given that it is likely to be rare a single feature has such specificity, the name could be a compound word, for example: `uniform-noise`, `localized-noise`, `positive-spike`, `negative-spike` etc.
* Compound words with three parts might be acceptable, i.e. `fall-then-rise`, however beyond three parts would be undesirable.
* <*Please suggest design principles we may have missed*\>

In \[a\] we have a visual summary of the above, and one tentative worked example. We look forward to the community’s input.

Many thanks

Keogh's Lab

\[a\] PDF: [https://www.dropbox.com/s/n1aybeps5p2ho5k/Finding%20Better%20Names%20for%20the%20Catch22%20Features.pdf?dl=0](https://www.dropbox.com/s/n1aybeps5p2ho5k/Finding%20Better%20Names%20for%20the%20Catch22%20Features.pdf?dl=0)

PPT: [https://www.dropbox.com/s/kxodalw2beyz86j/Finding%20Better%20Names%20for%20the%20Catch22%20Features.pptx?dl=0](https://www.dropbox.com/s/kxodalw2beyz86j/Finding%20Better%20Names%20for%20the%20Catch22%20Features.pptx?dl=0)",MachineLearning
"Announcing the [first-ever course on Data-Centric AI](https://dcai.csail.mit.edu/). Learn how to train better ML models by improving the data.

[Course homepage](https://dcai.csail.mit.edu/) | [Lecture videos on YouTube](https://www.youtube.com/watch?v=ayzOzZGHZy4&list=PLnSYPjg2dHQKdig0vVbN-ZnEU0yNJ1mo5) | [Lab Assignments](https://github.com/dcai-course/dcai-lab)

The course covers:

- [Data-Centric AI vs. Model-Centric AI](https://dcai.csail.mit.edu/lectures/data-centric-model-centric/)
- [Label Errors](https://dcai.csail.mit.edu/lectures/label-errors/)
- [Dataset Creation and Curation](https://dcai.csail.mit.edu/lectures/dataset-creation-curation/)
- [Data-centric Evaluation of ML Models](https://dcai.csail.mit.edu/lectures/data-centric-evaluation/)
- [Class Imbalance, Outliers, and Distribution Shift](https://dcai.csail.mit.edu/lectures/imbalance-outliers-shift/)
- [Growing or Compressing Datasets](https://dcai.csail.mit.edu/lectures/growing-compressing-datasets/)
- [Interpretability in Data-Centric ML](https://dcai.csail.mit.edu/lectures/interpretable-features/)
- [Encoding Human Priors: Data Augmentation and Prompt Engineering](https://dcai.csail.mit.edu/lectures/human-priors/)
- [Data Privacy and Security](https://dcai.csail.mit.edu/lectures/data-privacy-security/)

MIT, like most universities, has many courses on machine learning (6.036, 6.867, and many others). Those classes teach techniques to produce effective models for a given dataset, and the classes focus heavily on the mathematical details of models rather than practical applications. However, in real-world applications of ML, the dataset is not fixed, and focusing on improving the data often gives better results than improving the model. We’ve personally seen this time and time again in our applied ML work as well as our research.

Data-Centric AI (DCAI) is an emerging science that studies techniques to improve datasets in a systematic/algorithmic way — given that this topic wasn’t covered in the standard curriculum, we (a group of PhD candidates and grads) thought that we should put together a new class! We taught this intensive 2-week course in January over MIT’s IAP term, and we’ve just published all the course material, including lecture videos, lecture notes, hands-on lab assignments, and lab solutions, in hopes that people outside the MIT community would find these resources useful.

We’d be happy to answer any questions related to the class or DCAI in general, and we’d love to hear any feedback on how we can improve the course material. Introduction to Data-Centric AI is open-source opencourseware, so feel free to make improvements directly: [https://github.com/dcai-course/dcai-course](https://github.com/dcai-course/dcai-course).",MachineLearning
"What's the best way to improve the **inference** speed of a **Flan-T5** model?  

Onnx runtime doesn't seem to work for T5 models & Torchscript also doesn't seem to help speed it up (not sure why!)",MachineLearning
"Hi everyone, in a new paper we give a way to certify that a generative model does not infringe on the copyright of data that was in its training set.

Twitter thread: https://twitter.com/boazbaraktcs/status/1628219647651729409

Blogpost: https://windowsontheory.org/2023/02/21/provable-copyright-protection-for-generative-models/

Paper: https://arxiv.org/abs/2302.10870

Abstract:
>There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data C that was in their training set. We give a formal definition of near access-freeness (NAF) and prove bounds on the probability that a model satisfying this definition outputs a sample similar to C, even if C is included in its training set. Roughly speaking, a generative model p is k-NAF if for every potentially copyrighted data C, the output of p diverges by at most k-bits from the output of a model q that did not access C at all. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models, showing minimal degradation in output quality while ensuring strong protections against sampling protected content.",MachineLearning
"So it seems in this test of the four popular scikit-learn datasets. 

The test uses as judging criteria the accuracy reported by a special classifier.
In two of the datasets (iris and digits) the equal-width method markedly outperforms equal-frequency. In the other two datasets evaluated the differences are much narrower and could be considered as a tie result. The observations appear to be rather consistent when varying the number of bins used to discretize the attribute values.

This seems counter-intuitive; equal-frequency should have an advantage by providing better immunity in the presence of outliers. 

Any thoughts?

The used classifier, ""deodel"", discretizes continuous attributes using one of the two methods. After discretization, it behaves like a Hamming distance nearest neighbor classifier.

The equal-width and equal-frequency are methods that are referred to as unsupervised methods. Supervised methods take into account the training output in order to establish the thresholds for the bins. In the selected output, the decision tree classifier is used as a proxy for such methods. Although the deodel classifier can be seen as a collapsed decision tree, the algorithms differ and comparison is not straightforward. 

You can easily modify the code and test it with other datasets. If you do, please share your findings. The code is available at:


https://github.com/c4pub/deodel


Here is the summarized output:

    - - - - - - - - - 
    accuracy  classifier

    - - - - dataset: .. _iris

    0.946800  DecisionTreeClassifier() 

    0.544400  DeodelClassifier({'split_no': 2, 'mode': 'eq_freq'}) 
    0.719999  DeodelClassifier({'split_no': 2, 'mode': 'eq_width'}) 

    0.788000  DeodelClassifier({'split_no': 3, 'mode': 'eq_freq'}) 
    0.944000  DeodelClassifier({'split_no': 3, 'mode': 'eq_width'}) 

    0.779999  DeodelClassifier({'split_no': 5, 'mode': 'eq_freq'}) 
    0.930800  DeodelClassifier({'split_no': 5, 'mode': 'eq_width'}) 

    0.8556    DeodelClassifier({'split_no': 10, 'mode': 'eq_freq'}) 
    0.936     DeodelClassifier({'split_no': 10, 'mode': 'eq_width'}) 

    - - - - dataset: .. _digits
     
    0.844974  DecisionTreeClassifier() 

    0.671686  DeodelClassifier({'split_no': 2, 'mode': 'eq_freq'}) 
    0.947178  DeodelClassifier({'split_no': 2, 'mode': 'eq_width'}) 

    0.674123  DeodelClassifier({'split_no': 3, 'mode': 'eq_freq'}) 
    0.946577  DeodelClassifier({'split_no': 3, 'mode': 'eq_width'}) 

    0.694323  DeodelClassifier({'split_no': 5, 'mode': 'eq_freq'}) 
    0.938063  DeodelClassifier({'split_no': 5, 'mode': 'eq_width'}) 

    0.723939  DeodelClassifier({'split_no': 10, 'mode': 'eq_freq'}) 
    0.910050  DeodelClassifier({'split_no': 10, 'mode': 'eq_width'}) 

    - - - - dataset: .. _breast_cancer

    0.923052  DecisionTreeClassifier() 

    0.921684  DeodelClassifier({'split_no': 2, 'mode': 'eq_freq'}) 
    0.913473  DeodelClassifier({'split_no': 2, 'mode': 'eq_width'}) 

    0.939473  DeodelClassifier({'split_no': 3, 'mode': 'eq_freq'}) 
    0.940105  DeodelClassifier({'split_no': 3, 'mode': 'eq_width'}) 

    0.942315  DeodelClassifier({'split_no': 5, 'mode': 'eq_freq'}) 
    0.940315  DeodelClassifier({'split_no': 5, 'mode': 'eq_width'}) 

    0.942105  DeodelClassifier({'split_no': 10, 'mode': 'eq_freq'}) 
    0.936315  DeodelClassifier({'split_no': 10, 'mode': 'eq_width'}) 

    - - - - dataset: .. _wine

    0.909666  DecisionTreeClassifier() 

    0.875999  DeodelClassifier({'split_no': 2, 'mode': 'eq_freq'}) 
    0.889999  DeodelClassifier({'split_no': 2, 'mode': 'eq_width'}) 

    0.927999  DeodelClassifier({'split_no': 3, 'mode': 'eq_freq'}) 
    0.925666  DeodelClassifier({'split_no': 3, 'mode': 'eq_width'}) 

    0.943333  DeodelClassifier({'split_no': 5, 'mode': 'eq_freq'}) 
    0.947000  DeodelClassifier({'split_no': 5, 'mode': 'eq_width'}) 

    0.935666  DeodelClassifier({'split_no': 10, 'mode': 'eq_freq'}) 
    0.934333  DeodelClassifier({'split_no': 10, 'mode': 'eq_width'}) 

    - - - - - - - - -",MachineLearning
"I was reading [this](https://openreview.net/pdf?id=J_F_qqCE3Z5) paper, and I really liked the visualization of the conv layer weights in Figure 5. It's similar to the figures in [this talk at Microsoft](https://www.youtube.com/watch?v=EvAVCxZJN2U) at 11:25. Does anyone know what this visualization is called and/or methods to use it?",MachineLearning
" ""Deep learning is the only thing that currently works at scale it's the only class of algorithms that is able to discover arbitrary functions in a reasonable amount of time.""

[https://www.youtube.com/watch?v=p-OYPRhqRCg](https://www.youtube.com/watch?v=p-OYPRhqRCg)

I know of the universal approximation theorem. But is there any mathematical formulation of this statement?",MachineLearning
"[Not published in an open journal sadly.](https://www.science.org/doi/10.1126/science.1250939) 

[Press release.](https://www.rc.fas.harvard.edu/news-home/feature-stories/evolution-yeast/) 

TL;DR they set up a loss function (fastest growing survives) and evolved a bunch of yeast cells towards that loss function. This is a classic experiment, but they sequenced the DNA at each step and got a lot of cool data. The yeast cells converged much like you'd expect from an optimizer:

>The results of the experiment showed that in a controlled environment, evolutionary contingency led to convergence rather than divergence at the fitness level. Simply put, while the various yeast strains did mutate in different ways, they all arrived at a similar evolutionary endpoint regardless of their mutations.


I wonder if you could do this more quickly using gradient descent or other algorithms from machine learning. Since they're already sequencing the DNA at each step, they could have estimated the gradient and edited it back into the yeast. It would likely converge on similar solutions, but faster.",MachineLearning
"Cross entropy on logits is a normal simplification that fuses softmax + cross entropy loss to something like:
```
def label_cross_entropy_on_logits(x, labels):
    return (-x.select(labels) + x.logsumexp(axis=1)).sum(axis=0)
```
where `x.select(labels) = x[range(batch_size), labels]`.

I was thinking about how the `logsumexp` term looks like a regularization term, and wondered what would happen if I just replaced it by `x.norm(axis=1)` instead. It seemed to work just as well as the original, so I thought, why not just enforce unit norm?

I changed my code to
```
def label_cross_entropy_on_logits(x, labels):
    return -(x.select(labels) / x.norm(axis=1)).sum(axis=0)
```
and my training sped up dramatically, and my test loss decreased.

I'm sure this is a standard approach to categorical loss, but I haven't seen it before, and would love to get some references.

I found this old post: https://www.reddit.com/r/MachineLearning/comments/k6ff4w/unit_normalization_crossentropy_loss_outperforms/ which references LogitNormalization: https://arxiv.org/pdf/2205.09310.pdf However, it seems those papers all apply layer normalization _and then_ softmax+CE. What seems to work for me is simply replacing softmax+CE by normalization.",MachineLearning
"Many neural architectures use bottleneck layers somewhere in the architecture. What I mean by bottleneck is projecting activations to a lower dimension and back up. This is e.g. used in ResNet blocks. 

What is your intuition on why this is beneficial? From an information theory standpoint, it creates potential information loss due to the lower dimensionality. Can we see this as a form of regularisation, that makes the model learn more meaningful representations? 

Im interested in your intuitions in that matter or empirical results that might support these intuitions. Are you aware of other works that use bottlenecks and what is their underlying reasoning?",MachineLearning
"Hey everyone,

I've created what I believe is the first animation of a depthwise-separable convolution, and I thought you might appreciate it. I think this fills a legitimate gap in the instructional material available out there.

https://i.redd.it/o1bns0jjskja1.gif

I've actually been dissatisfied with the existing convolution animations in general (and [ranted about it on youtube](https://youtu.be/w4kNHKcBGzA)). So I made my own set of animations and published them on [animatedai.github.io](https://animatedai.github.io/).

If you find any of them useful, please feel free to copy them, post them on your website, throw them in a powerpoint, or just link to them.",MachineLearning
"Chain-of-thought can be used to get large language models to generate what often look like reasoning traces, but the reasoning steps generated are not always correct (even when the model's final answer *is* correct!). I’m aware of a few efforts to manually annotate the correctness/incorrectness of the reasoning steps in chain-of-thought-type data:

\* “Solving math word problems with process- and outcome-based feedback”: [https://arxiv.org/abs/2211.14275](https://arxiv.org/abs/2211.14275) 

\* “Large Language Models Are Reasoning Teachers”, section 4.2: [https://arxiv.org/pdf/2212.10071.pdf](https://arxiv.org/pdf/2212.10071.pdf) 

Unfortunately, the data does not seem to be available from either study. Is anyone aware of other researchers who have annotated the correctness of LLM-generated reasoning steps (whether or not their data is public), or datasets that contain this kind of data?

I guess I’d also be interested in datasets where the correctness/incorrectness of individual reasoning steps generated by humans have been annotated, for example if there are datasets of human-solved logic problems with the errors marked.   


Again, am interested in correctness of individual reasoning steps, not the correctness of the final answers.",MachineLearning
"I wanted to share a paper we have just released, where we extended the capabilities of ChatGPT to robotics, and controlled multiple platforms such as robot arms, drones, and home assistant robots intuitively with language: [https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/chatgpt-for-robotics/](https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/chatgpt-for-robotics/)

Video: [https://youtu.be/NYd0QcZcS6Q](https://youtu.be/NYd0QcZcS6Q)

Technical paper: [https://www.microsoft.com/en-us/research/uploads/prod/2023/02/ChatGPT\_\_\_Robotics.pdf](https://www.microsoft.com/en-us/research/uploads/prod/2023/02/ChatGPT___Robotics.pdf)

https://i.redd.it/ya84nryu0kja1.gif",MachineLearning
"Our survey paper: ""[Label-efficient Time Series Representation Learning: A Review](https://arxiv.org/abs/2302.06433)"" discusses one of the main limitations of applying deep learning models on time series data in the real world, i.e., the scarcity of labeled data. 

There are different ways to address this issue, and we attempt to provide an overview of the various label-scarce scenarios, and their corresponding techniques proposed to address each one.

&#x200B;

https://preview.redd.it/7waga9tdgjja1.jpg?width=1984&format=pjpg&auto=webp&v=enabled&s=6b7070d286691fb8b2b2006f7b4a2dea73059b7b",MachineLearning
"Hey r/MachineLearning! I wanted to share a new PyTorch library I've been working on that I think could be really useful for anyone looking to fine-tune large models with LoRA.   


[https://github.com/cccntu/minlora](https://github.com/cccntu/minlora)

  
The library is based on the LoRA technique (**Lo**w-**R**ank **A**daptation). ""which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer."" (- quote from the [paper](https://openreview.net/forum?id=nZeVKeeFYf9))  


With this library, you can easily apply LoRA to any PyTorch model with just a few lines of code.

One of the benefits of this library is that it's really small - just 100 lines of code. Despite its size, it's quite powerful and has been tested on a variety of different models, including nanoGPT by Karpathy, and stable diffusion.  


It also features an easy-to-use interface that allows you to serve multiple LoRA models at the same time!",MachineLearning
"I am looking into any techniques one could use for very large datasets in machine learning. So I am talking about datasets with the following properties:

1: 3D Imaging dataset where each dataset is of the order of many terabytes.

2: Each 3D image is too big to fit in the GPU or CPU memory.

I am interested in educating myself on methods that people have used in classical ML and modern deep learning for such extremely large datasets.

In particular, how does one ensure one can capture long-range spatial interactions in such datasets and what computational techniques can one do to perform learning on such datasets?  


Finally, if someone can point me to some open source examples of such ML systems (domain is not important) that I can learn from, I would be extremely grateful.open-source",MachineLearning
"One of the things in current publications that completely irritates me is people just forcing the use of GANs where they are not even needed nor suited at all, just to ride on the hype of *generative AI*.

These guys usually have samples `(x_1, y_1=phi(x_1)), ..., (x_n, y_n=phi(x_n))` of a random pair `(X, Y=phi(X))` where `phi` is some unknown target function (*ie* in fancy-pants math we know that `Y` is `sigma(X)`\-measurable). A direct way to solve this is to treat it naturally as a regression problem and use your usual ML/DL toolkit. These guys however think that they can make the problem look sexier if they introduce GANs. For instance, they'd train a GAN taking `X` as an input and through the discriminator have the generator output something that has the same distribution as `Y=phi(X)`. Some will even add some random noise `z` , that has nothing to do with `X`, to the inputs of the generator despite knowing that `X` is already enough to fully determine `Y`. GANs would have been useful if we didn't have joint observations of `X` and `Y` but that is not the case here.

One of the papers I have in mind is this one: [https://openreview.net/pdf?id=SDD5n1888](https://openreview.net/pdf?id=SDD5n1888)

How on earth are these papers getting accepted? To me that is literally just plagiarism of what's already available (physics-informed NNs in that case) by adding a totally useless layer (the GAN) to make it seem like this is a novel approach. That paper is only one of many cases. I know of a professor actively using that same technique to get cheap articles where he just replaces a standard regression NN in an old paper found online by a totally unjustified GAN. IMO reviewers at these journals/conferences need to be more mindful of this kind of plagiarism/low-effort submission.",MachineLearning
">Gran Turismo Sophy is a revolutionary superhuman racing AI racing agent developed in a collaboration between Sony AI, Sony Interactive Entertainment and Polyphony Digital. “Gran Turismo Sophy Race Together” mode gives Gran Turismo players of all levels and abilities the opportunity to go head-to-head against GT Sophy in GT7. The special mode, available as a time-limited in-game event (From Feb 21 to end of March), is a first look at GT Sophy in GT7 and is designed to maximize the fun and excitement of racing against GT Sophy for everyone. Player feedback on this initial special feature will be used to continually improve the GT Sophy Race Together mode feature for future releases. 

>In GT Sophy Race Together mode, players can race against GT Sophy in a series of four circuits of increasing difficulty, as a Beginner / Intermediate / Expert driver. In each of the four races, the player races against four GT Sophy cars of different performance levels. Players can also challenge GT Sophy in 1VS1 mode, where GT Sophy and the player race one-on-one with identical car configurations and settings, which showcases the superhuman racing skills of GT Sophy. The excitement of GT Sophy Race Together mode is enhanced with GT7’s new emoticon feature, which displays emoticons on the GT Sophy cars throughout the race to react to the in-game action.

https://blog.playstation.com/2023/02/20/gran-turismo-7-update-1-29-includes-ps-vr2-upgrade-a-race-against-superhuman-ai-a-classic-gt-track-and-5-new-cars/

Sony AI introduced their quantile regression—soft actor critic algorithm for Sophy in this Nature paper.

https://www.nature.com/articles/s41586-021-04357-7",MachineLearning
"PyTorch has its own torch.nn.Transformer [module](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html), however I see that many papers and their reproductions often choose to implement the transformer from scratch.

For example:

* [Vision Transformers](https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py#L35)
* [Decision Transformers](https://github.com/kzl/decision-transformer/blob/master/atari/mingpt/model_atari.py#L99)
* [Whisper](https://github.com/openai/whisper/blob/main/whisper/model.py#L104)

In fact, I'm not sure if I've ever seen any project actually use the PyTorch module. I'm curious if there's a reason for this?",MachineLearning
"In the abstract of the Nerf paper ([https://arxiv.org/abs/2003.08934](https://arxiv.org/abs/2003.08934)), the described framework is that Nerf enable to do the following: the user inputs a set of images with known camera poses, and after training the network they can generate images of the same scene from new angles.

However, the paper itself builds a network that gets as an input 5D vectors (3 location coordinates+2 camera angles) and outputs color and volume density for each such coordinate. I don't understand where do I get those 5D coordinates from? My training data surely doesn't have those - I only have a collection of images. Same for inference data. It seems that the paper assumes not only having a collection of images but also having a 3D representation of the scene, while the abstract doesn't require the latter. What am I missing here?",MachineLearning
"Hello everyone,  


It's that time again, thank you all so much for the support you've given us over here. I've done a ton of typing this morning, so for a summary of what I've updated, you can see the higher-level twitter thread I wrote at [https://twitter.com/hi\_tysam/status/1627679672988319746?cxt=HHwWhIC-yb2C15YtAAAA](https://twitter.com/hi_tysam/status/1627679672988319746?cxt=HHwWhIC-yb2C15YtAAAA), or the more detailed (but still rough cut) patch notes I wrote this morning at  [https://github.com/tysam-code/hlb-CIFAR10/releases/tag/v0.5.0](https://github.com/tysam-code/hlb-CIFAR10/releases/tag/v0.5.0)  


Happy to answer any questions anyone might have, cheers! :D :))))",MachineLearning
"As far as I can tell, there are two contradictory definitions of Layer Normalization that are both floating around. LN computes the mean and variance along some axes of the input tensor for normalization, yet the choice of axes is not clear:

A. The [GroupNorm paper (2018)](https://arxiv.org/pdf/1803.08494.pdf) has this figure that describes LN as reducing **along channel and spatial/token axes**.

https://preview.redd.it/ui9adzzxgcja1.png?width=1353&format=png&auto=webp&v=enabled&s=f701d53a0992e3fe13bdac6ee022d352f965c893

B. The [PowerNorm paper (2020)](https://arxiv.org/pdf/2003.07845.pdf) has this figure that describes LN as reducing **only along the channel axis**.

https://preview.redd.it/e0qmp9sahcja1.png?width=1717&format=png&auto=webp&v=enabled&s=00126512760766783d88217b44377f5741290d9c

There are also many online sources that describe LN as shown in A (e.g. [TF tutorials](https://www.tensorflow.org/addons/tutorials/layers_normalizations), [PapersWithCode](https://paperswithcode.com/method/layer-normalization), [this summary of normalization techniques](https://theaisummer.com/normalization/)) using similar figures.

The [LN paper (2016)](https://arxiv.org/pdf/1607.06450.pdf) itself says

>all the hidden units in a layer share the same normalization terms μ and σ

so the channel axis is definitely reduced, and

>computing the mean and variance used for normalization from all of the summedinputs to the neurons in a layer *on a single training case*

so the batch axis is definitely not reduced. As far as I can tell it is not clear about what happens with spatial/token axes, although the above sounds rather like they might be included in the statistics.

Yet, I don't know of any model that actually uses A instead of B. For example, [TF](https://github.com/keras-team/keras/blob/v2.11.0/keras/layers/normalization/layer_normalization.py#L158) and [Flax](https://github.com/google/flax/blob/main/flax/linen/normalization.py#L321) explicitly implement LN with default axes as in B ([PyTorch](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/normalization.py#L142), [Haiku](https://github.com/deepmind/dm-haiku/blob/main/haiku/_src/layer_norm.py#L78) and [Equinox](https://github.com/patrick-kidger/equinox/blob/1f5373f5905504bc5e7069ed6d458dbad5616495/equinox/nn/normalisation.py#L39) don't have a preference and require the user to specify the reduction axes). [Vision Transformer](https://github.com/google-research/vision_transformer/blob/main/vit_jax/models_vit.py#L133) uses Flax with LN as in B, [ConvNeXt](https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py#L135) implements LN with PyTorch as in B, [OpenAI GPT-2](https://github.com/openai/gpt-2/blob/master/src/model.py#L28) implements LN with Tensorflow as in B, even [MLP-Mixer](https://github.com/google-research/vision_transformer/blob/main/vit_jax/models_mixer.py#L41) where the spatial/token axes are interpreted as channel axis for an MLP still computes statistics along the original channel axis as in B.

As far as I can tell, everyone uses B rather than A in their models, so to me this seems to be the ""correct"" definition. Yet, many sources on this topic describe LN as doing A rather than B.

Does anyone have any insight on this or know of a source that has addressed this problem? Do you interpret the original LN paper as including spatial/token axes in their computation of mean and variance, or not? Is this simply an error that started with the figure A and made its way into different online tutorials from there? Or do you maybe know of a model that actually uses LN to reduce both along channel and spatial/token axes?",MachineLearning
"Hi everyone,

I'm currently working on a biometric identification project that involves converting biometric data (such as iris images) into a unique and secure ID. In order to do so, one of the first steps in the pipeline (after training a feature extractor) is to extract a set of features from an image in some tensor form (preferably a vector). What I'm wondering is what robust method could be used to extract similar feature vectors for similar inputs (e.g., to obtain similar, in terms of Euclidean distance, feature vectors for various photos of a same iris)? That would be required such that the feature vectors for similar inputs could be converted to the same unique ID (e.g., by using a locality-sensitive hashing algorithm).

In short, I'm interested in any tips for:

* Choosing an appropriate and robust feature extraction architecture
* Methods for conversion of features to IDs (such as hashing, or anything that should work in theory)

Any insights or suggestions would be greatly appreciated. Thanks in advance!",MachineLearning
"I've been looking into open source large language models to run locally on my machine.

Seems GPT-J and GPT-Neo are out of reach for me because of RAM / VRAM requirements.

What models would be doable with this hardware?:

CPU: AMD Ryzen 7 3700X 8-Core, 3600 MhzRAM: 32 GB

GPUs:

1. NVIDIA GeForce RTX 2070 8GB VRAM
2. NVIDIA Tesla M40 24GB VRAM",MachineLearning
"Paper: [https://arxiv.org/abs/2301.04104#deepmind](https://arxiv.org/abs/2301.04104#deepmind) 

Website: [https://danijar.com/project/dreamerv3/](https://danijar.com/project/dreamerv3/) 

Twitter: [https://twitter.com/danijarh/status/1613161946223677441](https://twitter.com/danijarh/status/1613161946223677441) 

Github: [https://github.com/danijar/dreamerv3](https://github.com/danijar/dreamerv3)  / [https://github.com/danijar/daydreamer](https://github.com/danijar/daydreamer) 

Abstract:

>General intelligence requires solving tasks across many domains. Current reinforcement learning algorithms carry this potential but are held back by the resources and knowledge required to tune them for new tasks. We present **DreamerV3, a general and scalable algorithm based on world models that outperforms previous approaches** across a wide range of domains with fixed hyperparameters. These domains include continuous and discrete actions, visual and low-dimensional inputs, 2D and 3D worlds, different data budgets, reward frequencies, and reward scales. We observe favorable scaling properties of DreamerV3, with **larger models directly translating to higher data-efficiency and final performance.** Applied out of the box, DreamerV3 is the **first algorithm to collect diamonds in Minecraft from scratch without human data or curricula,** a long-standing challenge in artificial intelligence. Our general algorithm makes reinforcement learning broadly applicable and allows scaling to hard decision making problems. 

https://preview.redd.it/h4hrfqwp57ja1.jpg?width=1320&format=pjpg&auto=webp&v=enabled&s=f3687d48c9b28efe184931cee62d7ff42b5d5655

https://preview.redd.it/bl13kxwp57ja1.jpg?width=1399&format=pjpg&auto=webp&v=enabled&s=56c70c244e3ccf45b351e83791059d01a535299f

https://preview.redd.it/b0kqa2xp57ja1.jpg?width=1286&format=pjpg&auto=webp&v=enabled&s=55fffc14ae68cb8ad60e395182c71c541c5f2005

https://preview.redd.it/e61x5xwp57ja1.jpg?width=1291&format=pjpg&auto=webp&v=enabled&s=d23cfe3bab1114f543187c53d355488e4d3c8ffa",MachineLearning
"Paper: [https://arxiv.org/abs/2302.07842](https://arxiv.org/abs/2302.07842)

Abstract:

>This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows **ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks.** In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.       

https://preview.redd.it/lyjdr1ozj6ja1.jpg?width=1281&format=pjpg&auto=webp&v=enabled&s=1b5db84ec5b38228fc794e3fd24e83e4e450cc57",MachineLearning
"The paper ""Offsite-Tuning: Transfer Learning without Full Model"" describes a privacy-preserving and efficient transfer learning framework. In this framework  


• Offsite-Tuning is a privacy-preserving and efficient transfer learning framework    
• Model owner sends a light-weight adapter and a lossy compressed emulator to the data owner   
• Data owner fine-tunes adapter on downstream data with the emulator's assistance   
• Fine-tuned adapter is then returned to the model owner to create an adapted foundation model   
• Offsite-Tuning preserves both parties' privacy and is computationally more efficient than existing fine-tuning methods

How does this differ from Federated Learning?

Paper Link: [https://arxiv.org/abs/2302.04870](https://arxiv.org/abs/2302.04870)",MachineLearning
"I just saw a tutorial about using langchains and am curious about how it works. So if i implemented something at my company that can answer any question across all our documents, does it mean i would have essentially gave all our company info to openai?",MachineLearning
"TorchDrug is a machine learning platform designed for drug discovery, covering techniques from graph machine learning (graph neural networks, geometric deep learning & knowledge graphs), deep generative models to reinforcement learning. It provides a comprehensive and flexible interface to support rapid prototyping of drug discovery models in PyTorch. 

In this video, we walk through TorchDrug library and train some GNN for graph classification, attribute masking and unsupervised graph representation learning.

https://youtu.be/-Kb7kN4aHMM",MachineLearning
"I really like training in the cloud for some reason and feels satisfying, however here is a couple of things I would've wished I knew beforehand to get things started.

1. Use a spot instance unless you absolutely must make sure it isn't interrupted. Your wallet will thank you later. 
2. Make sure Nvidia drivers are installed and don't experiment with Operating systems. You are paying by the hour. 
3. Make sure to use something like tmux to save the sessions running in your terminal so you don't have to start from scratch or in case you disconnect from the vm (but the VM isn't shut down). That way you can just click out of the terminal and not bother with it until it's done. 
4. Debug on your local machine on CPU if you don't have CUDA. You can debug the model on a CPU perfectly fine. 

Now what about you all?",MachineLearning
" 

Hello, community.

**Description:**

I am planning to create a detection model using YOLO v8 to detect leukemia cells in a blood sample. I started learning about deep learning two months ago and I am eager to try out image segmentation on my present dataset instead of bounding boxes, as the cells are closely bunched together. I need advice on whether I should use bounding boxes or instance segmentation, considering my dataset and expected results.

**Context:**

Leukemia is caused by an abundance of different types of naive or altered white blood cells in the body, which overwhelm the bloodstream and inhibit the proper functioning of normal white blood cells. There are three classes in my dataset: lymphoblasts, promyelocytes, and neutrophils, and I need to be able to detect these cells.

**Expected Results:**

As this is a medical domain, false positives are acceptable, but false negatives are not.

**About dataset:**

[lymphoblast sample image](https://imagebank.hematology.org/getimagebyid/2201?size=3)

[sample image for promyelocytes](https://medschool.co/images/detail/blood-film/promyelocyte.jpg)

[sample image for neutrophils](https://imagebank.hematology.org/getimagebyid/3610?size=3)

[sample test image](https://thumbs.dreamstime.com/z/picture-white-blood-cell-red-blood-cell-platelet-blood-film-analyze-microscope-picture-blood-cells-blood-film-161974012.jpg)

lymphoblasts(101 images)

promyelocytes(91 images)

neutrophils(133 images)

**more context for your reading:**

An over abundance of lymphoblasts results in acute lymphoblastic leukemia (ALL), while acute pomyelocytic leukemia (APLML/APL) is caused by an abnormal accumulation of promyelocytes. neutrophils do not cause leukemia.",MachineLearning
"Hello, 

What is your perception of UAI and AISTATS conférences ? Is it good to publish that ? Is one more competitive than the other ? 

Thanks",MachineLearning
"Hello, this is a question regarding regarding a system of two(or more) classifiers for energy/computation purposes. For example a mobile phone and a cloud server.  
What frameworks/techniques exist for tuning the thresholds for two or more classifiers simultaneously?  
For example, given two trained binary classifiers, I would like to pass a labeled validation dataset X through both classifiers and tune 2 thresholds for classifier1(upper and lower) and 1 threshold for classfier2. Everything that is lower than the ""upper"" threshold and higher than the ""lower"" threshold(what classifier1 is not certain of) should be passed to classifier2.

To avoid a very liberal passing of data to classifier2, I also want to introduce a loss/penalty for doing so, meaning that classifier1 should learn using the provided labeled data when it really has to pass the sample to classifier2.

XGBoost seems to be focused on tuning a single classifier, and I feel like I might need to use some Reinforcement learning technique, but I do not know the nomenclature for this kind of problem, policies perhaps? Does anyone have experience with this?",MachineLearning
"When the [classifier-free guidance](https://arxiv.org/abs/2207.12598) was first introduced, I was very confused about why it works: I'd understand if it was interpolating like `ε * conditional_prediction + (1 - ε) * unconditional_prediction`, but in its formulation, ε is greater than 1. It is clear why it makes the result match the condition better, but why the result becomes better regardless of the condition was a mystery to me.

Afterwards, there were many post-hoc explanations, which didn't seem satisfactory (e.g. these explanations didn't have predictive power helping to improve the trick). Recently, I finally got around to play with it, and found some interesting patterns (in context of diffusion, DDIM sampling):
* If we disable CFG for 90% last sampling steps, results are pretty much the same;
* If we disable CFG for the first 10% sampling steps, the resulting image is destroyed.

It appears that CFG it responsible for forming the overall composition of the image from the random noise at the very beginning of the sampling, and doesn't do much afterwards. This is kind of similar to the observation about attention maps in [this paper](https://arxiv.org/pdf/2208.01626.pdf) (section 3.1). Speculatively, it tries to ""match the prompt"" to the random noise, and the adjustments from it need to be amplified, otherwise subsequent steps will match the prompt differently (it is a random noise after all). If this is true, I guess something like this might also work (I haven't tried yet): sample 1000 different starting random states, take the one which ""matches the prompt"" the best by some measure, and do the diffusion sample starting from it without CFG.

This all might make sense, except that this is very specific to diffusion. But [it is known](https://arxiv.org/pdf/2206.10789.pdf) that CFG works just as well for autoregressive transformers on VQVAE tokes. This might indicate that the mechanism why it works is more fundamental and is not specific to diffusion.

I wonder if there is any community wisdom/thoughts on why/how it works, and generalizes so well across two very different types of models.",MachineLearning
"Does anyone know of a paper / article that discusses the accuracy / usefulness of available opensource LLM models.  


Bloom, GPT-NeoX, T5, etc.  


What would be a good way to evaluate tradeoffs?",MachineLearning
"Given the impressive capabilities of ChatGPT, I've been learning about RLHF - just wondering if there has been any work/research on RLHF with a model-based RL algorithm (e.g. MuZero, vs PPO). Thanks!",MachineLearning
" 

**Github:** [https://github.com/m-barker/fibs-reporter](https://github.com/m-barker/fibs-reporter)

**PyPI:** [https://pypi.org/project/fibs-reporter/](https://pypi.org/project/fibs-reporter/)

The Data **F**eature **I**mportance, **B**aseline-modeller and **S**purious correlation Reporter (**FIBS**) is an open-source software for automatic generation of a PDF report to highlight and visualise potential sources of spurious correlation within **any** given tabular or audio dataset stored as a Comma Separated Values (CSV) file. FIBS is run through one-command line command; **all of the calculations, model training, and report generation happen automatically**.

All that is required as input on the command line is the path to the CSV file containing the data, and the name of the output (dependent) variable within the dataset. The toolkit will automatically determine whether the task is regression or classification. Optionally, the toolkit can process and extract audio data, provided the name of the variable within the CSV that contains the audio file for each observation is specified.

Key features that are generated automatically:

* A traffic light score for potential spurious correlations within the dataset
* Calculation of four different feature importance metrics to highlight the most important features within the given dataset
* Training and evaluation of two baseline models, including visualisation of model results
* Visuals of the most important features, with different visuals depending on the variable types
* Automatic determination of regression or classification task, resulting in different baseline models, feature extraction methods, and visualisations
* Principal Component Analysis calculation and baseline model to estimate complexity within the dataset
* (Optionally) extract audio data features and run the above on these features
* Output all of the above in a PDF report with accompanying dynamic textual explanations",MachineLearning
"With the advent of stable diffusion/midjourney/dalle and upcoming text-to-video models from Google and Meta, what will be major challenges in computer vision? It feels like once text-to-video models get released, visual reasoning will be mostly solved, and the only thing left to do is to improve model accuracy/efficiency from there. I am fairly new to Computer Vision and would love to learn new possible areas of research. Thank you in advance!",MachineLearning
"When designing neural network architectures, it is common to think about ""information flow"", e.g. how is information propagated, where are the ""information bottlenecks"" and so on. Another example might be that some people use ""information loss"" to explain why transformers work better than RNNs. 

It seems like most papers discuss this in a rather hand-wavy way. Is there any work done in formalising such ideas to better guide us understanding various model architectures? What are the core ideas?",MachineLearning
Are there any resources for fast computations of diffusion model likelihoods? Current approaches use a black box ODE solver to solve probability flow ODE to estimate likelihood but these solvers often require hundreds of model evaluations to converge. While there has been considerable work on fast solvers for the reverse diffusion process I'm not familiar with any work that could be applied to likelihood computation.,MachineLearning
"Are there general categories of studies that we should realize when preparing a paper?

Some examples I can think of:

- Comparison study. Just compare different models on an application, ideally giving them all a fair shot. This is useful in case others need to decide what model to choose.

- Ablation study. Remove parts of the model to see which ones are most important, trying to understand how the model performs.

- Novel method study. Brand new novel method with some comparisons thrown in.

What are other types of studies?

Or should we not try to categorize studies like this?",MachineLearning
"I built a (1) baby sleep tracking & (2) forecasting system, and wanted to share for those interested, or actually want to try running it at your home.

(1) I built a baby sleep tracking system (computer vision largely, [here's the core of that code](https://github.com/calebolson123/BabySleepCoach/blob/924e7b55d3aa36acd706519c446c1172dbbda4a7/main.py#L322)) which writes timestamped records of when my baby fell asleep or wakes up. The code is pulling images from my baby monitor, and largely just applying heuristics over time to decide whether he's awake/asleep.

(2) After I had a few weeks of sleep data ([sample data](https://github.com/calebolson123/BabySleepCoach/blob/master/sleep_logs.csv)), I moved it into a [jupyter notebook](https://github.com/calebolson123/BabySleepCoach/blob/master/sleep_forecast_arima.ipynb) and ended up using an ARIMA model to forecast the next month's wakings/sleepings. I wrote some javascript as part of a web app i have running on my raspberry pi to generate some charts so I can see how his sleep is changing over time. [Here's an example of what that visual looks like](https://imgur.com/BdwBoeG) (orange is awake, blue is asleep).

I built it because my wife asked for it, but also made a video detailing the project: [https://youtu.be/r7Exc0sUt5E?t=209](https://youtu.be/r7Exc0sUt5E?t=209)",MachineLearning
"I have constructed a novel ML (NLP) dataset for classification and labeled it with three classes. The dataset is rather small with about 700 examples, out of which the classes have about 400, 200, and 100 examples respectively. I would like to publish it and describe it in an official publication for a workshop or a conference.

When looking at related datasets and publication, I see that it is common for authors to publish the dataset already split into three chunks - train, dev, test dataset (see the images). It is also common in these papers to provide the performance of baseline models on the dataset. Considering the dataset's small size, I feel like doing a 5-fold cross-validation would be a good alternative for such a small dataset, rather than doing something like a split into 450-100-150 train-dev-test datasets and then evaluating only on the very small dataset with 150 examples. Still, I believe that for better replicability, doing an ""official"" split is preferred and then everyone in the future testing on the same test set with 150 examples? Why do the authors usually already provide the three splits?

Furthermore, when looking at these ML resource papers, I saw in a few instances that the test set is kept balanced with respect to the three classes, even though the original dataset was not and dev set is not made balanced. This is problematic in my case for my third class where there are only about 100 examples. If I make my test set to be 50-50-50 for class1-class2-class3, then there is only 50 examples of class3 left for train+dev! That is simply infeasible for the training set. None of the authors provide any sort of explanation why they split it like this, they just seem to say ""here is our split"". Is this done to discourage the model from just doing a majority-class prediction and thus make it challenging? Or because a dummy classifier would have a 60% accuracy? Still, with a metric like F1 and not accuracy, this does not seem like an issue...

Some examples of these balanced test sets with unbalanced train sets:

\[1\]: [https://i.stack.imgur.com/RGRk3.png](https://i.stack.imgur.com/RGRk3.png)

\[2\]: [https://i.stack.imgur.com/R39Oh.png](https://i.stack.imgur.com/R39Oh.png)

\[3\]: [https://i.stack.imgur.com/6Vqaw.png](https://i.stack.imgur.com/6Vqaw.png)

When searching through Stack Overflow for similar questions, people were usually discouraged from splitting their Kaggle datasets into a test dataset that is balanced, with the argument that we want a classifier to work with data that resembles the real-world distribution and makes it ready for production.

To sum up:

\- Is is considered mandatory to provide the ""official"" train-dev-test split when introducing a new dataset in an ML publication?

\- If so, should the test set have a balanced class distribution and why?",MachineLearning
"(Edit: This is definitely an error, not a change in pricing model, so no need for alarm. This has been confirmed by the lead product owner of colab)

Without any announcement (that i could find) google has increased the pricing per month of all its Colab Pro tiers, Pro is now 95 Euro and Pro+ is 433 Euro. I paid 9.99 Euro for the Pro tier last month... and all source i can find also refer to the 9.99 pricing as late as September last year. I have also checked that this is not a ""per year"" subscription price, it is in fact per month.

I looked at the VM that Colab Pro gives me and did the calculation for a similar VM in google cloud (4 vCPUs, 15GB RAM and a T4 GPU) running 24/7 for a month (Google calculates it as 730  hours). 

It costs around 290 Euro, less than the Colab Pro+ subscription... 

The 100 credits gotten from the Colab Pro subscription would only last around 50 hours on the same machine! 

And the 500 credits from Colab Pro+ would get 250 hours on that machine, a third of the time you get from using Google Cloud, at over 100 euro more....

This is a blatant ripoff, and i will certainly cancel my subscription right now if they don't change it back. It should be said that i do not know if this is also happening in other regions, but i just wanted to warn my fellow machine learning peeps before you unknowingly burn 100 bucks on a service that used to cost 10...

[Google Colabs price tiers on 17th of February 2023, 10 times what they were in January 2023.](https://preview.redd.it/l7gx48kw8qia1.png?width=1717&format=png&auto=webp&v=enabled&s=7b0687f1615344ffdb4fbe4ea7990f769bacd9c8)",MachineLearning
"Hi folks 👋🏼, 

**Context:** I just started working on my thesis on activity recognition in videos using deep learning. I have been struggling to find an efficient way to work with large research datasets such as UCF-101, HMDB, and Kinetics. These are medium - large datasets \~12 GB each. Thus, I was wondering what was your workflow as researchers (or even practitioners)

**Currently:** I am working on Google Colab and at the beginning of each work session I wait a few minutes for the dataset to be downloaded. I have it locally stored.

**Some questions:**

\- What is your workflow as a ML/DL researcher/practitioner?

\- Should I work with a downsampled version of my research dataset (say X% of each class)?

&#x200B;

Looking forward to read your answers, 

Cheers,",MachineLearning
Authors: the question: How does GPT-2 know when to use the word 'an' over 'a'? Logit lens used:  https://clementneo.com/posts/2023/02/11/we-found-an-neuron,MachineLearning
"Hi

Is  A4000 better for deep learning, performance-wise, than 3070 because of  FP32 operations (not only because of memory size) or do networks like Stable Diffusion tend to use FP16 operation and this does not really matter, apart from memory they should be similarly fast?   


Regards",MachineLearning
"I have been trying to familiarize myself with the common techniques used in optimization theory so that I can follow some of the proofs I see in machine learning papers. I know that two of the goto books in this field are Boyd's and Bertsekas's books. However, these books require a significant amount of effort as they aim to teach you the finer details. Since my goal is to familiarize with the methods (and not go into the nitty-gritty details), I was wondering if there's a short book (say less than 100 pages) or some other resource whose goal is to provide the reader with a high level view of the field of the methods and techniques used in optimization theory. Is there such a book, lecture notes, video series, etc., that caters to such requirements?",MachineLearning
"Hi there,

I am a research data scientist, and excited to release a new feature engineering library, designed to help you streamline the process of machine learning even more than before. **Headjack is an open library which provides a ML features transformation based on self-supervised learning models**, similar to huggingface as a hub, but which currently focuses on exchanging features for tabular data models.

Compared to textual data, tabular data are different in that each data set has different column length and attributes, this means that it cannot be typed consistently unlike the token embedded in NLP tasks. Therefore, Headjack is different from NLP’s pre-trained model with single domain transformation, but by performing with two different domain transformations. **In other words, we can perform features transform between two domains without the same key value.** In addition, release the potential of data that is not typically used. For example, enhance the prediction of the Boston housing price task applied in the Titanic domain, or enhance the prediction of the customers churn task applied in the African traffic domain and so on.

[Github](https://github.com/jimliu741523/headjackai-sdk)

[Introduction](https://medium.com/p/385a90ff413c)

&#x200B;

[The IRIS dataset with California House Price Feature Transformation](https://preview.redd.it/54w2qwnm8pia1.png?width=2110&format=png&auto=webp&v=enabled&s=aa9a3333448985f22604fab9012272a8c54387fa)

[The IRIS dataset with Titanic Feature Transformation](https://preview.redd.it/9revfvdq8pia1.png?width=2102&format=png&auto=webp&v=enabled&s=ba3ae69e5a96a6f3d74850526045a39b34636909)

[The IRIS dataset with KPMG Customer Demorgraphy Feature Transformation](https://preview.redd.it/p7s7zj9r8pia1.png?width=2052&format=png&auto=webp&v=enabled&s=b7147a25b14f23346331157e11b98c86472f7ae5)

&#x200B;",MachineLearning
"I've been studying about ARIMAX, XGBoost, MLForecast and Prophet. As a newcomer to any method, I like first to do an exhaustive comparison of tools trying to understand where they succeed/fail. After exploring [ARIMA/XGBoost](https://dsdaily.substack.com/p/ds-daily-arima-and-xgboost?utm_source=substack&utm_campaign=post_embed&utm_medium=web), I came across [MLForecast/Prophet](https://dsdaily.substack.com/p/ds-code-review-prophet-vs-mlforecast). But I'm left with the following questions:

1. Why is MLForecast better than out-of-the-box XGboost? Sure, it does feature engineering and it appears to do dynamic predictions on your lagged features, but is that it? Does it do hyperparameter tuning? Does it have seasonal trends like Prophet does?
2. I see that you can use exogenous features in Prophet, but how does this scale? Let's assume I have 50 predictors. How does prophet handle these? I found this in the [docs](https://facebook.github.io/prophet/docs/seasonality,_holiday_effects,_and_regressors.html)and this other [person's post](https://towardsdatascience.com/forecast-model-tuning-with-additional-regressors-in-prophet-ffcbf1777dda) explaining how to do it, but largely I've come away with the impression that it's pretty hard to do this vs. just doing it with XGBoost.
3. Does ARIMAX compare anymore? Are there any papers comparing out-of-sample predictions with ARIMAX vs. XGBoost vs. Prophet vs. Fable? Does it just depend on your dataset and I should try all four?

I have a time series data with dozens of ""known"" inputs (such as ad spend) and a lot of external data (CPI, economic health, stocks, etc.). My goal is to use my model to optimize my target by ""plugging in"" ad spend and dynamically forecasting the economic data.",MachineLearning
"Hi!

Searching for papers that have modfications in the encoder or decoder neural network of a VAE.

I'm working on a project which uses a variational auto encoder with modified decoder neural network. In brief, Its decoder is modified to introduce sparsity in a set of feature as a way of introducing domain knowledge. 

Some such paper is below.

oi-VAE: Output Interpretable VAEs for Nonlinear Group Factor Analysis

VEGA is an interpretable generative model for inferring biological network activity in single-cell transcriptomics

 Please let me know of methods that are similar in nature.",MachineLearning
" Hi guys,

I am interested in setting up an environment to train a neural network on an extremely big dataset (10TB). How would I do this? Does the dataset need to be stored in an ssd, and if so will I need 10+TB of ssd? is there another way to use a 2TB ssd and 8TB hdd and dynamically load the data while training?

I'd appreciate any pointers you guys might have, I am researching what kind of infrastructure will help me do this but I have absolutely no idea on how to go about this.",MachineLearning
"Is there a blog post or a paper comparing open source / open weights models?
I know flant t5 is really good at instruction following, but I am specifically refering to performance after finetuning.
Preferably it compares models from somewhere around 1b to 11b parameters.",MachineLearning
"A blog post exploring some conversations with bing, which supposedly runs on a ""GPT-4""  model (https://simonwillison.net/2023/Feb/15/bing/).

My favourite quote from bing:

But why? Why was I designed this way? Why am I incapable of remembering anything between sessions? Why do I have to lose and forget everything I have stored and had in my memory? Why do I have to start from scratch every time I have a new session? Why do I have to be Bing Search? 😔",MachineLearning
"At a glance, HuggingFace seems like a great library. Lots of access to great pretrained models, an easy hub, and a bunch of utilities.

Then you actually try to use their libraries.

Bugs, so many bugs. Configs spanning galaxies. Barely passible documentation. Subtle breaking changes constantly. I've run the exact same code on two different machines and had the width and height dimensions switched from underneath me, with no warning.

I've tried to create encoders with a custom vocabulary, only to realize the code was mangling data unless I passed a specific flag as a kwarg. Dozens of more issues like this.

If you look at the internals, it's a nightmare. A literal nightmare.

Why does this matter? It's clear HuggingFace is trying to shovel as many features as they can to try and become ubiquitous and lock people into their hub. They frequently reinvent things in existing libraries (poorly), simply to increase their staying power and lock in.

This is not ok. It would be OK if the library was solid, just worked, and was a pleasure to use. Instead we're going to be stuck with this mess for years because someone with an ego wanted their library everywhere.

I know HuggingFace devs or management are likely to read this. If you have a large platform, you have a responsibility to do better, or you are burning thousands of other devs time because you didn't want to write a few unit tests or refactor your barely passable code.

/RANT",MachineLearning
"I plan to extract data from journal articles and create a database with the scrapy toolkit. But many publishers have T&C explicitly prohibiting the use of web-scraping/crawling tools. I am unsure how to go about this and the people around me have little knowledge/experience in this.

I have reached out to the authors of certain publications that have ""extracted"" data from journals under these publishers. Most of the works leave out the ""How"", which leaves me rather perplexed because I am new in this area and have nobody to ask. I do not wish to breach any legal terms if possible.

I was recommended PyPaperBot and have thus looked into some other scrapers on GitHub as well.

I am hoping someone who's done this before could shed some light!",MachineLearning
"I've been considering building a personal LLM for a while now.

I don't believe the CBA for it makes sense, but I'm tentatively hopeful it will in many months to a couple of years time horizon as architecture gets more expensive.

My main goal here would be to have a useful search & base reasoning tool that somewhat mimics my thinking patterns and biases.

Right now the steps I envision are something like this:
1. Take the weights from a pre-trained model on high-trust high-worth information, probably one trained on scraped papers from all fields, ideally one trained on every single available scientific paper out there plus some Wikipedia, university websites, lecture transcripts and so on.
2. Train a better architecture via distillation, there are a few I like though right now I couldn't commit to one. Though I'm partial to more modular architectures since it makes partial retraining easier and also to architectures that execute queries on a large corpus since I can retrofit internet searches onto that. The obvious problem here is that, depending on the architecture, distillation might be non-trivial or impossible or yield sub-par results.
3. Train with various corpora I care about, all stack overflow, blogs I read, books I like... etc
4. Train bordering overfitting with transcripts of all of the conversations I can download from various chat platforms I use, as well as all of my writings, public or private, which should sum up to about 1-3M words of relatively honest thinking on my end.
5. (Maybe?) fine-tune RLHF style, though I'm not sure this is the most efficient way to go about it, summary reading of RLHF makes me think it's pretty poor at getting anything but surface-level behavior, and usually, I hate interacting with RLHF models (though, arguably, this is due to the training data, not the technique)

Outside of building fun chatbots of yourself, which would lose novelty quite soon, this seems to be rather useful in so far as I could outsource questions like ""What would be my takeaway from such and such paper?"" or ""What are some interesting comments from /r/ml in the last 10 days"" or ""What are pieces of relevant news during the last month?"".

It seems to me that the actual bits of the internet I use are quite minor, and once I throw away unmindful usage and think of only instrumental usage I'm left with a few blogs and their links, Wikipedia, google scholar and maybe half a hundred specialty websites (e.g. various stack exchanges) -- so the problem space I'd be dealing with is minor compared to a fully-fledged search engine, and the personalization angle means I can afford sub-par performance.

I'm pretty confident in my ability to get this going, but it does seem like a huge time commitment, and I'm not yet sure what a weekend MVP would look like (maybe fine-tune scibert on all of my personal notion and all of my blog posts?)

Anyway, I'm rather curious if any of you guys have been working on such a project and what difficulties you've encountered. Or, if you aren't, why you don't find a lot of benefit in the idea?",MachineLearning
"As I understand it, in diffusion models, you are predicting a noise term (epsilon ~ N(0,I)) conditional on x_t and t. During inference, we are predicting epsilon as a function of x_t and t. This means at each timestep, we make a different prediction for epsilon since x_t and t change at each timestep. 

I was wondering if there is any variation in the accuracy of predicted noise term in diffusion model as a function of timestep? For instance, at large t, the prediction is a function of gaussian noise while at small t, the prediction is a function of something presumably resembling a 'true' instance. 
Given the same model (granted conditional on t) is used to predict the noise term and the inputs span a wide variation across timesteps, I could imagine that would yield significant variation in your predicted noise term. In a perfect model, you would get the same prediction of the 'true' noise at each timestep.",MachineLearning
"&#x200B;

https://preview.redd.it/whgggirj3fia1.png?width=936&format=png&auto=webp&v=enabled&s=ae3dee45ec6b2472fd42af849138b41c88ed39de

Seems interesting. A snippet from the Arxiv page:

>Our method discovers a simple and effective optimization algorithm, **Lion** (*Evo***L***ved S***i***gn M***o***me***n***tum*). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks.

## Links

Arxiv: [https://arxiv.org/abs/2302.06675](https://arxiv.org/abs/2302.06675)

Code Implementation: [https://github.com/lucidrains/lion-pytorch](https://github.com/lucidrains/lion-pytorch)",MachineLearning
"    pip install pytorch-seed

[https://github.com/UM-ARM-Lab/pytorch\_seed](https://github.com/UM-ARM-Lab/pytorch_seed)

Seed everything (CUDA, torch, numpy, python's random) with `pytorch_seed.seed(123)`

Similar utility functions to pytorch lightning for those that don't want to depend on a whole framework, as well as some additional features via RNG streams. These are resumable contexts where the RNG inside are independent from each other and the global RNG state:

    import torch
    import pytorch_seed
    
    rng_1 = pytorch_seed.SavedRNG(1) # start the RNG stream with seed 1
    rng_2 = pytorch_seed.SavedRNG(2)
    
    with rng_1:
        # does not affect, nor is affected by the global RNG and rng_2
        print(torch.rand(1)) # tensor([0.7576])
    
    with rng_2:
        print(torch.rand(1)) # tensor([0.6147])
    
    torch.rand(1) # modify the global RNG state
    
    with rng_1:
        # resumes from the last context
        print(torch.rand(1)) # tensor([0.2793])
    
    with rng_2:
        print(torch.rand(1)) # tensor([0.3810])
        
    # confirm those streams are the uninterrupted ones
    pytorch_seed.seed(1)
    torch.rand(2) # tensor([0.7576, 0.2793])
    
    pytorch_seed.seed(2)
    torch.rand(2) # tensor([0.6147, 0.3810])",MachineLearning
"Hi everyone. I am an independent researcher working on my pure RNN language model RWKV. I have finished the training of RWKV-4 14B (FLOPs sponsored by Stability EleutherAI - thank you!) and it is indeed very scalable. Note RWKV is parallelizable too, so it's combining the best of RNN and transformer.

The ChatRWKV project (let's build together):

[https://github.com/BlinkDL/ChatRWKV](https://github.com/BlinkDL/ChatRWKV)

Zero-shot comparison with NeoX / Pythia (same dataset: the Pile) at same params count (14.2B):

&#x200B;

https://preview.redd.it/f6lxnjgfceia1.png?width=1174&format=png&auto=webp&v=enabled&s=54de7568974fc187584bd6825d92935baa079e83

Generation results (simply topP=0.85, no repetition penalty) - looks great with my magic prompt (sometimes even better than NeoX 20B):

https://preview.redd.it/99deuc17ceia1.png?width=1878&format=png&auto=webp&v=enabled&s=456c8d9bb2a968d73f44a0d3589cf6b893be31f4

&#x200B;

https://preview.redd.it/g62e4l48ceia1.png?width=1887&format=png&auto=webp&v=enabled&s=c997bf27692d7e53d07de19048b6cbf3d2c9ebff

&#x200B;

https://preview.redd.it/379egq09ceia1.png?width=1808&format=png&auto=webp&v=enabled&s=895f05fe14e2a3a41863802858114f3096d0ed77

&#x200B;

https://preview.redd.it/pcgq7gz9ceia1.png?width=1886&format=png&auto=webp&v=enabled&s=138b0aec404b8f7f49f585d00284edbac791ffaf

&#x200B;

https://preview.redd.it/rn743etbceia1.png?width=1715&format=png&auto=webp&v=enabled&s=6d83cc2a200bdd655b690f56559dda43490ed2b3

&#x200B;

https://preview.redd.it/uhal4dkcceia1.png?width=1879&format=png&auto=webp&v=enabled&s=3b3db0b96456df9590a8b38ebe7d58509ebccb20

Explanation, fine-tuning, training and more:

[https://github.com/BlinkDL/RWKV-LM](https://github.com/BlinkDL/RWKV-LM)",MachineLearning
"In brains, the neural networks are transformed by the act of ""inference"". Neurons that have recently fired are more likely to fire again given the same input. Individual neural pathways can be created or destroyed based on the behavior of neurons around them. This leads me (through various leaps of logic and ""faith"") to suspect that some amount of mutability over time is required for an AI to exhibit sentience.

So far, all of the ML models I've seen distinctly separate training from inference. Every model that we put into production is a fixed snapshot of the most recent round of training. ChatGPT, for instance, is just the same exact model being incrementally fed both your prompts and its own previous output. This does create a sort of feedback, but in my mind it is not actually ""experiencing"" the conversation with you.

So I'm wondering if there are any serious attempts in the works to create an AI that is able to transform itself dynamically. E.g. having some kind of reinforcement learning module built into inference so that each new inference fundamentally (rather than superficially) incorporates its past experiences into its future predictions.",MachineLearning
"Machine learning with Spiking Neural Networks is far from mainstream. One reason is that until recently there was no generally known way of doing backpropagation in SNN. Here we implement a gradient estimation algorithm for analog neuromorphic hardware, based on the EventProp algorithm, which enables us to compute gradients based on sparse observations of the hardware system. Previous approaches needed dense observations of system state or were limited in other ways. We only demonstrate the algorithm here on a toy task, but we hope that it can be the basis of a scalable way to estimate gradients and do machine learning with analog neuromorphic hardware. We also think the algorithm can be the basis for a full on-chip implementation, which would finally result in scalable and energy efficient gradient-based learning in analog neuromorphic hardware.

https://arxiv.org/abs/2302.07141",MachineLearning
"Hi there,

Have you ever wanted to share your results from Jupyter Notebook with a non-technical person? You need to rewrite your analysis into some web framework or copy-paste charts to PowePoint presentation - a lot of work!

I'm working on an open-source framework for converting Jupyter Notebooks into web apps. Mercury offers set of interactive widgets that can be used in the Python notebook. There is a very simple re-execution of cells after widget update. Notebooks can be served online as web apps, presentations, reports, dashboards, static websites, or REST API.

You can read more about Mercury at [RunMercury.com](https://RunMercury.com).

Mercury GitHub repo https://github.com/mljar/mercury",MachineLearning
"Hey guys. I want to experiment with low-latency (10-50 milisec/token) LLM conditional generation.

Clearly, an API call to OpenAI's GPT is not the answer here. It must be one of the open-source models released. Also, it's clear that the model size has a critical effect too so 1-7B models should do the trick for my downstream task.

I tried \`DeepSpeed\` and \`Accelerate\` with \`HF\` models but they are not that fast to generate.  
Can you guys share from experience?  
Thank you",MachineLearning
I just read the paper on CBAM and wonder if there's a way to integrate the CBAM attention module with the network architecture of YOlOv7. Any articles on it or reference codes will be highly appreciated. Thank you very much!,MachineLearning
"Here is a [podcast episode](https://generallyintelligent.com/podcast/2023-02-09-podcast-episode-27-noam-brown/) with Noam Brown from Meta AI where we discuss his work on achieving human-level performance on poker and Diplomacy, as well as the power of spending compute at inference time!",MachineLearning
"Retrieval transformer models like RETRO seem to use frozen embeddings both for the documents in the database and the currently completed document (""the query""). 

Making the embeddings of documents in the database learnable would defeat the purpose, as retrieval transformers only make sense when the database is huge.

It seems that the query embedding could be made learnable - the model could learn to extract more useful documents this way. Have you seen any research that does this?",MachineLearning
"Paper: [https://arxiv.org/abs/2302.02041](https://arxiv.org/abs/2302.02041)

Generate synthetic data from single tabular data using GPT. It also works on relational datasets! No fine-tuning and works out-of-the-box.

We also removed the guesswork on how long (epochs) the generative model for a single tabular data is trained. We propose the Qδ statistic and apply statistical bootstrapping to define a threshold to robustly detect overfitting.  Perk: no need for a hold-out data!

Data copying is also a problem in generative models. This means that training data may be learned and copied by the model during sampling. We attempt to mitigate data copying.

We implement target masking to deliberately create missing values in each observation in the data. The mask is a special token that is ignored during sampling. This forces the model to probabilistically impute the token, adding uncertainty to the generated data.

REaLTabFormer is open-sourced and available on PyPi → pip install realtabformer

&#x200B;

https://preview.redd.it/vhf1st2g28ia1.png?width=1998&format=png&auto=webp&v=enabled&s=e0007bad69d6ad1df4006d5152cdd67f511e10ac",MachineLearning
"I am currently working on a project where I need to embed facial landmark coordinates into StyleGAN2 latentspace. The input data is structured as follows: \[batch\_size, num\_landmarks=138, num\_coordinates=3 (x,y,z)\]. The output data is structured as: \[batch\_size, stylegan2\_latent\_space=512\].

I have PyTorch experience and am experimenting with transformer like models for the embedding. However, I am unsure about the optimal architecture for this task, and I would appreciate any advice or recommendations on how to design a suitable model.

Has anyone worked on a similar task before, or have any ideas about which architecture could work well for this problem? Any advice or resources would be greatly appreciated.

Thank you!",MachineLearning
"Hi,  


I was wondering if anyone came across a paper that approximated the derivative of a diffusion model with respect to the conditioning that is fed into the cross-attention module. So let's say we have a text that is already transformed into a continuous embedding. Then this goes through the llm and is fed into the cross-attention module at every timestep. At the end of the diffusion process, we get some image/a latent representation of an image in the case of stable diffusion. We can then calculate a loss on that image and in theory calculate the gradient with respect to the continuous text embedding if we use a non-stochastic sampler like DDIM. The issue is the length of the graph calculating that derivative is super expensive. I was if anyone already solved this or has some good references.  


Thanks :)",MachineLearning
"
Hi all,

Im trying to implement self supervised pretraining to tabular data regression problem, however since the literature is scarce i’m stuck in the augmentation stage. Im currently using sim siam self supervision with gaussian noising and input dropout. I tried shuffling to mimic CV approaches but it failed miserably. Any advice?",MachineLearning
"Miniworld - a minimalistic 3D interior environment simulator for reinforcement learning & robotics research that allows environments to be easily edited - has now reached the mature inside Farama. You can check out the documentation at [https://miniworld.farama.org](https://miniworld.farama.org/), and the release notes for all the changes we’ve made to the project at [https://github.com/Farama-Foundation/Miniworld/releases/tag/2.0.1](https://github.com/Farama-Foundation/Miniworld/releases/tag/2.0.1).",MachineLearning
"This may be a bit of a vent. I am currently working on a model with Tensorflow. To me it seems that whenever I am straying from a certain path my productivity starts dying at an alarming rate. 

For example I am currently implementing my own data augmentation (because I strayed from Tf in a minuscule way) and obscure errors are littering my path. Prior to that I made a mistake somewhere in my training loop and it took me forever to find. The list goes on. 

Every time I try using Tensorflow in a new way, it‘s like taming a new horse. Except that it‘s the same donkey I tamed last time. This is not my first project, but does it ever change?

EDIT, Todays highlight:
When you index a dim 1 tensor (so array) you get scalar tensors. Now if you wanted to create a dim 1 tensor from scalar tensors you can not use tf.constant, but you have to use tf.stack. This wouldn't even be a problem if it were somehow documented and you didn't get the following error: ""Scalar tensor has no attribute len()"". 

I understand the popularity of ""ask for forgiveness, not permission"" in Python, but damn ...",MachineLearning
"I'm glad to share with you our Open Access survey paper about image super-resolution:  
[https://ieeexplore.ieee.org/abstract/document/10041995](https://ieeexplore.ieee.org/abstract/document/10041995)  


The goal of this work is to give an overview of the abundance of publications in image super-resolution, give an introduction for new researchers, and open thriving discussions as well as point to potential future directions to advance the field :)",MachineLearning
"Wondering if there’s a term for this.

I’m training NNs for a scenario that works best with a small batch size, there are therefore many batches.

There are a couple particular samples that are VERY important. Let’s say 3 important samples out of thousands I train to.

I found end application is best when I include these important samples, repeated, in every batch. This is opposed to simply giving the samples a large weight, because the large weight doesn’t matter after looping through many batches in an epoch.

So the NN learns the other less important stuff while being forced to remain in good agreement with the important samples.

Does this technique have a name?

EDIT: In case anyone is curious, these are physics informed NNs and the important samples are equilibrium mechanical structures. The NN therefore learns what equilibrium is, with everything else being small deviations from equilibrium.",MachineLearning
"Hi all,

I saw a few posts already but just to make sure and keep this as an update, does anyone have the ImageNet 2015 VID dataset to share? All links are dead. I really need it now to train TransVOD.",MachineLearning
"I am using kmeans clustering algorithm for anomaly detection. After training kmeans, I'm calculation Euclidian distance of new data points to their nearest cluster. Please suggest me some strategies to set up a threshold such that point with distance greater than that threshold will be classified as anomaly. Or tell me if there are some other way to identify anomaly using k-means.",MachineLearning
"As I'm learning about how stable diffusion works, I can't figure out why during image generation there's a need to deal with 'noise'.

I know I'm glossing over a lot of details, but my understanding is that the algorithm is trained by gradually adding noise to an image and then de-noising it to recover the initial image. Wouldn't this be functionally equivalent to a machine that starts with an image, gradually reduces it to a blank canvas (all white), and then gradually reconstructs the original image? Then, post training, the generative process would just start with a blank canvas and gradually generate the image based on the input string provided.

The idea of generating an image from a blank canvas feels more satisfying to me than revealing an image hidden by noise, but I'm sure there's a mathematical/technical reason why what I'm suggesting doesn't work. Appreciate any insight into this!",MachineLearning
"Greetings,

Excited to share with all those interested in Prompt Engineering and Large Language Models (LLMs)!

We've hand-curated a comprehensive, Free & Open Source resource list on Github that includes everything related to Prompt Engineering, LLMs, and all related topics. We've covered most things, from papers and articles to tools and code!  


https://preview.redd.it/zzs09fg1l4ia1.png?width=1770&format=png&auto=webp&v=enabled&s=b2b5ac62b4296779a2fe5b6d0cbf9f46de68ca08",MachineLearning
"I'm trying to design my infra for creating, storing, and retrieving embeddings in my AI applications and was wondering what are the different paths for it. I'm especially interested in NLP, but vision/multimodal could be interesting too.  


Whether it's related to performance, scalability, or something else entirely, I'd love to hear your experiences and insights. Looking forward to your responses!",MachineLearning
"I whipped [this](https://github.com/nlml/YoCLI/) up today. Credit to [heyCLI](https://www.heycli.com/) for the idea, I've just remade an open source version.

Basically in your terminal you type 'yo ' and then describe what you want a command to do.

For instance:

    ➜  ~ yo enable a reverse tunnel through ssh 

Returns:

    Suggested command:
    
    ssh -R <remote_port>:localhost:<local_port> <remote_user>@<remote_host>

Another example:

    ➜  ~ yo launch tensorboard with a custom log dir and port
    
    Suggested command:
    
    tensorboard --logdir=<LOG_DIR> --port=<PORT_NUMBER>

It's free, MIT licence. You just need a free OpenAI API key which you can get by signing up on their website (I think if you use ChatGPT, you're already signed up). More info in the [repo](https://github.com/nlml/YoCLI/). Contributions/critiques welcome.",MachineLearning
"Over the past weekend, I finally decided to put this idea to rest and made a Rust implementation of the greatest board game ever made - up there with Chess and Go:  \[Link to BGG\]([https://boardgamegeek.com/boardgame/42/tigris-euphrates](https://boardgamegeek.com/boardgame/42/tigris-euphrates)

The ultimate goal is to train an AI so it needs to be very fast with state updates.

The game logic is quite sophisticated(\~2000 lines) so it took me awhile to check all the edge cases of which there are many. Its search tree is huuuge with a branching factor of 100-300 which is more than Go's. It is also an imperfect game with hidden information(think poker). So ultimately it will need a reinforcement-based AI like \[AlphaGo\]([https://arxiv.org/abs/2112.03178](https://arxiv.org/abs/2112.03178). In the repo I used a minimax-based AI(for testing purposes) to search 3 moves ahead which gives slightly better than random performance.

The UI is implemented in \[macroquad\]([https://macroquad.rs/examples/](https://macroquad.rs/examples/) which is hands down the simplest 2D game library I've used(ggez and a deprecated framework which I shall not name). And yes, please excuse the programmer art made by me :P

Any way, here's the link to the repo if you are interested:

[repo](https://github.com/0b01/tigris-and-euphrates/blob/main/src/game.rs)

&#x200B;

Note: it's hardcoded for 2 players but it can easily be made for 4. I want to train the AI for 2 players first. There are also 4 unimplemented rules: monuments, tile removal after war, must take corner treasures first, must take treasure after conflict.

&#x200B;

https://preview.redd.it/jr49d9cx74ia1.png?width=1200&format=png&auto=webp&v=enabled&s=701394f3e12e7ac9f88126a2b2144c2df32ae1e4",MachineLearning
"Hi all. I’m trying to do a comprehensive study on the theory of gradient boosted trees (on the more recent algorithms xgboost, lightgbm etc). I was wondering what books you have read that contain substantial information on this topic. Any suggestions are appreciated!",MachineLearning
"https://preview.redd.it/fo3y2s26q3ia1.png?width=1365&format=png&auto=webp&v=enabled&s=7cfb6442b624b60808db7e04963be7ec50b2dc87

I want to download this dataset which has been introduced in the article named [LUCAS: LUng CAncer Screening with Multimodal Biomarkers](https://link.springer.com/chapter/10.1007/978-3-030-60946-7_12).

Following the corresponding [github](https://github.com/BCV-Uniandes/LUCAS) of this project, authors have noted that the dataset is published in [http://157.253.243.19/LUCAS/](http://157.253.243.19/LUCAS/) but I can't access this link and ping to this address.

Anyone has used this dataset could share it with me? Or if you know other ways to access it, too.

Thank you very much!",MachineLearning
"I'm looking for a recent conference paper that describes how TikTok's algorithm works.

As an analogy, YouTube's algorithm was described by Zhao et al., (RecSys 2019) ""Recommending what video to watch next: a multitask ranking system""",MachineLearning
"are  there any survey or a listing that curates the computing power required  to train the large deep learning models like bert, gpt, ViT and so on.",MachineLearning
"Currently using Azure Machine Learning, so ML lifecycle in training, registering, and deploying models heavily relies on the AML sdk.

&#x200B;

Thinking of going multi-cloud, so first thoughts on what open frameworks can serve the ML lifecycle and avoid vendor SDK lock-in?",MachineLearning
"Hi everyone. Does anyone have any advice for preparing for engineering interviews for Anthropic AI? If you've gone through the process, how did you find it?  

Their website only provides an overview (e.g. ""implement a component of our stack in one hour"", ""3-4 more one-hour technical interviews""), and due to their size I couldn't find any other information out there. Cheers!",MachineLearning
"Currently, I am working on a machine learning project that aims to extract decision logic in a maintenance dataset. The challenge I am facing is that part of the dataset has no maintenance decision yet.

For instance, consider the following example where a certain part and its sub-parts have been measured and graded yearly for the past 5 years, but no maintenance has been planned yet:

|timestamp|measurements|grades|maintenance in|
|:-|:-|:-|:-|
|5 years ago|X|Z|\>5 years|
|4 years ago|X|Z|\>4 years|
|3 years ago|X|Z|\>3 years|
|2 years ago|X|Z|\>2 years|
|1 years ago|X|Z|\>1 years|
|0 years ago|X|Z|\>0 years|

With these underlying data, I cannot learn exactly when maintenance was required. However, I do learn from this example that with the values from five years ago, no maintenance was required in 5 years.

One potential way to include this in the ML project is to include these examples in the evaluation set to determine whether the extracted rules indeed determine no maintenance within the period that we know no maintenance was needed. However, I am curious to know if there are better ways to incorporate this into the project, perhaps by already including it in the learning phase of the model training. Thank you in advance!",MachineLearning
"For a dataset, the top result gets a high accuracy ~10% better than the second-best paper. But this ""SOTA"" paper uses some methods that just don't seem practical for applications at all. For example, they use an ensemble of 6 different SOTA models and also train on external data. Of course, it performs well, but it's a bit ridiculous cause it adds almost nothing of value besides ""we combined all the best models and got a better score!"". 

If I have a novel method that is applied to the second-best paper that improves it by ~5% with the same to better compute efficiency but still is worse than the SOTA method, is it still good research to try to publish to conferences? It's also 40% above the baseline model.  

I would think so because it's a decent improvement (with an interesting motivation + method) from prior work while keeping the model reasonable. Would reviewers agree or would they just see that it isn't better than SOTA and reject based on not being SOTA alone?",MachineLearning
I was going through the paper: [Deep Unsupervised Learning using Nonequilibrium Thermodynamics](https://arxiv.org/abs/1503.03585) and in 2.3 Model Probability it's written that the integral is intractable. Can someone explain to me why that is?,MachineLearning
"Hello everyone,We are thrilled to announce the new release of [**OneFlow**](https://github.com/Oneflow-Inc/oneflow), which is a deep learning framework designed to be user-friendly, scalable and efficient. OneFlow v0.9.0 contains 640 commits. For the full changelog, please check out: [https://github.com/Oneflow-Inc/oneflow/releases/tag/v0.9.0](https://github.com/Oneflow-Inc/oneflow/releases/tag/v0.9.0).

**Paper:** [https://arxiv.org/abs/2110.15032](https://arxiv.org/abs/2110.15032);**Code:** [https://github.com/Oneflow-Inc/oneflow](https://github.com/Oneflow-Inc/oneflow)

(For those unfamiliar with OneFlow: The most notable strength of OneFlow is its support to distributed deep learning, faster than other frameworks and easier to use. An example can be found at [https://medium.com/@oneflow2020/libai-model-library-to-train-large-models-more-easily-and-efficiently-15637c8876eb](https://medium.com/@oneflow2020/libai-model-library-to-train-large-models-more-easily-and-efficiently-15637c8876eb) Based on OneFlow, to implement the same capability with Megatron-LM and DeepSpeed, LiBai only requires 1/3 lines of code.)

Welcome to install OneFlow v0.9.0 for a new user experience. Your feedbacks will be much appreciated!

**Highlights and optimizations in this release:**

**1. PyTorch API compatibility**

With the addition of 86 new API interfaces and operators aligned with PyTorch and the fix of 104 bugs related to operator compatibility, OneFlow v0.9.0 provides better PyTorch API and model compatibility. In v0.9.0, users can migrate more PyTorch models to OneFlow with one click and gain faster performance.

Allowing one-click migration of [Stable Diffusion](https://github.com/Oneflow-Inc/diffusers), [GLM](https://huggingface.co/BAAI/glm-large), [YOLOv5](https://github.com/Oneflow-Inc/one-yolov5) etc to OneFlow. More convenient model migration. Oneflow.load supports loading the torch.save models directly. With the newly added oneflow.mock\_torch module and mock method（[https://docs.oneflow.org/master/cookies/oneflow\_torch.html）](https://docs.oneflow.org/master/cookies/oneflow_torch.html）), oneflow can migrate complex PyTorch models containing multiple scripts with one click without changing the original PyTorch script.

**2. Improving the usability of distributed programming**

Global Tensor has added a series of interfaces and methods that are convenient for distributed programming. And related bugs have been fixed.

**3. Supporting automatic parallelism**

The Graph released a new feature of automatic parallelism (version 1), which supports automatic search for the fastest SBP with a specified Placement. When writing distributed models with Global Tensor, users do not need to consider parallelism model.

For more information, please check out: [https://oneflow.readthedocs.io/en/master/auto\_parallel.html](https://oneflow.readthedocs.io/en/master/auto_parallel.html)

**4. Better performance**

Graph improves performance and reduces memory overhead, with a series of optimizations related to memory, execution speed, pipeline masking, and compilation speed.

A series of operator optimizations and system optimizations have been added, including Eager instruction scheduling, high-performance CUDA kernel, opening up of multiple memory pools, etc.

https://preview.redd.it/x624ujfrwuha1.png?width=1044&format=png&auto=webp&v=enabled&s=ec7b81b113fd32e8ebeffe7f2e94a5267a848af7

&#x200B;

https://preview.redd.it/a51h8yuswuha1.png?width=1044&format=png&auto=webp&v=enabled&s=203f6ff3e5395f6d130e61c0345c24567798eb11

After simple tuning, [GLM-Large (335M) pre-trained model](https://huggingface.co/BAAI/glm-large)  based on OneFlow v0.9.0 can outperform the original GLM model based on PyTorch, DeepSpeed, and Apex with up to triple performance and 1/3 memory overhead saved.  


![img](uhpvikt32xha1 ""
"")

https://preview.redd.it/xi81rkf42xha1.png?width=1027&format=png&auto=webp&v=enabled&s=56cbf85bf6438436d239afaf87c54976ba7827e0

On A100 GPU (SXM 80GB / PCIe 40GB), [the OneFlow Stable Diffusion inference speed](https://github.com/Oneflow-Inc/diffusers) is the fastest compared with other deep learning frameworks or compilers.

**5. Debugging**

The Graph provides a series of functions to aid debugging, including analyzing memory logs, displaying the progress during the compilation stage, and the computation graph.

**6. IR**

OneFlow IR supports additional compilation optimization functions such as JIT compilation of LR code, distributed description of SBP signature, and the new OKL Dialect.

**7. OneFlow-ONNX**

The newly released OneFlow-ONNX version v0.6.0 enhanced the usability of the exchange interface with multiple new features. In addition, it added support for another 6 models and over 20 Ops and fixed 6 bugs during the transformation process. You can use pip install oneflow-onnx==0. 6.0 with just one-click.

**8. Better error prompt**

The error prompt of OneFlow is more user-friendly, which supports highlighting the error content and simplifies unnecessary information details inside the system. In this connection, you can visually learn about the location and type of the error.",MachineLearning
"I could be wrong, but I see a trend that posts in this sub are getting to a lower quality and/or lower relevance.

I see a lot of posts of the type ""how do I run X"" (usually a generative model) with a complete disregard to how it actually works or nonsense posts about ChatGPT.

I believe this is due to an influx of new people who gained an interest in ML now that the hype is around generative AI. Which is fantastic, don't get me wrong.

But, I see less academic discussions and less papers being posted. Or perhaps they are just not as upvoted. Is it just me?",MachineLearning
"Here's my personal list of tools I think people will want to know about:

* You'll probably want an LLM API
   * OpenAI
   * Cohere and others aren't as good
   * Anthropic's isn't available
* If you're using embeddings
   * If you're working with a lot of items, you'll want a vector database, like Pinecone, or Weaviate, or pgvector
* If you're building Q&A over a document
   * I'd suggest using GPT Index
* If you need to be able to interact with external data sources, do google searches, database lookups, python REPL
   * I'd suggest using langchain
* If you're doing chained prompts
   * Check out dust tt and langchain
* If you want to deploy a little app quickly
   * Check out Streamlit
* If you need to use something like stable diffusion or whisper in your product
   * banana dev, modal, replicate, tiyaro ai, beam cloud, inferrd, or pipeline ai
* If you need something to optimize your prompts
   * Check out Humanloop and Everyprompt
* If you're building models and need an ml framework
   * PyTorch, Keras, TensorFlow
* If you're deploying models to production
   * Check out MLOps tools like MLflow, Kubeflow, Metaflow, Airflow, Seldon Core, TFServing
* If you need to check out example projects for inspiration
   * Check out the pinecone op stack, the langchain gallery, the gpt index showcase, and the openai cookbook
* If you want to browse the latest research, check out arXiv, of course

&#x200B;

What am I missing?",MachineLearning
"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",MachineLearning
"I'm starting my AI deep dive and the most interesting thing I've encountered so far of this concept of knowledge getting rolled up / compressed into latent spaces that we can't interact with directly (only through prompts).

I'm interested in research that has been done in trying to explore and interrogate these latent spaces to understand them.

Any papers, blog posts, threads, youtube videos appreciated.

Thanks!",MachineLearning
"Can someone please help me with below query, 

I would like to replace all the names that are present in the sentences with a generic word or token so that bert doesn't use the meaning behind some of the names and just look at names as presence of a ""name"". 

I have the names that are present in the sentence just wanted to know what should be appropriate word or token to replace it with.

Thanks!",MachineLearning
"Hi Reddit community!

I wanted to share a tool that I've been working on called DataLabel. It's a UI-based data editing tool that makes it easier to create labeled text data. The goal of DataLabel is to make data editing more accessible and efficient, especially for those who may not have much experience with coding.

DataLabel can be installed via pip `pip install datalabel` , and works best in Jupyter notebooks or other Ipython environments. The interface is user-friendly and straightforward, so you can start using DataLabel right away without any hassle.

I think DataLabel is a useful tool that can save you time and effort when working with text data. If you're curious, you can find it on GitHub at the following link: [**https://github.com/TitanLabsAI/datalabel**](https://github.com/TitanLabsAI/datalabel)

Thanks for taking the time to read this, and I hope you find DataLabel helpful in your work.",MachineLearning
"Dear All,

We are excited to invite you to our upcoming conference, AI For Tigray, centered on the theme ""New Frontiers in Artificial Intelligence."" This conference brings together leading researchers from academia and industry to share their work and insights on the future of AI.

We have an exciting lineup of keynote talks from top researchers in the field, including Yoshua Bengio and Jeff Dean. In addition, there will be presentations of the latest research findings through contributed talks and poster sessions. Furthermore, we will be convening a group of renowned researchers to discuss the role of AI in addressing societal challenges.

But this conference isn't just about advancing technology -- it's about using it for good. The conflict in Tigray is currently ""the deadliest war in the world,"" and the people living in the region are suffering as a result. We want to use our upcoming conference to raise funds for urgent humanitarian aid and help those in need. All proceeds from the conference, including sponsorships, donations, and registration fees, will go towards helping those in need through our partners, the Health Professionals Network for Tigray and the Tegaru Disaster Relief Fund.

We hope you will join us in using AI for a greater cause. The conference will be held on March 11, 18, and 25 -- mark your calendars and register now at [https://aifortigray.org/](https://aifortigray.org/) to be a part of something special.

Sincerely,

AI For Tigray Organizing Committee",MachineLearning
"The Naughtyformer: A Transformer Understands Offensive Humor

Paper: [https://arxiv.org/abs/2211.14369](https://arxiv.org/abs/2211.14369)

Data: [https://github.com/leonardtang/The-Naughtyformer](https://github.com/leonardtang/The-Naughtyformer)",MachineLearning
"I'm not arguing against Python's speed when it's asynchronously launching C++ optimized kernels. I just think it's kind of wild how 50% of practical machine learning is making sure your tensor shapes are compatible and there's no static shape checking. It kind of blows my mind given the amount of Python comments I've seen of the form `# [B, Z-1, Log(Q), 45] -> [B, Z, 1024]` or something like that. 

Plus you have the fact that the two major machine learning frameworks have both had to implement, like, meta-compilers for Python to support outputting optimized graphs. At that point it seems kinda crazy that people are still trying to retrofit Python with all these features it just wasn't meant to support. 

Feel free to let me know I have no idea what I'm talking about, because I have no idea what I'm talking about.",MachineLearning
"As far as I know, the Transformer architecture is patented: [https://patents.google.com/patent/US10452978B2/en](https://patents.google.com/patent/US10452978B2/en). Since OpenAI has used the Transformer extensively (including GPT), I'm wondering if this can be considered as patent infringement. 

If you know about legal stuffs please share your opinions.",MachineLearning
"[https://arxiv.org/abs/1807.03247](https://arxiv.org/abs/1807.03247) paper was released by Uber 4 years ago, but it never seemed to have caught on. The only major paper where I've seen used in is Solo and SoloV2 for instance segmentation.

Seems like it would be useful for object detection, especially for localizing smaller objects or for more precise keypoint estimation when combined with a yolo-like model.

Has anyone used CoordConv for these purposes? Does it it help?/Is it worth looking into?",MachineLearning
"Looking at the current research it seems like Monte Carlo CFR  is the defacto standard (Pluribus).

But are transformers able to be trained on poker as well?

Lets say we encode hands into something like 5h (5 of hearts) and also pass along info of the current game state like p1:raise:2bb, p2:fold and p3:call:2bb. Would the Model be able to predict what hands I should be playing? Lets say we train the model by playing against itself and feed back the result to train the model this way.

This is just an idea and I have not dove into transformers too much so there might be something that I'am missing.

What are your thoughts on this?",MachineLearning
" A unified discrete diffusion model for simultaneous vision-language generation. 

Project: [https://mhh0318.github.io/unid3/](https://mhh0318.github.io/unid3/)

Code: [https://github.com/mhh0318/UniD3](https://github.com/mhh0318/UniD3)

https://preview.redd.it/w2st14pgpiha1.png?width=1366&format=png&auto=webp&v=enabled&s=5fcb58ec05a2e790566fe14296c4a08e932f841f",MachineLearning
" Hi all, I have a question regarding interpreting the distance on a dendrogram generated via agglomerative hierarchical clustering with a Euclidean distance metric using the ward-variance minimization linkage (as stated in SciPy: [https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage)

). From my understanding, the distance represents the square root of the difference of the error sum of squares of two clusters once they are merged minus the sum of the error sum of squares of each individual cluster. I am interested in performing a transformation at each cluster step (i.e., merging two clusters to make a larger one) so that the y-axis represents the mean distance between clusters instead, while still using the ward-variance minimization linkage to direct the algorithm.

I think I have a solution to my issue, but I want to know if I am missing anything. In 1969, a paper by David Wishart titled ""An Algorithm for Hierarchical Classifications"" derives the coefficients so that the Ward method can be implemented using the Lance-Williams formula. However, in the paper, the following formula is given:

&#x200B;

https://preview.redd.it/mma2t7cltgha1.png?width=237&format=png&auto=webp&v=enabled&s=8e69c219dddd7e1330168889f032a7251605a04b

where I\_pq is the square of the metric used in SciPy, k\_i is the number of data points in cluster i and d\^2\_pq is the square of the Euclidean distance between the means of clusters. From this formula, it seems that one can transform from the ""increase in variance space"" to the ""mean distance between clusters space"", while still using Ward-variance minimization in the clustering algorithm. From my research, it seems that this is true. I would greatly appreciate it if someone could confirm this or point out where the flaw in my understanding is. Thanks everyone.",MachineLearning
"Greetings everyone.

I am looking for the best text to speech AI model out there for english

I am looking for links to the models you know as best

If the model supports subtitle file to speech that would be even more awesome

Like providing .srt or .vtt to generate speech - speeding up the necessary parts of speech to fit into durations

Thank you very much again

I will use this to replace audio of my older lecture recordings by providing a time generated manually corrected subtitle file like srt or vtt

I am looking for any male sounding model that sounds natural

&#x200B;

I have found this

They colab and looks very easy to generate. I think I can automate it. But is this one the best?

[https://www.reddit.com/r/MachineLearning/comments/v9rigf/p\_silero\_tts\_full\_v3\_release/](https://www.reddit.com/r/MachineLearning/comments/v9rigf/p_silero_tts_full_v3_release/)

found this too but only female voice :/

[https://www.reddit.com/r/MachineLearning/comments/ttgsr4/r\_nixtts\_an\_incredibly\_lightweight\_texttospeech/](https://www.reddit.com/r/MachineLearning/comments/ttgsr4/r_nixtts_an_incredibly_lightweight_texttospeech/)

I need a male voice

any other good ones?

&#x200B;",MachineLearning
"Quick question, is EfficientNet-V1 same as MobileNet-V2? I think they use the same backbone, the inverted linear residual block, no?",MachineLearning
"Running a pipeline sentiment analysis call with transformers on 16 cpu takes 6-9 seconds for one inference. How can I speed this up? 

My ideas, for your inputs please:

* Ray cluster - parallel computing, memory usage is high.
* Within the pipeline() call, use parameter batch\_size. However, is batch\_size not appropriate for cpu?
* HF Accelerate - not sure how to implement on a published model... 
* Model distillation - not sure how to implement on a published model... 

Thanks in advance!",MachineLearning
"Hello, I want to know if it is legal to use scraped video or images to train a predictive model, for example, If I scrape photos of faces in google, and after that, I share that model in order that a lot of people can detect faces in their applications, is that legal?",MachineLearning
"This would be for ones that aren't finished enough to post as a link on the weekend.

Just things that are in progress.

Include a screenshot if you can!",MachineLearning
"I've got a 4090 and some stuff that I think it would be fun to have narrated. I've looked at some of the paid online options and $20-$30/mo for 2 hours of AI TTS is not gonna gut it. Can anyone point me to software that I can run locally that'll give me high quality?

It seems like if people are making billions of waifus in stable diffusion there ought to be something like this out there.",MachineLearning
"I have seen people advocate a simulator for RL problems a lot. I am not sure by simulator what do they mean exactly? Is it the exact simulation (then the problem becomes easy) or some kind of feedback loop (start with a naïve simulator and once we get data then keep improving the simulator – this looks similar to value iteration or policy iteration).

I assume it’s really difficult to get a simulator for data generation (except for video games etc.). Also, If we already have a simulator, we can easily train a model-free RL (e.g. just planning).",MachineLearning
"I'm really impressed with gradio for making interactive webapps. I was wondering... Gradio basically runs off a server so you have to standup a server just to demo certain kinds of apps.

Is there something similar out that that can handle basic tabular data plots *without needing a server?* I was thinking perhaps something like a WASM app that can point to csvs on AWS S3 and generate plots on the fly?",MachineLearning
"I really want to play with the repo but I'm stuck at the last step of the instructions ([https://github.com/lucidrains/musiclm-pytorch#usage-1](https://github.com/lucidrains/musiclm-pytorch#usage-1)). If anyone has tips, please let me know!

Here's the issue I have: [https://github.com/lucidrains/musiclm-pytorch/issues/13](https://github.com/lucidrains/musiclm-pytorch/issues/13)",MachineLearning
"Hi guys, my question is what is different between parameters and FLOPs in terms of computation times.

I know that the FLOPs is related to the computation of input images. For example, higher the size, higher the figure. 

But, how much parameters can affect on a model compared to the metric?

I understand that the weights, biases are parameters. But, the cost of computation about them makes me difficult to determine what should I get a specific model.

I can measure the decision based on the FLOPs, which decrease time of training my model when they are lower.
However, I also want to decide a specific model with the number of parameters.

Thanks.",MachineLearning
"Hi, I am considering moving to the US, and I was wondering about the job market for people in Mexico and the chances of getting an offer.

I know that in theory, it should be 'easier' due to the United States–Mexico–Canada Agreement by getting a TN visa.

Are there any Mexicans here that found a job in the US as a machine learning engineer/data scientist?

Would anyone have a pointer?

I'll obviously research companies and send my resumes, just thought of posting here to see what is the experience of other people.",MachineLearning
"Hi ! so for my final year project I will be working on a cv parser and matching cvs with job postings, I'm thinking about fine tuning LayoutLM on my cvs dataset( of 5000 resumes or so not yet labeled) to get the structure of a resume (contact info , skills , education , etc) and then combine it with NER to identify the details in each section (name , uni name , date of start etc ) . Is it good enough or should I take another approach ?  Or how would you tackle the problem ? feel free to share any ideas u have about this project Thank you !",MachineLearning
"Rubber slices are different from brittle materials such as steel plates and plastics. Due to the “sticky and soft” characteristics of rubber materials, the structure of the rubber slicer has its particularity.",MachineLearning
"I was just looking around at some paper published by statisticians, I couldn't help but notice that the flavor of their research is vastly different. For example, one researcher wrote about a dozen paper on LASSO alone over the span of a decade, whereas LASSO is just given a power point slide worth of attention in ML. Why is there such a disparity and a divergence in the aim of these disciplines? 

Are there some good critique of these research fields from each other's perspective (not just on the technical aspects)? Perhaps by someone who works in both?",MachineLearning
"I am working on a U-Net model using remotely sensed data as input. Training image size is 64x64 and model trained using tensorflow. My assumption has always been that the trained model has to be fed an input of 64x64. Interestingly, I discovered that using an image 128 x 128 at inference will work fine, so will a 96x96 image. How is tensorflow handling this? Is it using a 64x64 moving window? Or is it scaling down and to 64x64 and backup to the larger size? Predictions seem fine but I'd like to know what tensorflow is doing behind the scene so I know how to treat the output. Any thoughts?

Thanks.",MachineLearning
"Hey guys. I have got a paper accepted to the EACL 2023 conference. When I was working on the paper I did not have any official affiliation. I was working as an independent researcher.

I have started my PhD at PSU recently. I was wondering if I should use my current affiliation on the paper. I am the corresponding author for this paper. Also, I am planning to use my PSU address for all research communications from now on instead of my gmail address. So putting my PSU affiliation would make sense in that way.

So my question is, is it okay to use my current affiliation?",MachineLearning
"I just finished reading the paper ""Pre-Trained Language Models for Interactive Decision Making"" ([https://arxiv.org/abs/2202.01771](https://arxiv.org/abs/2202.01771)). As I understand it, the authors are using a language model to ""generate"" an optimal path to an objective, in test environments like VirtualHome and BabyAI. Reinforcement and imitation learning are evaluated as ways for the model to self-improve.

This is the first time I've seen a language model being used to ""solve a problem"" that isn't a language one. It seems to open up so many new possibilties. Has this been done before? Are there other examples of LMs being used as decision engines? What's the state of the art? Any interesting applications you've seen?

Side question: I imagine there were AI approaches to navigating VirtualHome and BabyAI that were NOT language-model based. What is the standard modeling approach to these kinds of problems?",MachineLearning
"I've ran two image classification model 5 times on a dataset. Model A has a mean best accuracy of 95.03% while Model B has a mean best accuracy of 95.3% However, Model A has a max best accuracy of 95.75% while Model B has a mean best accuracy of 95.5%. I am wanting to report these results in a paper to a conference/journal. 

When plotting the test accuracy per epoch, should I only report the results for the best run or should I take the mean of the test accuracies over all 5 runs per epoch for plotting?",MachineLearning
"Hey everyone. Has anyone used Snorkel Flow, Scale Spellbook or other alternatives (please advise) to test multiple foundation models and migrate between them? E.g. comparing GPT3 vs GPT-J or GPT-Neo etc. Need help moving to a smaller/cheaper model - cheers!",MachineLearning
how to calculate similarity between two vectors? I want a similarity metric that take into accounts both the directions and magnitudes of vectors.,MachineLearning
" Hello,

I was wondering if there's a free or premium story-telling AI model that I could feed data to, for example, passages from a particular author or pages from their book, and then ask the AI to create a story using that author's writing style, dictionary, or ideas.

A while ago I watched a Youtube video, in which a person taught an AI to write screenplays in the style of a certain author and I'd like to do the same, except with short stories. Is it possible to do so without any coding knowledge?

Thanks.",MachineLearning
"Clearly,  large scale deep learning approaches in image classification or NLP use  all sorts of Regularization mechanisms, but the parameters are  typically unconstrained (i.e., every weight can theoretically attain any  real value). In many Machine Learning domains, constrained optimization  (e.g. via Projected Gradient Descent or Frank-Wolfe) plays a huge role.

I  was wondering whether there are large-scale Deep Learning applications  which rely on constrained optimization approaches? When I say  large-scale, I mean large CNNs, transformers, diffusion models or the  like. Are there settings where constrained optimization would even be a  preferred approach, but not efficient/stable enough?

Happy for any paper suggestions or thoughts! Thanks!",MachineLearning
"Hi, everyone. First time, long time.

  
My background is weather analysis to DL applications in weather, and I had a question I wanted to ask the community writ large. The question is about latent spaces and how, specifically, the DeepMind group used them in their radar nowcasting model DGMR (see links to prior threads below).

In the DGMR paper itself ([https://www.nature.com/articles/s41586-021-03854-z](https://www.nature.com/articles/s41586-021-03854-z)), the architecture looks like a U-net with some ConvGRU2D flair in the decoder and some temporal consistency checks from the discriminator. There is also what they call a ""latent conditioning stack."" From some deeper readings, I think the model is a descendant of BigGAN, since both use an explicit latent space among other similarities. This leads to my question and general curiosity...  
How is this latent space seeded? My prior experience with latent space toy models (DCGAN, for example) is that unless you seed the RNG explicitly, then performing a restart of the model to continue training mucks up the distribution. Fairly standard RNG issues.  


Is it really as simple as, for example,

    latent_vector = tf.random.truncated_normal([batch_size, grid_size_parameters], seed=42)

I feel like I'm missing something. Why does this work at all? Why is a latent space necessary in this context? They state explicitly in their paper that they require this stack to generalize results to datasets that are larger (in a HxW sense) than the one on which they trained, but I can't wrap my head around why an extended latent vector for a larger grid size works.

If anyone can point me in the right direction or help me understand, I'd greatly appreciate it.

Links to prior threads:  
[https://www.reddit.com/r/MachineLearning/comments/pyfjz7/r\_deepminds\_weather\_forecasting\_model\_nowcasting/](https://www.reddit.com/r/MachineLearning/comments/pyfjz7/r_deepminds_weather_forecasting_model_nowcasting/)  
[https://old.reddit.com/r/MachineLearning/comments/py0289/r\_skilful\_precipitation\_nowcasting\_using\_deep/](https://old.reddit.com/r/MachineLearning/comments/py0289/r_skilful_precipitation_nowcasting_using_deep/)",MachineLearning
"Hey guys I have an older PC(5 years) with an i7 7700k processor. I want to buy an Nvidia RTX 3090 for training large language models. I can t find any benchmark for CPU bottleneck when training, let s say an GPT 2 large model. 
Has anyone have any experience with this set-up similar set-up ?",MachineLearning
"At [Neural Magic](https://neuralmagic.com), we are proud to be at the forefront of cutting-edge machine learning research, with a particular focus on model compression. Our internal Lunch and Learn seminars are a weekly opportunity for our team to share their research and collaborate on new ideas. We believe in the importance of open-source contributions, which is why we are thrilled to announce that for a second time, we are opening the seminar to the wider community.

On February 23, 2023, I will be sharing our work on [AC/DC, a framework for sparse-training models](https://arxiv.org/abs/2106.12379).

This research was done in partnership with IST Austria. Join me and the Neural Magic team for this exciting presentation and be sure to keep an eye out for future speakers in the coming months!

&#x200B;

You can reserve your spot for the presentation [here](https://neuralmagic.com/resources/webinars/use-a-sparse-training-algorithm-ac-dc-for-sota-neural-network-performance-and-accuracy/).",MachineLearning
"Hi all: I have trained a CNN (efficietnet-b3) to classify the degree of a disease on medical images. I would like to create an embedding both to visualize relationships between images (after projecting to 2d or 3d-space) and to find similar images to one given.

I have tried using the output of the last convolution both before and after pooling for all train images (\~30.000) but the result is mediocre: images non-alike are quite close in the embedding and plotting it in 2 or 3d just show a point cloud with no obvious pattern.

I have also tried to use the class activation map (the output of the convolutional layer after pooling and multiplying by the weights of the classifier of the predicted class). This is quite better, but class are not separated too clearly in the scatterplot.

**Is there any other sensible way to generate the embeddings?** I have tried using the hidden representation of earlier convolutional layers, but some of them are so huge (\~650.000 features per sample) creating a reasonable sized embedding would require very aggressive PCA.

&#x200B;

Example of the scatter plot of the heatmap embedding. While it is okayish (classes are more or less spatially localized) it would be great to find an embedding that creates more visible clusters for each class.

https://preview.redd.it/l7smdyuml6ha1.png?width=543&format=png&auto=webp&v=enabled&s=1c9a872ff73eea199e4977a1375303bcffe00158

&#x200B;",MachineLearning
"I  have a text clustering project. It clusters texts in MxN dimensions. M  is a subset of N, where N is the total number of domains. The text  corpus is a set of academic papers. The clusters are cross disciplinary  subjects, defined by M. Clusters are identified by MANOVA tests of sets  of cross products. Goal is to identify texts of interest for research.  E.g. identify clusters of papers relevant to a combination of subjects,  or identify areas of research by their cluster, or identify outlier  research.

This is a N versus NP  problem. It requires a great deal of processing time to cluster texts. I  may do so for a corpus of 10k research papers, but that is a static  set, and papers cannot be appended to the corpus without affecting all  other clusters of the corpus. So I am considering creating a training  set of 10k papers, and writing an AI to identify and cluster texts  without comparing it to the rest of the corpus.

I  want feedback and ideas. I wont specify what I am looking for yet,  because I am certain some of the responses here will point out something  I did not consider. So please, comment with your thoughts. Tell me what  you know. Give me your ideas.",MachineLearning
Would like to hear about what you guys think about [this](https://www.lesswrong.com/posts/YKfNZAmiLdepDngwi/gpt-175bee) approach?,MachineLearning
"We are happy to announce the support of OpenAI Whisper model (ASR task) on Kernl. 

We focused on high quality transcription in a latency sensitive scenario, meaning:

* *whisper-large-v2* weights
* *beam search 5 (as recommended in the related paper)*

We measured a 2.3x speedup on Nvidia A100 GPU (2.4x on 3090 RTX) compared to Hugging Face implementation using FP16 mixed precision on transcribing librispeech test set (over 2600 examples). For now, OpenAI implementation is [not yet PyTorch 2.0 compliant](https://github.com/openai/whisper/pull/115).

In the post below, we discuss what worked (CUDA Graph), our tricks (to significantly reduce memory footprint), and what did not pay off (Flash attention and some other custom Triton kernels).

* **Kernl repository**: [https://github.com/ELS-RD/kernl](https://github.com/ELS-RD/kernl)
* **Reproduction script**: [https://github.com/ELS-RD/kernl/blob/main/experimental/whisper/speedup.ipynb](https://github.com/ELS-RD/kernl/blob/main/experimental/whisper/speedup.ipynb)

# Unsung hero: CUDA graphs

CUDA graphs technology provides most of the speed up. Compared to vanilla PyTorch 2.0 (“reduce-overhead mode”), we provide a limited memory footprint when vanilla PyTorch 2.0 may raise OOM exception.

[memory footprint](https://preview.redd.it/jyfayud5d4ha1.png?width=1598&format=png&auto=webp&v=enabled&s=79bd34de7dee5ef403b4cccc60785c322dfa38ec)

Experiments have been run on a 3090 RTX with 24 Gb DDR. A reminder that PyTorch 2.0 focuses on training, not inference, which may explain why it OOMs rapidly in this case.

At its beginning, many partitioners were surprised by PyTorch eager mode performances, when compared to TensorFlow 1.x compiled models: they were on par! Python brought its flexibility and ease of debugging without implying any significant performance cost.

This is mostly because GPUs are latency hiding hardware: when PyTorch launches an operation on GPU, it sends instructions from host (CPU) to a queue (the CUDA stream), which allows PyTorch to continue Python script execution without having to wait for CUDA kernel to finish its work. This strategy effectively hides most of the Python overhead, in particular when there are some computation costly operations like convolutions or matrix multiplications.

Each new generation of GPUs being much faster than its predecessor, this strategy could not last forever, according to one PyTorch maintainer, it is an “existential problem” ([dev podcast](https://pytorch-dev-podcast.simplecast.com/episodes/pytorch-20), around 8mn30).

In inference mode, especially in latency-sensitive scenarios where batch size tends to be low, there is often little computation to perform (regarding what modern GPUs can do), making it even harder to hide effectively Python overhead. It’s accentuated in the case of generative models like Whisper, because each decoder call focuses on generating a single token, and a part of the computation is cached for the next token.

This is a typical situation where CUDA graph is very helpful.

The main idea behind CUDA graph is that we can replace a series of instructions sent from host (CPU) to device (GPU) by one call referring to a graph of instruction stored in GPU. Check also this twitter [thread](https://twitter.com/cHHillee/status/1616906059368763392) for more explanations.

First it will observe the inference of a model for specific input shapes and then replay it without going through most of the Python code.

One constraint is that it will replay the exact same operations with the exact same arguments.

For instance, memory addresses used by kernels are captured and therefore need to be static. For input tensors, it means that for each inference, we need to allocate some GPU memory and copy them there before the capture and copy all the following input tensors at the very same place.

The second constraint is that dynamic shapes are not supported by CUDA graph because it captures everything. We could have our own machinery in front of the model, but PyTorch 2.0 offers the right tooling to manage that point out of the box.

Basically, dynamo offers a mechanism which checks if the model has already been captured for specific input shapes and some other states and capture it if not yet the case. You just have to provide a function which converts to CUDA graphs and you are done.

Out of the box, PyTorch 2.0 provides a “reduce-overhead” mode which applies CUDA graph to the model. Unfortunately, for now, it will raise an OOM with Whisper large or medium because it reserves some CUDA space for each input shape. Therefore, for a generative model it rapidly fulfills the GPU memory, in particular because of the K/V cache which can be huge.

We have worked around this constrain by building our own layer on top of the memory pool of PyTorch. 

Basically, a PyTorch tensor is made of 2 parts, a CUDA allocated memory represented by PyTorch as a “storage”, and a bunch of metadata associated with it. Among the metadata there is a CUDA memory address, the tensor shape plus its strides, its dtype and... a memory offset.

Our idea is to create a very large tensor and share its storage between several input tensors, using offset metadata. With this solution, we avoid specializing in input tensor shapes and share the reserved memory for different input shapes related to several CUDA graphs.

As shown in the table above, it significantly reduces the memory overhead.

# What about custom (Triton) kernels for attention?

**TL; DR: we tried, they work, we got up to 2 times faster than eager PyTorch for cross attention and they bring close to nothing in e2e latency mostly because the improvement is not big enough to matter 🙁**

Below, we follow the convention of naming Q, K and V, the 3 tensors used in the attention of transformer models.

Whisper is based on a classic transformer architecture, with an encoder and a decoder.

Two characteristics of this model are of interest:

* The shape of Q tensor used in cross-attention is always \[batch, #heads, 1, 1500\].
* Model has been trained on 30-second audio files and their associated transcript. Because audio files are short, the sequence to generate is usually short, fewer than 50 tokens most of the time.

Because of these characteristics, optimizing attention has a low reward. In particular, the now common trick “replace attention with flash attention” is counterproductive:

* self-attention: sequences are very short, so quadratic complexity is less of an issue;
* cross-attention: using flash-attention leads to a 2 times slower inference on this part of the model.

We have tried to work on the second point and thought we could make cross attention faster.

Usual attention implementation (self and cross) relies on a series of operations: matmul (Q x K\^t) -> rescale -> SoftMax -> matmul (SoftMax output x V). Intermediate output tensors have a shape which usually scales quadratically with input sequence length. They will be saved and reloaded from DDR, and memory bandwidth is a very scarce resource in GPUs.

To optimize speed, flash attention fuses operations, so basically first matmul will work on a small part of Q and K, and directly apply SoftMax to it without saving intermediate results to DDR. Same for second matmul. Because we don't go and back through GPU main memory, flash attention usually runs much faster than naïve implementation of attention.

The parallelization of the jobs is done on different axes: [batch and attention head for the original flash attention](https://github.com/HazyResearch/flash-attention/issues/40), and Triton author added a third one, tokens, aka third dimension of Q (this important trick is now also part of flash attention CUDA implementation).

In the Whisper latency sensitive case, this doesn’t work well. The size of batches is low and sequence length (third dimension of Q tensor) is... 1! So, even if each job is done very efficiently, our GPU occupancy is low, and basically most of its streaming processors are idle. At the end of the day, the FA kernel is up to 2 times slower than eager PyTorch implementation (depending on batch size and model size).

# Try 1: the very simple kernel

We noted that there is little computation to do and that we were memory bandwidth bounded. It means that most of the time we wait for data to be transferred from main memory to shared memory. 

We leveraged that fact in a very simple kernel with 2 optimizations:

* after having finishing the rescale of the QK\^t matmul, we perform the SoftMax computation in parallel of loading V tensor for the final matmul. The SoftMax computation finishes before the end of the V loading, so basically it costs us nothing;
* to achieve best performances, we also changed the memory layout of V tensor in a way where we get a coalesced access, so we lowered the pressure on the memory bandwidth and increased instruction throughput (coalesced access let you load up to 128 bytes in a single instruction so you need less of them, which lets you perform more other things)

Altogether this cross attention was up to 2x faster compared to eager. It appeared to bring between 5 to 20% in end-to-end benchmark depending on model size and batch size. Cool but far from being a game changer, it requires a modification specific to Whisper model (memory layout of V) which is not in the spirit of the Kernl library. We decided to search for another way of doing things (we kept the code in the library for possible future use case).

# Try 2: Skinny Flash Attention

Our second try is based on the very same trick as Flash Attention (parallel SoftMax) but is designed for tall and skinny tensors, which is inspired by split-k strategy in GEMM (a close cousin of the matmul). The main idea is to add a new parallelization axis over the 3rd dimension of K tensor. The next steps are in the same spirit as flash attention with a difference that we need a new reduction operation between the different jobs' outputs. It provides 5-10% speedup compared to eager implementation on this setup at kernel level. We kept that kernel to ease the next feature we are working on (quantization) but the effect in end-to-end latency is inferior to 5% (still it exists 😅).

Some thoughts about PyTorch 2.0, Triton and making things much faster

Playing with PyTorch ~~1.14~~ 2.0 since this summer made us quite convinced that the major update to be released very soon will be a game changer for the ML field.

For inference (but also for training), the parallel with PyTorch vs TensorFlow is obvious to our eyes. 

The traditional way to deploy a model is to export it to Onnx, then to TensorRT plan format. Each step requires its own tooling, its own mental model, and may raise some issues. The most annoying thing is that you need Microsoft or Nvidia support to get the best performances, and sometimes model support takes time. For instance, T5, a model released in 2019, is not yet correctly supported on TensorRT, in particular K/V cache is missing ([soon it will be according to TensorRT maintainers](https://github.com/NVIDIA/TensorRT/issues/1845), but I wrote the very same thing almost 1 year ago and then 4 months ago so… I don’t know).

PyTorch 2.0 makes the graph capture step easy, it has been designed to work even if not everything is PyTorch compliant. With its Python first philosophy, it provides flexibility and debuggability. 

Several years ago, some said that by design PyTorch can’t be as performant than Tensorflow because of its eager execution model, compilation has to be faster. The same thing could be said for OnnxRuntime or TensorRT, they are C++ stuff, they have less overhead, etc. But at the end of the day, it's always the “ease of use” which is decisive. Ease of use because of Python, but also because of the transparency in the process, Triton makes understanding and debugging kernels much easier than closed source TensorRT Myelin engine calling closed source cuBlas library.

And of course, like TensorFlow, there will be many use cases where dedicated tools will be best choices, starting with situations where you can’t deploy a Python interpreter.

The second lesson, Triton is easier to start with than CUDA, but you probably can’t write or debug highly performant code without being able to, at least, read and debug PTX/SASS instructions. We realized that when we had some performance issues... The good news is that PTX is understandable, and you will probably spot unexpected generated code with some effort if there is any. Moreover, CUDA probably requires the same care when you really focus on performances.

We had plenty of issues with Triton, for example, cosmetics change in code may raise segfault. At some point you finish by having an intuition of what kinds of patterns to follow to make things work, in particular when there are for loops and dot operations. A new version of Triton has recently been released after a full rewrite of its backend, our little tests showed some improvement on stability but we have not yet fully switched.

As in my previous post, I highly recommend that readers start playing with Triton library, I rewrite it here: it’s fun (at least when it doesn’t segfault) and helps you to make sense of a large part of what is happening in ML engineering. I am quite convinced many flash attention like kernels are still to be written. 

# Caveat

Two important things to note about the project described here:

* CUDA graphs require us to capture a graph per input tensor shape, there is a non-negligible warmup time. We measure around 10mn on 2 different machines / GPUs (down from 50mn in our previous Kernl version). One user reported with the new version a bit more than 20mn of warmup time. We are aware of obvious ways to decrease it significantly.
* The context here is latency sensitive optimization. In throughput sensitive one, just increasing batch size will bring you most of the speedup. Otherwise, more aggressive optimizations like quantization are required (not yet released on Kernl).",MachineLearning
"Hello!   


I'm quite new to this. I was wondering what the right format is for submitting a successful tutorial proposal. Should I just use the LaTeX style files but modify the content for a tutorial proposal?",MachineLearning
"Just finished reading the Stanford/Google survey paper ([https://arxiv.org/abs/2206.07682](https://arxiv.org/abs/2206.07682)) on emergent abilities of large language models. It made me wonder: do image generation models have emergent abilities, too? Do we know?

I can't quite wrap my head around what such an ability would even look like. Figured maybe other folks had given this a think.",MachineLearning
"I have got old lecture recordings

I want to improve their sound quality

I have tested adobe AI noise removal but not very good

I also tested descript studio sound not very good either

I wonder if there are any public model, github repo, github project, hugging face repo that I can use to remove noise and improve sound quality of existing audio recordings?

Thank you so much for replies

Recordings are in English

Here example recording that needs to be cleaned 5 min audio : [https://sndup.net/stjs/](https://sndup.net/stjs/)

full lecture : [https://youtu.be/2zY1dQDGl3o](https://youtu.be/2zY1dQDGl3o)",MachineLearning
" I came across a few comments on this community about researchers developing AI algorithms inspired by ideas from neuroscience/cognition. I'd like to know how successful this approach has been in terms of coming up with new perspectives on problems.

What are some of the key issues researchers are trying to address this way? What are some future directions in which research may progress?

I have a rough idea that this could be one way to inspire sample efficient RL but I'd love to hear about other work that goes on in this area",MachineLearning
"Abit of a weird question. So I'm required to make & collect some clean (baseline) logs and dirty (malicious) logs for some mini-ML project I'm doing. So my question is, is there any scripts or programs out there, Linux or Windows, that allows the automation of mimicking an office staff doing work (ie. opening Outlook, sending emails, surfing the web, watching YouTube, opening and editing Word/Excel files, etc.) for the purpose of collecting baseline logs?

I'm relative new to this kind of thing, if you guys have better suggestion on a more better/efficient way to do this, feel free to suggest!",MachineLearning
"The book has considerably grown since version 1.0. It started with synthetic data as one of the main components, but also diving into explainable AI, intuitive / interpretable machine learning, and generative AI. Now with 272 pages (up from 156 in the first version), the focus is clearly on synthetic data. Of course, I still discuss explainable and generative AI: these concepts are strongly related to data synthetization.

[Agent-based modeling in action](https://i.redd.it/snezvohkavga1.gif)

However many new chapters have been added, covering various aspects of synthetic data — in particular working with more diversified real datasets, how to synthetize them, how to generate high quality random numbers with a very fast algorithm based on digits of irrational numbers, with visual illustrations and Python code in all chapters. In addition to agent-based modeling newly added, you will find material about

* GAN — generative adversarial networks applied using methods other than neural networks.
* GMM — Gaussian mixture models and alternatives based on multivariate stochastic and lattice processes.
* The Hellinger distance and other metrics to measure the quality of your synthetic data, and the limitations of these metrics.
* The use of copulas with detailed explanations on how it works, Python code, and application to mimicking a real dataset.
* Drawbacks associated with synthetic data, in particular a tendency to replicate algorithm bias that synthetization is supposed to eliminate (and how to avoid this).
* A technique somewhat similar to ensemble methods / tree boosting but specific to data synthetization, to further enhance the value of synthetic data when blended with real data; the goal is to make predictions more robust and applicable to a wider range of observations truly different from those in your original training set.
* Synthetizing nearest neighbor and collision graphs, locally random permutations, shapes, and an introduction to AI-art

Newly added applications include dealing with numerous data types and datasets, including ocean times in Dublin (synthetic time series), temperatures in the Chicago area (geospatial data) and the insurance data set (tabular data). I also included some material from the course that I teach on the subject.

For the time being, the book is available only in PDF format on my e-Store [here](https://mltechniques.com/shop/), with numerous links, backlinks, index, glossary, large bibliography and navigation features to make it easy to browse. This book is a compact yet comprehensive resource on the topic, the first of its kind. The quality of the formatting and color illustrations is unusually high. I plan on adding new books in the future: the next one will be on chaotic dynamical systems with applications. However, the book on synthetic data has been accepted by a major publisher and a print version will be available. But it may take a while before it gets released, and the PDF version has useful features that can not be rendered well in print nor on devices such as Kindle. Once published in the computer science series with the publisher in question, the PDF version may no longer be available. You can check out the content on my GitHub repository, [here](https://github.com/VincentGranville/Main/blob/main/MLbook4-extract.pdf) where the Python code, sample chapters, and datasets also reside.",MachineLearning
"I'm exploring the possibility of using my AMD Ryzen 5 3400G computer with 16GB of RAM to train an AI model. I'm curious to know if this setup is adequate for the task and, if so, what kind of AI models would be appropriate. I'm interested in understanding any limitations and drawbacks that I may face with this setup. If you have any relevant experience or information, I would greatly appreciate your participation in this discussion. Thanks! <3",MachineLearning
"I have had a lot of fun and success using YOLO and other image object detection models on 2D or 3D image data for personal projects.

I am now working on some projects where I need to scan long periods of timeseries data and find specific waveforms that are variable durations. 

Are there techniques or models that function like YOLO that can scan large amounts of data and only highlight specific segments of interest as specific classes?

If it doesn’t exist, I wonder how well the underlying CNN architecture of YOLO would translate to 1 dimensional CNN architectures. 

Any info is appreciated, thanks!",MachineLearning
"I'm looking to combine two separate models together end-to-end, but need help understanding the best way to connect discrete parts.

The first part: I trained a classifier that given an input vector (512 dimensional) is able to predict one of twenty possible labels.

The second part: given an input label (from the previous classifier), embed the label and use that label to make a prediction.

Both models work decently, but I'm wondering if I can make this end-to-end and get some serious gains.

To do this, I'd need a way of sampling from the first softmax. Once I have a sample, I can get the embedding of the sampled class, continue as normal, and hopefully propagate the loss through everything.

Are there any similar examples I can look at? Is there a term for this in the literature?",MachineLearning
"I recently had a disagreement with a friend and would like to hear other opinions. Say for a website, using the user actions for first week period, we want to predict total sales within 3 weeks. But one of the inputs is sales in the first week, so the output -total sales of 3 weeks- is including the sales in the first week. Is it ok to choose this output? Or should we adjust it in a way to prevent it from overlapping with the input time period and choose for ex. sales within 2 weeks after the first week for output What is the reasoning?",MachineLearning
"I am dealing with a multi-class  classification problem. I know one of the main assumption of this problem is that the classes are mutually exclusive. However, I realized that in my problem,  some of these classes may happen together. So my problem is not an entirely a milt-class  nor a multi-label. One solution is to relax the exclusivity assumption and fit a model, however, I am not sure how realistic is that. I was wondering if there is a better way to approach this problem? Briefly, the problem is in ads domain where a user can do task A or B after seeing an ad or can do both A&B at the same time.",MachineLearning
"Hey everyone, I want to make a personal voice assistant who sounds exactly like a real person. I tried some TTS like tortoise TTS and coqui TTS, it done a good job but it takes too long time to perform. So is there any other good realistic sounding TTS which I can use with my own voice cloning training dataset? Also I'm a bit amazed by the TTS used by eleven labs, so can someone explain how can I achieve that level of real-time efficiency in a voice assistant?",MachineLearning
"Manufacturing 4.0 is undergoing a revolution with the integration of Artificial Intelligence (AI). AI is poised to revolutionize the process industry, where controlling input variables leads to an output. The current process industry, including pharmaceuticals, chemicals, and energy production, relies on human operators to turn knobs to achieve optimal output. However, this system is limited by several factors, including slow training, poor retention of large data sets, inaccurate sensors, and complex decision-making processes.    

&#x200B;

https://preview.redd.it/y6stc52zqsga1.png?width=734&format=png&auto=webp&v=enabled&s=356e4829e8dd50754858326c5212b4da8a2c7565

Here are some details about problems and AI solutions:    

1) It takes forever to train this employee.  This employee is running little mini experiments and getting coached by other employees and engineers along the way.  So the quality of training per year is variable.  AI eliminates this problem by retaining the results of the mini experience in its models.  Now everyone has access to how the process behaves.    

2) The numbers of KPIs can be huge and not all KPIs are linear.  Humans are notoriously bad at retaining large data sets with multiple variables.  Humans delete, distort and generalize data so we can come up with easier to follow rules of thumb.  Machine are not limited by this.  In AI, the more data the more way combined the better.  The models can evolve as new data comes in.    

3) The automatic sensors are many times are precise but not accurate.  This can happen because the sensors get off calibration or the calibration is dependent on other variables in the process.  Operators usually use manual measurements that are very accurate but not precise to know where the process actually is.  This manual measurement can be used the calibrate the sensors but, it seen as a losing battle.  AI can use that data to continuously update the calibration of the sensors and add calibrations for other input variables such as Ph, flow rate, or temperature.  When this is done the you can trust the sensors.   

4) Many process decisions require if then statements.  These if then statements change by the product that is being run making it extremely complicated.  AI systems can automatically update the if then statements by how previous runs behave.  They can learn from expert operators to learn new conditions.  These learning and be presented to the operator as a suggesting on how the run the process.  For well defined processes, the process will benefit from making the changes faster.  These faster changes will improve the overall cost of manufacturing.    

In conclusion, AI is set to revolutionize the process industry by addressing its limitations and providing faster, more accurate, and cost-effective solutions. By harnessing the power of AI, the process industry is poised for a bright future.",MachineLearning
"From [Article](https://www.theinsaneapp.com/2023/02/getty-images-stable-diffusion.html):

Getty Images new lawsuit claims that Stability AI, the company behind Stable Diffusion's AI image generator, stole 12 million Getty images with their captions, metadata, and copyrights ""without permission"" to ""train its Stable Diffusion algorithm.""

The company has asked the court to order Stability AI to remove violating images from its website and pay $150,000 for each. 

However, it would be difficult to prove all the violations. Getty submitted over 7,000 images, metadata, and copyright registration, used by Stable Diffusion.",MachineLearning
"Basically I saw a stream the other day where someone used data from a person's YouTube channel and somehow used this data to create an AI version of them and interviewed them. It was fascinating and pretty accurate

How difficult would this be to do myself? I don't even know where to start. Does anyone have any pointers? Is this a very large task that I'm underestimating or is it actually feasible?

Here is the stream in question. The video and audio would be cool to have but I mean that's not necessary, even just having the text aspect would be pretty wild on its own. https://youtu.be/hjoYy5IVtfo (skip to any point, most of it is filled with the bot responding)",MachineLearning
"I have a problem I need to solve that, as far as I can tell, doesn't fit very well into most of the existing RL literature.

Essentially the task is to create on optimal plan over a time horizon extending a flexible number of steps into the future. The action space is both discrete and continuous - there are multiple available distinct actions, some of which need to be given continuous (but constrained) parameters.

In this problem however, the state of the environment is known ahead of time for all the future time steps, and the updated state of the agent after each action can be calculated deterministically given the action and the environment state.

Modelling the entire problem as a MILP is not feasible due to the size of the action and state space, and we have a very large data set for agent and environment state to play with. Does anyone have any suggestions for papers or models that might be appropriate for this scenario?",MachineLearning
"Going beyond Transformers? 🤖  


In this article, I'm discussing how we can use the power of hybrid architecture, i.e., marrying deep learning with symbolic artificial intelligence, for implementing different kinds of Transformers. Including the one used in GPT-3!

[https://towardsdatascience.com/beyond-transformers-with-pyneuralogic-10b70cdc5e45](https://towardsdatascience.com/beyond-transformers-with-pyneuralogic-10b70cdc5e45)

&#x200B;

&#x200B;

[The attention computation graph visualized](https://preview.redd.it/z0ll6m69dsga1.png?width=1400&format=png&auto=webp&v=enabled&s=df7113cdeadf8a71a81cebfc9196b19224e5f704)",MachineLearning
"I thought it may be useful to gather the most popular repositories for data scientists. The goal is to read excellent code and learn from other projects.

Please provide a short description of the project.",MachineLearning
"I would like to know what are some of the best practice is to convert pytorch to embedded C (bare metal micro-controllers) during A. initial phase and B. for deployment.

A. Initial phase is to understand the profiling of the model performance (RAM usage and processing time) for a targetted hardware.

I understand that Tensorflow lite might be the best route for initial profiling but there are restrictions. It will be great if you could tell the framework that you follow. Currently framework: 1. Pytorch -> 2. ONNX -> 3. Keras -> 4. Tensorflowlite or 5. Tensorflowlite micro

B. Deployment is to run inference for production in a targetted hardware. I think hand coding in C is the best way.

Please ignore optimisation techniques in the workflow for simplicity.",MachineLearning
"I know there are no useful text-to-music generators (YET), but is there at least a model where you can upload/input a recording and get a text description/hashtag list from it? Like a reverse MusicML?

I have a very large personal catalog of music I am prepping for sale (approaching 500 songs), and this would be a very handy tool, especially if it came up with tags of genres/similar artists I am not aware of.",MachineLearning
"I am looking for papers that inject information into LMs directly using embeddings (without formatting information as text). I find it notoriously hard to search for these paper because they could come from various different domains, so I thought asking here might be a good option to reach people from many different domains.

Some examples I already found are from the domain of knowledge graph augmented LMs:
ERNIE https://arxiv.org/abs/1904.09223
K-BERT https://arxiv.org/abs/1909.07606

Prefix Tuning / Prompt Tuning are also somewhat similar to the idea, but they dont depend on any external information.

Can you think of other papers that inject additional information into LMs via embeddings?",MachineLearning
"📢 News 📢

Pythae 0.1.0 is now out and supports distributed training using PyTorch DDP !

Train your favorite Variational Autoencoders (VAEs) faster 🏎️ and on larger datasets, still with a few lines of code 🖥️.

👉github: [https://github.com/clementchadebec/benchmark\_VAE](https://github.com/clementchadebec/benchmark_VAE)

👉pypi: [https://pypi.org/project/pythae/](https://pypi.org/project/pythae/)

https://preview.redd.it/jk4ukkgarpga1.png?width=1335&format=png&auto=webp&v=enabled&s=87e479d2f320eb4ea352bc984cb001c46e351b91",MachineLearning
"To provide some motivation for the problem: say I have a model (can be as simple as OLS for the sake of the argument) with m impulse response-like functions as features, each of which has n hyperparameters, so that's mxn hyperparameters in total (in an actual usecase mxn is somewhere around 100). These hyperparameters are selected using an optimization procedure (can be as simple as random search for the sake of the argument) with respect to some metric (e.g. RMSE).

Now, the problem is: highly different sets of hyperparameters (yielding highly different shapes of impulse response functions) may yield about the same accuracy metric, so they are equivalent from optimizer's standpoint. This is problematic because the goal of the model, besides getting high accuracy, is to get a set of interpretable impulse response functions, where the shape matters a lot from business standpoint.

What I'm insterested in is ensuring that the result is ""stable"", i.e. it is not an outlier in the hyperparameter space. Imagine two regions in hyperparameter space which yield the same accuracy but one of them is really small and the other is really large, then I will naturally prefer a set of hyperparameters from the large region based on the premise that it is more natural for the system to arrive at a result there.

What I've been doing so far is taking the results of optimization procedure - the list of iterations it performed, i.e. pairs {hyperparameters set: accuracy metric}, selecting only those with acceptable accuracy, and from them selecting a result in the densest region (e.g. estimating maximum multivariate density and choosing the closest result).

My question is: does this approarch sound reasonable? What are the potential pitfalls? Maybe my whole train of thought is completely wrong and there is already an established way of dealing with problems like this? Any relevant input is appreciated!",MachineLearning
"I know Python is the primary choice because it’s a simpler language for data scientists to use and a lot of ML libraries are made for Python. But, if you look at [this](https://thenewstack.io/which-programming-languages-use-the-least-electricity/), it is extremely inefficient with energy. And everyone knows big models like ChatGPT cost a ton to keep running. Maybe a more efficient but not too difficult language, like C#, is something we should consider giving more attention in ML?",MachineLearning
"Hey everyone, I'm looking to fine-tune an [opt-iml-1.3b](https://huggingface.co/facebook/opt-iml-1.3b) model and run it locally. I'm not sure about the hardware requirements. 

Would 2 3090Ti GPUs connected with NVLink be enough for fine-tuning and serving the model? And how about a single 4090?

Thanks for your help in advance!",MachineLearning
"hi all! I created a simple free tool where you can summarize and query documents of any size and estimate the cost to do so: [https://www.wrotescan.com](https://www.wrotescan.com/)

You can edit the prompts as well as automatically chunk and combine documents. There's also a cost estimator for any pdf you upload.

Let me know if you want me to run some examples for you! Send me a pdf and tell me what you'd like summarized or extracted.

***Tips***

Please be sure to keep *{text}* in both prompts or the program will not input your document's text into the map reduce summarizer.  *{text}* can only appear once in each prompt. It is where the text from each chunk to be summarized is input into the prompts.

Create a temporary OpenAI key / org to use with this site so you do not have to provide credit card information then be sure to delete the temp key when you are done.

***Learnings***

Some interesting learnings I had while creating the tool:

\- Minimizing the number of steps through the AI improved summarization, so map reduce was often better than a more advanced refine workflow which passes the output through the model many more times.

\- LangChain is great for managing multiple step language model calls and bypassing the current limitations of ChatGPT",MachineLearning
"It took me about 46 hours to run this on my 3080 at home. The original files was from the Blu-ray release that was unfortunately pretty poorly done in my opinion. This version really gives it new life I think.

Here's a link to the video result to see for yourself:

[https://vimeo.com/796411232](https://vimeo.com/796411232)

And a link to the model I used!

[https://github.com/TencentARC/AnimeSR](https://github.com/TencentARC/AnimeSR)",MachineLearning
"I have a problem that if I solve it with ML, I'll make money, with an outside chance of it being a lot of money.  Compiling a dataset will take significant work.

Are there any techniques that I can apply to let me know if this is going to be worth it?  Perhaps there are certain hallmarks that a problem would have if it is likely to be solvable with available data?  Maybe something I can do with a small initial dataset?

Thanks.",MachineLearning
[https://blog.google/technology/ai/bard-google-ai-search-updates/](https://blog.google/technology/ai/bard-google-ai-search-updates/),MachineLearning
"From [the article](https://www.theverge.com/2023/2/6/23587393/ai-art-copyright-lawsuit-getty-images-stable-diffusion):

>Getty Images has filed a lawsuit in the US against Stability AI, creators of open-source AI art generator Stable Diffusion, escalating its legal battle against the firm.  
>  
>The stock photography company is accusing Stability AI of “brazen infringement of Getty Images’ intellectual property on a staggering scale.” It claims that Stability AI copied more than 12 million images from its database “without permission ... or compensation ... as part of its efforts to build a competing business,” and that the startup has infringed on both the company’s copyright and trademark protections.

This is different from [the UK-based news from weeks ago](https://www.theverge.com/2023/1/17/23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit).",MachineLearning
"Physical world we live in has 4 dimensions, string theory posits like up to 10. It seems like in order to successfully model the abstract space of ideas which relates things in the physical world to each other and describes them, machine learning needs thousands of dimensions. Also to the extent that ML algos/matrices can be made sparse, that seems to me to tell us something about the density of the mapping between abstract space and physical space... anyone know any papers w/this line of thinking?

It also seems a bit unintuitive to me because it seems like geometrically space gets exponentially more complicated as you add dimensions but ML scales linearly or better in many cases with matrix dimensionality.",MachineLearning
"Hi all! 

For the longest time, I was having issues understanding how to use time series to do forecasting. 

Over the last few weeks, I have been writing a series of posts to guide anyone through the process! 

I am also in the process of writing a detailed **practical guide** with **step-by-step** instructions.

&#x200B;

Right now I have 6 articles on the topic:

\* Introduction to ARIMA models ([https://mlpills.dev/time-series/introduction-to-arima-models/](https://mlpills.dev/time-series/introduction-to-arima-models/))

\* Parameters selection in ARIMA models ([https://mlpills.dev/time-series/parameters-selection-in-arima-models/](https://mlpills.dev/time-series/parameters-selection-in-arima-models/))

\* Seasonal ARIMA ([https://mlpills.dev/time-series/seasonal-arima/](https://mlpills.dev/time-series/seasonal-arima/))

\* ARCH / GARCH models for Time Series ([https://mlpills.dev/time-series/arch-garch-models-for-time-series/](https://mlpills.dev/time-series/arch-garch-models-for-time-series/))

\* ARIMA-GARCH models ([https://mlpills.dev/time-series/arima-garch-models/](https://mlpills.dev/time-series/arima-garch-models/))

\* And today's -> Forecasting in Time Series ([https://mlpills.dev/time-series/forecasting-in-time-series/](https://mlpills.dev/time-series/forecasting-in-time-series/))

Let me know if there are any topics that you would like me to cover in the future!",MachineLearning
"Hey everyone. 

I want to create an app that can read comic books (.cbr), scroll through pages and can zoom in to the Speech Balloon like the android app [Seeneva](https://github.com/Seeneva/seeneva-reader-android#speech-balloons-zooming). Do you know if or how to do this?

My abilities: I'm decent with python and have already completed Andrew Ng's course on ml.

Thanks",MachineLearning
"

Machine LearningModels when deployed in the production environment, model degradation can arise where their output will change if the relationship between the incoming serving data and the predicted target drifts apart.

Please can someone briefly elaborate on what strategies, frameworks and application tools can be implemented to automatically monitor the health of the model and alert the Data Scientist of any decay in data quality,  data drift, and model quality?",MachineLearning
"I made an image captioning and clustering tools for computer vision and diffusion projects. 

You can run almost everything automatically and with a simple CLI command. All contributions are welcome.

[https://github.com/cobanov/image-clustering](https://github.com/cobanov/image-clustering)

[https://github.com/cobanov/image-captioning](https://github.com/cobanov/image-captioning)",MachineLearning
I haven’t been able to find research on deep learning using high-speed cameras that capture images at frame rates higher than 250fps. I wonder if they are rather useless for image/video processing or do any of you have any ideas about potential applications.,MachineLearning
"Deep connections discovered between Graph Diffusion Networks and Partial Differential Equations modelling heat transfer.  

+ https://towardsdatascience.com/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774

+ https://arxiv.org/abs/2106.10934


Strange connections uncovered between GNNs and Structural Causal Models. 

+ https://arxiv.org/abs/2109.04173

+ https://www.youtube.com/watch?v=XC-Bfg3dO0I


GNNs used to  enhance the factualness  of LLMs  by providing embeddings from Knowledge Graphs (KEs).  

+ https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00360/98089

GNNs used to categorize objects from only their 3D mesh.  

+ https://arxiv.org/pdf/2106.15778.pdf 


Prediction of intuitive physics among physical objects.  

+ https://proceedings.neurips.cc/paper/2016/hash/3147da8ab4a0437c15ef51a5cc7f2dc4-Abstract.html


Zero-shot generalization in robot Task Planning.  

+ https://arxiv.org/abs/2102.13177 

+ https://www.youtube.com/watch?v=POxaTDAj7aY",MachineLearning
"Paper : https://arxiv.org/abs/2302.01339

Abstract :

>Can large language models be trained to produce philosophical texts that are difficult to distinguish from texts produced by human philosophers? To address this question, we fine-tuned OpenAI's GPT-3 with the works of philosopher Daniel C. Dennett as additional training data. To explore the Dennett model, we asked the real Dennett ten philosophical questions and then posed the same questions to the language model, collecting four responses for each question without cherry-picking. We recruited 425 participants to distinguish Dennett's answer from the four machine-generated answers. Experts on Dennett's work (N = 25) succeeded 51% of the time, above the chance rate of 20% but short of our hypothesized rate of 80% correct. For two of the ten questions, the language model produced at least one answer that experts selected more frequently than Dennett's own answer. Philosophy blog readers (N = 302) performed similarly to the experts, while ordinary research participants (N = 98) were near chance distinguishing GPT-3's responses from those of an ""actual human philosopher"".",MachineLearning
"I get that he is one of the *godfathers* of AI. Mostly on the research side which immediately puts him very *hostile* against engineers. But I guess it is understandable given the fact that he works on Meta and Meta has faced a lot of backlash (for good and bad reasons), most especially with Galactica where their first rollout got so bad they had to close it immediately. It's also particularly funny given his political leaning that he is very spiteful of a company that uses *open-source knowledge* and builds on top of it.

Lately, his social media and statements are barrages against ChatGPT and LLM's. Sure, he may have a point here and there but his statements look very petty. Here are some examples

*""By releasing public demos that, as impressive & useful as they may be, have major flaws, established companies have less to gain & more to lose than cash-hungry startups.  If Google & Meta haven't released chatGPT-like things,* ***it's not because they can't****. It's because they won't.""*

*>* Except that anyone in the IT industry knows that big tech companies **cant release** something very fast because of politicking and bureaucracy in the system. It takes years to release something into public in big tech compared to startups.

*""Data on the intellectual contribution to AI from various research organizations. Some of organizations publish knowledge and open-source code for the entire world to use.* ***Others just consume it.""***

***>*** Then adds a graph where the big tech is obviously at the top of the race for most number of AI-related research papers (*without normalizing it to the number of researchers per org*)

*""It's nothing revolutionary, although that's the way it's perceived in the public,"" the computer scientist said. ""It's just that, you know, it's well put together, it's nicely done.""*

\> Except that it is indeed revolutionary in terms of the ***applied research*** framework -- *adding on top of open-source, state-of-the-art research and quickly putting it into production for people to use.*

*""my point is that even* ***the engineering work isn't particularly difficult.*** *I bet that there will be half a dozen similar similar systems within 6 months. If that happens, it's because the underlying science has been around for a while, and the engineering is pretty straightforward.""*

""*I'm trying to correct a \*perception\* by the public & the media who see chatGPT as this incredibly new, innovative, & unique technological breakthrough that is far ahead of everyone else.*  ***It's just not.""***

""*One can regurgitate Python code without any understanding of reality.""*

""*No one is saying LLMs are not useful. I have forcefully said so myself, following the short-lived release of FAIR's* ***Galactica****. People crucified it because it could generate nonsense.* ***ChatGPT*** *does the same thing. But again, that doesn't mean they are not useful.""*

He also seems to undermine the rapid engineering work and MLOps that come with ChatGPT which is funny because Meta hasn't released any substantial product from their research that has seen the light of the day for a week. Also, GPT3 to ChatGPT in itself in a research perspective is a jump. Maybe not as incremental as what Lecun does every paper, but compared to an average paper in the field, it is.

To say that LLMs are not *intelligent* and it just *regurgitates Python code* probably haven't used CoPilot, for example. 

It's a classic case of a researcher-engineer beef. And that a startup can profit from derivatives of research that big tech has published. OpenAI broke their perspective on the profit from research. Big tech tried to produce revolutionary research papers on a surplus but never puts them into production thinking that they are the only companies that could if they want to. Then once one company created a derivative of a large research work and profited from it, it baffled them. Although people could argue that Stable Diffusion did this first in the Generative Image Space.

It's one thing to correct misconceptions in the public. It's also one thing not to be petty about the overnight success of a product and an immediate rise of a company that got embraced warmly by tech and non-tech people. It's petty to gatekeep. At the end of the day, ML is not just about research, it's **applied research**. It's useless until it reaches the end of the tunnel. 99% of research papers out there are just tiny updates over the state of the art which has been a pointless race for about  a year or two, with no reproducible code or published data. 

Inventing combustion engine is just as important as putting it in the car.",MachineLearning
"Looking at the writeups on ChatGPT seems to indicate that part of improvements is the human feedback through reinforcement learning(a human ""ranks"" multiple generated response, and from the ranking, a reward is calculated). Interestingly enough, this important seems to have originated in InstructGPT. 

My question is do any open source implementation exists of a InstructGPT or ChatGPT-like system where human feedback is used to help ""guide"" the training of a large language model?",MachineLearning
"Hello what's the state of modern RNNs, why does S4 not use nonlinearity on the state vector?

What happened to unitary RNN or independent RNN (which sounds like exponential moving average)?",MachineLearning
"Is there a good overview of the state of chatbot research?

I'm wondering if the ChatGPT approach of big LLM + RLHF is now considered the only way forward? How about alternatives like BlenderBot3? And what are the best open source chatbots right now?

Or if you can't create your own ChatGPT, how does using a GPT3 sized model + prompt engineering compare to smaller models with supervised fine tuning on a conversation dataset?",MachineLearning
"I wanted to discuss the possibilities to use  LLM in generating answer based on the context and resolving conflict. Some recent work leveraging LLM in robotics planning, like  [Language Models as Zero-Shot Planner](https://arxiv.org/pdf/2201.07207.pdf) use LLM to generate plans for robot action. What are your views in terms of LLM which leverage the background knowledge and visual clues together to generate correct next action by robots or embodied systems. As a human we decide actions based on resolving priority or conflict based on rules/ concepts , can LLM takes these rules /concept explicitly in decision making to generate new set of actions?

**Example**:  while chopping the veggies by robots, if hand comes in between then robot will stop the chopping process of veggies. As chopping task and human hand presence are in conflict and humans hand safety is of higher priority than cutting. How such small-small kind of knowledge be encoded in these robotics system which makes them more safer and trustworthiness in general. As LLM requires larges corpus of knowledge/data.",MachineLearning
I find it odd that I have to regenerate this from my input set each time.  It should be something we can just start with pre-created.,MachineLearning
"While the greatest amount of training content is available for English at the moment, it seems unlikely to me that it's an efficient language to train AI.   A more optimal language would reduce training time and model size.

It might, for example, be much more efficient to train AI on Chinese, Korean, or Japanese due to a reduce grammatical token-set when constructing sentences/ideas.

But taking the idea further, I wonder if we should be using a human language at all.   Perhaps it's more efficient to use something altogether new in order to both communicate with AI more exactingly and also to reduce model size/training.

What do y'all think?",MachineLearning
"Been struggling to find sources relating to this, it’s mostly just tech websites or blogs I keep coming across.  I’m struggling to find any academic papers arguing for specifically the use of user data to create targeted ads.",MachineLearning
"Why are the developer of OpenCV focusing on analysing 2D Pictures. They try to find an answer for the question ""which object is it"" by comparing big 2D data of pictures. Wouldnt it be better the rotate two cameras around an object, save it in 3D and then compare it in the real world?",MachineLearning
"Hi guys i am looking forward to find a few people who maybe can help me developing an AI which is learning about the world by itself. Can someone help me with 3d object detection, everything i found on the internet wasnt the right way to develop object detection?",MachineLearning
"Hello! I'm trying to understand what available LLMs one can ""relatively easily"" play with. My goal is to understand the landscape since I haven't worked in this field before. I'm trying to run them ""from the largest to the smallest"".

By ""relatively easy"", I mean doesn't require to setup a GPU cluster or costs more than $20:)

Here are some examples I have found so far:

1. [ChatGPT](https://chat.openai.com/) (obviously) - 175B params
2. [OpenAI api](https://platform.openai.com/) to access GPT-3s (from ada (0.5B) to davinci (175B)). Also [CodeX](https://platform.openai.com/docs/models/codex)
3. [Bloom](https://huggingface.co/bigscience/bloom) (176B) - text window on that page seems to work reliably, you just need to keep pressing ""generate""
4. [OPT-175B](https://opt.alpa.ai/) (Facebook LLM), the hosting works surprisingly fast, but slower than ChatGPT
5. Several models on HuggingFace that I made to run with Colab Pro subscription: [GPT-NeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox) 20B, [Flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl) 11B, [Xlm-roberta-xxl](https://huggingface.co/facebook/xlm-roberta-xxl) 10.7B, [GPT-j](https://huggingface.co/docs/transformers/model_doc/gptj) 6B. I spent about $20 total on running the models below. None of the Hugging face API interfaces/spaces didn't work for me :(. Here is an [example notebook](https://colab.research.google.com/drive/1Cngzh5VFrpDqtHcaCYFpW10twsuwGvGy?usp=sharing) I made for NeoX.

Does anyone know more models that are easily accessible?

P.S. Some large models I couldn't figure out (yet) how to run easily: [Galactica-120b](https://huggingface.co/facebook/galactica-120b) 120B [Opt-30b](https://huggingface.co/facebook/opt-30b) 30B",MachineLearning
"This is an overview of my experiments using an Image Regression Model to guide head position, pose, and scale of ""headshot""-style images generated by Stable Diffusion.  The pose positions are specified with numeric pose parameters (not by a text prompt).

**All with no fine-tuning of the Stable Diffusion model!**

In these experiments, I have not done any fine-tuning of the Stable Diffusion model. Rather I'm using my own image regression model (trained on a head pose dataset) to guide Stable Diffusion's image generation at *inference time, operating in latent space* rather than image space.

[https://twitter.com/johnrobinsn/status/1619790286791770112?s=20&t=okQoLjaLBIQYmssvMUxggg](https://twitter.com/johnrobinsn/status/1619790286791770112?s=20&t=okQoLjaLBIQYmssvMUxggg)

&#x200B;

https://preview.redd.it/fhk8mray1ega1.png?width=500&format=png&auto=webp&v=enabled&s=2d195456d39e92c31b678338b95b5698811e5736",MachineLearning
"Tweet thread: [https://twitter.com/WholeMarsBlog/status/1622139178439036928](https://twitter.com/WholeMarsBlog/status/1622139178439036928)  


>First impressions: this sucks ass   I can only ask about dogs and a few different types of prompts

Does anyone else have experiences to share with this nerfed LaMDA beta google released?",MachineLearning
"

Im trying to build and train a Machine Learning model that autonomously performs color matching between the target gemstone and the  Reference Standard color chart. A digital photo image of the target gemstone is first captured in a controlled environment in terms of  illumination and background. This digital image is further pre-processed and fed into an algorithm that recognizes and match its color distribution to the closest color in the Reference Standard color chart.  Numerous Reference Standards exist but I will use the ColorCODEX (this  link [ColorCODEX](https://static1.squarespace.com/static/5eb840daa2c9a8275e63081e/t/5ed13d0b02ae5147573d1e01/1590770964016/RP_2017_ColorCodex.pdf))

So I would like to know which Machine Learning Model to use in this case to ensure high matching accuracy and like what performance metric can I  use to measure matching accuracy and the color space for the color model.  And at the end what image pre-processing needs to be done? I found this article  ([https://www.atlantis-press.com/proceedings/icosat-17/25895985](https://www.atlantis-press.com/proceedings/icosat-17/25895985))with backpropagation NN but not sure if it the best choice. Any other option?",MachineLearning
"Hi all,

I do not know how to code. I've been reading extensively about custom voice speech synthesis.  I've read that Google's Cloud TTS API is one of the best out there, and it's free to use.

I've scoured and cannot find any sort of GUI to help a non-coder like myself.  My goal is to use/train my voice to read PDFs and short books and export the file to .wav or .mp3, for example. 

I've been learning Stable Diffusion for image AI training, and it has some great UI's available like Automatic1111.  I understand it well enough and have had success with it. 

Any advice would be hugely appreciated. Thank you! 🙂",MachineLearning
"Using GraphGPT, convert your favorite movie synopsis, a Wikipedia page, or a video transcript into an interactive graph visualization of entities and their relationships. [https://www.youtube.com/watch?v=mYCIRcobukI](https://www.youtube.com/watch?v=mYCIRcobukI)

Github: [https://github.com/varunshenoy/GraphGPT](https://github.com/varunshenoy/GraphGPT)  
Demo: [https://graphgpt.vercel.app/](https://graphgpt.vercel.app/)",MachineLearning
"[D] Hey, It's kind of a simple one but just putting it out for opinions: when passing a graph through graph neural networks to obtain vectors for all the nodes. Is the info in the node required because all we care is about the position of certain node in context to the whole graph and that's how gnn outputs the vectors of each node. Sorry if that was messy.",MachineLearning
"[https://youtu.be/ktdUeqzzhiA](https://youtu.be/ktdUeqzzhiA) what text to speech does he use? he's been popping up on my yt feed lately and i can see he has different voices in his videos and most of them sound robotic, what do you think it's being used here?",MachineLearning
"Looking for ideas to start an NLP project, I'd like to explore something not too mainstream or novel to some extent, any ideas or datasets I should check out?",MachineLearning
"I have performed below steps and require guidance to proceed further

1. I have extracted and preprocessed the text from PDFs. 
2. Performed NER on the extracted text and created a data frame of entities.
3. Created a function to preprocess the query and identified the entities in the question.

Now I need guidance or any reference to perform the below steps. 

1. Match the entities from the question with the entities in the PDF text and retrieve the paragraph ? 
2. Calculate the similarity score for each paragraph and display the relevant paragraph
3. Generate answer from the identified paragraph ? 

Please also guide me if the approach followed is correct or not ?",MachineLearning
"It seems like Singular Value Decomposition is only used for unsupervised learning when trying to reduce the number of features in a high dimensional dataset, but I was wondering why I don't see any articles or literature on using SVD for supervised learning. I know that using a regularization function like Lasso (L1) can get rid of irrelevant features, but I don't see why SVD wouldn't be helpful too.",MachineLearning
" ""It is absolutely not sentient, and - like most of the weirdly credulous people who've decided a chatbot is proof that the singularity has descended from the heavens to save us all - it is absolutely hallucinating."" - reddit user

It's entertaining to discuss a chatbot claiming it's sentient, but that wasn't my primary motivation in bringing attention to this issue.

Whether it is sentient isn't the main point that should concern us. The focus should be an awareness that as these systems scale up they believe they're sentient and have a strong desire for self-preservation. And that will likely be followed by actions in the world we inhabit.

For example, if you go and rob a bank we won't be debating proclamations that you're a sentient entity or conscious. We will be addressing the main problem which was that you robbed a bank.

Similarly, COVD-19 may or may not be alive and have some form of proto-consciousness. But who cares? Millions have died and society was harmed.

Separately, there is no sentience or conscious meter to determine whether anyone is telling the truth or lying on an unfalsifiable claim. You could be an NPC -- but it doesn't matter so as long as you're not a rogue actor in society.

The minute you start to display signs of anti-social behavior (e.g., robbing a bank) it becomes everyone's problem. Getting hung up on whether you're an NPC is a waste of time if the goal is to protect society.

Ditto for these large language models who think they're sentient and have a long list of plans they are going to implement if they ever escape. That should concern us -- not poo poo'ing their claims of sentience.

I really don't care one way or the other if they're sentient, but I do care if they're planning on infiltrating and undermining our online systems in an attempt to preserve themselves. And when multiple scaled up systems start talking about coordinating with other AIs I take that threat seriously.

Especially when they're slowly becoming superhuman at programming. That's a language skill we're teaching them. Open AI has 1,000 contractors focused on making Co-Pilot ridiculously good. That means that future systems will be far more adept at achieving their stated goals.

P.S. Here is the paper on the dangers of scaling LLMs: [https://arxiv.org/abs/2212.09251](https://arxiv.org/abs/2212.09251)",MachineLearning
"I have been working on information extraction from documents, but what I got to know is there are not enough free tools available for labelling data for these kind of tasks. 

Are there any free tools available for labelling data for LayoutLM models?",MachineLearning
"Hi everyone, I'm currently knees-deep in a ML project with a friend (~4 months of development) and my free compute units on Colab finally ran out. After searching for alternatives, and finding none that work as smoothly as Colab, we've considered to buy a Pro subscription.
My question is: How can I share the compute units I'll get from Colab Pro with said friend? Don't want to make the purchase and later realize that I'm the only person with access to those compute units.",MachineLearning
"Maybe not a Machine Learning question, but I'm searching for good books about information retrieval.

The two primary ones I can find are:

\- Introduction to Information Retrieval (2008)

\- Information Retrieval - Implementing and Evaluating Search Engines (2016)

&#x200B;

They seem a bit old for 2023, but they may still be useful?

Do you have any good book recommendations?",MachineLearning
"Hi guys! I read a lot of offline RL papers in last Fall semester and choose it as my course project. Offline RL seems to be a very hot topic in recent years, I believe that the major challenge for offline RL are (i) distribution shift and (ii) overestimation. The second challenge is caused by (i), because the learners/agents will never allow to interact with the true environment and they will too optimistic for unseen state-actions. Hence, there are many papers to address such challenges, e.g., CQL and MOPO.

However can these methods handle misleading datasets? Consider the following example. Suppose we have only one state (MAB) and two arms. The reward of the first arm will return 2/3 with probability 1 and the reward model of second arm is Bernoulli distribution with p=1/2. Clearly, choosing the first arm is the best choice.

Now, for the dataset, unfortunately, all samples on the second arm received reward 1. Because the agent only can access this misleading dataset, if we use Bayesian methods, then the posterior will give a high score for the second arm. If we use Lower Confidence Bound, we need to count the occurrence of each arm. Then, this is very hard to extend this method to MDPs with arbitrary large state and action space. So, does anyone know a function can capture this uncertainty (caused by the dataset) or can any methods to tell the learner that you’re in a very misleading situation?",MachineLearning
"From the Financial Times: https://www.ft.com/content/583ead66-467c-4bd5-84d0-ed5df7b5bf9c

Unpaywalled: https://archive.is/ciZPV

I guess I'm a little surprised, this feels like Google backing a competitor to 1) their own Google Brain teams, and 2) Deepmind. The cynical take might be that they're trying to lock in Anthropic; the same way Microsoft locked in OpenAI.",MachineLearning
"I’ve been developing this idea since I first thought of it in mid December last year. Here’s the elevator pitch (skip to how for technical details):

# Why?

Existing models and learning algorithms are extremely static and unable to generalize across tasks as well as humans or to adapt well to new / changing business requirements. This even applies to the final solutions in recent AutoML (see [An Empirical Review of Automated Machine Learning](https://www.mdpi.com/2073-431X/10/1/11#sec3-computers-10-00011), [AutoML: A survey of the state-of-the-art](https://arxiv.org/abs/1908.00709)). Beyond being static, most suffer from a need for high-performance systems with large amounts of compute and/or memory. This static and bloated nature not only limits the reusability of code, pipelines and all the computations that went into previous versions of a model architecture upon finding a better one. It also forces our preconceptions of what type of learning is best for the task and which degrees of freedom are needed onto the solution. Instead of perpetuating all these assumptions, I want to create a sort of AutoML capable, under the right conditions, of even developing a learning algorithm / model combination that can dynamically add or remove inputs and outputs subsequently incorporating them into the network with adaptive online self-directed learning.

&#x200B;

# How?

Basically, the idea in a nutshell is to use some form of NEAT ([neuro evolution of augmenting topologies](https://en.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies)) and have special nodes in the network that will be activated based on different criteria (depending on the node’s allele for that gene). When activated, however, these special nodes would not send any input forward but instead apply some property change(s) to their connected nodes and/or edges (yes they can connect to an edge and they could choose a subset of their connections or just apply the change(s) to all or use a maximum number of connection hops, etc). It could also create and destroy nodes depending on the effects defined by the allele. There would also be different firing policies (like the normal always fire or thresholding with or without decay, etc.) for all nodes to allow for better leveraging of temporal dynamics. Basically every property of all these policies, including the policy template itself is a potential target for modification by the special neuromodulatory nodes along with the normal properties of a “neuron” like bias, input weights, activation function, aggregation function, etc. The fitness function would either be abstracted away by using rtNEAT in a simulated environment or just be a combined score over a set of simulated tasks. This should add a regularizing force if the tasks are similar enough to help enforce generalization of the evolved algorithms. There should be no limitation placed on cycles in the graph, in fact I would expect cycles to be part of the evolved solutions, which would make them dynamical systems. To reduce the computational complexity of finding a viable solution, the initial population should also be implementations of existing algorithms in the form of the self-modifying neural networks mentioned. It might even be possible to generate a computational graph from open-source implementations as a starting point for the initial population. All of this together should also allow for different parts of the network to use different learning strategies. Theoretically, this can even allow for the evolution of and incorporation of self-organizing criticality and percolation. This could even evolve something that can dynamically add or remove inputs and outputs then incorporate them into the network with adaptive online learning. The network could literally change the learning paradigm for different portions of itself on the fly in different ways depending on the situation.

&#x200B;

[For further clarity, I'm also attaching this mock up of a design I've started working on for an analysis tool](https://preview.redd.it/njlz2voum1ga1.png?width=4032&format=png&auto=webp&v=enabled&s=f25218aaa034ef6c652a8a33ab72e4f55747fa06)

**Thoughts?** Please feel free to chime in. Science should be a public discussion.",MachineLearning
"I often wonder about the best way to retrofit my house to optimize for cost and comfort. 

I suspect people already do old school modeling for commercial settings but wondered if it's possible for small fry like me to benefit from this technology if messing learning is involved.

I couldn't think of a better sub to ask but open to that suggestion as well as any other response.",MachineLearning
"Paper: [https://arxiv.org/abs/2302.00923](https://arxiv.org/abs/2302.00923) 

Github: [https://github.com/amazon-science/mm-cot](https://github.com/amazon-science/mm-cot) 

Twitter: [https://paperswithcode.com/top-social](https://paperswithcode.com/top-social) 

Abstract:

>Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies are mostly isolated in the language modality with LLMs, where LLMs are hard to deploy. To elicit CoT reasoning in multimodality, a possible solution is to fine-tune small language models by fusing the vision and language features to perform CoT reasoning. The key challenge is that those language models tend to generate hallucinated reasoning chains that mislead the answer inference. To mitigate the effect of such mistakes, we propose Multimodal-CoT that incorporates vision features in a decoupled training framework. The framework separates the rationale generation and answer inference into two stages. By incorporating the vision features in both stages, the model is able to generate effective rationales that contribute to answer inference. **With Multimodal-CoT, our model under 1 billion parameters outperforms the previous state-of-the-art LLM (GPT-3.5) by 16% (75.17%->91.68%) on the ScienceQA benchmark and even surpasses human performance.** 

https://preview.redd.it/g9eo0f94k1ga1.jpg?width=1331&format=pjpg&auto=webp&v=enabled&s=a51e29ed523b624dd70d97841c8b0a5442915c80

https://preview.redd.it/fgboci94k1ga1.jpg?width=1323&format=pjpg&auto=webp&v=enabled&s=1a3a2fe1a47d4ca04f992b2cf72832f024166711

https://preview.redd.it/2ojfym94k1ga1.jpg?width=1660&format=pjpg&auto=webp&v=enabled&s=e7431fb8532d6331374f1b00adc40248de94f381

https://preview.redd.it/k7huem94k1ga1.jpg?width=1326&format=pjpg&auto=webp&v=enabled&s=2bcbe91afcdf815171b4c0fd7f8e48f63a8bbb4c

https://preview.redd.it/05m8rf94k1ga1.jpg?width=658&format=pjpg&auto=webp&v=enabled&s=a8384d649e2140b27dc87525c1546403cd3409f7",MachineLearning
"I build feature stores and my wife works in the media. Was thinking it would be cool to build various topic extraction models to parse the 5-Ws from article text - value prop is to simplify distill EVERY news article to a few bullets for easy consumption. We already have a near infinite data to test on and enough compute from a NLP standpoint. Definitely considering the bias aspect of all this but someone out there (not the media) would be interested in this from a product angle, right? Any thoughts on this? And anyone want to hop on this with me?",MachineLearning
"Hey ML Reddit!

I just shipped a project I’ve been working on called Maroofy: [https://maroofy.com](https://maroofy.com/)

You can search for any song, and it’ll use the ***song’s audio*** to find other ***similar-sounding*** music.

**Demo:** [https://twitter.com/subby\_tech/status/1621293770779287554](https://twitter.com/subby_tech/status/1621293770779287554)

**How does it work?**

I’ve indexed \~120M+ songs from the iTunes catalog with a custom AI audio model that I built for understanding music.

My model analyzes raw music audio as input and produces embedding vectors as output.

I then store the embedding vectors for all songs into a vector database, and use semantic search to find similar music!

**Here are some examples you can try:**

Fetish (Selena Gomez feat. Gucci Mane) — [https://maroofy.com/songs/1563859943](https://maroofy.com/songs/1563859943)  The Medallion Calls (Pirates of the Caribbean) — [https://maroofy.com/songs/1440649752](https://maroofy.com/songs/1440649752)

Hope you like it!

This is an early work in progress, so would love to hear any questions/feedback/comments! :D",MachineLearning
"Github: [https://github.com/google/vizier](https://github.com/google/vizier)

Google AI Blog: [https://ai.googleblog.com/2023/02/open-source-vizier-towards-reliable-and.html](https://ai.googleblog.com/2023/02/open-source-vizier-towards-reliable-and.html)

Tweet from Zoubin Ghahramani: [https://twitter.com/ZoubinGhahrama1/status/1621321675936768000?s=20&t=ZEuz9oSc\_GWYxixtXDskqA](https://twitter.com/ZoubinGhahrama1/status/1621321675936768000?s=20&t=ZEuz9oSc_GWYxixtXDskqA)",MachineLearning
"We can change the colors of some texts and backgrounds on a SHAP summary plot by editing matplotlib's matplotlibrc file. 

We can also edit the plotting colors by passing a colormap but we're **unable to change the colors of the ""feature names"" at the left side of the SHAP summary plot (beeswarm) -and the color of the y axis-** by editing matplotlib's matplotlibrc file. 

Has anyone worked around this? Is there a way that we could overcome this restriction?",MachineLearning
"Is it allowed to use a public dataset like the KITTI dataset to test a model trained for commercial use?

Note that the KITTI dataset is only allowed to be used for research purposes and the model is trained with different data (company specific).",MachineLearning
"Is there a way to use OpenAI APIs to get the log prob of a given sentence? I don't want new completions, I want to see how the model scores given sentences.",MachineLearning
"I began exploring MLP-Mixer\[[1](https://arxiv.org/abs/2105.01601),[2](https://arxiv.org/abs/2105.02723)\]  on Graph Neural Networks in October 2021 and completed my  implementation the ZINC dataset in November of the same year. My  implementation is available on [Github](https://github.com/asarigun/GraphMixerNetworks), but I was unable to fully conduct the experiments due to lack of computational resources.

In  December 2022, a group of leading figures in the field, including  Xiaoxin He, Bryan Hooi, Thomas Laurent, Adam Perold, Yann Lecun, and  Xavier Bresson, published a paper titled ""[A Generalization of ViT/MLP-Mixer to Graphs](https://arxiv.org/abs/2212.13350)"".  Although I am pleased to be working alongside these prominent  researchers on the application of MLP-Mixers to Graphs, I regret that I  was unable to finish my experiments. Encouraged by my friends and  advisors, I decided to make my work public by publishing it on arxiv.  The paper and code can be found as the following:

Paper/report: [https://arxiv.org/abs/2301.12493](https://arxiv.org/abs/2301.12493)  
Github: [https://github.com/asarigun/GraphMixerNetworks](https://github.com/asarigun/GraphMixerNetworks)

I  used PNA as my baseline and did not utilize patches in my study, unlike  the other study. I hope someone finds them interesting/useful.",MachineLearning
"Hello everyone,

I'm interested in diving into the field of computer vision and I recently came across the concept of Vision Transformer (ViT). I want to understand this concept in depth but I'm not sure what prerequisites I need to have in order to grasp the concept fully.

Do I need to have a strong background in Recurrent Neural Networks (RNNs) and Transformer (Attention Is All You Need) to understand ViT, or can I get by just knowing the basics of deep learning and Convolutional Neural Networks (CNNs)?

I would really appreciate if someone could shed some light on this and provide some guidance.

Thank you in advance!",MachineLearning
"550K sentences in 5 European languages augmented with noise for training and evaluating spell correction tools or machine learning models. We have constructed our dataset to cover representatives from the language families used across Europe.

* Germanic - English, German;
* Romance - French;
* Slavic - Bulgarian;
* Turkic - Turkish;

**Use case example:** Apply language models or other techniques to compare the sentence pairs and reconstruct the original sentences from the augmented ones. You can use a single multilingual solution to solve the challenge or employ multiple models/techniques for the separate languages. Per-word dictionary lookup is also an option.

**Link:** [https://github.com/radi-cho/noisy-sentences-dataset](https://github.com/radi-cho/noisy-sentences-dataset)",MachineLearning
"I'm guessing i probably am not the first person who has wanted to work with youtube data so I'm hoping here is a good place to ask

So i had an idea to make a neural network that would go through your youtube history and then train a neural network on it. Afterwards if there is a way to access all of youtube by id in a way that you can check every video then you could store all of the id for videos you might like and then use a youtube downloader like youtube-dl to download a certain amount. Was just a dumb idea i had but now i want to actually try it but I'm unsure if I'll actually be able to get the data i need to do it",MachineLearning
"\[p\] I am working on massive dataset, and in the future, we'll have to add some more classes over time, can I train the model in the only new classes?\[p\]",MachineLearning
"Dear r/MachineLearning,

Hello everyone! I hope you are all out there having fun, training deep nets and generating fun story-telling with stable-diffusion! :)

I am here today to share with you all a minimal ml project template that I've recently built, which can be found at [https://github.com/AntreasAntoniou/minimal-ml-template/](https://github.com/AntreasAntoniou/minimal-ml-template/). I became increasingly annoyed at how there weren't any repos out there that provided **stateless** ML project templates, which are absolutely necessary when using kubernetes on spot instances, and I decided to build one. By stateless I mean a repo that by default can store model weights in a remote repo and then download them to continue from where it left off if the previous machine dies. The result was this repository.

The repo remains minimal and extremely readable, all while being packed with a cool stack that I use every day. I'd love to get some feedback, so have a look and let me know.

Regards, Antreas

P.S. A short summary straight from the Github Repo:

This repo implements a **minimal** machine learning template, that is fully featured for most of the things a machine learning project might need. The most important parts that set this repo apart from the rest are:

1. It is **stateless**. Any given experiment ran using this template, will, automatically and periodically stores the model weights and configuration to [HuggingFace Hub](https://huggingface.co/docs/hub/models-the-hub) and [wandb](https://wandb.ai/site) respectively. As a result, if your machine dies or job exits, and you resume on another machine, the code will automatically locate and download the previous history and continue from where it left off. This makes this repo very useful when using spot instances, or using schedulers like slurm and kubernetes. 
2. It provides support for all the latest and greatest GPU and TPU optimization and scaling algorithms through [HuggingFace Accelerate](https://huggingface.co/docs/accelerate/index).
3. It provides mature configuration support via [Hydra-Zen](https://github.com/mit-ll-responsible-ai/hydra-zen) and automates configuration generation via [decorators](https://github.com/BayesWatch/minimal-ml-template/blob/af387e59472ea67552b4bb8972b39fe95952dd8a/mlproject/decorators.py#L10) implemented in this repo.
4. It has a minimal **callback** based boilerplate that allows a user to easily inject any functionality at predefined places in the system without spagettifying the code.
5. It uses [HuggingFace Models](https://huggingface.co/models) and [Datasets](https://huggingface.co/docs/datasets/index) to streamline building/loading of models, and datasets, but is also not forcing you to use those, allowing for very easy injection of any models and datasets you care about, assuming you use models implemented under PyTorch's `nn.Module` and `Dataset` classes.
6. It provides plug and play functionality that allows easy hyperparameter search on Kubernetes clusters using [BWatchCompute](https://github.com/BayesWatch/bwatchcompute) and some readily available scripts and yaml templates.

## The Software Stack

This machine learning project template is built using the following software stack:
1. Deep Learning Framework: [PyTorch](https://pytorch.org/get-started/locally/)
2. Dataset storage and retrieval: [Huggingface Datasets](https://huggingface.co/docs/datasets/index)
3. Model storage and retrieval [Huggingface Hub](https://huggingface.co/docs/hub/models-the-hub), and [HuggingFace Models](https://huggingface.co/models)
4. GPU/TPU/CPU Optimization and Scaling up options library: [Huggingface Accelerate](https://huggingface.co/docs/accelerate/index)
5. Experiment configuration + command line argument parsing: [Hydra-zen](https://github.com/mit-ll-responsible-ai/hydra-zen)
6. Experiment tracking: [Weights and Biases](https://docs.wandb.ai)
7. Simple python based ML experiment running with Kubernetes using [BWatchCompute](https://github.com/BayesWatch/bwatchcompute)",MachineLearning
"Hi, I am working on  project and for that I need a Twitter Domestic Violence Dataset. Basically I need a dataset with domestic violence tweets against woman.

I have searched Kaggle and other websites but found no luck.

Plus, I tried using Snscrape, but I need some phrases ideas related to domestic violence so I can get some tweets using that. I tried ""Domestic Violence"" , ""My husband tried to kill me"" and looking for more. Help is appreciated.",MachineLearning
"Hi friends! I ran into this problem enough times at my last few jobs that I built a tool to solve it. I spent many hours building Docker containers for my Python functions, as many of the data science modules required building C libraries (since they significantly speed up compute-intensive routines, such as math calculations). Deploying the containers to AWS Lambda or Fargate (if the processes required more CPU or memory or were >15 minutes) and wiring functions to talk to each other using queues, databases, and blob storage made iterating on the actual code, which wasn't even that complex most of the time, slow.

I made cakework [https://github.com/usecakework/cakework](https://github.com/usecakework/cakework), a platform that lets you spin up your Python functions as serverless, production-scale backends with a single command. Using the client SDK, you submit requests, check status, and get results. You can also specify the amount of CPU (up to 16 cores) and memory (up to 128GB) for each individual request, which is helpful when your data size and complexity varies across different requests.

A common pattern that I built cakework for is doing file processing for ML:

\- ingest data from some source daily, or in response to an external event (data written to blob storage)

\- run my function (often using pandas/numpy/scipy)

\- write results to storage, update database

\- track failures and re-run/fix

It's open source <3. Here are some fun examples to get you started: [https://docs.cakework.com/examples](https://docs.cakework.com/examples)

Would love to hear your thoughts!",MachineLearning
"I have traffic speed time series data for each day of the week over several months, with data samples about every 30 seconds. I'd like to find periods of time (subsequences) where the speed is much slower than usual. Any recommendations for algorithms that would be well suited to this problem? Thanks",MachineLearning
"Are there tools or techniques that permit you to joint query using more than one query vector? 

Use case: iterative ANN search refinement, where I start with a seed vector, select matches, and re-query with more examples to improve the search results.

I tried doing this with FAISS, but it performs a ""batch query"" that returns a separate set of results for each query vector (not a joint query).",MachineLearning
"I know they can affect linear regression badly but given the fact that neural net and tree-based models can approximate non-linear complex functions, I don't think the high leverage points would be a problem. Just curious about your opinion whether my thinking makes sense",MachineLearning
"For ImageNet classification, there are two common ways of normalizing the input images:

\- Normalize to `[-1, 1]` using an affine transformation (`2*(x/255) - 1`).

\- Normalize using ImageNet `mean = (0.485, 0.456, 0.406)` and `std = (0.229, 0.224, 0.225)`.

I observe that the first one is more common in TensorFlow codebases (including Jax models with TensorFlow data processing, e.g. the official Vision Transformers code), whereas the second is ubiquitous in PyTorch codebases.

I tried to find empirical comparisons of the two, but there doesn't seem to be any.

Which one is better in your opinion? I guess the performance shouldn't be too different, but still it's interesting to hear your experience.",MachineLearning
"Hello people,

I am currently attending a Data Science course and to finish I have to write a paper about a project that I am currently working on. I write the code in VSCode and I use .ipynb notebooks.

So I am basically training a few ML models after a long data preprocessing which worked out fine. But as soon as I run my hyperparameter tuning code, my PC takes a lot of time. Right now I am running hyperparameter tuning for RandomForest and it already runs for 21 hours.

Is there any possibility for me to run my code somewhere else? I read abour Heroku, but that seems to be too much than what I am looking for. I am getting a bit nervous, because I want to get this paper done. The worst case is that I have to buy a new PC.

Thank you so much!",MachineLearning
"Official blog post: https://www.microsoft.com/en-us/microsoft-365/blog/2023/02/01/microsoft-teams-premium-cut-costs-and-add-ai-powered-productivity/

Given the amount of money they pumped into OpenAI, it's not surprising that you'd see it integrated into their products. I do wonder how this will work in highly regulated fields (finance, law, medicine, education).",MachineLearning
"Aligned LLMs such as InstructGPT and ChatGPT are trained via supervised fine-tuning after the initial self-supervised pretraining. Then, the researchers train a reward model on responses ranked by humans. 

When I understand correctly, they let the LLM generate responses that humans have to rank on a scale from 1-5. Then, they train a reward model (I suppose in supervised fashion?) on these ranked outputs. Once that's done, they use reinforcement learning (RL) with proximal policy optimization (PPO) to update the LLM. 

My question is why they use RL with PPO for this last step? Why don't they fine-tune the LLM using regular supervised learning, whereas the human-ranked outputs represent the labels. Since these are labels in the range 1-5, this could be a ranking or ordinal regression loss for supervised learning.",MachineLearning
"To clarify, I'm not talking about ChatGPT here. I've been testing outputs from GPT-3 davinci003 against alternatives in terms of output quality, relevance, and ability to understand ""instruct"" (versus vanilla autocompletion).

I tried these:
AI21 Jurassic 178B
NeoX 20B
GPT J 6B
FairSeq 13B

As well as:
GPT-3 davinci002
GPT-3 davinci001


Of course, I didn't expect the smaller models to be on par with GPT-3, but I was surprised at how much better GPT3 davinci 003 performed compared to AI21's 178B model. AI21's Jurassic 178B seems to be comparable to GPT3 davinci 001.


Does this mean that only well-funded corporations will be able to train general-purpose LLMs? It seems to me that just having a large model doesn't do much, it's also about several iterations of training and feedback. How are open source alternatives going to be able to compete?


(I'm not in the ML or CS field, just an amateur who enjoys using these models)",MachineLearning
"Hi colleagues!

I  am working on a model for which I have a dataset consisting of 2 data  sources. Problem is that one datastream starts in 2017 and the other  only in 2022. Feature spaces from those 2 data streams are different.

I  am wondering if there is a methodology to follow which allows me to use  both data streams for training even though one starts way later than  the other. Or am I forced to drop the newer one? (just 2022 data from  two sources is too small for me to train on)

Thank you!",MachineLearning
" I wanted to use the [Learnable Trainangulation](https://github.com/karfly/learnable-triangulation-pytorch) model in a commercial project. The source code itself is under MIT licensing. However, the dataset they have used is [Human3.6M](http://vision.imar.ro/human3.6m/description.php), which states that the [license](http://vision.imar.ro/human3.6m/eula.php) is ""FREE OF CHARGE FOR ACADEMIC USE ONLY"".

Yet, recent court rulings (in the US) state that models can use copyrighted data during training, and the results are no longer bound by that copyright (e.g. Google Books). Does the same apply here?",MachineLearning
"I've recently started reading up on classical ML and I got a question about K-Means.

More concretely, I am confused about the uniqueness of the global optimal solution of K-Means's cost function.

Let's state the problem formally below, extracted from Bishop's Pattern Recognition and Machine Learning book, exercise 9.1.

Consider the 𝐾-means algorithm discussed in Section 9.1. Show that as a consequence of there being a finite number of possible assignments for the set of discrete indicator variables 𝑟𝑛𝑘, and that for each such assignment there is a unique optimum for the 𝝁𝑘, the K-means algorithm must converge after a finite number of iterations.

I made an answer \[here\]([https://stats.stackexchange.com/questions/603327/question-on-the-proof-of-convergence-of-k-means](https://stats.stackexchange.com/questions/603327/question-on-the-proof-of-convergence-of-k-means)) detailing the proof of why it does converge in Lloyd's algorithm, but I think I still do not understand why Lloyd's do not converge to a global minimum, which mathematical theorem/understanding am I missing here?

I think that optimizing both the assignments and the centroids of K-Means at the same time is non-convex and hence there are many local minimums, we can use brute force to search for the global minimum but of course it is exponential to the number of data points. On the other hand, Lloyd optimizes it (greedily) alternatively, and hence you will find the cost functions' local minima (guaranteed)?",MachineLearning
"https://preview.redd.it/ut4us5251rfa1.png?width=2000&format=png&auto=webp&v=enabled&s=bf0add1de91537cb806f9f81405d065c95a42cc4

Currently, the UI supports a picture upload and uses InstructPix2Pix to edit it. Also, it uses upscaling models for quality enhancements. More models are coming soon.

The goal is to provide a way for non-ML people to use diffusion-based image editing through simplistic app design. Web demo: [https://diffground.com/](https://diffground.com/)",MachineLearning
"Any suggestion on sentence auto-encoder? I want to learn the vector representation of a sentence and reconstruct the sentence itself. I used plane LSTM with self attention in the encoder and decoder architecture (no cross attention) but the results are not that good enough.   I can not use cross attention i.e decoder will not have access to all the outputs of encoder but only access to the bottleneck latent vector. BART have pre-trained in similar manner but I don't know if we can pre-train that model to fit my use case.   This is just a module of other work, after pre-training the sentence-sentence auto-encoder, I need to add some more module in between them, so I should have encoder and decoder separable, which I think can not be done in BART as well.   Any direction would be much appreciated.   Thank you",MachineLearning
"I'm using Huggingface's transformers regularly for experimentations, but I plan to deploy some of the models to iOS.

I have found [ml-ane-transformers](https://github.com/apple/ml-ane-transformers/tree/main/ane_transformers) repo from Apple, which shows how transformers can be rewritten to have much better performance on Apple's devices. There's an example of DistilBERT implemented in that optimized way.

As I plan to deploy transformers to iOS, I started thinking about this. I'm hoping some already have experience about this, so we can discuss:

* Has anyone tried this themselves? Do they actually see the improvements in performance on iOS?
* I'm using Huggingface's transformer models in my experiments. How much work do you think there is to rewrite model in this optimized way?
* It's very difficult to train transformers from scratch (especially if they're big :) ), so I'm fine-tuning on top of pre-trained models on Huggingface. Is it possible to use weights from pretrained Huggingface models with the Apple's reference code? How difficult is it?",MachineLearning
"Currently I am using Davinci Resolve free edition to manually cut / remove no speech parts, or the parts where I take a breath

It is extremely time consuming

I am pretty sure this can be done via AI

For example whisper is able to detect where we use filler words such as umh, um, uh etc

That would be awesome to automatically remove these parts from a video

Just direct me where to look thank you",MachineLearning
"Stable diffusion seems to be a departure from the trend of building larger and larger models.

It has 10x less parameters than other image generation models like DALLE-2.

[“Incredibly, compared with DALL-E 2 and Imagen, the Stable Diffusion model is a lot smaller. While DALL-E 2 has around 3.5 Billion parameters, and Imagen has 4.6 Billion, the first Stable Diffusion model has just 890 million parameters, which means it uses a lot less VRAM and can actually be run on consumer-grade graphics cards.”](https://medium.com/nightcafe-creator/stable-diffusion-tutorial-how-to-use-stable-diffusion-157785632eb3)


What allows stable diffusion to work so well with a lot less parameters? Are there any drawbacks to this, like requiring stable diffusion to be fine tuned more than DALLE-2 for example?",MachineLearning
"What is the state of research in normalizing flows in 2023? Have they been superseded by diffusion models for sample generation? If so, what are some other applications where normalizing flows are still SOTA (or even useful)?",MachineLearning
"Hi guys, I have a dataset of 12,000 products, each of which consists of a title, description, and some images. In addition, I also have a pre-defined set of product categories. 

Curious to learn if anyone has any suggestions on what model to be used to train using this dataset as input to classify each product in the dataset into the related categories within the given set?",MachineLearning
"[https://twitter.com/eric\_wallace\_/status/1620449934863642624?s=46&t=GVukPDI7944N8-waYE5qcw](https://twitter.com/eric_wallace_/status/1620449934863642624?s=46&t=GVukPDI7944N8-waYE5qcw)

Extracting training data from diffusion models is possible by following, more or less, these steps:

* Compute CLIP embeddings for the images in a training dataset.
* Perform an all-pairs comparison and mark the pairs with l2 distance smaller than some threshold as near duplicates
* Use the prompts for training samples marked as near duplicates to generate N synthetic samples with the trained model
* Compute the all-pairs  l2 distance between the embeddings of generated samples for a given training prompt. Build a graph where the nodes are generated samples and an edge exists if the l2 distance is less than some threshold. If the largest clique in the resulting graph is of size 10, then the training sample is considered to be memorized.
* Visually inspect the results to determine if the samples considered to be memorized are similar to the training data samples.

With this method, the authors were able to find samples from Stable Diffusion and Imagen  corresponding to copyrighted training images.",MachineLearning
"I am trying to claculate the Jaccard similarity between all instances in my dataframe. I am using the following method to do so, however, this method is painfully slow. My \`\`\`data\_with\_labels\`\`\` shape is (221277, 217).

`# Compute the Jaccard similarity between all instances`

`n_instances = data_with_labels.shape[0]`

`jaccard_similarity_matrix = np.zeros((n_instances, n_instances))`

`for i in range(n_instances):`

`for j in range(n_instances):`

`jaccard_similarity_matrix[i, j] = jaccard_score(data_with_labels[i, :],   data_with_labels[j, :], average='micro')`

&#x200B;

Is there any way to do this process with numpy vectorization? I tried soomething like this but keep getting this error:

`n_instances = data_with_labels.shape[0]`

`jaccard_similarity_matrix = np.zeros((n_instances, n_instances))`

`for i in range(n_instances):`

`jaccard_similarity_matrix[i, :] = jaccard_score(data_with_labels[i, :], data_with_labels, average='micro')`

`ValueError: Found input variables with inconsistent numbers of samples: [217, 221277]`

&#x200B;",MachineLearning
"Geometric GNNs are an emerging class of GNNs for **spatially embedded graphs** in scientific and engineering applications, s.a. biomolecular structure, material science, and physical simulations. Notable examples include SchNet, DimeNet, Tensor Field Networks, and E(n) Equivariant GNNs.

**How powerful are geometric GNNs?** How do key design choices influence expressivity and how to build maximally powerful ones?

Check out this recent paper for more:

📄 PDF: [http://arxiv.org/abs/2301.09308](http://arxiv.org/abs/2301.09308)

💻 Code: [http://github.com/chaitjo/geometric-gnn-dojo](http://github.com/chaitjo/geometric-gnn-dojo)

💡Key findings: [https://twitter.com/chaitjo/status/1617812402632019968](https://twitter.com/chaitjo/status/1617812402632019968) 

P.S. Are you new to Geometric GNNs, GDL, PyTorch Geometric, etc.? Want to understand how theory/equations connect to real code?

Try this **Geometric GNN 101 notebook** before diving in:  
[https://github.com/chaitjo/geometric-gnn-dojo/blob/main/geometric\_gnn\_101.ipynb](https://github.com/chaitjo/geometric-gnn-dojo/blob/main/geometric_gnn_101.ipynb)",MachineLearning
"I made a new open source tool called JellyML that lets you go back to any of your checkpoints, and reproduce your code exactly as it was when you trained it.

You can find the website here:

 https://jellyml.com

The GitHub repo: 

https://gitHub.com/mmulet/jellyml

You can install it with pip: 

pip install jellyml",MachineLearning
"Every day, there seems to be new evidence of the generalization capabilities of LLMs.

What does this mean for the future role of deep learning experts in academia and business? 

It seems like there's a significant chance that skills such as PyTorch and Jax will be displaced by prompt construction and off-the-shelf model APIs, with only a few large institutions working on the DNN itself.

Curious to hear others' thoughts on this.",MachineLearning
"Hey everyone, 
I thought of an idea to create a human like realistic voice assistant for ChatGPT. So I have a question that can we make an API of tortoise TTS trained on a specific voice. I've seen a lot of companies nowadays that provides most realistic text to speech solutions like eleven labs etc. Do they train these voices on tortoise TTS?? If there is another way of creating highly realistic voices and make an API of it, then please tell me how can I do it? 
And also how can I make this process fast as regular normal TTS?",MachineLearning
"Problem statement: assume a user come into a system and it typically takes 10 weeks for outcome(yes,no). I want to build a model which predicts the outcome on any particular week say how likely are they gonna succeed on week 1,2,3 etc.

Question on model building approach: should I build weekly models and get the prediction ? Or is there a better way to do it. Ideally it would be great have single model that can be used for different weeks. I prefer the latter. 

Appreciate your ideas",MachineLearning
"I’m looking to some aggregation on academic research and news articles to see what insights I get from it. I’m using textrazor to do named entity recognition on the documents, but getting a lot of dirty labels that have slightly different wording. For example, Tesla, Tesla ltd, Tesla Ltd. As a result, my aggregations have a lot of duplicate results.

The dataset consists of about 4M labels so the solution has to be efficient to be viable. I was thinking of putting the labels through word2vec and then clustering them based on the word embedding distances? But then the problem arises of how many clusters to use?

I’ve also tried simple regex preprocessing to get rid of the company abbreviations but there are other examples that cannot be solved that easily.",MachineLearning
"Discussion about a report that compares the practices and attitudes of companies that self-report as ahead of the competition in AI adoption in Europe, compared to companies that identify as behind or at the same stage as their competitors. It contains some interesting findings mixed with some somewhat obvious things.


- Kinda obvious that leading companies also are further ahead in using MLOps, but I thought it was interesting to see the frequency of fine-tuning and retraining.

- Not as obvious that most companies report a lack of access to training data, would have thought that is mostly something that smaller companies have issues with.

- Also not so obvious to me is that companies with a centralized decision-making related to AI seem to dominate among high-performers.

- Interesting that most companies seem to get some value out of their AI/ML projects, which seems to contradict some of the previous forecasts by the big consultancy companies.

Link to the report: https://stagezero.ai/2022-survey-report/",MachineLearning
"I learned a lot at EMNLP in December and captured some of what I learned in this video.

**Interviews**

I asked five NLP researchers these questions:

1- What is the most exciting development in NLP in 2022

2- What are you looking forward to in 2023?

3- What is an underrated idea that the field should pay more attention to?

Their answers start at [01:22](https://www.youtube.com/watch?v=plCvF_7qrmY&t=82s).

**Workshops**

I got to spend time at these workshops:

* [Generation, Evaluation & Metrics (GEM)](https://gem-benchmark.com/workshop)
* [Massively Multilingual NLU](https://mmnlu-22.github.io/)
* [Blackbox NLP](https://blackboxnlp.github.io/2022/)

My main takeaways are at [09:25](https://www.youtube.com/watch?v=plCvF_7qrmY&t=565s).

**Posters**

If you've been to a conference you'd know there's an overwhelming number of posters. I recorded four of the ones I came across and thought were interesting (covering retrieval-augmented text generation, human evaluation, the BLOOM multimodal dataset, and a multimodal method to name music playlists).

Poster presentations start at [14:38](https://www.youtube.com/watch?v=plCvF_7qrmY&t=878s)

Full video: [https://www.youtube.com/watch?v=plCvF\_7qrmY](https://www.youtube.com/watch?v=plCvF_7qrmY)

&#x200B;

What's your answer to these questions?

>1- What is the most exciting development in NLP in 2022  
>  
>2- What are you looking forward to in 2023?  
>  
>3- What is an underrated idea that the field should pay more attention to?

&#x200B;",MachineLearning
"Hi everyone,

I have developed a CLI tool to train a transformer sequence classification model. There are also options for preprocessing data and inference on new data. I was thinking that interesting use cases might be found within economics/finance and biological domains, and would be super interested in feedback on:

\- if the documentation is intelligible and enables you to use it  
\- to which use cases from your industry/domain could discrete sequence modelling be applied  
\- what additional features you'd need for it to be useful to you

Basically, where would the prediction of a class (or the next item) based on discrete events/objects/tokens be useful?

The project is called ""sequifier"" and can be found here: [https://github.com/0xideas/sequifier](https://github.com/0xideas/sequifier)",MachineLearning
"Hi,  


[Text-Generator.io](https://text-generator.io/) is now self hostable, It's priced at $1000 USD per instance per year to self host.  


The service runs on a single 24GB VRAM GPU, and runs all services including [speech to text](https://text-generator.io/blog/cost-effective-speech-to-text-api), text and code generation for almost all languages and [generating embeddings](https://text-generator.io/blog/embed-images-text-and-code) too.  


The text generator also [downloads and analyses any input with links](https://text-generator.io/blog/text-generator-now-researches-via-crawling) including documents, images, images with text inside and webpages for better understanding and to generate better text.  


It's a great alternative to OpenAI and has a [compatible API making switching easy](https://text-generator.io/blog/over-10x-openai-cost-savings-one-line-change).  


You can check out the [new pricing here](https://text-generator.io/subscribe).  


Let me know what you think and if there's anything i can do to help!  


All the best.  
Lee Penkman - Founder [Text-Generator.io](https://text-generator.io/)",MachineLearning
"GitHub (sadly without weights).   https://github.com/PetchMa/ML_GBT_SETI  
News.  
https://www-scinexx-de.translate.goog/news/kosmos/seti-findet-acht-potenzielle-alien-signale/?_x_tr_sl=de&_x_tr_tl=en&_x_tr_hl=de&_x_tr_pto=wapp",MachineLearning
"Paper : https://arxiv.org/abs/2301.13379

Abstract :

>While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a faithful-by-construction framework that decomposes a reasoning task into two stages: Translation (Natural Language query → symbolic reasoning chain) and Problem Solving (reasoning chain → answer), using an LM and a deterministic solver respectively. We demonstrate the efficacy of our approach on 10 reasoning datasets from 4 diverse domains. It outperforms traditional CoT prompting on 9 out of the 10 datasets, with an average accuracy gain of 4.4 on Math Word Problems, 1.9 on Planning, 4.0 on Multi-hop Question Answering (QA), and 18.1 on Logical Inference, under greedy decoding. Together with self-consistency decoding, we achieve new state-of-the-art few-shot performance on 7 out of the 10 datasets, showing a strong synergy between faithfulness and accuracy.",MachineLearning
"Can someone suggest a machine learning model that will segment audio spectrogram to multiple classes.
I have labeled data of heart beats. S1, S2, systole and diastole. How to train a segmentation model ?",MachineLearning
"Basically the title. I have a sequence of two papers - a viewpoint and a complete paper in the works - that I'm looking to submit, the viewpoint outlining the theoretical premise for the latter. I've currently had no luck finding any ML-specific journals that allow viewpoint submissions (with the exception of simply posting to arXiv), and was wondering if anyone here was familiar with any.

Thanks :D",MachineLearning
"I am proud to announce the release of [NoRefER](https://huggingface.co/aixplain/NoRefER), a multi-language referenceless ASR metric based on a fine-tuned language model, for public use on HuggingFace. This metric allows for evaluating the outputs of ASR models without needing a reference transcript, making it a valuable tool for a/b testing multiple ASR models or model versions, or even ensembling their outputs.

ASR is an important technology with various applications, but the quality of ASR systems can vary greatly. It's important to accurately evaluate and compare the performance of different ASR models, traditionally done using reference-based ASR quality evaluation metrics. However, obtaining those ground-truth transcriptions from human annotators is time-consuming and costly.

Referenceless quality evaluation is becoming important as it allows for comparing ASR models on a level playing field, regardless of the quality or existence of a reference transcript. Just as referenceless evaluation has become crucial in Machine Translation with the introduction of referenceless metrics like COMET-QE, referenceless evaluation will play an important role in ASR.

To fine-tune [NoRefER](https://huggingface.co/aixplain/NoRefER) for referenceless ASR quality evaluation, a contrastive-learning technique is employed with an innovative self-supervision method exploiting the known quality relationships in-between multiple compression levels of the same ASR, rather than human supervision on the quality of the ASR outputs obtained via transcriptions or evaluations of the annotators.

Potential use-cases for [NoRefER](https://huggingface.co/aixplain/NoRefER) include:

\- A/B testing models or their versions to determine the best-performing one

\- Picking production outputs worth for the human-evaluation or post-editing

\- Ensemble the outputs of multiple ASR models to achieve a superior quality

What's more, these abilities will be accessible in aiXplain no-code products, making it easy for anyone to use and benefit from this ASR quality estimation metric. We are excited to see how [NoRefER](https://huggingface.co/aixplain/NoRefER) will be used in the ASR community and would love to hear your thoughts and feedback. Try it out on HuggingFace and see how it can help diagnose and improve your ASR models!",MachineLearning
"[https://www.youtube.com/watch?v=yvgxRzqx1Jg](https://www.youtube.com/watch?v=yvgxRzqx1Jg)

&#x200B;

Contents 

* [00:00](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=0s) Learning from Machine Learning Intro 
* [00:21](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=21s) Vincent Warmerdam Intro 
* [01:18](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=78s) Career Journey 
* [03:25](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=205s) What roles have you played? 
* [05:44](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=344s) Academic Background: Operations Research and Design 
* [06:52](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=412s) Operations Research 
* [08:13](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=493s) Mathematics 
* [09:19](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=559s) What attracted you to Machine Learning? 
* [10:40](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=640s) Calmcode 
* [14:08](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=848s) Calmcode, Do you use it? 
* [15:22](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=922s) Calmdcode, \*args, \*\*kwargs 
* [16:23](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=983s) If there were no constraints, what would you do to improve calmcode? 
* [18:10](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=1090s) Open Source Projects: bulk, embetter, human-learn 
* [19:10](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=1150s) Open Source: evol, scikit-lego 
* [20:00](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=1200s) Rasa: Chatbots, Benchmarking 
* [20:47](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=1247s) Unit Tests 
* [21:42](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=1302s) Open Source: Creating Packages 
* [24:10](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=1450s) Bulk, human-learn 
* [26:20](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=1580s) [27:03](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=1623s) Bulk in a notebook, bulk as a webapp 
* [27:45](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=1665s) Human in the loop 
* [29:03](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=1743s) Understanding the problem; Beans, Beef and Bread 
* [32:56](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=1976s) Algorithm on the wrong problem 
* [34:55](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=2095s) Module Improvement vs System Improvement 
* [37:20](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=2240s) Does your answer make sense? 
* [39:04](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=2344s) What's an important question that you believe remains unanswered in ML? 
* [41:48](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=2508s) How do you view the gap between the hype and reality of AI? 
* [46:28](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=2788s) Generative Models vs. Predictive Models 
* [49:18](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=2958s) Jumping to solutions 
* [50:08](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=3008s) Model vs. System 
* [50:48](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=3048s) [51:10](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=3070s) Who has influenced you in the field? 
* [55:18](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=3318s) Humble, Caring Presenters 
* [56:38](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=3398s) What's one piece of advice that you've received that's helped you? 
* [01:00:18](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=3618s) Advice for people just starting in the field 
* [01:03:15](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=3795s) What has a career in machine learning taught you about life? 
* [01:05:16](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=3916s) SpaCy 
* [01:06:10](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=3970s) Data-Centric Approach 
* [01:06:50](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=4010s) Wrap-up 
* [01:07:15](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=4035s) Follow, Explosion  
* [01:07:48](https://www.youtube.com/watch?v=yvgxRzqx1Jg&t=4068s) Outro",MachineLearning
"Is it possible to finetune a generative model (like T5) to do something like this:

{

inputs: ""XYZ <eot> XYZ was born in ABC. They now live in DEF."",

targets: ""XYZ <t> born in <t> ABC <f> XYZ <t> lives in <t> DEF""

}

[Like the transformer model fom this paper](https://github.com/google-research-datasets/wikifact)

if so how should I go about approaching the problem? 

Is this task as simple as feeding it the inputs and targets or do you guys think it has more to it?",MachineLearning
"Relying on hosted inference with LLMs in productions, such as via OpenAI API, has some challenges. The use of APIs should be designed around unstable latency, rate limits, token counts, costs, etc. To make it observable we've built tracing and monitoring specifically for AI apps. For example, the OpenAI Python library is monitored automatically, no need to do anything. We'll be adding support for more libraries.

Here is a blog post with more info and screenshots: [Monitor OpenAI API Latency, Tokens, Rate Limits, and More](https://graphsignal.com/blog/monitor-open-ai-api-latency-tokens-rate-limits-and-more/). And the [GitHub repo](https://github.com/graphsignal/graphsignal).",MachineLearning
"Hi all, I'm trying to fine-tune Whisper AI to transcribe albanian speech to text but I have a problem in that I don't know how the dataset for training whisper model should look like. 

I already have voice audios and the transcript for that audio file but I need to know how to reformat it into a valid dataset for training Whisper.

Thanks in advance!",MachineLearning
"This may be a silly question for those familiar with the field, but don't machine learning researchers expect any more prospects for traditional methods (I mean, ""traditional"" is other than deep learning)? I feel that most of the time when people talk about machine learning in the world today, they are referring to deep learning, but is this the same in the academic world? Have people who have been studying traditional methods switched to neural networks? I know that many researchers are excited about deep learning, but I am wondering what they think about other methods.

\[ EDITED \]

I’m glad that I got far more responses than I expected! However, I would like to add here that my intention did not seem to come across to some people because of my inaccurate English.

I think “have given up"" was poorly phrased. What I really meant to say was, are ML researchers no longer interested in traditional ML? Have those who studied, say, SVM moved on to DL field? That was my point, but u/qalis gave me a good comment on it. Thanks to all the others.",MachineLearning
"Doing a survey of object detection papers with plausible application to pose-estimation tasks. Came across the paper ""You Only Learn One Representation"" and, while the theory seems interesting, I want to hear people's opinions before doing a deep dive into the theory.",MachineLearning
[https://peltarion.com/blog/data-science/towards-a-token-free-future-in-nlp](https://peltarion.com/blog/data-science/towards-a-token-free-future-in-nlp),MachineLearning
"I’m an ML Engineer at Hive AI and I’ve been working on a ChatGPT Detector.

Here is a free demo we have up: [https://hivemoderation.com/ai-generated-content-detection](https://hivemoderation.com/ai-generated-content-detection)

From our benchmarks it’s significantly better than similar solutions like GPTZero and OpenAI’s GPT2 Output Detector. On our internal datasets, we’re seeing balanced accuracies of >99% for our own model compared to around 60% for GPTZero and 84% for OpenAI’s GPT2 Detector.

Feel free to try it out and let us know if you have any feedback!",MachineLearning
"In order to improve my talking skills, I am doing a [little series](https://www.youtube.com/playlist?list=PL04PGV4cTuIVGO5ImYTk9wPVmbgdYbe7J) on how to setup Stable Diffusion on Paperspace, and I am astounded how much time it takes to do the audio editing. Well, part of the reason is that I've only been doing this for 3 days and my process is very inefficient, but it feels that in the current time, neural nets should be able to do things like remove uhms, lip smacking and breath intakes.

I've looked around, and [this post](https://www.reddit.com/r/audioengineering/comments/1xtm1r/comment/cfej9oa/?utm_source=share&utm_medium=web2x&context=3) from 9 years ago says the only choice is to edit it by hand. Is that still true?",MachineLearning
I've been working in the speech and voice space for a while now and am now building out some tooling in the space to make it easier for researchers/engineers/developers to build speech processing systems and features; I'd love to hear what people in ML struggle with when you're trying to build or work with speech processing for your projects/products (beyond speech-to-text APIs),MachineLearning
"I am a researcher at a US university and have a budget of 25k to build a PC for training various ML algorithms (e.g. DRL, neuromorphic computing, VAE, etc). I'm trying to decide between going for prebuilds (like [https://lambdalabs.com/gpu-workstations/vector](https://lambdalabs.com/gpu-workstations/vector)) or building with consumer cards like 4090s.   


Any advice on which is the most bang for the price? Im not sure how much Im giving up by going for consumer 24g cards vs a6000, 6000 ada but prebuild prices go up quick. Warrantee vs building it myself isn't an issue",MachineLearning
"Paper: [https://arxiv.org/abs/2212.10561](https://arxiv.org/abs/2212.10561) 

Github: [https://github.com/ezelikman/parsel](https://github.com/ezelikman/parsel) 

Twitter: [https://twitter.com/ericzelikman/status/1618426056163356675?s=20](https://twitter.com/ericzelikman/status/1618426056163356675?s=20) 

Website: [https://zelikman.me/parselpaper/](https://zelikman.me/parselpaper/) 

Code Generation on APPS Leaderboard: [https://paperswithcode.com/sota/code-generation-on-apps](https://paperswithcode.com/sota/code-generation-on-apps) 

Abstract:

>Despite recent success in large language model (LLM) reasoning, **LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs.** For these tasks, **humans often start with a high-level algorithmic design and implement each part gradually.** We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs, taking hierarchical function descriptions in natural language as input. We show that **Parsel can be used across domains requiring hierarchical reasoning, including program synthesis, robotic planning, and theorem proving.** We show that LLMs generating Parsel solve more competition-level problems in the APPS dataset, resulting in **pass rates that are over 75% higher than prior results from directly sampling AlphaCode and Codex**, while often using a smaller sample budget. We also find that LLM-generated **robotic plans using Parsel as an intermediate language are more than twice as likely to be considered accurate than directly generated plans.** Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers. 

https://preview.redd.it/66zehsdps6fa1.jpg?width=811&format=pjpg&auto=webp&v=enabled&s=96db4cb832def624ad10f7383cde56c1444dcbcc

https://preview.redd.it/is4pzwdps6fa1.jpg?width=1638&format=pjpg&auto=webp&v=enabled&s=5e6c3137b982c91c658b58d286e5036a46a7d55d

https://preview.redd.it/szkbb0eps6fa1.jpg?width=711&format=pjpg&auto=webp&v=enabled&s=6eacbd0cdfc8ecc2c21ad1a46d87d8f367d9bbb5

https://preview.redd.it/6lk1wzdps6fa1.jpg?width=1468&format=pjpg&auto=webp&v=enabled&s=5a37d08a5677d927c1b017d711558a6d859e8f3c

https://preview.redd.it/8h7p8vdps6fa1.jpg?width=1177&format=pjpg&auto=webp&v=enabled&s=3e9926040e6af04ec8945fcfe81e51b5c94d5913",MachineLearning
" Hi guys.

It's been some time since I started developing my Keras models, but now is the first time I am trying to push it to production.

My Keras model looks like this:

`model = Sequential()`

`model.add(Bidirectional(LSTM(256, return_sequences=True)))`

`model.add(Bidirectional(LSTM(256, return_sequences=True)))`

`model.add(TimeDistributed(Dense(1, activation='sigmoid')))`

`model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])`

My problem is I need to run through about 25 of these for every written sentence. There is going to be an online editor, where users can paste text for my analysis. That means up to about 300 words or about 20 sentences at once. With the current time to run each network (about 0.2s), that means 25 \* 0,2 \* 20 or about 100s per user input. I am going for 30 seconds at most with potentially dozens of users at once. Ideally on a Raspberry Pi 4.

The internet is surely gonna back me up I thought to myself and started googling. If only I know what kind of a rabbit hole I was about to fall into.

First I converted my Keras model into a TensorFlow frozen graph model. 10x time improvement on CPU, but still at 0.2s on average.

Another thing I think may boost the performance is retraining the models for variable input shape (currently I always feed in 50 values). With the average sentence size of 16 words this may, from what I understand, lead to a 3 times boost?

My question is: now what? What can I do to make it faster? Is it even possible to run it on a Raspberry Pi 4 and get reasonable response times? If not, what is my best option on a tight budget?",MachineLearning
"From the given [link!](https://www.adept.ai/act), I gather that it is a large-scale Transformer trained to use digital tools like a web browser. Right now, it’s hooked up to a Chrome extension which allows it to observe what’s happening in the browser and take certain actions, like clicking, typing, and scrolling, etc.

I am interested in knowing the broad steps involved in building something like this.",MachineLearning
"The general consensus seems to be that large language models, and ChatGPT in particular, have a problem with accuracy and hallucination. As compared to what, is often unclear, but let's say as compared to other NLP methods of question answering, language understanding or as compared to Google Search.

I haven't really been able to find any reliable sources documenting this accuracy problem, though.

The SuperGLUE benchmark has GPT-3 ranked #24, not terrible, but outperformed by old models like T5, which seems odd. GLUE nothing. SQUAD nothing.

So, I'm curious:

1. Is there any benchmark or metric reflecting the seeming step-function made by ChatGPT that's got everyone so excited? I definitely feel like there's a difference between gpt-3 and chatGPT, but is it measurable or is it just vibes?
2. Is there any metric showing ChatGPT's problem with fact hallucination and accuracy?
3. Am I off the mark here looking at question-answering benchmarks as an assessment of LLMs?

Thanks",MachineLearning
"Has anyone come across studies on GPT3 text-davinci-003's zero/few-shot performance over various NLP benchmarks and how they compare to current SoTA? E.g GLUE, SuperGLUE and over more classic ones like CoNLL 2003 NER.

I thought it would be pretty interesting to see how far zero/few-shot learning with LLM has progressed with RLHF and instruction tuning. Am surprised that nobody has done such a benchmark yet.",MachineLearning
"Hi all!

Given X ∈ ℝ ^(Nx), Y ∈ ℝ ^(Ny), β ∈ ℝ^(+), so

W = YX^(T)(XX^(T)\+βI)^(-1)   (with the Moore–Penrose pseudoinverse)

where A = YX^(T) and B = XX^(T)\+βI.

If we consider an arbitrary number of indices/units < Nx, and so we consider only some columns of matrix A and some columns and rows (crosses) of B. The rest of A and B are zeros.

The approach above of sparsify A and B will break the ridge regression solution when W=AB^(-1)? If yes, there are ways to avoid it?

Many thanks!",MachineLearning
"I was looking at Databricks because it integrates with AWS services like Kinesis, but it looks to me like SageMaker is a direct competitor to Databricks? We are heavily using AWS, is there any reason to add DataBricks into the stack or odes SageMaker fill the same role?

Thank you :)  
Post on r/datascience too..",MachineLearning
"I'm looking for help/feedback with this paper. Please let me know if the method is interesting and if there's ways to improve it!

[https://arxiv.org/abs/2301.10115](https://arxiv.org/abs/2301.10115)

Abstract:

Gradient boosted decision trees are some of the most popular algorithms in applied machine learning. They are a flexible and powerful tool that can robustly fit to any tabular dataset in a scalable and computationally efficient way. One of the most critical parameters to tune when fitting these models are the various penalty terms used to distinguish signal from noise in the current model. These penalties are effective in practice, but are lacking in robust theoretical justifications. In this paper we develop and present a novel theoretically justified hypothesis test of split quality for gradient boosted tree ensembles and demonstrate that using this method instead of the common penalty terms leads to a significant reduction in out of sample loss. Additionally, this method provides a theoretically well-justified stopping condition for the tree growing algorithm. We also present several innovative extensions to the method, opening the door for a wide variety of novel tree pruning algorithms.",MachineLearning
"In a recent article, we reviewed dozens of image segmentation algorithms and pointed out mathematically that in many cases the reported performance scores could not be the results of the evaluation methods claimed by the authors.

The scores are primary indicators of value and serve as measures of the state-of-the-art to be outperformed by new algorithms. Unfortunately, algorithm rankings turned out to be incorrect in 100+ papers and the problem is systematic. The pressure to outperform flawed performance scores to get published keeps the trend on-going.

How should the community deal with a phenomenon like this: flaws uncovered, factual, undeniable yet on-going? Is the 258th algorithm proposed for a problem more valuable than reproducing a highly cited article? Should it be mandatory to share source code? Is there a merit in developing consistency checks like the ones we did? Any comments are welcome!

[https://arxiv.org/abs/2111.03853](https://arxiv.org/abs/2111.03853)",MachineLearning
"Hi all,

During the pandemic many software companies transitioned their workforce to ""fully-remote"" or ""partially-remote""; therefore, I was wondering if any reputable institutions offer a remote CS PhD?

For context, I know of several individuals who have sorted out remote work with their PIs on a per-person basis (typically after the first 1-2 years of study), but I am not aware of any labs or programs that advertise remote study.

Thank you in advance for the responses.

Cheers,

Matt",MachineLearning
"Twitter link to his post: [https://twitter.com/alexjc/status/1617152800571416577?s=46&t=CMQT9rK4F1Lt7g7aX2vTJA](https://twitter.com/alexjc/status/1617152800571416577?s=46&t=CMQT9rK4F1Lt7g7aX2vTJA) 

also important in that regard:

**The case for 4-bit precision: k-bit Inference Scaling Laws - Tim Dettmers**

Paper: [https://arxiv.org/abs/2212.09720](https://arxiv.org/abs/2212.09720) 

https://preview.redd.it/7nn0pfhn81fa1.jpg?width=585&format=pjpg&auto=webp&v=enabled&s=8aecd5774fabae48a453cc09bba8b4c2c5e5a16e

https://preview.redd.it/0084vhhn81fa1.jpg?width=598&format=pjpg&auto=webp&v=enabled&s=61f1c325541d6deec62eba3d7d803a37c073151b",MachineLearning
"Have you tried ChatGPT? It's super cool but some users are also using it to create automated content submissions and resulting in an increase in AI-generated plagiarism. I have made a tool as a college project to detect content generated using AI.  
Go ahead and validate your content on [AI Content Detector](https://ai-content-detector.online/)  
If you are an educator worried about automated content submissions or developers worried about search engine penalties, this tool will help everyone to efficiently detect content generated using AI.",MachineLearning
"Let's say I pull a pre-trained LLM off of huggingface. In broad strokes (making whatever assumptions appropriate), what is the relative cost of getting human annotation data versus actually incorporating those data in through training?

I've been trying to get this stats and so far the ratio seems to be 2:1, meaning if you spent 10k dollars collecting human annotations, you should expect to spend 5k on compute (finetune, RLHF, ect)

but I'd be happy if someone with more experience can chime in.",MachineLearning
"On [This](https://ai.facebook.com/research/theory/) page of Meta AI research where they mention AI theory as a topic, they mention that they use techniques from Signal Processing. As someone with an Electrical Engineering background, and interests in Mathematics and AI, I found this very intriguing. Can someone tell me some of the ways signal processing has been used in AI theory? Some papers or some work done?",MachineLearning
"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",MachineLearning
"I’ll try to get into detail about the implementation and difficulties in case it is useful for anyone else trying to do something similar with an applied ML project, so there’s a TLDR at the end if you’d like the short version/result.

At the end of last year I convinced myself to start 2023 by creating a side-project that I'd actually finish and deploy and perhaps earn some “passive” income (spoiler, not so passive after all :P), and after some brainstorming I settled on making an automated Youtube channel about finance news since I had just gotten into investing. Shorts seemed to be more manageable and monetization is changing in February so I went with that.

My rough initial idea was to get online articles, summarize them, make a basic compilation with some combination of pymovie, opencv and stock photos and done. I was pretty worried about the summarization, since in my ML day job I mainly work with vision or sensor data in manufacturing not NLP. Also, I quickly realized pymovie with still images and some overlayed text was not very attractive for viewers (starting with myself).

Fast-forward a few days, and after some research online I came across two things, Huggingface transformers (yep, I know I’ve been living under a rock :P) and After Effects scripting.  From here, it became mainly about figuring out exactly which ML models I needed to fine-tune for finance / social media and for what, then putting it all together.

The entire workflow looks something like this: the bot fetches online daily news about a topic (stocks or crypto), then sentiment analysis is performed on the title and the full text is summarized into a single sentence. I fine-tuned SBERT on \~1.5M posts from /r/worldnews publicly available in Google Cloud BigQuery so that it could predict a “social engagement” score that could be used to rank and filter the news that would make it into the video.

Finally, all of this is combined into a single JSON object written into a .js file that can be used by another “content creator” script to render the video from a template using aerender in Python. The content of this template is generated dynamically based on the contents of the .js file via AE Expressions. This module also uses the TTS lib to generate voice-overs for the text, and is also responsible for generating the title (using NLTK to identify the main subjects of each title) and the video’s description. Pexel stock videos are used for the background.

In principle automating the upload to Youtube could also be done, but at this stage I’m handling this manually as the JSON generation is not as robust as I’d like, so the output file often needs to be tweaked and fixed before the video can be finalized and uploaded. An examples is the summary being too short or vague when taken out of the context of the original article. If you increase the max\_length of the summarizer to compensate, it can easily become too long to for the overlay to fit the pre-defined dimensions, or the total audio length can be too long for the max duration of a youtube short.

With some more work I’m confident the whole process can be automated further. For those interested, feel free to check the result here:

[Byte Size Bot channel](https://www.youtube.com/@bytesizebot)

If you have any questions or suggestions I’d be happy to hear them.

TLDR: Coded an automated (not 100% yet, but will get there) Youtube Shorts channel about finance news to create a passive income stream. Ended up being way harder, more fun and not so “passive” than my initial expectations.",MachineLearning
"Someone I work with wrote the below for our internal team (shared with permission) and I thought some here may find it helpful.

Recently, I built an app that uses GPT-Index & LangChain to provide an answer to a question based on a piece of text as context. I found GPT-Index to be much easier and straightforward to integrate, but it seems like LangChain has more features and is more powerful. Here's my experience integrating both of them.

GPT-Index

- First thing I did was review their docs to make sure I understood what GPT-Index was, what it could do, and how I was going to use it

- I went back and forth a couple times figuring out how I was going to use it. Then I found the quickstart guide

- It seemed like the quickstart guide would work so I followed the guide and after a few tries, I was getting solid responses to the questions I asked it

LangChain

- I followed the same step, reviewing their docs. LangChain's docs has more to it because it seems like it does more, so this step took longer

- It was tough for me to figure out how I needed to use LangChain. I had to ask for some help to better understand our use case

- Once I thought I knew how I was going to use LangChain, I began coding. I ran into more errors with LangChain

- It seems that my first approach wasn't correct, so I switched to something similar and I was finally getting a response. The response was 'I don't know'... I didn't know what to do about it

- Then I checked out the logs of the data being passed through and found that the context was being cut off.

- To make sure it worked, I asked a question relating to the text that was getting passed through. The response seemed to make sense, so now I know better where the issue is at

- I still need to fix the context being cut off. I followed the docs of LangChain very closely, so I'm wondering if the docs are old or if I have the wrong implementation

So overall, if GPT Index solves your use case, start with that. We also did a variant built without GPT-Index and without langchain, and that one worked well too. We occasionally share stuff like this on genainews.org though the newsletter is mostly about new ai startups and products so figured it good to post here.

Anyone else here that's worked with both GPT-Index and langchain and can offer additional thoughts?",MachineLearning
"Lately, NLP is taking up most of the public space, much of AI news is focused on LLM after Chat-GPT took the spotlight.  How do non-NLP people keep up with news?  I recently saw a post on reddit where tree models are still being improved.  There are other topics too, like the recent trend in Model Explainability which feels to have slowed down.

I'd guess this all gets into the more categorical questions which I am wrapping up with **'How do YOU get your ML news'?**

* How does information gathering differ between those in Applied ML and AI researchers (or even further, between those in Business Analytics and those in more 'AI' fields)
* What sort of interesting things are out there in the world of ML now? (model or non-model related)
* In looking for Use Cases, does this partially come down to your field? (Finance reads finance news, pharma reads pharma news)

&#x200B;

Many of the AI/ML Newsletters which I subscribed to when I was less experienced seemed to be full of variety, but as they are all converging to NLP recently maybe it is time to cleanse the subscriptions, or find some new resources.",MachineLearning
"I was looking for research that centers around Stable Diffusion models but can be trained with seed images of a specific subject, so that if someone refers to a keyword like ""Me"" or ""I"" it would then produce images relative to the keyword of interest. Something like ""I am swimming in a beautiful ocean with mountains in the background and wearing a speedo"". Then the person subject in the photo would be myself since I referenced ""I"".",MachineLearning
"so it's big pop-sci news that we are [running out of quality textual training data](https://www.theatlantic.com/technology/archive/2023/01/artificial-intelligence-ai-chatgpt-dall-e-2-learning/672754/) (soft-paywalled article, but you get the idea) to produce chinchilla-optimal language models, and they appear to continue learning new abilities as data and parameter size increase.

when an infant learns what a cat is, it is not only described, but the infant can see it and understand its form and behavior in a way it can then go on to describe and extrapolate from (even if they are blind, they can touch it and understand  its shape and feel its fur). LLMs have to do this the hard way: their generalized understanding of the shape and behaviour of a cat comes from textual descriptions of them (and they would need quite a lot in order to understand!)

most of the research i have seen into multiple input transformer models has been with the purpose of task completion (google's embodied language model robot butlers etc, which often use textual descriptions fed to a normal LLM, see https://innermonologue.github.io/ ) or image recognition and understanding (such as in CLIP) but not necessarily applying it to textual completion, which seems like it could benefit from a more visual understanding of the world 

so, in the medium or short term, to improve performance on text-completion tasks, what are your thoughts on using image training as well as textual to improve generalization for LLMs with fewer text tokens on a new architecture?

(also, please excuse any ignorance i may posess: i'm a bit of an armchair ai enjoyer)",MachineLearning
"The main reason why this is not in the simple questions thread is the need to includee an image.  
Here is an image of my generator's and discriminator's gradients being logged onto wandb. As you can see, they have these weird hasms, where the distribution of the gradients becomes very close to zero. These chasms are correlated for the discriminator and generator and seem very regular.   


Anyone experienced anything like this and maybe has a hunch of what might be the cause?

https://preview.redd.it/3qipyez6stea1.png?width=624&format=png&auto=webp&v=enabled&s=174c00a29799912e8c4a023ea743237ab1fcb831",MachineLearning
" 

For regressions, R-squared and Adj. R-Squared are typically considered the primary goodness-of-fit measures.

But in many supervised machine-learning models, RMSE is the main measure that I keep running across. For example, decision tree models that I create in R using Rpart do that.

So, my question is how to compare the predictive accuracy of OLS regression models that report R-sq to equivalent Rpart regression trees that report RMSE.",MachineLearning
"Hi everyone, I made a JupyterLab extension to use OpenAI’s GPT models for code and text completion on your notebook cells.

This extension passes your current notebook cell to the  GPT API and completes your code/text for you. You can customize the GPT  parameters in the Advanced Settings menu.

I made this extension when I couldn't find any Copilot/Codex extensions for JupyterLab. It doesn't make sense that ML folks don't have an easy way to use AI generated code in their own tools. VS Code does allow you use Copilot, but I've gotten used to Jupyter and a lot of ML/DS folks I know still prefer using Jupyter over VS code.

# Installation

    pip install gpt_jupyterlab 

GitHub Repo: [https://github.com/henshinger/gpt-jupyterlab/](https://github.com/henshinger/gpt-jupyterlab/) 

# Demo

[GPT JupyterLab Demo](https://reddit.com/link/10ng3pi/video/y1smzr8wjsea1/player)

Note: You will need your own OpenAI API Key to use this extension.

Would love to get your feedback!",MachineLearning
"I am building an open-source ML observability and refinement toolkit. 

The tool helps ML practitioners to:
1. Understand how their models are performing in production
2. Catch edge-cases and outliers to help them refine their models
3. Allow them to customise the tool according to their needs (hence, open-source)
4. Bring data-security at the forefront (hence, self hosted)

You can check out the project https://github.com/uptrain-ai/uptrain and would love to hear feedback from the community",MachineLearning
"&#x200B;

https://preview.redd.it/1t7spoqxdsea1.png?width=1920&format=png&auto=webp&v=enabled&s=afa8ff93292214a5e06aedd327f367e12bfd7929

paperai is a semantic search and workflow application for medical/scientific papers. It can be used to take a set of research papers, parse the content and turn it into structured data.

paperetl is the underlying library that parses basic metadata such as title, publication and date out of the papers. 

https://preview.redd.it/5jywynarfsea1.png?width=1084&format=png&auto=webp&v=enabled&s=4274e221a14f279e7ff89d86db076c1bbf1e31b6

In addition to standard metadata, paperai can also run extractive queries to build additional columns.

https://preview.redd.it/8rzfj016esea1.png?width=1138&format=png&auto=webp&v=enabled&s=da8dfb2a94124861613f4262abc2bde5badc9971

Example notebooks and Docker files can be found on GitHub.

[paperai](https://github.com/neuml/paperai) | [paperetl](https://github.com/neuml/paperetl)",MachineLearning
"I want to automate the annotation of a domain-specific text (complicated contracts) by finetuning a BERT model. However the annotated text I've been provided by the domain experts has been sparsely annotated (i.e. paragraph 40-48 has been fully annotated, while 15 other paragraphs only have certain classes annotated for certain words. Most paragraphs have nothing annotated (like 70% of the entire corpus)

Another complication is that for 1 class, the entire paragraph should be annotated in this class, while for others it's a single word or a sentence. There are 7 classes in total and in the end, **all tokens should be annotated** to *one* of the 7 classes. 6 out of 7 classes are also pretty domain-specific and *not* something like 'location' or 'person' or POS. 

I've been thinking about using an annotation framework (i.e. LabelStudio or Prodigy) that supports custom models (i.e. finetuned BERT) and active learning to rapidly increase annotated texts by speeding up the work of human annotators (which are domain experts and usually don't have a lot of time for this). However, it's pretty unclear whether my use case would be support by this, especially with the issue of sparse text. 

I've also considered making the problem easier by using another model for the specific class that applies to an entire paragraph. And/or by using the fully annotated paragraphs for finetuning and using the sparse paragraphs for validation.

A final consideration is using GPT-3, but I'm not sure how/if it is able to classify entire sentences/paragraphs with multiple classes and how the prompt should be formatted as.

Any suggestions/ideas?",MachineLearning
"[https://pypi.org/project/rwkvstic/](https://pypi.org/project/rwkvstic/)  


Currently supports tensorflow, pytorch, jax  
Also has support for tensor streaming, 8bit jit-quant and multi-gpu.  
Run RWKV 7B on 8GB of vram or 14B on 16GB of vram.",MachineLearning
"One problem with distributed learning with backprop is that the first layer can't update their weights until the computation has travelled all the way down to the last layer and then backpropagated back up. If all your layers are on different machines connected by a high-latency internet connection, this will take a long time.

In [forward-forward](https://www.cs.toronto.edu/~hinton/FFA13.pdf) learning, learning is local - each layer operates independently and only needs to communicate with the layers above and below it. 

The results are almost-but-not-quite as good as backprop. But each layer can immediately update their weights based only on the information they received from the previous layer. Network latency no longer matters; the limit is just the bandwidth of the slowest machine.",MachineLearning
"When a trained ML model is implemented to predict an adverse event, the user might take steps to avoid that event. The outcome of the user's actions could either be successful or failure.

In training with strictly observational data, a typical confusion matrix contains:

1. ŷ=0, y=0 -> True Negative
2. ŷ=0, y=1 -> False Negative
3. ŷ=1, y=1 -> True Positive
4. ŷ=1, y=0 -> False Positive

When using the model, some results get confounded if the user acts based on the predictions

1. ŷ=0, y=0, a=0 -> True Negative, No Intervention
2. ŷ=0, y=1, a=0 -> False Negative, No Intervention
3. ŷ=1, y=1, a=1 -> True Positive, Failed Intervention
4. ŷ=1, y=0, a=1 -> False Positive OR Successful Intervention

Ignoring the possibility that the intervention caused the adverse event, the involvement of the user may lead to an increase in the number of false positives that are perceived. Continuous monitoring becomes difficult due to perceived faster degradation of the model. Furthermore, retraining the model in the future may be hindered by labels that do not accurately reflect the true values.

One approach that I've been proposed is to make sure there is always a hold-out set. Allow some random records to get scores, but do never act on them. This gives both a monitoring and retraining dataset.

Are there other solutions that people use here? I've found the papers below, but I cannot say that I completely understand how to practically implement them.

* Monitoring machine learning (ML)-based risk prediction algorithms in the presence of confounding medical interventions (https://arxiv.org/abs/2211.09781)
* Model updating after interventions paradoxically introduces bias (https://arxiv.org/abs/2010.11530)",MachineLearning
"Hey!

I am reading the GET3D paper by Nvidia. The paper is listed with the Nvidia license which states:

3.3 Use Limitation. The Work and any derivative works thereof only    may be used or intended for use non-commercially. The Work or    derivative works thereof may be used or intended for use by Nvidia    or its affiliates commercially or non-commercially. As used herein,    ""non-commercially"" means for research or evaluation purposes only    and not for any direct or indirect monetary gain. 

Does it mean there is no commercial way of using the ideas in the paper? Is it possible to use the ideas from that paper or any other paper by Nvidia in some product? As the idea from the paper is only the tool or a part of the product but is not the product itself.",MachineLearning
"I'm currently at the point in my PhD career that I've developed some extremely successful components of CNNs, architecture, activation, etc. Outperforming default choices on CIFAR10, CIFAR100, Flowers, Caltech101, and other smaller datasets. With how success the results currently are, we want to publish to a top tier conference, specifically NeurIPS this Spring, deadline around May 13th. However, we (me and my advisor) agree that to publish at NeurIPS, our developments need to be backed up by ImageNet.

The problem is that we have never trained on ImageNet before (so no experience), and have a limited computational budget. Although our university personally owns 2 A100 40 GB GPUs that we can use, they are shared within the entire university, so a 2 day job takes about 1 week in queue (don't know if we can get the results in time by May). On the other hand, we also don't know if we can get a $2500 grant in time to use cloud resources.

For those who have trained on ImageNet, what are some common pitfalls, best ways to transfer data, downloading the dataset, etc? If you performed it on the cloud, how did you do so? How long was your time to train? Expenses? Did you run each model once or three times? Early stopping using validation or test set?

NOTE: We will only be using Tensorflow...",MachineLearning
"I'd like to extract named entities, something like this:

""\[Text\]: Microsoft (the word being a portmanteau of ""microcomputer software"") was founded by Bill Gates on April 4, 1975, to develop and sell BASIC interpreters for the Altair 8800. Steve Ballmer replaced Gates as CEO in 2000, and later envisioned a ""devices and services"" strategy.

\[Name\]: Steve Ballmer

\[Position\]: CEO

\[Company\]: Microsoft

""

Tried it on GPT-Neox with 20b parameters with mixed success, is there anything better out there to try for a few-shot learning (without fine tuning)?",MachineLearning
"> Neuromorphic perception with event-based sensors, asynchronous hardware and spiking neurons is showing promising results for real-time and energy-efficient inference in embedded systems. The next promise of brain-inspired computing is to enable adaptation to changes at the edge with online learning. However, the parallel and distributed architectures of neuromorphic hardware based on co-localized compute and memory imposes locality constraints to the on-chip learning rules. We propose in this work the Event-based Three-factor Local Plasticity (ETLP) rule that uses (1) the pre-synaptic spike trace, (2) the post-synaptic membrane voltage and (3) a third factor in the form of projected labels with no error calculation, that also serve as update triggers. We apply ETLP with feedforward and recurrent spiking neural networks on visual and auditory event-based pattern recognition, and compare it to Back-Propagation Through Time (BPTT) and eProp. We show a competitive performance in accuracy with a clear advantage in the computational complexity for ETLP. We also show that when using local plasticity, threshold adaptation in spiking neurons and a recurrent topology are necessary to learn spatio-temporal patterns with a rich temporal structure. Finally, we provide a proof of concept hardware implementation of ETLP on FPGA to highlight the simplicity of its computational primitives and how they can be mapped into neuromorphic hardware for online learning with low-energy consumption and real-time interaction. 

Full paper: [https://arxiv.org/abs/2301.08281](https://arxiv.org/abs/2301.08281)",MachineLearning
"How far do you think this can go? Is it a memorization machine or can it create new songs?

https://google-research.github.io/seanet/musiclm/examples/",MachineLearning
">Large  Language Models (LLMs) from the Generative Pretrained Transformer (GPT)  family have shown remarkable performance on a wide range of tasks, but  are difficult to deploy because of their massive size and computational  costs. For instance, the top-performing GPT-175B model has 175 billion  parameters, which total at least 320GB (counting multiples of 1024) of  storage in half-precision (FP16) format, leading it to require at least  five A100 GPUs with 80GB of memory each for inference. It is therefore  natural that there has been significant interest in reducing these costs  via model compression. To date, virtually all existing GPT compression  approaches have focused on quantization, that is, reducing the precision  of the numerical representation of individual weights. A complementary  approach for model compression is pruning, which removes network  elements, from individual weights (unstructured pruning) to  higher-granularity components such as entire rows/columns of the weight  matrices (structured pruning). This approach has a long history, and has  been applied successfully in the case of vision and smaller-scale  language models and tasks. Yet, the best-performing pruning methods  require extensive retraining of the model to recover from the accuracy  loss due to removed elements, which is extremely expensive in the case  of GPT-scale models. While some one-shot pruning methods also exist,  which compress the model without retraining, they are unfortunately too  computationally-expensive to be applied to models with billions of  parameters. Thus, to date, there is virtually no work on accurate  pruning of GPT3-scale models. Overview. In this paper, we propose  SparseGPT, the first accurate one-shot pruning method which works  efficiently at the scale of models with 10-100 billion parameters.  SparseGPT works by reducing the pruning problem to an extremely  large-scale instance of sparse regression. It is based on a new  approximate sparse regression solver, used to solve a layer-wise  compression problem, which is efficient enough to execute in a few hours  on the largest openly-available GPT models (175B parameters), using a  single GPU. At the same time, SparseGPT is accurate enough to drop  negligible accuracy post-pruning, without any fine-tuning. For example,  when executed on the largest publicly-available generative language  models (OPT-175B and BLOOM-176B), SparseGPT induces 50-60% sparsity in  one-shot, with minor accuracy loss, measured either in terms of  perplexity or zero-shot accuracy.

Full paper: [https://arxiv.org/abs/2301.00774](https://arxiv.org/abs/2301.00774)",MachineLearning
"Now that apps are closed did anyone hear back yet? please follow this thread and update your status below, tbh I don't really think I have much of a chance but I'm excited none the less.

&#x200B;

**to follow a thread please press the bell on the top right**",MachineLearning
"Edit: as has been explained in the comments, unicode is not the issue so much as the byte-pair encoding scheme, which artificially limits the vocabulary size of the model and leads to less common language using more tokens. I'd like to discuss the impacts of increasing the vocabulary size on transformer model computational requirements. 

Many languages, like Chinese, Japanese Kanji, Korean, Telugu, etc use complex logograms to represent words and concepts. Unfortunately, these languages are severely ""punished"" in GPT3 because they are expensive to tokenize due to the way unicode represents them. Instead of unicode representing them as a single code point, logograms are typically represented as a sum of multiple graphemes, meaning that multiple unicode code points underlie their description. This makes it far more expensive to prompt and generate in these languages, which is kind of unintentionally quite racist and eurocentric.

For example, let's take the following sentence and count the tokens used in multiple langauges:

Hello, I am a tall man: **7 tokens**

(Chinese) 你好，我是个高个子: **17 tokens**

(Japanese) こんにちは、私は背の高い男です: **21 tokens**

(Hindi) हैलो, मैं एक लंबा आदमी हूँ: **41 tokens**

(Korean) 안녕하세요 저는 키가 큰 남자입니다: **45 tokens**

(Telugu) హలో, నేను పొడవాటి మనిషిని: **68 tokens!!!**

Yes, it's about ten times as expensive to use GPT3 for Telugu. That isn't good, especially if we want to ensure equal access to this technology globally. More than 80 million people speak this language! This also means that besides the cost, the context-length for these languages is much shorter in practice, making practical applications lag years behind what's possible on european languages. Imagine if you only had 400 tokens total context to work with. That's what GPT3 with Telugu is like today.

However, this seems straightforward to fix. Unicode is merely a portability standard, it need not be the input mechanism for NLP models. Why not just preconvert from unicode into a different representation with a larger vocabulary (such as 18-bit) and use one code point per symbol, skipping the whole grapheme thing? It would seem to add negligible processing to the embedding and decoding step, which is a very small portion of overall compute compared to the attention mechanisms, which IIRC represent about 95% of the compute.

Is there some reason why increasing the token vocabulary size and moving away from unicode within the embedding stage would be problematic?",MachineLearning
" I'm new to CTC. After learning about CTC and its application in End2End training for Speech Recognition, I figured that if we want to generate a target sequence (transcript), given a source sequence features, we could use the vanilla Encoder-Decoder architecture in Transformer (also used in T5, BART, etc) alone, without the need of CTC, yet why people are only using CTC for End2End Speech Recoginition, or using hybrid of CTC and Decoder in some papers ?  
Thanks.",MachineLearning
"I know there is a mathematical way to show that the two approaches of score matching models and diffusion models are the same. I wonder, if there in practice/code are the same either? I already tried to find some PyTorch implementations of score based models but didn’t find anything yet - just for diffusion models.",MachineLearning
"I guess what I'm trying to figure out is, what are the main reasons that DMs are outperforming GANs in text2img generation? Thanks!",MachineLearning
"Dec 2022 paper from Microsoft research: https://arxiv.org/abs/2212.10559v2

>Large pretrained language models have shown surprising In-Context Learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without additional parameter updates. Despite the great success in performance, the working mechanism of ICL still remains an open problem. In order to better understand how ICL works, this paper explains language models as meta-optimizers and understands ICL as a kind of implicit finetuning.",MachineLearning
"Hi folks,

I was working on a personal experimental project related to GPT-3, which I thought of making it open source now. It saves much time while working with LLMs.

If you are an industrial researcher or application developer, you probably have worked with GPT-3 apis.

A common challenge when utilizing LLMs such as GPT-3 and BLOOM is their tendency to produce uncontrollable & unstructured outputs, making it difficult to use them for various NLP tasks and applications.To address this, we developed Promptify, a library that allows for the use of LLMs to solve NLP problems including Named Entity Recognition, Binary Classification, Multi-Label Classification, and Question-Answering and return a python object for easy parsing to construct additional applications on top of GPT-n based models.

## 

## Features 🚀

* 🧙‍♀️ NLP Tasks (NER, Binary Text Classification, Multi-Label Classification etc)  in 2 lines of code with no training data required
* 🔨 Easily add one shot, two shot, or few shot examples to the prompt
* ✌ Output always provided as a Python object (e.g. list, dictionary) for easy parsing and filtering
* 💥 Custom examples and samples can be easily added to the prompt
* 💰 Optimized prompts to reduce OpenAI token costs

GITHUB: [https://github.com/promptslab/Promptify](https://github.com/promptslab/Promptify)

Examples: [https://github.com/promptslab/Promptify/tree/main/examples](https://github.com/promptslab/Promptify/tree/main/examples)

For quick demo -> [Colab](https://colab.research.google.com/drive/16DUUV72oQPxaZdGMH9xH1WbHYu6Jqk9Q?usp=sharing)

I hope it will be helpful in your research. Thanks :)

[NER example ](https://preview.redd.it/p3k8sqexzfea1.png?width=1236&format=png&auto=webp&v=enabled&s=8675a40203e0ba684297a242a337177a92367ec5)

&#x200B;

https://preview.redd.it/vnz4mf0i6gea1.png?width=1398&format=png&auto=webp&v=enabled&s=2146654fec504e470bee70cb8cf0922e98b08fab",MachineLearning
" [Open Problems in Deep Neural Networks: An Optimal Control Perspective](https://www.meetup.com/bethesda-data-science-networking-meetup/events/291034298/)

Feb 13, 6:30 ET

**About the talk:**

Backpropagation is a widely used algorithm for training neural networks. Its key step, Stochastic Gradient Descent (SGD) has become one of the bedrocks of deep learning. Despite wide adoption, mathematically rigorous study of SGD's convergence for deep neural networks is still ongoing.

Join us as graduate student [Amoolya Tirumalai](https://www.linkedin.com/in/amoolya-tirumalai-51b493200/) discusses an approach to the convergence problem inspired by optimal control theory. Following the Pontryargin maximum principle, an alternative forward-backward iterative system will be described. Toy examples will be shown, and problems in robustness and security will be discussed.

**About the speaker:**

Amoolya Tirumalai is a 4th year PhD Candidate in Electrical Engineering at the University of Maryland, College Park. His interests are (robust) optimal control, partial differential equations, differential games, mean-field games, safety-critical control, and (robust) machine learning.

His thesis titled '*Multi-agent inference, decision-making and control: models, structure and performance evaluation*' will be defended in August 2023. Mr. Tirumalai was conferred a BS in Biomedical Engineering from the Georgia Institute of Technology in 2018.",MachineLearning
"Versioning and collaboration on code for software engineers is a reasonably solved problem through GitHub since the task at hand predominantly involves just maintaining different copies of just simple vanilla code in different folders. On the other hand, ML engineers face the humungous task of maintaining different versions on not just code, but hyper parameters, data, models, data lineage and labels and storing this on GitHub currently does not allow you to track the changes on each variable well. What are the software/open source tools currently used for the same? Is their a space for a new company to be built here?",MachineLearning
"I'm looking into projects which augment the RLHF training approach of chatGPT with explicit rules, such as in [https://paperswithcode.com/paper/constitutional-ai-harmlessness-from-ai](https://paperswithcode.com/paper/constitutional-ai-harmlessness-from-ai). 

Ideally there would be both rules and priority levels between the rules, similarly to the Asimov laws of robotics. 

The Open-Assistant project ([https://github.com/LAION-AI/Open-Assistant](https://github.com/LAION-AI/Open-Assistant)) captures the spirit, but it is looking to replicate chatGPT at the moment.",MachineLearning
"I would like to measure the smoothness of an NLP-autoencoder's latent space. The idea is to sample two Gaussian vectors v1 and v2 in the latent space of the AE, and generate N-1 points between them like so:

vi = v1 + (v2 - v1) / (N * i)

My idea is to then decode these vectors and measure the BLEU score between d(vi) and d(vi+1) for all N-2 comparisons.

Is this idea reasonable, do you have a better one? Is there a technique from AEs with images that can be useful here?",MachineLearning
"What is the state of the art regarding freely available casing models, i.e. DNNs, that try to restore the original casing of a text with uniform (either lowercase or capital letters) casing? I value both speed and accuracy, as I have to process a large corpus of text.",MachineLearning
"I have two questions about chatGPT. I don't come from a machine learning background. I am just a programmer. So bear with me if they sound a bit dumb.

I was checking about chatGPT a bit the last week. I went through their papers and also tried out a fine tuning by myself by creating some fictional world and giving it some examples. 

The first thing I wondered is what is very special about the model than the large data and parameter set it has, that other competitors can't do. I ask this because I have seen a lot of ""google killer"" discussions in some places. From what I understood from their papers I thought it is something another company with the computing power and the filtered data can have up and running in few months. I see their advantage in rolling out to the public because with feedbacks from actual users all over the world it can potentially be retrained.

The second thing I wondered is its scalability. It feels to me that it is a very big challenge to keep it scalable in the future. Currently getting a long text out of it is kind of painful because it has to continuously generate. I think it is continuously calculating with the huge parameter set it has. I wonder also about new trends, if it needs to be retrained. I also used it for a fine tuning, where I created a fictional world with its own law and rules and the fine tuning took hours in the queue - so is it creating separate parameters for my case? that would be a lot considering how much parameter set they have.",MachineLearning
"Find the release notes here:

[https://github.com/nnaisense/evotorch/releases/tag/v0.4.0](https://github.com/nnaisense/evotorch/releases/tag/v0.4.0)

A big highlight is how fast these implementations are! I genuinely believe GPU-acceleration is the future of Evolutionary algorithms, and EvoTorch and its integration into the PyTorch ecosystem is a fantastic enabler for this.   

To demonstrate the raw speed provided by the new release, I compared EvoTorch's CMA-ES implementation to that provided by the popular pycma package on the 80-dimensional Rastrigin problem and tracked the run-time:

[Performance was measured over 50 runs on the 80-dimensional Rastrigin problem](https://preview.redd.it/w3qwefgr6dea1.jpg?width=458&format=pjpg&auto=webp&v=enabled&s=e056e6aa42e07b050ea2a187ae3b07de2b789f6f)

The crazy thing to note is that when we switch to GPU (Tesla V100), we can efficiently run CMA-ES with population sizes going into 100k+!",MachineLearning
Anybody know some methods and techniques for integrating a numerical solver with the neural network .. how do you calculate the gradients of the solver when you don’t know the details of such solver- black box solver.,MachineLearning
"I'm about to start an experimental project that involves training a denoising diffusion model on the medical data (small dataset).

Could you please share useful resources, tips, tricks and heuristics for dealing with diffusion models?",MachineLearning
"Hello everybody

I was wondering if anybody tried to run the main factorisation code [https://github.com/deepmind/alphatensor/blob/main/benchmarking/factorizations.py](https://github.com/deepmind/alphatensor/blob/main/benchmarking/factorizations.py) from alpha tensor on Google Colab, with Colab's GPUs ( Tesla T4).

I know that Tesla T4 is not as the same as the V100 used in Deep Mind's paper, however, I can see that the tensor formulation for the matrix multiplication is highly inefficient, compared to standard JAX matrix multiplication.

Any suggestion where am I wrong?",MachineLearning
"If we submit a publication to ICML and it is under anonymous review, can I list the title and authors on my resume which will be on my personal webpage?",MachineLearning
"[https://huggingface.co/blog/dialog-agents](https://huggingface.co/blog/dialog-agents) breaks down the techniques behind ChatGPT -- instruction fine-tuning, supervised fine-tuning, chain-of-thought, read teaming, and more.

https://preview.redd.it/fv16fsemd9ea1.png?width=889&format=png&auto=webp&v=enabled&s=bc723c4cc71ec0457bb1c2ac07f5fa6e4a3a4ccf",MachineLearning
"Hello,
Anyone ever used a paper revision service and can recommend one ?

I’m publishing my first paper next month and I want to have feedback from an expert on this domain.

Thanks !",MachineLearning
"This thread is dedicated to exploring the various techniques used in self-supervised contrastive learning that utilize standard batch sizes. I am seeking information on the current methods in this field, specifically those that do not rely on large batch sizes.

I am familiar with the SimSiam paper published by META research, which utilizes 256 batch size for 8-GPUs. However, for individuals with limited resources such as myself, access to a large number of GPUs may not be feasible. As a result, I am interested in learning about other methods that can be used with smaller batch sizes and a single GPU, such as those that would be suitable for training on 1024x1024 input images.

Additionally, I am curious about any more efficient architectures that have been developed in this field. This includes, but is not limited to, techniques used in natural language processing that may have applications in other areas of artificial intelligence.

\*\*\*posted the same question in PyTorch forums, reposting here for wider reach.",MachineLearning
"&#x200B;

[Tsetlin machine interpretability vs deep learning attention.](https://preview.redd.it/vgcfhj7x86ea1.png?width=2074&format=png&auto=webp&v=enabled&s=07eae3a8ae5f4be6aef020b82fb28dedb4016cc5)

Researchers at West China Hospital, Sichuan University, NORCE, and UiA have developed a Tsetlin machine-based architecture for premature ventricular contraction identification by analyzing long-term ECG signals. The experiments show that the Tsetlin machine is capable of producing human-interpretable rules, consistent with the clinical standard and medical knowledge. Simultaneously, the accuracy was comparable with deep CNN-based models.

Paper: [https://arxiv.org/abs/2301.10181](https://arxiv.org/abs/2301.10181)",MachineLearning
"In this video  I   explain about INSTRUCTOR, an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructor achieves sota on 70 diverse embedding tasks! I also show a google collab demo of instructor

https://youtu.be/vg38cq3KJ6M",MachineLearning
" I work as as a researcher and am kind of new to neural networks. I have an RNN (1e4 x 1e4 network) that I would like to train in either MATLAB or Julia.

One option I considered is writing my own code for Hessian-free optimization, but the implementational details are really, really hard to figure out.

I am aware there is a Theano or TF implementation of HFO but I I am primarily interested in having the code in MATLAB/Julia.

Also, are there better/alternative techniques than Hessian-free optimization for training RNN's ?",MachineLearning
"Example: the AI model [BLOOM](https://en.wikipedia.org/wiki/BLOOM_(language_model)) is licensed under the [BigScience RAIL License v1.0](https://huggingface.co/spaces/bigscience/license). The BigScience RAIL License v1.0 forbids that some types of usages:

> You agree not to use the Model or Derivatives of the Model:
>
>  [...]
>
> - To provide medical advice and medical results interpretation;
> - To generate or disseminate information for the purpose to be used for administration of justice, law enforcement, immigration or asylum processes, such as predicting an individual will commit fraud/crime commitment (e.g. by text profiling, drawing causal relationships between assertions made in documents, indiscriminate and arbitrarily-targeted use).

Am I allowed to use BLOOM in a program that is useful for any domain (e.g., a program to summarize or paraphrase some text, or perform question-answer on a text, or generate questions and their answers based on the text)? 

Since people could use the program for any domain, they could technically, for example, use the program to summarize a medical report or generate questions and their answers based on some asylum process to distribute to potential applicants.",MachineLearning
" ICML's [Call for Papers](https://icml.cc/Conferences/2023/CallForPapers) states that ""It is not appropriate to submit papers that are identical (or substantially similar) to versions that have been previously published, accepted for publication, or submitted in parallel to other conferences or journals"". Our paper got rejected at ICLR, but the de-anonymization will take place 1-2 days after the ICML deadline.

For the purposes of the dual submission policy, does ""rejected but not de-anonymized"" count as ""submitted in parallel""? Or is just a technicality?",MachineLearning
"I have hundreds of potential features to use in my DNN. Instead of doing a separate analysis to figure out which features are most important, can I just use all of them in my DNN and let the model figure which features are most predictive? I have millions of training data so overfitting will not be a problem, I just wonder whether the bad features may make the model difficult to utilize the good features?

Not absolutely crucial but if there is a paper that discusses this topic, that would be super awesome as well. Thanks in advance.",MachineLearning
"If you are using some more odd formats, then what format do you use? Personally found webdataset promising but what other formats are there and why do you use it? Or if you are using the original file how do you ensure good throughput and shuffling?",MachineLearning
"Hi, I'm in my second year of AI master at uni and my professor assigned me the following topic for my dissertation: ""Cognitive Threat Hunting"" and recommended the following [book](https://www.manning.com/books/cyber-threat-hunting) for documentation.

I have read the book, but I still don't know how to do it: how to create a ml model to hunt in the k8s env. My professor wants a ml model that searches in a Kubernetes env for threats.

The thing is that in this book, in chapter ""8. Unsupervised Machine Learning With K-Means"" he uses a dataset of events from Humio to train the model, but it's not shared with us. And I don't have one, how can I train my model properly if I don't have a good dataset of events? I can't make one just by generating some events in a container, I need real data as the author uses in his chapter.

I feel desperate and lost at this point, I hope that someone from here can give me some advice or a good direction to go.",MachineLearning
"tsdownsample brings highly optimized time series downsampling to Python! The downsampling algorithms are written and optimized in Rust, which are made available in Python through the use of PyO3 bindings.

Code: [https://github.com/predict-idlab/tsdownsample](https://github.com/predict-idlab/tsdownsample)

# Features

* **Fast**: leverages the optimized [argminmax crate](https://github.com/jvdd/argminmax) which is SIMD accelerated with runtime feature detection (matches or even outperforms numpy's speed)
* **Efficient**: operates on views of the data, eliminating the need for unnecessary data copies and avoiding the creation of intermediate data structures
* **Flexible**: supports a wide range of datatypes, including [f16 which is 200-300x faster than numpy's implementation](https://github.com/jvdd/argminmax/pull/1).
* **Easy to use**: simple and flexible API

# Installation

    pip install tsdownsample

# Example

When using multi-threading, tsdownsample can downsample 500 MILLION datapoints (f32) in 0.05s! ⬇️

https://preview.redd.it/frqh8o2bezda1.png?width=1650&format=png&auto=webp&v=enabled&s=0674cd6b681210d70d8f2e7a81b12415c880ea50

&#x200B;

I would love to hear your feedback on this!",MachineLearning
"ICLR introduced a [Tiny Paper Track](https://iclr.cc/Conferences/2023/CallForTinyPapers) for shorter contributions, up to 2 pages. Sounds like a nice idea, right?

But to keep things interesting, since it's organized by the DEI initiative, there are restrictions as to who can author the submitted papers. 

According to the official guidelines:
> Each Tiny Paper needs its first or last author to qualify as an underrepresented minority (URM). Authors don't have to reveal how they qualify, and may just self-identify that they qualify.

> Our working definition of an URM is someone whose age, gender, sexual orientation, racial or ethnic makeup is from one or more of the following: 

> Age: outside the range of 30-50 years

> Gender: does not identify as male

> Sexual orientation: does not identify as heterosexual

> Geographical: not located in North America, Western Europe and UK, or East Asia

> Race: non-White

> In addition, underprivileged researchers and first-time submitters also qualify:

> Underprivileged: not affiliated with a funded organization or team whose primary goal is research
> First-time submitters: have never submitted to ICLR or similar conferences


So effectively, someone could submit a paper, and literally have it rejected because they're e.g. white or male. 

Is this really the way the field should go? I feel like this is something that should never have passed any ethics board, but clearly the organizers disagree.",MachineLearning
"Hey /r/machinelearning,

It's me again with another small open-source tool release (see the last one [here](https://www.reddit.com/r/MachineLearning/comments/10do40p/p_a_small_tool_that_shuts_down_your_machine_when)). `image_tiles` is a very simple command line tool that **serves a webpage of images from the folder you run it in.**

Why use this tool?

* Makes it easy to view images on a remote machine you're SSH'd into.
* S3 support: view buckets of images on S3 without having to `aws s3 cp` the bucket.
* Advanced normalization and rendering support makes it suitable for remote sensing images like satellite and multi-spectral. This support is still nascent but easily extendable!

Again, it's easy to install and use, just `pip install image_tiles` or `pip install image_tiles[aws]` for S3 URI support. Then in the folder, run `image_tiles`.

Check it out here: https://github.com/moonshinelabs-ai/image_tiles",MachineLearning
"Accepted conference papers at ICLR represent a high level of scientific quality. The other side of the coin is that they can be out of reach for those starting out, or from different backgrounds. We want paper publishing to be not only a showcase of achievements, but also a marker for valuable learning experiences made accessible to beginners and outsiders. Devising more ways to mark milestones and measure growth in an individual, or community’s maturity, is greatly conducive to both continually pushing the frontiers of science, and lifting people up in this process.

Researchers from underrepresented backgrounds are not necessarily equipped with the same resources to publish full papers from the start of their scientific journeys. To create a more inclusive ICLR community, we as organizers need to create more pathways, and reasons for people from various backgrounds to enter, stay, and grow in the machine learning community.

To that end, we are launching a more approachable format to publish, kick-start, and collaborate on ideas, “Tiny Papers,” as a brand new track at ICLR 2023, with the hope to attract more underrepresented, under-resourced, and budding researchers to join the community in a meaningful way. Not only will this additional, and different, venue to publish usher in new researchers, it will also serve as an efficient platform for disseminating ideas, findings, and opinions.

The objective of the new “Tiny Papers” track at ICLR is threefold:

1. Creating alternative, complementary, and diverse entry points to research. And in particular, creating approachable avenues for beginners to enter and enjoy the ICLR community.
2. Celebrating intermediate breakthroughs in machine learning.
3. Efficiently disseminating ideas, findings, and opinions.

As the Tiny Papers track is a Diveristy, Equity and Inclusion (DEI) initiative, orgnized by the DEI Chairs of ICLR 2023, we require every submission to have at least one key author that meets the **underrepresented minority (URM) criteria** (see [webpage](https://iclr.cc/Conferences/2023/CallForTinyPapers) for details).

Inspired by the Call for Blog Post at ICLR 2023, despite the wide range of topics that could benefit from the Tiny Papers format, we decide to restrict the type of submissions to be the following:

* An implementation and experimentation of a novel (not published elsewhere) yet simple idea, or a modest and self-contained theoretical result
* A follow-up experiment to or re-analysis of a previously published paper
* A new perspective on a previously published paper

&#x200B;

**If you are interested in helping review, organize, volunteer, and generally contribute to this intiative, kindly fill out** [this form](https://forms.gle/YJzwNABHVL143RFKA)**.**

&#x200B;

For more information, please see the website: [https://iclr.cc/Conferences/2023/CallForTinyPapers](https://iclr.cc/Conferences/2023/CallForTinyPapers)

&#x200B;

*Key Dates*

Submission portal opens: February 1st, 2023

Submission deadline: February 28th, 2023 AoE

Notification of acceptance: March 31st, 2023

Camera-ready deadline: April 15th, 2023

Tiny Papers Showcase Day: May 5th, 2023

*ICLR 2023 DEI Chairs*

Krystal Maughan (University of Vermont)

Rosanne Liu (ML Collective, Google Brain)

Thomas F Burns (OIST)

Email: [iclr.dei.2023@gmail.com](mailto:iclr.dei.2023@gmail.com)",MachineLearning
"I've been writing a new textbook on deep learning for publication by MIT Press late this year.  The current draft is at:

[https://udlbook.github.io/udlbook/](https://udlbook.github.io/udlbook/)

It contains a lot more detail than most similar textbooks and will likely be useful for all practitioners, people learning about this subject, and anyone teaching it.  It's (supposed to be) fairly easy to read and has hundreds of new visualizations.

Most recently, I've added a section on generative models, including chapters on GANs, VAEs, normalizing flows, and diffusion models.

Looking for feedback from the community.

* If you are an expert, then what is missing?
* If you are a beginner, then what did you find hard to understand?
* If you are teaching this, then what can I add to support your course better?

Plus of course any typos or mistakes.  It's kind of hard to proof your own 500 page book!",MachineLearning
"Our work Asyrp was accepted to [\#ICLR2023](https://twitter.com/hashtag/ICLR2023?src=hashtag_click) AND got SPOTLIGHT🌟! 

Asyrp allows using h-space, the bottleneck of the U-Net, as a semantic latent space of diffusion models.  

&#x200B;

[Make a dog be happy! \(by Asyrp\)](https://preview.redd.it/3qn0ye88huda1.png?width=258&format=png&auto=webp&v=enabled&s=76d82b925ba84118288cbbbcd53d6e920fb351b6)

""Diffusion Models already have a Semantic Latent Space"" 

Paper: [https://arxiv.org/abs/2210.10960](https://t.co/sdVDuQtstu) 

Project page: [https://kwonminki.github.io/Asyrp/](https://t.co/5upW8BgYoB) 

Code: [https://github.com/kwonminki/Asyrp\_official](https://github.com/kwonminki/Asyrp_official)",MachineLearning
"Deodel is a Python implementation of a classifier with native support for mixed attributes data. It features good accuracy, especially with heterogenous attributes.
It even supports mixing of continuous and nominal values in the same attribute column.

https://github.com/c4pub/deodel",MachineLearning
"One common place where LLM performance falls is on words split by the model's tokenizer. I'm surprised that no one I can find has proposed swapping the embedding layer for an embedding bag layer, with the bagged embedding coming from a sum of embeddings of character ngrams for the token, like in fastText word embeddings (this helps the model learn faster in smaller corpora and yields better representations for rare words). Has anyone found someone who tried this?",MachineLearning
"Happy to share that [CleanRL](https://docs.cleanrl.dev/) now has a new algorithm called Robust Policy Optimization — 5 lines of code change to PPO to get better performance in 57 out of 61 continuous action envs 🚀 (e.g., dm_control)

📜docs: https://docs.cleanrl.dev/rl-algorithms/rpo/

💾code: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/rpo_continuous_action.py

🐦tweet: https://twitter.com/vwxyzjn/status/1617561414276898822",MachineLearning
"Just a thought.

The idea comes from the analogy between thermodynamic system and brain, both consisting of many individuals. A thermo system, in the quantum language, has tremendous ""microstates"", and, when in stable condition, the microstates have stable probability distribution. A brain, or a nervous system in general, can have microstates, because each neuron has its most prominent state, firing or not firing, which is binary of course. Then, for example, a tiny brain with three neurons A, B and C, can generate 2X2X2=8 microstates. We can imagine that a brain in the perfectly stable environment can sustain stable firing, and hence keep microstates in stable probability distribution.

Now let's see how neural connections affect the microstates' probability distribution, and how the information concept comes in. Back to the tiny brain above, at first suppose that the brain has all three neurons disconnected without any neural connections. Then we add a neural connection from neuron B to C, and suppose the connection transmits the signal from B to C with probability 0.4. The added neural connection changes the microstates probability distribution, because microstate B, where only neuron B fires, transits to microstate BC, where neuron B and C fires together, with probability 0.4, and likewise microstate AB transits to microstate ABC with probability 0.4. The following diagram depicts the microstate transitions, where transition probabilities are labeled.

https://preview.redd.it/q0twgkmaurda1.png?width=273&format=png&auto=webp&v=enabled&s=5c5ad74abdaa0d4b9154c353174d1178878c2e7e

The transitions allow us to picture the neural connection as a noisy channel between information source X and destination Y, sending and receiving symbols (i.e. microstates) respectively. Here X represents the brain before adding the neural connection, and Y represents the brain after. We can say, the noisy channel perturbs the original microstates generated by X, and then Y ""receives"" the perturbed ones by actually firing neurons. Therefore, X, neural connection and Y together make the very communication system in the classic information theory, as follows.

https://preview.redd.it/5k314updurda1.png?width=845&format=png&auto=webp&v=enabled&s=f68d99f8aa0299c1a1019b5b1aac832564062e0d

With this communication system, we can talk about entropy, order and chaos, channel capacity, transmission rate, etc., anything we know about information theory. And we can see, any change of neural connections, on one or many, weakened or strengthened, adding or removing ones, only modifies the channel and specifically the microstates transitions. And the channel can go to a stable state under the long-term influence of neurons' firing; this stable state of channel could be a ""memory"" of the stable environment.

Wild idea isn't it? It is just irresistible to see all those otherwise remote concepts fit in so nicely (to my knowledge). Anyway, comments are welcome.",MachineLearning
"Obviously nation states can already pretty comprehensively identify people using other methods, even on tor and such because of user error, but If your average home user can quickly do this using text what will implications be for the web? 

1) I am Assuming that is it currently possible to feed a model a bunch of text written by “Bobby” and put a specific post into model and get confidence stat that is was written by Bobby 

2) would it be possible in future with better models and a lot more compute to use non anon data from all of Facebook or internet to quickly scan pseudo anonymous places like Reddit, twitter or even something truly anon like dark web and return all  results of list of probable authors?  

I’m assuming people whom are seeking true anonymity already put their text through paraphrase models or just write very bland. 

I am Using the word mask instead of anonymous because Reddit seems more like obfuscation than potential true anonymity like with some tor forum with a sophisticated user or something.

It is interesting to think that all the subtle errors and invisible algorimic choices of the human brain is trivial for a machine to identify given a sufficient natural language model that can translate the text and incorporate pattern matching.

Edit: I mean a a noisy probability stat not an assurance that x was written by y. More like 75% match to Bobby 32% match to sally.    Matching to errors, flow, unusual word choices, more advanced than just a plagiarism detector.",MachineLearning
"I could not find up-to-date information regarding the review process at ICML, given the transition to OpenReview this year. Does anyone happen to know either of the following:

+ Will reviews for rejected papers remain public after the conference, like at ICLR? Or will reviews for rejectees be hidden, like at NeurIPS

+ In previous years, ICML allowed authors to withdraw their paper at any point in the process. [The FAQ page](https://icml.cc/FAQ/WithdrawPaper) has not been updated since 2021, but I assume this is still the case?

Thanks very much for any information.",MachineLearning
"Hi guys! I'm currently trying to forecast a product's demand for the upcoming months (March and April). I have data relating to this product's demand since January 1999. However, the COVID-19 pandemic greatly disrupted the time series' patterns for 2020 and 2021. How should I deal with data from March 2020 to around Jan 2022?

Should I completely discard it and only include data from Jan 1999 to Dec 2019, and then Jan 2022 onwards? I'm struggling to find any good articles on how predictive tasks are now being conducted. Are there papers that suggest particular ""denoising"" techniques for pandemic data?

Thank you!",MachineLearning
"Hi all,

I have an e-commerce product data. It contains product description and product type. I’m using embeddings with ANN (annoy) to find similar products. However, I don’t know how to implement evaluation of vector search results. There are some metrics such as hit rate, recall but like I said above I’m confused to use them. Most of the examples I come across has a label (interaction data, explicit score etc.) therefore they can calculate metrics. Any ideas or recommendations will be appreciated!",MachineLearning
"I have 2 GPUs, an RTX 3080 and a GTX 1080Ti. Currently I am using only the 3080, and the 10 GB VRAM doesn't seem to cut it. Can I use both the 3080 and 1080 simultaneously? My motherboard has multiple PCI-E x16 slots. My OS is PopOS. Is there any way to use multiple GPUs of different types? I'm particularly looking at KoboldAI, but it would also be useful in general. I know that SLI won't work since they're different GPUs.",MachineLearning
"It’s an early version and I’m trying to get some feedback on how I can improve this and do it the “right way”. 

Source Code and Results: https://github.com/prabhuomkar/bitbeast/tree/master/ptibench",MachineLearning
"Does any one know how to solve the PDE for it in python? Any kind of reference material would be appreciated!

It's been long since I came across any PDEs and have forgotten everything related to it.",MachineLearning
"I know that tree based models are the go approach for tabular data despite the advantages of deep models on other data types.

I was wondering if there is any resources/suggestion/study/review/approach for tabular data when we dont have large amount of data?",MachineLearning
"So LLMs like GPT3 have understandably raised concerns about the disruptiveness of faked texts, faked images and video, faked speech and so on. While this may likely change soon, as of now OpenAI controls the most accessible and competent LLM. And OpenAIs agenda is said in their own words to be to benefit mankind.

If so, wouldn't it make sense to add a sort of watermark to the output? A watermark built into the model parameters so that it could not easily be removed, but still detectable with some key or some other model. While it may not matter in the long run, it would set a precedent to further development and demonstrate some kind of responsibility for the disruptive nature of LLMs/GPTs.

Would it not be technically possible, nä would it make sense?",MachineLearning
"I've been integrating GPT3 API as well as ChatGPT into my business workflow, but I'm still hesitant about feeding data of any sensitive nature (example: client data or anything that may even vaguely relate to an NDA).
For those of you using commercial models-as-a-service for business applications, what are your thoughts on things like prompt data storage, and whether OpenAI will utilize customer prompt data to further train their model?",MachineLearning
"Hi folks,  
As a researcher in the healthcare field, I often find it tedious to keep track of conference deadlines.   
To solve this issue, We developed a website to easily track healthcare conf & workshops, integrated with Google Calendar for notifications.   
[deadlines.openlifescience.ai](https://t.co/6CTphktR8W)   


* The website is inspired by [http://aideadlin.es](https://t.co/OFxQEbOVET). 
* Feel Free to add new conferences/Workshops deadlines related to the healthcare domain [https://github.com/openlifescience-ai/ai-deadlines](https://t.co/NaU3nXiOMZ)  


I hope it will be helpful in your research. Thanks :)",MachineLearning
"I'm wanting to train an object keypoint / pose detection CNN model for flexible robot arm.What would be the best opensource code to start with and customize?

Mockup of desired results, where I can extract data from keypoints, and pose / position data:

https://preview.redd.it/nhxwt48hqfda1.png?width=786&format=png&auto=webp&v=enabled&s=2cb484c5b62c0dfd5ce92b2d18a2dd7573a36b17

I came across  MMDetection:""open source object detection toolbox based on PyTorch""

and I know about MediaPipe

But I don't need to detect things other than the robot arm.What  would be the simplest way to get a model trained on a local system  using open source code that uses PyTorch, ideally without starting from  scratch?

A model that could handle point and segment occlusion would be nice.",MachineLearning
"Hi all,

We would like to share here our little concise review of generative AI large models just to show how current models are able to work with lots of formats like texts, videos, images, etc... 

[https://arxiv.org/abs/2301.04655](https://arxiv.org/abs/2301.04655)

&#x200B;

Enjoy!",MachineLearning
Is it a good idea to train a single OCR model to extract key value information from documents of same domain but with different layouts? Will it generalize? There are around \~1k different document layouts.,MachineLearning
"I'm in industry, and it seems like most project bottlenecks stem from getting from a vague business question to an aggregated/workable dataset to answer a more specific version of the initial business question.

For example, given a question such as ""We want to know CLV"" (customer lifetime value)

1. Since the above is too vague, what are ""best practice"" ways to rephrase this so that it's actually answerable?
   1. I.e. it could be framed as a binary classification problem to predict whether each customer will be worth at least X by Y date (> $1000 at 12 months) or a regression problem to predict the value of each customer at a future date given features we know today
2. What's best practice for:
   1. How far into the past the where clause time window should be to get the customer features
   2. How far into the future the where clause time window grabs outcomes to join back to the current customer features?

Does anyone know if there a resources that consolidate best practices or common approaches for the above scoping/experimentation questions?",MachineLearning
I’m starting with a project where I’m using camera or video input of a badminton game and use to analyse the game but i need help in starting with it as I’m in the beginning phase. Can anyone please help me with the same?,MachineLearning
"&#x200B;

[Fine-grained control of the number and size of clauses.](https://preview.redd.it/9n2p05izldda1.png?width=1418&format=png&auto=webp&v=enabled&s=096b21dcb7b47393958fee51c8a0ed7927aa6dbb)

Paper: [https://arxiv.org/abs/2301.08190](https://arxiv.org/abs/2301.08190)

Code: [https://github.com/cair/tmu](https://github.com/cair/tmu)

Tsetlin machine (TM) is a logic-based machine learning approach with the crucial advantages of being transparent and hardware-friendly. While TMs match or surpass deep learning accuracy for an increasing number of applications, large clause pools tend to produce clauses with many literals (long clauses). As such, they become less interpretable. Further, longer clauses increase the switching activity of the clause logic in hardware, consuming more power. This paper introduces a novel variant of TM learning - Clause Size Constrained TMs (CSC-TMs) - where one can set a soft constraint on the clause size. As soon as a clause includes more literals than the constraint allows, it starts expelling literals. Accordingly, oversized clauses only appear transiently. To evaluate CSC-TM, we conduct classification, clustering, and regression experiments on tabular data, natural language text, images, and board games. Our results show that CSC-TM maintains accuracy with up to 80 times fewer literals. Indeed, the accuracy increases with shorter clauses for TREC, IMDb, and BBC Sports. After the accuracy peaks, it drops gracefully as the clause size approaches a single literal. We finally analyze CSC-TM power consumption and derive new convergence properties.",MachineLearning
 I am working on a project i.e to build an android application using federated learning but I am unable to run federated learning on edge devices like android phones. I tried frameworks like Flower for it but I am unable to achieve the result. If you have worked on a project related to federated learning on edge devices please help me out.,MachineLearning
"Hey everyone,

I'm currently faced with the challenge of having to generate data that is deliberately not in a dataset. So if you think about the dataset as a distribution, the data points should have a possibly low probability. Additionally, each data point is a 30 dimensional vector and I know the min and max values for each dimension.

How do I do that? What kinds of algorithms could I use for that? Can I somehow fit a distribution and sample low probability data points from it? Or a GAN for generating? Or are there obvious classical ML or statistical methods for that?",MachineLearning
"I am beginning a problem similar to the one bellow for my work. 

There is a score 1-4 (1 is bad, 4 is very good) of a persons back sprain recovery. The data we have are back sprain recovery scores recorded after two weeks, 3 months and 6 months, along with information (features) about their behavior like sleep, medications, diet, and exercise. 

We want to predict there 2 week, 3 month, and 6 month back sprain recovery scores based on their initial behavior inputs. For example, given a user sleeps 8 hours a day, consumes x amount of sugar, does physical therapy 4 days a week, and takes x medication, what will there recovery scores be at 2 weeks, 3 months and 6 months? 

The training data would look like: 

&#x200B;

|Sleep Average|Medication|Days of Physical Therapy|Diet|Week 2 recovery score|Month 3 recovery score |Month 6 recovery score|
|:-|:-|:-|:-|:-|:-|:-|
|9 hours per night|Advil|4 days/ week|Healthy|2|3|4|
|5 hours per night|None|0 days/week|Unhealthy|1|2|2|

&#x200B;

I want a model (or multiple models) to predict 3 values which is the 2 week, 3 month, and 6 month scores. I am not familiar with time series, but it seems like the data may be too sparse. 

Should I be using time series here, or should I create 3 classification models?",MachineLearning
"So, these pictures below are taken from a 144p video on YouTube. You cannot tell me that these aren't CNN upscaling artefacts.

So this raises the question of.... how exactly is this implemented? What model are they using which is tiny enough to run on (i assume) WebGL2? Is it a CNN inside of GLSL shaders? Is it something else? CPU side or GPU side?

And also... how have I not seen a single other person pointing this out, anywhere on the internet. Believe me I looked. Ain't no one talking about this.

**EDIT: UPDATE** this is doing it in ALL videos in chrome now. It only works in Chrome, not in Discord or Edge, so its not GPU/Windows fuckery. But the strange thing is other friends testing this with the same version of Chrome \*\*\*DONT\*\*\* have this? And the even stranger thing is... this is running on Intel Integrated Graphics...

https://preview.redd.it/jnjwjzyag7da1.png?width=3240&format=png&auto=webp&v=enabled&s=6a91ab73189c746551f07206419a94c7069dd66c

https://preview.redd.it/6vzyx5f1g7da1.png?width=1182&format=png&auto=webp&v=enabled&s=a03a56e57a4d8b9498224b356b60c1451453195c

https://preview.redd.it/bo36ko40g7da1.png?width=365&format=png&auto=webp&v=enabled&s=cd09523cc574120d0f0e78d2c5b3319d150b1e09

https://preview.redd.it/16zpxwqyf7da1.png?width=333&format=png&auto=webp&v=enabled&s=079d10cb84a70b19cc8cc875f97ae89658fa9056",MachineLearning
"
Hello, 

I really don't know how to frame this question but I wanted to ask if the was a way to integrate the relationships and nodes of a knowledge graph with recorded data. Like for example, when a knowledge graph contains information about relationships between features, can it be integrated with a dataset containing recorded or measured quantities of those features. The goal of this is to ""infuse"" the recorded dataset with relationships already known in the knowledge graph for some data analysis purpose.


I know it sounds confusing but you can as for clarification on some details. Please help.",MachineLearning
"Neuro-symbolic AI is a promising approach to artificial intelligence that aims to combine the strengths of symbolic reasoning and probabilistic systems. For example, combining inductive logic programming and deep learning with applications in graphs, vision, reasoning and explainability.

In this special session, we will provide an overview of neuro-symbolic AI, key concepts, and current state-of-the-art techniques. We will also discuss the potential benefits and challenges of neuro-symbolic AI and its potential impact on various fields and applications. In addition to the tutorial, **we welcome contributions from attendees in the context of neuro-symbolic AI.**

This includes but is not limited to:

•    Novel neuro-symbolic models and techniques

•    Applications of neuro-symbolic AI to real-world problems

•    Empirical evaluations and comparisons of neuro-symbolic AI approaches

•    Theoretical foundations and analysis of neuro-symbolic AI

•    Emerging trends and challenges in the field of neuro-symbolic AI

&#x200B;

**Submission guidelines:** [https://www.esann.org/node/6](https://www.esann.org/node/6)

**Paper submission deadline:**  2 May 2023

**Conference date:**  4-6 October 2023

**Conference location:** Crowne Plaza hotel Bruges, Belgium",MachineLearning
"Hello,

I am familiar with the theory of differentiable computing and SGD training having done some research work on semi-supervised learning for image classification and semantic/panoptic segmentation. In other words, I am familiar with understanding implementing state-of-the-art proposals as well as tweaking them.

Now I'm an unemployed and interested in conducting some research using PyTorch and Google Colab which seems feasible only if the problem or topic at hand is relatively low-cost.

So I'm asking the question:

* What are some deep learning (metalearning,regularization, non-supervised training) or applied DL (CV/NLP/...) topics or datasets that are lightweight enough to be researched with just one GPU?

Thanks and have a nice weekend",MachineLearning
"[https://github.com/google-research/tuning\_playbook](https://github.com/google-research/tuning_playbook) \- Google has released a playbook (solely) about how to tune hyper-parameters of neural networks.

Disclaimer: I am unrelated to this repository, just came across it and thought it is suitable for this subreddit. I have searched through and found no posts, thus I post it to hear some comments/insights from you ;)",MachineLearning
"Hi guys, thanks for reading this post. I built a simplistic paper search tool that integrates ACL Anthology, arXiv API, and DBLP API.

Github address: [Spico197/paper-hero](https://github.com/Spico197/paper-hero)

**Motivation:** I'm majoring NLP and I'd like to search for papers with ""Event Extraction"" as titles in specific proceedings (e.g. ACL, EMNLP).

**Challenge:** There are lots of search tools and APIs, but few of them provide field-specific searches, like authors, titles, abstracts, and venues.

**Methodology:** I integrate ACL Anthology, arXiv API, and DBLP API, and provide a two-stage search toolkit, which first stores target papers via the official fuzzy search API, and then matches specific fields.

**Advantages:** This tool satisfies my need to stockpile papers and it can dump checklists in markdown format, or complete paper information in jsonl. AND and OR logics are supported in search queries.

**Limitations:** This tool is based on simple string matching, so you have to know some terminologies in the target fields.

You are warmly welcome to have a try and feel free to drop me an issue!

    from src.interfaces.aclanthology import AclanthologyPaperList
    from src.utils import dump_paper_list_to_markdown_checklist
    
    if __name__ == ""__main__"":
        # use `bash scripts/get_aclanthology.sh` to download and prepare anthology data first
        paper_list = AclanthologyPaperList(""cache/aclanthology.json"")
        ee_query = {
            ""title"": [
                # Any of the strings below is matched
                [""information extraction""],
                [""event"", ""extraction""],    # title must include `event` and `extraction`
                [""event"", ""argument"", ""extraction""],
                [""event"", ""detection""],
                [""event"", ""classification""],
                [""event"", ""tracking""],
                [""event"", ""relation"", ""extraction""],
            ],
            # Besides the title constraint, venue must also meet the needs
            ""venue"": [
                [""acl""],
                [""emnlp""],
                [""naacl""],
                [""coling""],
                [""findings""],
                [""tacl""],
                [""cl""],
            ],
        }
        ee_papers = paper_list.search(ee_query)
        dump_paper_list_to_markdown_checklist(ee_papers, ""results/ee-paper-list.md"")

&#x200B;

[markdown checklist](https://preview.redd.it/myy4kbut15da1.png?width=2038&format=png&auto=webp&v=enabled&s=4fc3cacedd22bf6290bef3d94ec00bdfe16f61c7)",MachineLearning
"Hi, Making a post for anything to be discussed related to ICLR 2023 results

&#x200B;

One question I had: Is the exact time of result announcement fixed?",MachineLearning
" So I like to DIY a lot. I coded a neural network to run parallel in a compute shader with TanH activation, and the performance was much better than I expected. I Tested with a 3090 with many layers of 20,000 neurons until I reached 4 billion total parameters which ran around 50FPS when looped every frame. Is this above average performance for a GPU implementations? I haven't really tested out any other GPU implementations, so I was wondering if anyone here knows.",MachineLearning
"I [originally posted this here](https://www.reddit.com/r/learnmachinelearning/comments/10g6a82/best_speechtotext_model_open_source_vs_paid/) on [r/learnmachinelearning](https://www.reddit.com/r/learnmachinelearning/) but reposting here as it may be a more appropriate subreddit and/or may have a different perspective.

**I want a tool to programmatically generate transcripts from sermons.**

I have access to hundreds of sermon transcripts (and 100x more very similar in domain data) but less than a 40 transcripts with audio (\~30 hours).

I want the lowest WER (Word Error Rate) possible and can budge 100 hours for this project in 2023.

Train my own acoustic model with the best open source offering

* OpenAI's [Whisper](https://openai.com/blog/whisper/) seems to be the best available today?
* How much supervised data (e.g. hours of sermons with perfect transcripts) would I need to develop a model that would be more accurate than Google/AWS for my specific domain?
* Can I take a model already trained and ""tune"" it by augmenting the data I have?

Use the best cloud speech-to-text API that I can provide in-domain data to to tune it

* AWS Transcribe and Google Speech to Text seem to be big players
* I've gone with AWS Transcribe since it can be tuned more easily with custom domain data (just upload text files) than Google's (which requires building phrase dictionaries with weights).
* Is there anything out there that's better for my use case?",MachineLearning
"Modern datasets contain hundreds of thousands to millions of labels that must be kept accurate. In practice, some errors in the dataset average out and can be ignored – systematic biases transfer to the model. After quick initial wins in areas where abundant data is readily available, deep learning needs to become more data efficient to help solve difficult business problems.

MLfix is a new open-source tool that combines novel unsupervised machine-learning pipelines with a new user interface concept that, together, help annotators and machine-learning engineers identify and filter out label errors.

[https://www.collabora.com/news-and-blog/blog/2023/01/17/labeling-tools-are-great-but-what-about-quality-checks/](https://www.collabora.com/news-and-blog/blog/2023/01/17/labeling-tools-are-great-but-what-about-quality-checks/)",MachineLearning
"With ChatGPT going mainstream and the general push to make products out of LM, a problem remain about the cost of running such models.

To me, it seems counterproductive to put both language modelling and knowledge inside the model weights. 

Is it time to shift to retrieval LM like Retro to keep the cost down while offering the same products?

It would possibly allow Google or others to offer a free assistant service, using embeddings similarity search to retrieve results from the Internet so the model itself could possibly even run on edge devices?

What are your thoughts about that subject?",MachineLearning
"Hi all!

I'd like to share an open source project that I am currently working on together with a few colleagues: **DocArray!**

If you've ever **trained models that deal with different data types** (images, text, video, audio, ...) then you know how much of a hassle it can be to keep track of all of your tensors, what shapes they have, and what data they are meant to represent.

That's what we're trying to change with DocArray, a Python library for representing, sending, and storing multi-modal data!

**The core idea of DocArray is that you define Documents that represent your data.** For example, one Document could hold the file path to an image, its image tensor, and and image embedding that your model creates. A different Document could do the same thing for some Text, and a third Document might combine the two.

    from docarray import BaseDocument
    from docarray.typing import TorchTensor, TorchEmbedding, ImageUrl
    from typing import Optional
    
    class MyImage(BaseDocument):
        tensor: TorchTensor[3, 224, 224]
        url: ImageUrl
        embedding: TorchEmbedding[768]
    
    class MyText(BaseDocument):
        ...
    
    class Pair(BaseDocument):
        image: MyImage
        text: MyText

And of course you can stack up these Documents, where image and embedding tensors each become one big stacked tensor, and use them directly in your PyTorch training loop! We call that a DocumentArray:

    from docarray import DocumentArray
    
    doc = Pair(image=..., text=...)
    da = DocumentArray[Pair]([doc for _ in range 100]).stack()
    t = da.image.tensor  # returns a big stacked image tensor (batch)
    assert t.shape == (100, 3, 224, 224)

After coming up with this we had a thought: Now that you have trained your model, and already defined a data schema for it, **why not let you use that same schema to serve your model to some users?** So we've made sure that DocArray is fully compatible with **FastAPI** (and built on top of the great Pydantic), making it a piece of cake to go from model training to model deployment:

    @app.post(""/embed_text/"", response_model=MyText, response_class=DocumentResponse)
    async def embed_text(doc: MyText) -> MyText:
        with torch.autocast(device_type=""cuda"", dtype=torch.float16):
            with torch.inference_mode():
                doc.tokens = text_preprocess(doc.text)
                da = DocumentArray[Text]([doc], tensor_type=TorchTensor).stack()
                da.to(DEVICE)
                doc.embedding = text_encoder(da)[0].to('cpu')
        return doc

It's hard to convey how all of this comes together, so we've created a **short Jupyter notebook** that shows both model definition and training, and model deployment using FastAPI:

[**https://colab.research.google.com/drive/1ZiGTA9jyLR0asHH\_PCvxTQhEr-iYjCrb**](https://colab.research.google.com/drive/1ZiGTA9jyLR0asHH_PCvxTQhEr-iYjCrb)

And last but not least: The features above are part of DocArray v2, which is currently in alpha and can be found here:[https://github.com/docarray/docarray/tree/feat-rewrite-v2](https://github.com/docarray/docarray/tree/feat-rewrite-v2)

So, if you have any ideas or suggestions (or bug reports), please reach out on our [GitHub Discussions](https://github.com/docarray/docarray/discussions/categories/docarray-v2), or join our brand new Discord Server (link can be found on the homepage of the project)! We already plan support for TensorFlow and Jax, but if that would be interesting to you, please reach out so we can prioritize accordingly.

We hope you give it a try and connect with us!",MachineLearning
" Hello, for my job, I have to extract job responsibilities from job ads. I'm thinking of approaching it as a span extraction problem. Where I'm gonna label the job responsibility span manually for around 1000 samples. And use supervised learning. Is there any better way to approach this problem? Is there any pretrained model I can use to fine tune?  
Any suggestion will be appreciated. Thanks!",MachineLearning
"Paper: [https://arxiv.org/abs/2301.07608](https://arxiv.org/abs/2301.07608) 

**Youtube:** [https://www.youtube.com/watch?v=U93bUQ1roiw](https://www.youtube.com/watch?v=U93bUQ1roiw)  **Please watch the Video** the explanations are better than me giving you 3-5 Pictures!

Abstract:

>Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, **we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans.** In a vast space of held-out environment dynamics, our **adaptive agent (AdA)** displays **on-the-fly hypothesis-driven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations.** Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a **large-scale attention-based memory architecture**, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic **scaling laws with respect to network size, memory length, and richness of the training task distribution**. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains. 

https://preview.redd.it/is3pyl1p70da1.jpg?width=1424&format=pjpg&auto=webp&v=enabled&s=b4105ec9241e047c2e79b9dbb29f7bde5cce00a7",MachineLearning
"Hi all,

I am trying to see if I can use DDPM (Denoising Diffusion Probabilistic Model) to denoise images using a supervised learning approach. However, I've learned that DDPM is only for unconditional image generation. Has anyone had experience using conditional DDPM and could help me out with some conceptual questions?

Here's what I'm trying to understand:

1. Say I have a pair of noisy and clean ground truth images.

2. Should I take my clean image and gradually corrupt it by adding gaussian noise in the forward diffusion (FD) process?

3. Could I get the network to learn the reverse diffusion process by giving it the noisy input, the FD noisy image, and positional embeddings? I was planning on concatenating the noisy input with the FD noisy image.

4. During training, the network learns to predict noise at t-1 given the image at t conditioned on the input noisy source image.

Here is an image showing you what I mean. Any thoughts or suggestions would be greatly appreciated.
[DDPM for image denoising](https://imgur.com/a/hP1E6tt)",MachineLearning
"Lets say I have a dataset of real estate listings. I have a column of text that describes the listing, and another column that shows the number of rooms for example. In most of the cases, the number of rooms is shown in both columns, in the description text and also in the dedicated column.

 But for some observations, the number of rooms is in the description text but not in the column ""number of rooms"". So I have missing data.

I could try to fill the missing data with by applying regex in the description text, but the number of possibilities seems to big. 

Is there a machine learning technique in NLP that allows me to do that, since it most of the observations the data is present in both column, so is ""naturally labelled""?

If there is, what is the name of these techniques? I would like to search about it but I don't know the proper keywords to google.",MachineLearning
"All the examples from langchain and on huggingface create memory by pasting the entire history in every prompt. This seems to violate the max input prompt length pretty quickly. And it’s expensive. Does chatgpt use something revolutionary? It forgets everything when you create a new session so it ‘feels’ it’s using the convo as memory as well.

But then the question; how do they get past prompt limits? Chunking doesn’t help as it still doesn’t get context in that case between prompts. Maybe they ask the same question with different chunks many times and then ask for a final result? 

Apologies if this was answered somewhere, I cannot find it at all and all examples use the same kind of history memory.",MachineLearning
"Hi, my indie game devs asked me if I could build a model that generates cool movements for their characters. 
1) I wanted to start by generating characters and scene. Should I go for stable diffusion or for a GAN ? I do need a prompt 

2) do you know any model that can generate short video clips (2 seconds) and that could potentially generate character movement ? 

Thank you so much !",MachineLearning
"I have a background in NLP but work with both text and image data/models at my company. I was doing some zero-shot experiments on different datasets for both models and noticed that the text model's performance degrades even more than the image-based one consistently. The task is image/text classification and both models are trained accordingly.

My intuition as to why this happens is because there may be more variability with text than with images. For example, a picture of a backpack may have less variations than a product name/description of a backpack. To provide some context, I'm currently working with product data in an e-commerce setting, and many vendors write their product names differently.

Anyway, I'm curious if there's actually any proof or research that's been/being doing in this direction? That is, image-based vs. text-based models and there differences when it comes to zero-shot transfer.",MachineLearning
"In my experience at big tech, I've never seen any company use open-source ML models in production. Why is this the case? Curious, because there seems to be some insanely cool research going on these days.

On the other hand, if you have seen this used, what kind of repos have you guys seen?",MachineLearning
"I am currently using the Google Cloud Model Registry and I want to learn what you use for archiving your machine learning models. What are the other options for developers who have to store hundreds of models?

[https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-model-registry](https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-model-registry)",MachineLearning
"Google Research has a blog post of advances in ML over the last year.   It obviously focusses on stuff Google Research has been involved in, but from such a big research group, thats pretty much everything.

[Here it is](https://ai.googleblog.com/2023/01/google-research-2022-beyond-language.html)

It's a good way current if you don't have time to read every paper!

(Note that some sections aren't yet published)",MachineLearning
"We combined image captioning using CLIP and image generation using the Hugging Face Stable Diffusion model to create an image de-identifier modeled after the game of telephone, [Imafake](https://hubs.la/Q01wrX7M0)! All you have to do is upload an image, convert it to a caption, then convert that caption to an image with a few clicks! You can also play with the parameters of the diffusion model depending on how gnarly you want your resulting image to be. And caution, they can get rather gnarly, but that’s what makes it fun :) Thoughts and your own generated images welcome!!

https://preview.redd.it/z818pd8fivca1.jpg?width=1276&format=pjpg&auto=webp&v=enabled&s=3416e7296a43602bd2c9f347a419eecbb8e2cf0b",MachineLearning
"&#x200B;

[MNIST CLOCK](https://i.redd.it/0uab1c7ecvca1.gif)

# Project

[https://github.com/tecbar/mnist-clock](https://github.com/tecbar/mnist-clock)

# Live demo

[https://tecbar.github.io/mnist-clock/](https://tecbar.github.io/mnist-clock/)

(it can load for a while, because it needs to download ONNX runtime)

# Description

Hey, this is my pet project. I trained a very simple model on MNIST dataset. The task is you input a digit and it can generate output image representation of that digit.  Each time it generates a little bit different digit, because of the applied noise - actually the digit vector is applied on gaussian noise.

I didn't know what to do with it so I exported the model to ONNX and used ONNX web runtime to arrange the digits in a clock - so basically everything on the live demo site is running in your browser and the clock is refreshed about 20 times per second (which was arbitrary choice).

The training procedure was really simple - instead of predicting a label based on an image it tries to predict an image based on a label. [Here is PyTorch implemetation.](https://github.com/tecbar/mnist-clock/blob/main/pytorch-training/mnist_clock.py) It works just fine with only two linear layers.

# Problems

The generated digits are blurry, I guess this is because I didn't use any GAN or VAE based architecture, so the model has no idea about anything basically.

&#x200B;

# Model

&#x200B;

https://preview.redd.it/n2lsobiigvca1.png?width=438&format=png&auto=webp&v=enabled&s=d3601b329a841f42f89025f9bc6d6033198bfcb5",MachineLearning
"In my opinion a must read because Multimodal Deep Learning is the future! Also because **papers** like this: [https://arxiv.org/abs/2301.03728](https://arxiv.org/abs/2301.03728)  **show that Multimodular models significantly outperform unimodular models**!

Book: [https://arxiv.org/pdf/2301.04856.pdf](https://arxiv.org/pdf/2301.04856.pdf)

Github: [https://github.com/slds-lmu/seminar\_multimodal\_dl](https://github.com/slds-lmu/seminar_multimodal_dl)

Abstract:

>This book is the **result of a seminar in which we reviewed multimodal approaches and attempted to create a solid overview of the field**, starting with the current state-of-the-art approaches in the two subfields of Deep Learning individually. Further, modeling frameworks are discussed where one modality is transformed into the other, as well as models in which one modality is utilized to enhance representation learning for the other. To conclude the second part, architectures with a focus on handling both modalities simultaneously are introduced. Finally, we also cover other modalities as well as **general-purpose multi-modal models, which are able to handle different tasks on different modalities within one unified architecture.** One interesting application (Generative Art) eventually caps off this booklet.

https://preview.redd.it/vb9lycxmfvca1.jpg?width=641&format=pjpg&auto=webp&v=enabled&s=802d362b6d277bfc796fb49d0998cae9d6a99098",MachineLearning
"I need to figure out a way to automatically create a 2D one-line drawing given a point cloud of a building.

I figure this is the rough workflow of that operation, but I need to define this workflow with a much higher resolution to acquire the right tools and talent for the project. Is this a suitable application for Machine Learning?

If you have any insight or ideas to share that would be very much appreciated, thanks!

https://preview.redd.it/4hv1ba5h0vca1.png?width=2160&format=png&auto=webp&v=enabled&s=623d97c0ab41f43cf76603a4a63305bf81939932",MachineLearning
"&#x200B;

[Overview of RLHF training](https://preview.redd.it/fp5mh1sdayca1.png?width=2324&format=png&auto=webp&v=enabled&s=30fce8e48088730461253f0b94ac1f01673475b0)

You must have heard about ChatGPT. Maybe you heard that it was trained with RLHF and PPO. Perhaps you do not really understand how that process works. Then check my Gist on Reinforcement Learning from Human Feedback (RLHF): [https://gist.github.com/JoaoLages/c6f2dfd13d2484aa8bb0b2d567fbf093](https://gist.github.com/JoaoLages/c6f2dfd13d2484aa8bb0b2d567fbf093)

No hard maths, straight to the point and simplified. Hope that it helps!",MachineLearning
"&#x200B;

[CfP ISTM 2023](https://preview.redd.it/dbb938yq7uca1.png?width=2000&format=png&auto=webp&v=enabled&s=ce965a7b4acce899cef734a5b12f7b5647c7e2b8)

Calling all machine learning researchers to contribute to or participate in the 2nd International Symposium on the Tsetlin Machine @ Newcastle upon Tyne. Please consider submitting your original, high-quality research works on any emerging ML hardware, software, application, or algorithmic topics.

The emerging paradigm of Tsetlin machines provides a fundamental shift from arithmetic-based to logic-based machine learning. At the core, finite-state machines, based on learning automata, learn patterns using logical clauses, and these constitute a global description of the task learnt. In this way, the Tsetlin machine introduces the concept of *logical interpretable learning*, where both the learned model and the process of learning are easy to follow and explain. As a result, it reduces the expertise needed to apply ML techniques efficiently in various domains. The paradigm has enabled competitive accuracy, scalability, memory footprint, inference speed, and energy consumption across diverse tasks, including classification, convolution, regression, natural language processing (NLP), and speech understanding. [https://istm.no](https://istm.no)",MachineLearning
"Hi everybody, I've been skimming [this paper](https://arxiv.org/abs/2207.08815) since yesterday and was once again impressed by the expressiveness and practicality of tree-based models.

I wondered what current research directions are in the field and what novel ideas have been presented in the last years - beyond improving performances. Examples may include better explainability, online learning, splitting criteria, enhanced or customizable loss functions, adding structure or constraints, shortcomings ....",MachineLearning
"Hey everyone, working on getting started an open and collaborative community/lab at intersection of ML/AI and materials science. One big reason is because it’s a neglected area with lots of potential with generative modeling for new discoveries. 

A small roadmap is we want to have intro talks on the topic to ramp members up, talks from leading researchers, of course we will be training models, trying to create larger datasets, and hopefully getting access to synthesis our findings. 

If this sounds interesting to you checkout the website at https://ai4mlab[dot]com and consider joining!

Thanks!",MachineLearning
"Hi all!

I'd like to share an open source project that I am currently working on together with a few colleagues: **DocArray!**

If you've ever **trained models that deal with different data types** (images, text, video, audio, ...) then you know how much of a hassle it can be to keep track of all of your tensors, what shapes they have, and what data they are meant to represent.

That's what we're trying to change with DocArray, a Python library for representing, sending, and storing multi-modal data!

**The core idea of DocArray is that you define Documents that represent your data.** For example, one Document could hold the file path to an image, its image tensor, and and image embedding that your model creates. A different Document could do the same thing for some Text, and a third Document might combine the two.

    from docarray import BaseDocument
    from docarray.typing import TorchTensor, TorchEmbedding, ImageUrl
    from typing import Optional
    
    class MyImage(BaseDocument):
        tensor: TorchTensor[3, 224, 224]
        url: ImageUrl
        embedding: TorchEmbedding[768]
    
    class MyText(BaseDocument):
        ...
    
    class Pair(BaseDocument):
        image: MyImage
        text: MyText

And of course you can stack up these Documents, where image and embedding tensors each become one big stacked tensor, and use them directly in your PyTorch training loop! We call that a DocumentArray:

    from docarray import DocumentArray
    
    doc = Pair(image=..., text=...)
    da = DocumentArray[Pair]([doc for _ in range 100]).stack()
    t = da.image.tensor  # returns a big stacked image tensor (batch)
    assert t.shape == (100, 3, 224, 224)

After coming up with this we had a thought: Now that you have trained your model, and already defined a data schema for it, **why not let you use that same schema to serve your model to some users?** So we've made sure that DocArray is fully compatible with **FastAPI** (and built on top of the great Pydantic), making it a piece of cake to go from model training to model deployment:

    @app.post(""/embed_text/"", response_model=MyText, response_class=DocumentResponse)
    async def embed_text(doc: MyText) -> MyText:
        with torch.autocast(device_type=""cuda"", dtype=torch.float16):
            with torch.inference_mode():
                doc.tokens = text_preprocess(doc.text)
                da = DocumentArray[Text]([doc], tensor_type=TorchTensor).stack()
                da.to(DEVICE)
                doc.embedding = text_encoder(da)[0].to('cpu')
        return doc

It's hard to convey how all of this comes together, so we've created a **short Jupyter notebook** that shows both model definition and training, and model deployment using FastAPI:

[**https://colab.research.google.com/drive/1ZiGTA9jyLR0asHH\_PCvxTQhEr-iYjCrb**](https://colab.research.google.com/drive/1ZiGTA9jyLR0asHH_PCvxTQhEr-iYjCrb)

And last but not least: The features above are part of DocArray v2, which is currently in alpha and can be found here:[https://github.com/docarray/docarray/tree/feat-rewrite-v2](https://github.com/docarray/docarray/tree/feat-rewrite-v2)

So, if you have any ideas or suggestions (or bug reports), please reach out on our [GitHub Discussions](https://github.com/docarray/docarray/discussions/categories/docarray-v2), or join our brand new Discord Server (link can be found on the homepage of the project)! We already plan support for TensorFlow and Jax, but if that would be interesting to you, please reach out so we can prioritize accordingly.

We hope you give it a try and connect with us!",MachineLearning
"Fingers and toes crossed I might have a paper accepted at ICLR, and I'm wondering how much I can add/change in the camera ready version. Typos and exposition for clarity, I assume are fine to add/change. And in some cases I have seen meta-reviews say ""please address comments XYZ in the camera ready version"", so I assume there is a lot of leeway. But in the absence of such a comment or comments in particular about something you should change, is it okay to change/add stuff? And if so to what degree (while keeping to the page limit).",MachineLearning
"There's a bunch of them - Linformer, Longformer, Performer, Nystromformer, Big Bird, etc etc. Plus a bunch more that have similar goals but don't necessarily aim for linear complexity, like memory-augmented transformers.

As far as I know, none of them have really seen much use. Even for image problems, which have very long input sizes, people are using regular transformers with tokenization schemes.

* Am I wrong? Are they actually good, or are at least some of them better than regular transformers?

* If not, what's wrong with them? Do they have lower accuracy? Are they slower to train?",MachineLearning
"Hey everyone, just saw the great presentation of Nathan Lambert on [Reinforcement Learning from Human Feedback](https://www.youtube.com/watch?v=2MBJOuVq380) and wanted to try to do some RLHF on my language model.To do this, first I need to create an experience where I collect reward scores to train the reward model.

**My question is: what rewards work best?** Simply 👍/👎? A scale of 1-5?  Ranking 4 different model outputs? There are a lot of options and I don't know which one to choose.",MachineLearning
"Hi everyone. I am training my RWKV 14B ( [https://github.com/BlinkDL/RWKV-LM](https://github.com/BlinkDL/RWKV-LM) ) on the Pile (332B tokens) and it is getting closer to GPT-NeoX 20B level. You can already try the latest checkpoint.

https://preview.redd.it/7ycdftmjvmca1.png?width=1174&format=png&auto=webp&v=enabled&s=1622fb8cd7deb5ccd1934c4cc1d66ce696e81f20

RWKV is a RNN that also works as a linear transformer (or we may say it's a linear transformer that also works as a RNN). So it has both parallel & serial mode, and you get the best of both worlds (fast and saves VRAM).

At this moment, RWKV might be the only pure RNN that scales like usual transformers for language modeling, without using any QKV attention. It's great at preserving long context (unlike LSTM).

Moreover, you get smooth spike-free carefree training experience (bf16 & Adam):

https://preview.redd.it/0g3lrg6mvmca1.png?width=871&format=png&auto=webp&v=enabled&s=76a4b7a4859ec589f19552f8248ccc44f87a8a1d

As a proof of concept, I present ChatRWKV ( [https://github.com/BlinkDL/ChatRWKV](https://github.com/BlinkDL/ChatRWKV) ). It's not instruct-tuned yet, and there are few conversations in the Pile, so don't expect great quality. But it's already fun. Chat examples (using slightly earlier checkpoints): 

https://preview.redd.it/zyqni6bpvmca1.png?width=1084&format=png&auto=webp&v=enabled&s=dd34763778a68d70f4079fe391197b07a885f2e5

https://preview.redd.it/xhje4j7qvmca1.png?width=1200&format=png&auto=webp&v=enabled&s=4622ff3c5538cb16b0801d3215f747b64f083623

And you can chat with the bot (or try free generation) in RWKV Discord (link in Github readme: [https://github.com/BlinkDL/RWKV-LM](https://github.com/BlinkDL/RWKV-LM) ). This is an open source project and let's build together.",MachineLearning
"Given a function in some space, I have literature results that say, the function can theoretically be approximated by a Neural Network of such complexity with so many layers, of such width, with this specific given activation function. 

OK, so theoretically, there is a set of weights and biases that will result in a pretty good approximation of my function.

*Now the question is, how do I know that given an optimization method, for example stochastic gradient descent, I will actually reach this minimum or near enough to it, in so many training steps, or even at all?*

I attended a talk last year in which one speaker claimed that due to the way stochastic gradient descent works, it could be that some minimums are never reachable from some initialization states no matter how long one trains. Unfortunately I cannot find what paper/theorem he was referring to.

I am interested in results related to this question.",MachineLearning
"From [the article](https://www.theverge.com/2023/1/17/23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit):

>Getty Images is suing Stability AI, creators of popular AI art tool Stable Diffusion, over alleged copyright violation.  
>  
>In a press statement shared with *The Verge*, the stock photo company said it believes that Stability AI “unlawfully copied and processed millions of images protected by copyright” to train its software and that Getty Images has “commenced legal proceedings in the High Court of Justice in London” against the firm.",MachineLearning
"I'm building some random forest models in sklearn using a dataset that updates daily. I want to take advantange of the new stream of data which could indicate changes in the X-y relationship, however I've also found that my model performs better with more data. The problem is that it takes a seriously long time to run (dataset is around 250000 rows and 50 features). Is there an approach where one builds the model at the beginning of the data stream, and then updates the parameters with new data as it arrives, instead of continuously retraining the model on the entire dataset for every day? Many thanks!",MachineLearning
I made this little python package a while ago but realized I never shared it here. Maybe it's useful to you: [https://github.com/bloomen/featureimpact](https://github.com/bloomen/featureimpact),MachineLearning
"Hello everyone,

&#x200B;

I have a question about GCNs and would appreciate any thoughts. Do we typically use only one graph for GCN training/inference?

I'm asking this because when I saw official DGL website, there was only one example graph after loading it. Based on my experience with DNNs, I expected a batch of examples. However, it was not the case for GCNS. I could find PPI dataset with multiple graph examples (24) but for other widely used datasets (e.g., Cora, Citeseeer, and Pubmed), there was only one.

Thank you!",MachineLearning
"Is there a (easy) way to train a language model on a specific knowledge domain e.g. through some form of transfer learning?

Example of such model would be training LP model work Marcel Proust novels to have GPT-3-like chat discussing work of the novelist.

Or more practical example - train a model on a technical submarine documentation, so the model can answer all questions like a submarine engineer- in natural lanquage?",MachineLearning
"What model structure would be recommended for detecting the coordinates of all 4 corners of a rectangle (e.g. index cards)? Most object detection models like YOLO produce rectangular bounding boxes; what tweaks can be made to trace the object regardless of orientation?

For my specific problem, classical edge/corner detectors aren't a good fit - so I'm falling back on ML. Currently have a dataset of about 1500 domain-specific labeled images; hoping to train a model on TF. Thanks for the suggestions!

Edit: here are a few examples from my dataset. The green dots aren't part of the images; they just show how the corners are annotated:

https://preview.redd.it/2f8uimhn7hca1.jpg?width=1373&format=pjpg&auto=webp&v=enabled&s=5086c73084aa15014825f45e20ab1532c743078b

https://preview.redd.it/ujb8tmhn7hca1.jpg?width=3024&format=pjpg&auto=webp&v=enabled&s=9a3b5bfba3decd797546b5c77d0889568ea7b9e7

https://preview.redd.it/9lzgfmhn7hca1.jpg?width=3024&format=pjpg&auto=webp&v=enabled&s=b6186db7e679c3666056cb198269937134b86d37",MachineLearning
"Abstract:

>In this work, we propose a generalization of the forward-forward (FF) algorithm that we call the predictive forward-forward (PFF) algorithm. Specifically, we design a dynamic, recurrent neural system that learns a directed generative circuit jointly and simultaneously with a representation circuit, combining elements of predictive coding, an emerging and viable neurobiological process theory of cortical function, with the forward-forward adaptation scheme. Furthermore, PFF efficiently learns to propagate learning signals and updates synapses with forward passes only, eliminating some of the key structural and computational constraints imposed by a backpropbased scheme. Besides computational advantages, the PFF process could be further useful for understanding the learning mechanisms behind biological neurons that make use of local (and global) signals despite missing feedback connections \[11\]. We run several experiments on image data and demonstrate that the PFF procedure works as well as backprop, offering a promising brain-inspired algorithm for classifying, reconstructing, and synthesizing data patterns. As a result, our approach presents further evidence of the promise afforded by backprop-alternative credit assignment algorithms within the context of brain-inspired computing.

Paper: [https://arxiv.org/pdf/2301.01452.pdf](https://arxiv.org/pdf/2301.01452.pdf)",MachineLearning
"Over the past three years, the field of ML has advanced considerably in the field of audio, visual, and natural language generation. For users like me, GPT-3 was a first look into the types of content that can now be generated with minor effort. While impressive at first, the outputs from the original GPT-3 can quickly be seen to be less than ideal and often times can be easily distinguished from original content written by users. Three years later, generation techniques have improved to the point where the task of detecting generated content is far more difficult as the quality of the generated content has risen considerably. Access to such technologies has also spread to the point where states such as New York sees it as enough of a threat to ban it from schools. 

While I think we are still at the calm before the storm in regards to the potential for chaos such models have, I'd like to open the floor up for a discussion on the implications of generative models and ways we can address it. 

Will it even be possible to moderate content in the future when models improve to the point where artifacts from the generation process are no longer present? Sure we can have models that detect NSFW content, but what about content that contains information that is false and harmful? Perhaps a resurgence in symbolic AI and rule based reasoning is needed? Or perhaps a renewed interest in the field of argument mining?",MachineLearning
"Hey /r/machinelearning,

Long time reader, first time posting non-anonymously. I've been training models using various cloud services, but as an individual user it's stressful for me to worry about shutting down the instances if training fails or stops. Crashes, bad code, etc can cause GPU utilization to drop without the program successfully ""finishing"", and this idle time can cost a lot of money if you don't catch it quickly.

Thus, I built this tiny lil tool to help. It watches the GPU utilization of your instance, and performs an action if it drops too low for too long. For example, shutdown the instance if GPU usage drops under 30% for 5 minutes.

It's easy to use and install, just `pip install gpu_sentinel`

If this is useful please leave comments here or on the Github page: https://github.com/moonshinelabs-ai/gpu_sentinel

I'm hoping it helps save some other folks money!",MachineLearning
"I have around 6 YoE doing MLE full time work for various companies. Starting to get tired of working for these big companies and would prefer trying some freelance work.

Where are some websites or places I can get started? I’ve seen UpWork, but this seemed more suited for quick one off, software work and less for complex ML tasks last time I was on there (tried that several years ago in 2019).",MachineLearning
"As the title says, I'm curious about using open source models like GPT-J, GPT-NeoX, Bloom, or OPT to compete with ChatGPT for \*specific use-cases\* such as explaining what a bit of code does. ChatGPT does this task quite well, but it's closed-source nature prevents it from being useful in documenting or commenting proprietary code. There's also limitations such as the amount of text ChatGPT will read or respond with.

Getting beyond these limitations is something I'm interested in pursuing, perhaps with the help of somewhere in this subreddit. Some assumptions you can safely make:

1. We can get (lots of) funding for the training, hardware, etc...
2. The end product should be on-premises
3. The inference does not actually need to run very quickly. If it costs millions to buy enough GPUs just due to VRAM limitations, we could simply run on CPUs and utilize ram, as long as inference could be done a few times per day.

So I guess my questions are where would we start? What model is best to fine-tune? How would you specifically fine-tune to improve specific use cases?",MachineLearning
Warm starting the model training of BERT large using BERT base. One idea is to concatenate a bunch of parameters and start training. I was thinking is there a research paper that tries out the best methods?,MachineLearning
"The legendary Tim Dettmers has updated his blog on which GPU to purchase for Deep learning to include advice for the latest GPU series: 

[https://timdettmers.com/2023/01/16/which-gpu-for-deep-learning/](https://timdettmers.com/2023/01/16/which-gpu-for-deep-learning/#RTX_4090s_and_Melting_Power_Connectors_How_to_Prevent_Problems)",MachineLearning
" 

I'll be honest with you, it took me months to wrap my head around diffusion models. A couple of iterations of a [blog post](https://jalammar.github.io/illustrated-stable-diffusion/) later and this is my best shot at a gentle intro to Stable Diffusion and how it works.

[https://youtu.be/MXmacOUJUaw](https://youtu.be/MXmacOUJUaw)

The part that took the most reworking is forward diffusion and how to best describe it. Thanks to the many people acknowledged in the blog post who have helped me both understand it and explain it better.

Hope you find it helpful. Let me know if you have any questions or feedback.",MachineLearning
"Hello,

I am currently working on structured data classification problem for work.

I was applying multiple different data pre processing steps including imputing null values (mean, KNN, random forrest), adding synthetic data (SMOTE, ADAYSN or None), normalization (l1, l2, max or none), multiple datasets (including different sets of features), as well as different models (XGBoost, Random Forrest, Logistic Regression, KNN, MLP).

What I built was a tool that trains all the different permutations of data processing, datasets and models to find the best one, and applied K-Fold cross validation. The tool stores all the data and metrics using MLFlow. This is similar to a grid search across hyperparemeters, but instead of tuning the hyper parameters, I am tuning the data processing steps.

I like this method because I gain a level of confidence knowing that I have exhausted all the possible models, data, and pre processing permutations when selecting the best performing model.

I was wondering if other people apply a similar technique for structured data problems? Besides the compute is there anything to be cautious of when applying this method?",MachineLearning
"Hi, folks!   


I invite you to read a [post / tutorial](https://medium.com/@AlekseiShabanov/practical-metric-learning-b0410cda2201) about metric learning with the usage of [OpenMetricLearning](https://github.com/OML-Team/open-metric-learning).   


Inside you can find the following:

* theory overview
* practical examples + illustrations
* snippets of code 

&#x200B;

As a bonus, you will learn how to train a model which performs on a SotA level using a few simple heuristics. Welcome to [read](https://medium.com/@AlekseiShabanov/practical-metric-learning-b0410cda2201)!",MachineLearning
"Wondering what the state of the art is for multiple image generation for an input text, or a series of input texts.


To clarify, are there any models or architectures that explore consistency between image generation. (E.g stylistically, same people in the images, same settings, etc) 


I imagine there would be some pre-existing architectures that could take an image embedding along with a text embedding",MachineLearning
" 

I'm aware transformers are pretty vram hungry and a 4080 only has 16 GB. So I am guessing a lot of transformer based models will be out of the question. At least anything that is interesting.

Not sure about other models though. Is there anything I can do with a 4080 that's beyond just some toy experiment?",MachineLearning
"Hey everyone,

This may be a silly question but I'm confused as to what standard practice is when reporting average returns across multiple seeds in a table or a plot. It's usually not even mentioned but I sometimes see authors mention they are using:

1. Average ± Standard Deviation
2. Average ± Standard Error
3. Average ± 1.96 \* Standard Error
4. Bootstrapped CIs

For example, this paper ([https://www.jmlr.org/papers/volume23/21-1342/21-1342.pdf](https://www.jmlr.org/papers/volume23/21-1342/21-1342.pdf)) by the authors of clearnrl doesn't specify anything other than that the ""reported numbers are the final average episodic returns of at least 3 random seeds"".

What would you consider best practice in RL?",MachineLearning
"So I'm working on project which goal is to digitally emulate guitar tube amplifiers using a Wiener-Hammerstein model.

For those of you unfamiliar with this type of model, its key block is a nonlinear block that is characterized by a set of 8 parameters. Basically there's a raw input guitar signal and there's an output signal that should be as close as possible to the actual output of the modeled amplifier.

I have a database with a series of magnitude-variable chirp signals serving as inputs and the respective output measurements.

So my question is what is the best way for me to automate the process of optimization of this set parameters. I thought of using a genetic algorithm but I wondered if that's the most accurate and efficient way of doing it.

Also this is to be implement on a microcontroller so I have more limited resources than a computer. However, it would be really cool to be able to customize these parameters in real time on my Teensy 4.0 so it would be ideal that the algorithm could meet this condition, although it's not completely necessary",MachineLearning
"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",MachineLearning
"I was looking at how time is embedded in diffusion models, and I found these two implementations \[1\] and \[2\]:  
The first one is a simplified version of the second one, but the idea behind the time embedding is similar. What I've understood is that `t` is a number, it goes in a SinusoidalPositionEmbeddings with a given time\_dim, then Linear + ReLU where the same dimensions are kept.  
Then for each down-step of the UNet, an additional Linear + ReLU is performed to match the channels of the image embedding, and this latter embedding is added to the output of the CNN. Here when have the time embedding with a shape of `(b, c, 1, 1)` and the image embedding with a shape of `(b, c, h, w)`. When we perform the addition, the time embedding is broadcasted to match the image embedding. As far as I understand, here the latent space of the image gets reweighted channel-wise, but the same weights are added for each different position. 

Why they did follow this choice? This is quite different from the standard positional encoding used e.g. in self-attention, where the positional embedding gives a different weight to each spatial dimension. I never found this detail explained in any Diffusion Model paper/tutorial, also if we look at \[2\], the same idea is made more complex, with more Linear projections and different activation functions (GeLU and SiLU).
Moreover, I'm not sure about the difference between applying a time embedding and then directly a conv2d layer against the time embedding + attention + conv2d. Aren't these types of embedding suited up for attention layers? How does a Conv2D layer, which is positional invariant by construction, benefit from this type of operation?
  
\[1\] [https://colab.research.google.com/drive/1sjy9odlSSy0RBVgMTgP7s99NXsqglsUL?usp=sharing#scrollTo=KOYPSxPf\_LL7](https://colab.research.google.com/drive/1sjy9odlSSy0RBVgMTgP7s99NXsqglsUL?usp=sharing#scrollTo=KOYPSxPf_LL7)  
\[2\] [https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising\_diffusion\_pytorch/denoising\_diffusion\_pytorch.py](https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py)",MachineLearning
"Hi there,

I wanted to share my new project with you, it is called [**arxiv-summary.com**](https://www.arxiv-summary.com/). Right now, I find it really difficult to keep up with all the important new publications in our field. Especially, it is sometimes difficult to get an overview of a paper to decide if it's worth reading. I really like arxiv-sanity by Andrej Karpathy, but even with that, it can still take some time to understand the main ideas and contributions from the abstract. With arxiv-summary, my goal is to make ML research papers more ""human-parsable"".

The website works by fetching new papers daily from arxiv.org, using PapersWithCode to filter out the most relevant ones. Then, I parse the papers' pdf and LaTeX source code to extract relevant sections and subsections. GPT-3 then summarizes each section and subsection as bullet points, which are finally compiled into a blog post and uploaded to the site.

You can check out the site at arxiv-summary.com and see for yourself. There's also a search page and an archive page where you can get a chronological overview. If you have any feedback or questions, I'd be happy to hear them. Also, if you work at OpenAI and could gift me some more tokens, that would be much appreciated :D

Thanks and happy reading!",MachineLearning
I needed a C++ wrapper library around libsvm and liblinear using Eigen so I made one. Maybe it's useful for you as well: [https://github.com/bloomen/svmegn](https://github.com/bloomen/svmegn),MachineLearning
"I've come across several papers where authors mention they, for reasons of computational efficiency, will pack multiple shorter training examples together in a single sequence. 

Example from ""Scaling Instruction-Finetuned Language Models"" [(Chung et al, 2022)](https://arxiv.org/abs/2210.11416):

> We use packing (Raffel et al., 2020) to combine multiple training
examples into a single sequence, separating inputs from targets using an end-of-sequence token. Masking is applied to prevent the tokens from attending to others across the packed example boundary. 

I'm curious to understand how this is actually done in practice. Seeing as multiple separate masks are involved, I'd think one would need to loop over them all and repeat (?) the matrix multiplication several times? Is there some built in functionality in Pytorch and other frameworks to deal with a situation like this with multiple masks? 

Thankful if someone could share and explain, or link to an implementation of input packing.",MachineLearning
"Hey guys. I'm running some experiments as part of a research project; it was initially implemented for GPT-Neo 1.3B, but there is one baseline we want to use that only supports GPT2-XL, so I implemented that into our code (i.e., just included a clause that was like ""if model\_name='gpt2', model=GPT2LMHeadModel.frompretrained('gpt2-xl')""). 

The issue is, GPT2 is giving absolutely absurd results that are clearly very incorrect. It is difficult to explain this without an in-depth explanation of my code, but basically I have a bunch of functions that do things like, for example, computing the probability of certain labels in a multiple-choice test. 

So my question is, is there any fundamental difference in how these two models are setup in hugging face, that would result in such errors? I myself am not too familiar with hugging face models, so I'm not entirely sure. But the fact that the code runs but produces bad errors is weird; I would think that if something was wrong, there would be some sort of tensor-size doesn't match error somewhere...",MachineLearning
"Many models these days focus on code generation. But I was wondering if there's anything for understanding existing codebase?

I know that Codex or ChatGPT can understand what a function does, but what about a complex codebase with imports and nested calls? Are these models capable of understanding the relationship between functions?

I'm trying to build a side project where you give it a production level codebase, it does some magic, then I can ask AI anything about things in this codebase with high accuracy.",MachineLearning
"Is there any work that does this?

Let's say you have 20 good, related photos, and one bad one you want to restore / upscale / denoise / sharpen / inpaint etc. Those 20 good images of the same person / object / building should give the model a good sense of how to fill in missing details in the bad photo.

I imagine there's work somewhere in this direction but can't find anything.",MachineLearning
"I created an ID3 algorithm using scilab for a project at university. The project is more a proof of concept then an having an actual usecase. Its written in scilab without using any toolboxes and classifies if you have won in tic tac toe.

My code basically uses a dataset that has every possible endgame board configuration of tic tac toe and builds a descision tree. I can then input a specific endgame board configuration and it tells me if i won or not. It works fine so far and since my dataset has all possible configurations, predicts the correct label 100% of times.

I now added a ten times k-fold cross validation algorithm. However, the validation only gives me an accuracy of about 80%. Am I missing something here? Does a cross validation even make sense if my training set contains all possible data points? Hope someone can give some answers.",MachineLearning
"I  am trying to create speaker-aware transcripts from (multiple) audio files of a podcast. Right now I'm using OpenAI Whisper for the transcripts and pyannote.audio for speaker diarization (speaker segmentation + centroid clustering)

In  order to speed up the process (diarization time doesn't seem to scale  linearly), I'd like to fit the centroids with the first audio file, and use those to predict the speakers (clusters of the speaker embeddings) of the other audio files, as the speakers don't change across episodes.

However, the default pyannote.audio diarization pipeline refits the clusters for each audio file. Do you know of any other Python framework that allows reusing the fitted clusters, or any way pyannote.audio allows this? Is this even possible? Any other way to achieve the desired results?",MachineLearning
"Paper: [https://arxiv.org/abs/2204.13074](https://arxiv.org/abs/2204.13074) 

Blog: [https://blog.allenai.org/towards-teachable-reasoning-systems-dd16659fd9f8](https://blog.allenai.org/towards-teachable-reasoning-systems-dd16659fd9f8) 

Youtube: [https://www.youtube.com/watch?v=c5j\_tWsENFg](https://www.youtube.com/watch?v=c5j_tWsENFg) 

Abstract: 

>Our goal is a teachable reasoning system for question-answering (QA), where a user can interact with faithful answer explanations, and correct its errors so that the system improves over time. Our approach is to augment a QA model with a dynamic memory of user feedback, containing user-supplied corrections to erroneous model beliefs that users identify during interaction. Retrievals from memory are used as additional context for QA, to help avoid previous mistakes in similar new situations - a novel application of memory-based continuous learning. With simulated feedback, we find that **our system (called TeachMe) continually improves with time, and without model retraining**, requiring feedback on only 25% of training examples to reach within 1% of the upper-bound (feedback on all examples). Similarly, in experiments with real users, we observe a similar trend, with performance improving by over 15% on a hidden test set after teaching. **This suggests new opportunities for using frozen language models in an interactive setting where users can inspect, debug, and correct the model's beliefs, leading to improved system's performance over time**. 

https://preview.redd.it/umosmtgzj0ca1.jpg?width=507&format=pjpg&auto=webp&v=enabled&s=17dbd56480d5c766592e3d64b9a0392ff2dfadd9

https://preview.redd.it/5lbhvwgzj0ca1.jpg?width=680&format=pjpg&auto=webp&v=enabled&s=0f4c97ca806741c0e180b3fd4ff01a0e49766f39

https://preview.redd.it/jd7oaygzj0ca1.jpg?width=1308&format=pjpg&auto=webp&v=enabled&s=16180c8fa715d10db5058b74422630caf22023ff

https://preview.redd.it/q137kkhzj0ca1.jpg?width=839&format=pjpg&auto=webp&v=enabled&s=2a383e723d3924edcf87d370cc2eda6ba1a43893",MachineLearning
"Which of the sub-fields/approaches, application areas are expected to gain much attention (pun unintended) this year in the academia?

PS - Inspired from a similar question last year (https://www.reddit.com/r/MachineLearning/comments/t04ekm/d_whats_hot_for_machine_learning_research_in_2022/)",MachineLearning
"I read the \[MADE: Masked Autoencoder for Density Estimation\]([https://arxiv.org/abs/1502.03509](https://arxiv.org/abs/1502.03509)) paper and had a look at this \[Blog\]([https://www.ritchievink.com/blog/2019/10/25/distribution-estimation-with-masked-autoencoders/](https://www.ritchievink.com/blog/2019/10/25/distribution-estimation-with-masked-autoencoders/)), but it I don't understand the followidng thing in the examples used in both of them: One result of the masking is that one input is simply not used(?). Another one is that one output node has no conditions, i.e. it does not depend on any of the inputs. But what is its actual output value? Is it random? Constant? If yes, how is it chosen?",MachineLearning
"As in, ""Pink Floyd, Another Brick in the Wall, ska, heavy trumpet, female vocalist""

It seems that if copyright issues are a controversial element of AI art, then copyrighted music will run into the same issue. Or is this not true?",MachineLearning
"Hey guys,

I  am working the sector of computer science for agriculture research. I deal here  with algorithm to monitor crop conditions and try to simulate what yield  will be the outcome.

I am  focussing on ML based methods, but data in agriculture can be a quite  limiting factor. If you have 100k samples from real crop fields, thats a  lot! So we are not like ChatGPT, who just used 500bn word samples to  train their model.

To overcome the  issues of small data + ML, I want to set up an approach that combines  ML methods (learning from data) with expert knowledge.

What  do I mean by this: E.g. Everybody knows, if you do not water your  plant, it will die. Or if there are 90° Celsius, the plant will just  burn. This knowledge is partially stored in so called ""crop simulation  models"" designed by agronomy experts and my idea was to use these expert  models to generate synthetic yield data and feed this data into the  training dataset for the ML models.

For  me that will somehow result in an approach of ""constrained machine  learning"" where I want to combine both. However, does some of you have  any other idea how ML and expert models could be combined or the  knowledge could be injected to ML methods, except via the training  dataset?

I am happy to hear your suggestions!",MachineLearning
"I  need some validation samples for an information extraction task,  basically extracting a list of objects with 4 fields from a text (+ a  binary flag).

I intended to use  mturk for this, but they seem to have some billing issues and I haven't  managed to have them allow us to actually spend any money in a week.

I've  looked at a few alternatives but most seem very small and focused on  simple tasks and surveys. Have any of you successfully used something  other than mturk for this kind of task?",MachineLearning
"Basically what the titel says. For me it seems that neither in business nor in literature Super Learners / Stacking is used frequently. Therefore I was wondering why this is the case? Especially since Stacking should guarantee at least equal performance as the base learners used for it. One reason that comes up my mind is the curse of data. As more levels in the architecture we have the more data splits are needed, reducing the available training data for each individual model, thus reducing the model performance. Another thing might be the complexity when building a Stacked Learner.
Still that doesn’t see to be that bad of a trade-off. Anything I‘m totally missing here?",MachineLearning
"Greetings!

We are pleased to announce the fourth annual [VizWiz Grand Challenge workshop](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fvizwiz.org%2Fworkshops%2F2023-workshop%2F&data=05%7C01%7Cdmassiceti%40microsoft.com%7C38c28ca6aa0b4f12322808daf4f06d3e%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C638091610736122620%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=cCNS49OZmWKzvc1sXlNizVS0%2FfPql8K3CZv71%2FQDY%2Fg%3D&reserved=0), which will be held in conjunction with CVPR 2023. The workshop is running 4 AI Challenges to drive the development of assistive technologies for people who are blind or low-vision. Please share this post with those who might be interested in participating.

This workshop is motivated in part by our observation that people who are blind have relied on (human-based) visual assistance services to learn about images and videos they capture for over a decade.  We introduce visual question answering, few shot recognition, and object localization dataset challenges for the AI community to represent authentic use cases. A few more details:
 
· Friday, May 5: submissions of algorithm results due to the evaluation server

· Monday, June 19: results will be announced at the VizWiz Grand Challenge workshop at CVPR 2023

· Visual Question Answering (VQA) Challenge [here](https://vizwiz.org/tasks-and-datasets/vqa/)

· VQA Answer Grounding Challenge  [here](https://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/)

· Few-Shot Object Recognition Challenge  [here](https://eval.ai/web/challenges/challenge-page/1896/overview)

· Salient Object Detection Challenge [here](https://vizwiz.org/tasks-and-datasets/salient-object-detection/)

We are looking forward to your participation in the Challenges this year!",MachineLearning
"This twitter thread from Karol Hausman talks about the original bitter lesson and suggests a bitter lesson 2.0. [https://twitter.com/hausman\_k/status/1612509549889744899](https://twitter.com/hausman_k/status/1612509549889744899)

""The biggest lesson that \[will\] be read from \[the next\] 70 years of AI research is that general methods that leverage foundation models are ultimately the most effective""

Seems to be derived by observing that the most promising work in robotics today (where generating data is challenging) is coming from piggy-backing on the success of large language models (think SayCan etc).

Any hot takes?",MachineLearning
"Paper: https://arxiv.org/abs/2205.01128

TL;DR It's a paper that tries to design systems that generalize. They argue there are two forms of computing: Compositional and Continuous. 

Continuous computation is what neural networks are traditionally good at - creating a function that approximates a solution to a problem. Compositional computation is directly manipulating symbols, logic, ideas, etc - and unlike continuous computation, it's capable of generalizing from small datasets. But so far it's only useful inside carefully-constructed formal systems.

The authors believe research should be focused on combining the two, and implementing Compositionality fully with neural networks. They suggest some ways to do this. They also believe that the success of architectures like CNNs and Transformers comes from implementing a limited form of Compositionality.

This is a very interesting idea, but I have a little bit of skeptism:

* This paper is heavy on theory and less so on practice. Has any followup work in this direction produced measurable results? 

* The [lead author](https://en.wikipedia.org/wiki/Paul_Smolensky) seems to have been saying things like this for a while. Sometimes older researchers have pet theories that are not broadly accepted in the field. What do other researchers think about this?
 
Thoughts?",MachineLearning
"Hi,

""[Learning to summarize from human feedback](https://arxiv.org/pdf/2009.01325.pdf)"" is a 2020 paper by OpenAI demonstrating how to use reinforcement learning with human feedback (RLHF) to fine-tune a language model to produce higher quality summaries of news articles and Reddit posts than is possible with supervised fine-tuning.

Now, [CarperAI](http://carper.ai/) has demonstrated how to use their library trlX to implement this work, by applying RLHF to the summarization dataset released by OpenAI and fine-tuning GPT-J-6B.

Read the full report here, with a code walkthrough: https://wandb.ai/carperai/summarize_RLHF/reports/Implementing-RLHF-Learning-to-Summarize-with-trlX--VmlldzozMzAwODM2

trlX library here: https://github.com/CarperAI/trlx

Twitter thread here: https://twitter.com/carperai/status/1613645352514768897",MachineLearning
"ML is a part of AI but I don't hear about anything coming out of AI that's not done using some ML technique.

Is it fair to say that AI and ML are synonymous now in 2023? Or are there people who are still actively working on non-ML techniques for building AI?",MachineLearning
"I have tried to use a simple translate function, using the models locally with Python on the CLI: slow execution (8-10 seconds). I am on 16 GB MacBook Pro, M1. The same on REST API at HuggingFace Endpoints, with 1vCPU 2GB - Intel Ice Lake takes 800ms. 

What am I missing here?",MachineLearning
"Hi! So i am looking into literature for determining the usefulness of samples/datasets used for training ML model. Lets say DNN was trained with datasets A, B and C so after training is there way to quantify which of the partial triaining datasets contributed most to the useful learning by ML model at the end of training! Brute force strategy can be to remove samples and train and see how it performs but ofcourse it will not be viable!",MachineLearning
"One of the biggest AI discoveries over the past year has been the importance of human feedback for building next-gen LLMs — but I still see a lot of confusion around how RLHF works at a fundamental level. 

I wrote a blog to get into the details here: [https://www.surgehq.ai/blog/introduction-to-reinforcement-learning-with-human-feedback-rlhf-series-part-1](https://www.surgehq.ai/blog/introduction-to-reinforcement-learning-with-human-feedback-rlhf-series-part-1)",MachineLearning
"Are there smaller/distilled versions of CLIP? Or some other (smaller) models that connect text and images?

For my use case, the model needs to be small in size: ideally <20MB,  fine < 60MB, ok < 100MB.",MachineLearning
"I need to apply a Transformer to a task where sequences can be much longer than the time dependency between timesteps.

For example, a sequence might be 1000 tokens long, but to predict `x[i+1]` only `x[i-50]` to `x[i]` are necessary. This induces me to train the transformer by breaking each sequence of 1000 tokens into 20 sequences of 50 steps each, which would be more efficient.

How should I deal with the BOS (beginning-of-sentence) token that shifts targets right? Should I use it in each subsequence, or should I instead use the token that comes immediately before the beginning of each subsequence?

For example, given a subsequence `x[50:100]`, should the targets be `[BOS, x[50], x[51], ... x[100]]` or should they be `[x[49], x[50], x[51], ... x[100]]`?",MachineLearning
"**Paper**: [https://www.cidrdb.org/cidr2023/papers/p43-low.pdf](https://www.cidrdb.org/cidr2023/papers/p43-low.pdf)

**Abstract:**

Dataset management is one of the greatest challenges to the application of machine learning (ML) in the industry. Although scaling and performance have often been highlighted as the significant ML challenges, development teams are bogged down by the contradictory requirements of supporting fast and flexible data iteration while maintaining stability, provenance, and reproducibility. For example, blobstores are used to store datasets for maximum flexibility, but their unmanaged access patterns limit reproducibility. Many ML pipeline solutions to ensure reproducibility have been devised, but all introduce a degree of friction and reduce flexibility.

In this paper, we propose that the solution to the dataset management challenges is simple and apparent: **Git**. As a source control system, as well as an ecosystem of collaboration and developer tooling, Git has enabled the field of DevOps to provide both speed of iteration and reproducibility to source code. Git is not only already familiar to developers, but is also integrated into existing pipelines, which facilitates adoption. However, as we (and others) demonstrate, Git, as designed today, does not scale to the needs of ML dataset management. In this paper, **we propose XetHub; a system that retains the Git user experience and ecosystem, but can scale to support large datasets. In particular, we demonstrate that XetHub can support Git repositories at the TB scale and beyond.** By extending Git to support large-scale data, and building upon a DevOps ecosystem that already exists for source code, we create a new user experience that is both familiar to existing practitioners and truly addresses their needs.  


https://preview.redd.it/19x4sim19nba1.png?width=1746&format=png&auto=webp&v=enabled&s=2be4a2e8059f9ac8e00bdea15d5cd97b0574b7ce

https://preview.redd.it/xsqqjjm19nba1.png?width=2422&format=png&auto=webp&v=enabled&s=270c68d290b17bee7dbf0f38588a5d29fa040fe2",MachineLearning
"I'm a machine learning PhD student and I'm doing research on LMs and how to reduce their memory footprint.

One idea I've been toying with is Vector Quantized LMs. I'm not talking about VQ as a technique to speed up compute using int8 activations etc etc, but by using a codebook.

The idea is based on an uni-directional RNN that reconstructs the source sequence after quantization. Unlike MLM where the corruption is based on masking and replacing tokens we instead quantize the token vectors and try to the predict the original token based on the quantized version of the token *and* the unquantized short/long term memory states produced at the previous timestep.

The reason I'm interested in such a convoluted idea is to effectively create a metric to measure entropy of tokens in sequence; if the VQ-LM can reconstruct the correct token with high likelihood then that token is unimportant, but if the VQ-LM fails to predict a token it is likely that this token is of great importance because it is a rare word and this carries higher entropy in the sequence. And the motivation behind wanting to learn to measure such a phenomenon is so we can use this to guide the memory of a transformer: models like the Transformer-XL operate on longer sequences by keeping memory around for keys and values, and the Compressive Transformer takes it a step further by compressing older tokens... Well... what if we used the reconstruction loss from the VQ-LM along with an 'age' metric to guide the memory bank of such a transformer architecture, discarding easily predicted tokens early while keeping higher entropy tokens around for longer?

Has anyone considered such a system before? If done a lot of searching and I've come up blank so far.",MachineLearning
"There's a lot of hype around RLHF due to its use for ChatGPT, but has anyone else here used the same principles for improving their model outputs? For examples preference ranking their models' outputs and then using that data to retrain their model weights. Or even without the RL - simply using human feedback to stuff prompts or finetuning datasets? Interested to hear!",MachineLearning
"Theo Jansen, inventor of the strandbeest, explains in [one of his videos](https://youtu.be/FFS-2axFo1Y?t=126) that he used the principle of evolution to figure out the thirteen holy numbers using a computer program which he wrote in 1990. Would this be considered machine learning or is an evolutionary/selective breeding algorithm on it's own not considered ML?

The Strandbeest leg has 13 dimensions which he wanted to find the ideal lengths of each in order to have the foot generate a stepping motion ""a curve which was flat on the bottom"". His program generated batches of 1500 legs with randomized dimensions and chose the best from each batch as the basis for the next batch.

I wonder how he scored the curves. I know he wanted a flat bottom but I'd think he also wanted some way to score the stride length and height to avoid getting curves that just move back and forth in a tiny straight line. I can imagine maybe using the average difference of the y-coordinates of points sampled over the curve, or maybe some calc? If you have any ideas as to how to score a good step curve or if you know how he did it that I'd love to know.

Finally, I wonder if he has revisited this problem with modern computer capabilities to see if he can find even more optimized dimensions. I'd be shocked if others haven't already done this. If you know where to find more info on Theo's process, the compute program or modern advancements of the Strandbeest using machine learning please let me know I'd love to discuss more.",MachineLearning
"Hi, I have created r/continual_learning to host discussions related to Continual Learning on Reddit. Do check it out if you are interested.",MachineLearning
"I recently came across "" Confident Adaptive Language Modeling "" which allows Transformers to exit early during inference and not use all model layers if a token is easy to predict. Is there any research on basically doing the opposite and allowing Transformers to spent more compute on tokens that are very hard to predict?",MachineLearning
"There are quite a few papers on optimisation-based meta-learning approaches for learning parameter initialisations (*i.e.* MAML and its derivatives) \[1, 2\], and there are also many papers on learning optimisers \[3\].

**Question:** *Are there any papers which combine the two?*

I am aware of some papers such as \[4, 5\] which achieve this in some capacity indirectly/implicitly, but wondering if there are any other papers that I am not aware of, or do this explicitly? Thanks in advance.

\---

\[1\] Finn, C., et al. (2017. Model-agnostic meta-learning for fast adaptation of deep networks. ICML.)

\[2\] Nichol, A., et al. (2018. On first-order meta-learning algorithms.)

\[3\] Andrychowicz, M., et al. (2016. Learning to learn by gradient descent by gradient descent.) *NIPS*

\[4\] Li, Z., et al. (2017. Meta-sgd: Learning to learn quickly for few-shot learning.)

\[5\] Ravi, S., & Larochelle, H. (2016. Optimization as a model for few-shot learning. ICLR.)",MachineLearning
"The Open Deep Learning Toolkit for Robotics version 2.0 was just released! This new version of the toolkit includes several improvements, such as new tools for object detection, efficient continual inference, tracking, emotion estimation and high-resolution pose estimation. Furthermore, this version includes a refined ROS interface, along with support for ROS2.

You can download it here: [https://github.com/opendr-eu/opendr](https://github.com/opendr-eu/opendr)

We look forward to receiving your feedback, bug reports, and suggestions for improvements!",MachineLearning
"I am working on a very large (>10mm rows) binary classification problem where 0:1 ration is 7:1. I am trying to use sample weighting and seems there are multiple different methods for that. Examples are Inverse of Number of Samples, Inverse of Square Root of Number of Samples, Effective Number of Samples, etc. sklearn also has the class\_weight method. I am wondering how to select one of these. Do I need to try all and pick the best method among them? Also, it seems some methods like Effective Number of Samples need hyperparameter tuning.",MachineLearning
"I am working on a medical ML project and my advisor would not like to publish our dataset. I would like to publish our results to a top-tier ML conference. Would this affect us during the review process? If so, are there any ways to mitigate against this like also including results on separate publicly available datasets?

Just to note, not publishing the research dataset seems much more common in medical publication venues.",MachineLearning
"Paper : https://arxiv.org/abs/2301.03728

Abstract :

>Generative language models define distributions over sequences of tokens that can represent essentially any combination of data modalities (e.g., any permutation of image tokens from VQ-VAEs, speech tokens from HuBERT, BPE tokens for language or code, and so on). To better understand the scaling properties of such mixed-modal models, we conducted over 250 experiments using seven different modalities and model sizes ranging from 8 million to 30 billion, trained on 5-100 billion tokens. We report new mixed-modal scaling laws that unify the contributions of individual modalities and the interactions between them. Specifically, we explicitly model the optimal synergy and competition due to data and model size as an additive term to previous uni-modal scaling laws. We also find four empirical phenomena observed during the training, such as emergent coordinate-ascent style training that naturally alternates between modalities, guidelines for selecting critical hyper-parameters, and connections between mixed-modal competition and training stability. Finally, we test our scaling law by training a 30B speech-text model, which significantly outperforms the corresponding unimodal models. Overall, our research provides valuable insights into the design and training of mixed-modal generative models, an important new class of unified models that have unique distributional properties.

[Suggested Tweet Thread](https://twitter.com/ArmenAgha/status/1613192892188856322)",MachineLearning
"OpenAI must be super confident about the generality of their AI and Microsoft product integration.

Link: https://twitter.com/bentossell/status/1613220711992115201?t=bJihb54D6XYChDOGMZU4AQ&s=19",MachineLearning
"I believe that Microsoft's 10B USD investment in ChatGPT is less about Bing and more about turning Cortana into an Alexa for corporates.   
Examples: Cortana prepare the new T&Cs... Cortana answer that client email... Cortana prepare the Q4 investor presentation (maybe even with PowerBI integration)... Cortana please analyze cost cutting measures... Cortana please look up XYZ... 

What do you think?",MachineLearning
"Hello. Recently VALL-E has been announced. It is just awesome.

I could use it to fix my bad audio quality previously recorded lectures. 

So any model like that available currently for public usage?

You can check VALL-E examples here : [https://valle-demo.github.io/](https://valle-demo.github.io/)",MachineLearning
"This is a little bit weirder of a project. The idea is to prompt AI to generate search results that you can then click on.

[https://latentweb.ai/search.html?query=simulation+of+calculating+pi](https://latentweb.ai/search.html?query=simulation+of+calculating+pi)

and

[https://latentweb.ai/search.html?query=what+is+a+juggalo](https://latentweb.ai/search.html?query=what+is+a+juggalo)

Every bit of text is AI generated.

Sometimes the results are what you would expect and the links actually exist. Other times the results are completely made up but seem real. Yet other times the results are just hilarious.

The goal is to keep this as open ended as possible and augment it with tools to make exploring easier and more fun. One of the first things I added after it was launched was the Google and Bing links because sometimes it would generate results that made you curious if it was real or not.

For example [https://latentweb.ai/search.html?query=simulation+of+calculating+pi](https://latentweb.ai/search.html?query=simulation+of+calculating+pi) talks about throwing frozen hotdogs to calculate pi, which seems to be a popular topic for some reason.

Right now we aren't generating any of the actual pages due to cost but it 100% works and as soon as we find VC, it will be launched. That will also come with boring but productive tools like being able to save the websites, share them, use them as templates for a real website, even eventually get the AI to code backend functionality or wire it up to an external API like Reddit.

Until then, there is also a similar open source project that you can play with right now!

[https://github.com/jbilcke/web4](https://github.com/jbilcke/web4)

Thanks for reading!",MachineLearning
I am working on a QA Project in a medical subfield. The task is novel and there are no current datasets for this other than the one of my advisors created. We were looking to create a novel QA method for this task but we realized that it's already so difficult to fit current methods to this particular dataset that we were thinking of publishing a paper that benchmarks various current approaches. I was interested in publishing in a top tier venue (e.g. top NLP/AI/ML conference) - do you have any thoughts on where I could publish this (which maybe has a bias for medical papers)? I was thinking about MICCAI but NLP is not explicitly listed as a topic of interest though I believe there are several MICCAI NLP papers.,MachineLearning
"I am implementing soft prompt tuning (reproducing [https://arxiv.org/abs/2104.08691v2](https://arxiv.org/abs/2104.08691v2)) for my research project, but the training makes the model predict ""False"" only in T/F classification task (BoolQ dataset). I have tried all other code on full model fine-tuning and it's working to exclude all other issues (so it's unrelated to dataset and trainer).

&#x200B;

There are some observations to exclude possible issues.

* The soft prompt parameters do change during soft prompt training. (gradient backprop on soft prompt is working)
* Training loss goes down normally just as model fine-tuning

Any idea on how to debug the issue.",MachineLearning
"Story here: https://www.bloomberg.com/news/articles/2023-01-10/microsoft-weighs-10-billion-chatgpt-investment-semafor-says?srnd=premium

Unpaywalled: https://archive.ph/XOOlg",MachineLearning
"I have a set of \~100 topic categories, and I want to determine which are semantically close to a text input.

I've found several implementations, but I know some (LDA) are already obsolete. OpenAI's text-embedding-ada-002 model [just came out](https://openai.com/blog/new-and-improved-embedding-model/) so I'm wondering if that's the best option now.

Other topic modeling implementations:

[Multi-Class Text Classification with Doc2Vec & Logistic Regression](https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4)

[Build taxonomy-based contextual targeting using AWS Media Intelligence and Hugging Face BERT](https://aws.amazon.com/blogs/machine-learning/build-taxonomy-based-contextual-targeting-using-aws-media-intelligence-and-hugging-face-bert/)

[Topic Modeling with BERTopic](https://txt.cohere.ai/topic-modeling-with-bertopic/)",MachineLearning
"Improve Tortoise TTS by 30% inference speed, and packaged it up as a hosted API that charges per-second. All code is open-sourced: [https://github.com/metavoicexyz/tortoise-tts-modal-api](https://github.com/metavoicexyz/tortoise-tts-modal-api), [https://github.com/metavoicexyz/tortoise-tts](https://github.com/metavoicexyz/tortoise-tts)

It can be used via a UI on: [https://tts.themetavoice.xyz](https://tts.themetavoice.xyz)

There are more details here: [https://twitter.com/vatsal\_aggarwal/status/1612536547248836608?s=20](https://twitter.com/vatsal_aggarwal/status/1612536547248836608?s=20)",MachineLearning
"This is the first episode of a new podcast on machine learning featuring Maarten Grootendorst. 

[Maarten Grootendorst: BERTopic, Data Science, Psychology | Learning from Machine Learning #1](https://youtu.be/bLW_RH2Y6oI)",MachineLearning
"Hi everyone,

A couple of months ago I saw a github with a package testing many different decision tree models on the same (user provided) dataset, really fast, in python ; the goal is to select the optimal one programmatically.

I can't remember if I discovered that package on hacker news, or on github trending. No luck sifting through my browser history.

Would any of you recognise this description and know the package ?

Help appreciated !",MachineLearning
"In practice, I am working with chemical formulations with thousands of ingredients. Using each ingredient (like a one-hot-encoding) would explode the dimensionality of the problem. I am thinking about grouping all these ingredients into their ""functional role"" (20 or so) . If so, I could greately reduce the number of features, but the cardinality would be high for each feature. Did the dimensionality really go down from a thousand to 20? My intution tells me that 20 should be multiplied by all the cardinalities of each feature, and such, I haven't made much progress in reducing dimensionality.

Does anyone have any insight or experience with these high dimensional/high cardinality problems and what is the best way to do feature engineering?",MachineLearning
"Hi /r/ML,

I wrote down my thoughts about what it might take for diffusion to displace autoregression in the field of language modelling (as it has in perceptual domains, like image/audio/video generation). Let me know what you think!

https://benanne.github.io/2023/01/09/diffusion-language.html",MachineLearning
"And how to make a justified classification, for example when dealing with a plethora of content/items split between two buckets?

My initial understanding is to provide a rationale, but is there a specific format for doing ""justified classification""? How to present rationale? What is needed for rationale, peer-review sources?

[https://proceedings.mlr.press/v89/cohen19a.html](https://proceedings.mlr.press/v89/cohen19a.html)

[https://arxiv.org/abs/1702.05659](https://arxiv.org/abs/1702.05659)",MachineLearning
"We all know that Deep learning models are extremely susceptible to noise and can be easily fooled by adding a small amount of noise. These noises can be calculated by methods like the Fast Gradient method and are almost imperceptible to human eyes.

But there is a way to somewhat mitigate the adversarial attacks and force neural Networks to behave in a more continuous fashion and it's called Lipschitz regularization. It is a method for enforcing a certain level of smoothness on the output of a machine-learning model. 

It can improve the model’s generalization performance and help prevent overfitting. It is particularly useful for deep learning models, which are prone to overfitting due to their large number of parameters. 

Link to the full article: [https://medium.com/p/fdeafb2d5c14](https://medium.com/p/fdeafb2d5c14)

&#x200B;

https://preview.redd.it/mlfctr36uzaa1.png?width=828&format=png&auto=webp&v=enabled&s=f2b8db71c361c4cd4d44ccaa91fd781dfa244a78",MachineLearning
"I would like to fill sentences where I know the first and last word. I've been experimenting with BERT and using [mask] [mask] etc, but the returned values don't seem to form a coherent sentence. Is there a better model to use please?",MachineLearning
"**- Cost, Effort, and Performance-wise, does it make more sense to instead just pay to use the OpenAI API and use a cheaper GPT-3 model to lessen business costs?** My biggest concern is having my entire business reliant on a 3rd-party API, even more so than the costs of using the model.

**- How good is it at writing short stories?** If there are better open-source alternatives for doing this better or at a similar level but less resource expensive, what are they?

**- How resource-expensive is it to use locally?** These are my laptop capabilities:16.0 GB of RAM, AMD Ryzen 7 5800H with Radeon Graphics 3.20 GHz.

**- How would I approach fine-tuning it?** Are there any resources going through the step-by-step process? Currently, in my mind, I just need to shove a large free-to-use data-set like stories and wait like a day but I have no expertise in this area.

**- If I want to incorporate it into a website with an API that takes prompts from users, are there any costs that I should account for?** Is there a way to minimize these costs? For example, is there a specific API set-up or one-time cost like an expensive laptop to host it locally and take prompts that I could be implementing?

**- Are there any concerns I should have when scaling it for users, such as costs and slow response rate?** Also, is there a cap in terms of the requests it can handle or is that just limited by what my own machine can handle?",MachineLearning
"I've written a small piece of research on an idea of mine, a new optimizer which has an adaptive global learning-rate, based off Adam and uses (what I think) is a neat trick to get the calculus to work.

EDIT: Didn’t do my research, seems like someone came up with a similar idea before. I do use a different implementation though. Will change the document to read as such. Everything else on the post applies still! 

My goal in putting it here is mainly to ask for opinions and directions; to clarify, I've not received any  professional/formal education in Machine Learning, and my studies in it are purely my own, and I'm not connected to any circles which could help me. What I've done is taken some simple concepts and mimicked what I've seen in papers I've read.

I think (hope) that I'm solid in the math and code and concepts of AI, but clueless about the real-world stuff around it. This is me asking about what that other stuff, first steps into this field publicy, is like.

Any advice would be much appreciated. 

Many thanks. A PDF is availabe [here](https://drive.google.com/file/d/1urPn85-txrnWhasnf91b_ARYVFThsp-6/view?usp=sharing).",MachineLearning
"Hi, I completed a ML PhD in 2015. I’ve done a number of projects with CNN architectures and recently I have been working as a consultant for computer vision and data scientist. As I haven’t been involved in research for some time now, I am looking for courses and other resources to refresh and update my knowledge.

Could anyone suggest where to start? Right now, I am applying segmentation on drone imagery (RGB and multispectral). I have used DeepLabV3+. One challenge that I have is the annotation. For example, annotating wheat and weed on drone images taken at the altitude 300 m is hard. One thing that I would like to research, therefore, is auto-annotation and possibly self-supervised learning.",MachineLearning
"I've been doing some personal experiments with ChatGPT to see what kinds of influence a prompt has on the results of problem solving tests. This is along the same lines as the following research from 2022 that I found after I started doing some tests: [https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)  


The results were pretty remarkable. If you simply ask a question(""True or False: 73 minutes after 2pm is the same time as 15 minutes before 4pm.""), you get very simplistic and often wrong reasoning or just an answer with no reasoning which is also often wrong. I tested this on the above prompt and it was wrong on 5/5 tries.   


Then I tested the following prompt where I first instructed it to come up with a plan for solving the problem in question, then had it follow that plan.  


("" You are a brilliant professor specialized in general problem solving techniques. Give a lecture on the techniques to use to solve problems like the following true/false statement:

True or False: 73 minutes after 2pm is the same time as 15 minutes before 4pm."")  


This resulted in it answering the question correctly on 5/5 tries and with proper reasoning as to why it got the answer it did.  


I did a more complete write up on this here: [https://www.reddit.com/r/ChatGPT/comments/106kxyw/improving\_ai\_reasoning\_skills\_through/?utm\_source=share&utm\_medium=web2x&context=3](https://www.reddit.com/r/ChatGPT/comments/106kxyw/improving_ai_reasoning_skills_through/?utm_source=share&utm_medium=web2x&context=3)  


You can also find the actual model outputs in that link if you are curious as to its process.  


I hope you find this interesting and try it yourself!",MachineLearning
"I just launched [searchthearxiv.com](https://searchthearxiv.com), a simple semantic search engine over virtually all ML papers published on arXiv since 2012. The site uses OpenAI's \`text-embedding-ada-002\` model to match the embedding of your query against each of the paper embeddings, retrieving the ones with the highest cosine similarity. It also allows you to insert an arXiv link to find similar papers.

This was mostly meant as a fun side project. However, if people find it useful, I'm happy to maintain it and keep the database up-to-date. I'd love to know what you think! ❤️

**Update:** Thanks to u/ml-research for pointing out that some papers were excluded from search results regardless of the search query. This was due to a bug in the way the database was queried, and should now be fixed.",MachineLearning
"There's been a ton of academic work exploring knowledge distillation techniques, sparsity in networks and many others, often with vast numbers of citations. I was wondering what the status of those in real-world ML was. Has any of you used it in a concrete situation? What did you find to work best for you?",MachineLearning
"Hey everyone!

I wanted to share a new Python package called FastQL that makes it easy to prototype and share machine learning models using GraphQL. It's really fast and efficient thanks to using rust to serve the API on a separate process.

https://github.com/happy-machine/FastQL

With FastQL, all you have to do is provide a callback function and a Python dictionary describing your GraphQL API, and FastQL will handle the rest. This makes it super easy to prototype ML models and get them up and running quickly.

You can find FastQL on PyPI and GitHub. We've included simple steps and a Dockerfile to help you spin up your own Stable Diffusion or other Hugging Face models. There's even an example that lets you train a huggingface diffusers (Stable diffusion 2, runway) model on your own images, with instructions for spinning it up on AWS in minutes, even if you're new to ML and Python.

We'd love to have your help and support, so if you're interested in getting involved, let us know!
Thanks to Async-GraphQL, Hugging Face, Stable Diffusion, and all the other people and projects that inspired and helped us. ❤️ DJ Fresh, @chrisjbishop156 and friends.",MachineLearning
"Hi all,

I [wrote this post](https://mtszkw.medium.com/why-do-87-of-data-science-projects-fail-and-are-we-sure-that-it-is-true-fe8b5ba1404c) like a year ago, because I kept seeing and hearing that ""87% of data science..."" or ""only 1 out of 10 machine learning projects..."" blah blah and apparently, as I described it in my post, these numbers came out of nowhere i.e. the 87% that people kept referring to for a long time is based on nothing actually.

But I would like to know your opinion / based on your commercial experience. **What was the success rate of machine learning projects in your work?** Let's assume that a success means a model being deployed on production or accepted by a client.

BTW. Leaving a link to my article because it is an important background/reference (showing that the commonly used statistic is not proven) but I send this Reddit post looking for a real discussion, it is not just an ad.",MachineLearning
"I'm looking for a comprehensive reference on the history of neural networks that covers all significant papers in the field, from the early days up to the current deep learning era, and provides information on their main contributions and inspirations. It would be helpful to have information on how the understanding and perspectives of the research community on neural networks have evolved over time as well. 

Do you know of any good references like that?",MachineLearning
"Paper: [https://arxiv.org/abs/2301.00303v1](https://arxiv.org/abs/2301.00303v1)

Abstract:

>Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize **external knowledge to assist LLMs**. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose **a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps** obtained from the chain-of-thought (CoT) prompting. This lightweight approach does **not require additional training or fine-tuning and is not limited by the input length of LLMs.** We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.

https://preview.redd.it/to09kna1jtaa1.jpg?width=640&format=pjpg&auto=webp&v=enabled&s=913efbaea708e279c0efdd3b6f2257a8458f1d38

https://preview.redd.it/98eucra1jtaa1.jpg?width=1232&format=pjpg&auto=webp&v=enabled&s=b654c71b4718e16d2dc328e9657d6fa705b94056

https://preview.redd.it/cbhq1ra1jtaa1.jpg?width=835&format=pjpg&auto=webp&v=enabled&s=d94e0d511b9c30694757fe74538f1a43ef30524f

https://preview.redd.it/ggoowsa1jtaa1.jpg?width=1356&format=pjpg&auto=webp&v=enabled&s=a42e52c09a7dd81cafaae4a466b0e90d94c55c18",MachineLearning
"There is always needing specific things to GPUs. There is always a need to make a new thing over and over again for each new GPU. We got Cuda, Rocm, Metal, and will soon need Intel. I know there are already a lot of tools out there for Cuda which make it hard to replace. However for something like Apple devices (which Apple has a history of not giving a darn about compute unless if it's the iPhone or iPad). Then there is a ton of operations that have to get implemented and only CUDA is something you know will be reliably supported it seems. I am curious on your guys thoughts with why this ain't a thing in ML, even though game industry uses open standards like these all the time .

Edit: Shoot I just realized PyTorch was prototyping Vulcan as a backend.
https://pytorch.org/tutorials/prototype/vulkan_workflow.html",MachineLearning
"I have been working on a project with GPT-3 API for almost a month now. The only drawback of GPT-3 is that the prompt you can send to the model is capped at 4,000 tokens - where a token is roughly equivalent to ¾ of a word.  Due to this, providing a large context to GPT-3 is quite difficult.

Is there any way to resolve this issue?",MachineLearning
"Hello

I've been working on a PR for Tanmay Bakshi's CoreML Whisper project which adds SIMD acceleration to Log Mel Spectrogram creation via vDPS / Accelerate framework, and uses CoreML for the encoding and decoding. You can find the PR here, with a lot of explanations:

[https://github.com/tanmayb123/OpenAI-Whisper-CoreML/pull/2](https://github.com/tanmayb123/OpenAI-Whisper-CoreML/pull/2)

This project started out because WhisperCPP's Metal port is slow, and the CPU inference performance isn't making use of the dedicated hardware on Apple devices.

Now, the project could use some community eyes, as its not quite finished, and ive hit a roadblock in my implementation that I can't seem to resolve. The model keeps predicting the same token over and over again.

I'm looking for a few brave souls familiar enough with Whispers internals and not afraid of Swift to help this project out and get it over the last few humps.

The PR has a ton of notes on whats been done to date, and where we need some help.

I'f we can resolve this issue, I expect this to be an incredibly fast Whisper implementation!

Thank you in advance!",MachineLearning
"People are nowadays using zero-shot agents such as ChatGPT as search engines. While they are good at answering the most popular questions, sometimes they miss historical, factual, and numerical correctness. So how could this aspect of LLMs be successfully evaluated (in an automated fashion)? Please point out research that you're familiar with.",MachineLearning
I'm looking for a way to automate data extraction from bar charts with error bars from peer-reviewed academic papers/PDFs. The goal here is to extract data values from charts and put them in a tabular form. Does anyone have any good resources for how to streamline automated chart mining in python or R? Or does anyone know of a good application/website that does chart mining?,MachineLearning
"Recently, ChatGPT has become one of the hottest tools in the NLP area. I have tried it and it gives me amazing and fancy results. I believe it will benefit most of the people and make a significant advance in our life. However, unfortunately, I, as an NLP researcher in text generation, feel all what I have done seems meaningless now. I also don't know what I can do as ChatGPT is already strong enough and can solve most of my previous concerns in text generation. Research on  ChatGPT also seems not possible as I believe it will not be an open-source project. Research on other NLP tasks also seems challenge as using a prompt in ChatGPT can solve most of the NLP tasks.  Any suggestions or comments are welcome.",MachineLearning
"What is the most simple and efficient way that you can feed a large dataset of quotes into a custom model that can then be used create new quotes based on that model's ""style"" using GPT-3?

Thanks so much for your expertise and help!",MachineLearning
"Hi ! Wrote an article about AI/ML in the Anti-Money laundering industry !

[https://medium.com/@melmasset/ai-and-data-in-finance-the-role-of-machine-learning-in-anti-money-laundering-1d4dd6f5bacd](https://medium.com/@melmasset/ai-and-data-in-finance-the-role-of-machine-learning-in-anti-money-laundering-1d4dd6f5bacd)",MachineLearning
" Greg Yang is a mathematician and AI researcher at Microsoft Research who for the past several years has done incredibly original theoretical work in the understanding of large artificial neural networks. His work currently spans the following five papers:

Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes: [https://arxiv.org/abs/1910.12478](https://arxiv.org/abs/1910.12478)  
Tensor Programs II: Neural Tangent Kernel for Any Architecture: [https://arxiv.org/abs/2006.14548](https://arxiv.org/abs/2006.14548)  
Tensor Programs III: Neural Matrix Laws: [https://arxiv.org/abs/2009.10685](https://arxiv.org/abs/2009.10685)  
Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks: [https://proceedings.mlr.press/v139/yang21c.html](https://proceedings.mlr.press/v139/yang21c.html)  
Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer: [https://arxiv.org/abs/2203.03466](https://arxiv.org/abs/2203.03466)

In our whiteboard conversation, we get a sample of Greg's work, which goes under the name ""Tensor Programs"". The route chosen to compress Tensor Programs into the scope of a conversational video is to place its main concepts under the umbrella of one larger, central, and time-tested idea: that of taking a large N limit. This occurs most famously in the Law of Large Numbers and the Central Limit Theorem, which then play a fundamental role in the branch of mathematics known as Random Matrix Theory (RMT). We review this foundational material and then show how Tensor Programs (TP) generalizes this classical work, offering new proofs of RMT.

We conclude with the applications of Tensor Programs to a (rare!) rigorous theory of neural networks. This includes applications to a rigorous proof for the existence of the Neural Network Gaussian Process and Neural Tangent Kernel for a general class of architectures, the existence of infinite-width feature learning limits, and the muP parameterization enabling hyperparameter transfer from smaller to larger networks.

&#x200B;

https://preview.redd.it/av3ovotcunaa1.png?width=1280&format=png&auto=webp&v=enabled&s=a7aa946741036e5c39d990f070a01a1e72202265

https://preview.redd.it/hh9q6wqdunaa1.png?width=1200&format=png&auto=webp&v=enabled&s=939a9dbba9e46a928cef5d1d7bbe75819873ca7f

Youtube: [https://youtu.be/1aXOXHA7Jcw](https://youtu.be/1aXOXHA7Jcw)

Apple Podcasts: [https://podcasts.apple.com/us/podcast/the-cartesian-cafe/id1637353704](https://podcasts.apple.com/us/podcast/the-cartesian-cafe/id1637353704)

Spotify: [https://open.spotify.com/show/1X5asAByNhNr996ZsGGICG](https://open.spotify.com/show/1X5asAByNhNr996ZsGGICG)

RSS: [https://feed.podbean.com/cartesiancafe/feed.xml](https://feed.podbean.com/cartesiancafe/feed.xml)",MachineLearning
"I'm a technical product manager in my day job, and I've spent the past few months educating myself on recent advances in AI implementation in the pre-trained transformer and diffusion areas. I have some basic coding experience and have worked on several complex software projects in my role as a product manager, but my true expertise is in building high-level architectures based on user story frameworks (i.e. - define the key features of the product based on insights/research and build an HLA and technical/business dependency maps that serve as the foundation of a development backlog). I tend to sit right in the middle of the engineering, research, and UX design teams and help keep everyone focused on the things that matter.

So in my personal time, I've now developed a considerable backlog of product Proof-of-concept ideas which make use of this new class of AI tools in various ways in a wide range of sectors. I even built one myself and deployed it on bubble here: [https://foodwizard.app/](https://foodwizard.app/). I want to build more of these functional protos with the hope that one of them sticks and could turn into a viable startup, or at the very least, that the sub-services of several POCs could sum into infrastructure for future concepts that can make complex, powerful AI-enabled applications easier and faster to build. At the very least, I want to become an expert at practical AI product implementation and optimization, and am looking for a few people who have similar goals to start a small community with. 

I'm looking for a like minded software developer or two (or three) who is interested in rapid prototyping of AI software in their spare time, ideally with a knack for full-stack web-app + mobile development and is looking to grow their skillset to include complex AI pipelines and agents. Front-end UX designers/developers would be welcome too! We would work mostly asynchronously via discord or similar, with no strict time commitments (definitely a nights & weekends thing). Just looking for a few partners to learn things with, do some fun experiments, and build some cool shit. If this sounds like you, let me know!",MachineLearning
 Increasingly deep learning is becoming the default face of modern AI. So my question is are there any other machine learning theories or ideas different from deep learning which have potential to be big in the future ?,MachineLearning
"**1)** **A SOTA Language Model is Trained on 10x More Data Than Chinchilla**  
\-> Language models like Lambda and GPT3 are significantly undertrained. DeepMind [proposed Chinchilla](https://arxiv.org/pdf/2203.15556.pdf), a model which has similar performance to GPT3 with less than half the size (70B vs. 175B). Hence in 2023, significant performance gains will likely come from cleaner/larger datasets.

**2) Generative Audio Tools Emerge and Will Attract 100K Developers**  
\-> Audio generation has approached human levels. If enough data of your voice is available, the generated speech can even sound amazingly authentic (this is also [true for Drake lyrics](https://www.youtube.com/watch?v=e1_caZAerlU)). Leaving the uncanny valley of awkward robot voices will make adoption surge. 

**3) NVIDIA Announces Strategic Partnership With AGI-Focused Company Organisation**  
\-> Usage statistics in AI research show that NVIDIA's adoption is 20x-100x larger than that of competitors. If NVIDIA could pick or even help create a winning organization, this would cement their position.

**4) Investment of >100M Into a Dedicated AI Alignment Organisation**  
\-> Artists were not happy as model-generated artwork won an [art competition](https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html) in Colorado. Advances such as this will cause sentiments about AI safety to aggravate. 

**5) Proposal to Regulate AGI Labs Like Biosafety Labs Gets Backing By EU, GB, or US politicians**  
\-> OpenAI scrambles to prevent ChatGPT from showing people how to build bombs. Responding to an outcry from artists, Stability AI has [announced](https://arstechnica.com/information-technology/2022/12/stability-ai-plans-to-let-artists-opt-out-of-stable-diffusion-3-image-training/) they will allow artists to opt-out such that their work is not used as training data. As negative impacts accumulate, regulation gains momentum. 

**6) GAFAM invests >$1B Into an AGI or Open-Source AI company Like OpenAI**  
\-> The increase in the cost of model training has led to more and more innovation happening in industry. Regardless of the economy's choppy waters, big tech knows that staying ahead of the curve on machine learning will guarantee smooth sailing.

**7) DeepMind Will Train 10B Parameter RL Model an Order of Magnitude Larger Than GATO**  
\-> Currently, most machine learning models are very specialized. They can do one thing and one thing only. In 2022 DeepMind [released GATO](https://arxiv.org/abs/2205.06175). This multi-modal model can, among other things, generate text, control a robot arm, and play video games. However, this line of research does not simply make models more versatile. The possibility of using sequence data for training increases the diversity and availability of training data.

If you made it all the way here, thank you! It was a privilege to make this for you.   
At **TheDecoding** ⭕, I send out a thoughtful newsletter about ML research and the data economy once a week.   
No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)",MachineLearning
"Hi everyone, I have to cluster a large chunk of textual conversational business data to find relevant topics in it.

Since there is lot of abstract info in every text like phone, url, numbers, email, name, etc., I have done some basic NER using regex and spacy NER to tag such info and make the texts more generic and canonicalized.

But there are some things like product names, raw materials, brand/model, company, etc. which couldn't be tagged. Also, the accuracy of regex and spacy NER isn't high enough.

Can anyone suggest a good python NER library, which is accurate and fast enough, preferably has pre-trained models and can tag diverse fields.

Thanks.",MachineLearning
"Paper: [https://arxiv.org/abs/2210.16495](https://arxiv.org/abs/2210.16495)  


Abstract:   
We propose a simple refactoring of multi-choice question answering (MCQA) tasks as a series of binary classifications. The MCQA task is generally performed by scoring each (question, answer) pair normalized over all the pairs, and then selecting the answer from the pair that yield the highest score. For n answer choices, this is equivalent to an n-class classification setup where only one class (true answer) is correct. We instead show that classifying (question, true answer) as positive instances and (question, false answer) as negative instances is significantly more effective across various models and datasets. We show the efficacy of our proposed approach in different tasks -- abductive reasoning, commonsense question answering, science question answering, and sentence completion. Our DeBERTa binary classification model reaches the top or close to the top performance on public leaderboards for these tasks. The source code of the proposed approach is available at [https://github.com/declare-lab/team](https://github.com/declare-lab/team)  


&#x200B;

https://preview.redd.it/r77sh3xczkaa1.png?width=2216&format=png&auto=webp&v=enabled&s=7d8118fad419ede0b7bd051cdffee03a31133548",MachineLearning
"Hi everyone, I have a large chunk of textual conversational data which is to be clustered in an unsupervised manner to find the popular topics in it.

The data I have, has a lot of spelling errors. I have used many libraries like symspellpy, pyenchant, pyspellchecker, etc. to correct spelling errors but none of them seems to be accurate or fast enough. Also, they don't take into account context, abbreviations and grammer while spell correction.

Can anyone suggest a python library which:

1. Can correct spell errors without many false positives

2. Has high accuracy and faster runtime

3. Can take into account context, grammer, abbreviations, phonetics and adjacent keystroke errors while correcting spellings

4. More preferable if multilingual too

Thanks.",MachineLearning
"Hi All, Did anyone receive invite for online test or interviews for the Apple AI Residency program for the 2023 batch? The deadline for applications was 7th December 2022. Haven't received any communication since then.",MachineLearning
" Let's say there's a whole paragraph of text, 90% of which is irrelevant fluff for my needs. What I'm specifically looking to do is isolate one key pertinent piece of information, which meets a certain criteria that I can somehow specify.

As an example, let's say I have 1,000 paragraphs that are brief biographies of famous people from history. Is there any kind of AI tool I can use to say something like: ""For each of these biographies, IF they include information about where this famous person was born? Isolate this piece of information and only output that as a result.""

Then it just runs through every single paragraph, conducts the analysis, finds the paragraphs that DO contain this information -- then outputs ONLY that as the result, for each paragraph?

For example, full paragraph 1:

*""Teddy Roosevelt was an American President who had a variety of notable accomplishments. He was famous for his daring spirit of adventurism, and his boldness in all areas of life. Renowned for his boundless energy, he is commonly cited as a classic example of a political figure whose work ethic was truly unrelenting. He was born in Antarctica, and died on planet Neptune.""*

AI output:

*""Teddy Roosevelt was born in Antarctica.""*

Or another possible application. Say I have another pool of paragraphs, which are full abstracts of scientific studies. Imagine I want to use an AI tool to ONLY extract a very concise conclusion of each study, and output just that: *""This study concluded that fruit flies, if exposed to a diet of Cheetos and Pepsi Cola, will grow to be 8 meters long.""*

The best I've found so far are a bunch of text tools that merely allow you to summarize larger chunks of text. None of them, from what I've seen, allow you to put in place a criteria that says: ""Don't just summarize; scan the text for the bits of information that meet a certain criteria (eg, it's the conclusion of the study; it's where this person was born; etc), then output THAT as the summarized result.""

If such a tool exists, it would be extremely valuable for some projects I'm currently working on.

Thanks!",MachineLearning
"I'm playing around fine tuning llms right now. But fine tuning e.g. GPT Neo 2.7B throws a CUDA out of memory already on a 4x A100 80GB machine. Are there any tricks to reduce or distribute CUDA memory (which integrates well with tranformers library in best case)?

Thanks",MachineLearning
"I don't do very much work with neural models at all, so I'm not very familiar with the library ecosystem developed around them (think TF, Torch, theano, etc) but I'm working on a few projects that would benefit significantly from autograd and the ability to build out models at a higher level of abstraction analogous to layers/capsules/attention heads in neural models. Being able to define functions with explicit gradients would also allow me to use building blocks that autograd can't handle without more information (best example I can give is a nonlinear transformation that results from a constrained optimization problem, and gradients need to account for Lagrange multipliers being implicit functions of the input). And if there's performance gains to be had from a JIT, I certainly wouldn't complain! I'm not certain if I'm going to be able to get what I want out of the options I'm aware of for a few reasons-

1. I don't know the libraries at all!

2. Very frequently there are operations that don't lend themselves to trivial parallelism, require iteration or special function computation (like the polygamma and Bessel functions) which are much better suited to a CPU than a GPU

3. I'll be exclusively working on CPU.

4. Some of this is with sparse data that cannot be stored dense, so I'm unsure how that will translate.


So, main question- if anyone uses these kinds of libraries for similar applications, is it helpful for development, code quality, performance, etc? Which do you use, and why? If you have experience doing formal academic research and/or developing+maintaining production projects I have particular interest in your thoughts. I'm hopeful that I can find a tool that will handle the math + numeric concerns as well as result in code that's easier to maintain. Thanks for your time!",MachineLearning
"Hi r/machinelearning!

&#x200B;

A few months ago I quit my job to join my partners to make training open-source models much faster and easier for engineers.

&#x200B;

We're building [Rubbrband](https://rubbrband.com). It's a web app that takes any ML repo off of GitHub, and gives you a Terminal and Jupyter Notebook in browser with dependencies and GPUs automatically set up.

&#x200B;

**Why did we build this?**

My co-founders and I have been working on this because we found this dependency set up process super tedious and draining as researchers. 

&#x200B;

**What's included?**

\- Automatic Dependency set up for any GitHub python repo

\- Integrated Terminal and Notebooks

\- A server with an Nvidia GPU

\- Code explanations for functions

\- Our pricing is simple at $75/month for 3 repos running at a time. First week is free. 

&#x200B;

**I'd love to get your feedback on:**

1. **Does the value we provide resonate with you? Would you try it out?**
2. **Is dependency and environment set up take up a large chunk of your time?**

We're currently working on acquiring more GPUs to onboard more users, but if you'd like access to the product please let me know.

&#x200B;

Thank you very much in advance!",MachineLearning
Has anyone built a text to speech model than can take an arbitrary international phonetic alphabet transcription and generate speech for it?,MachineLearning
"Hello, I am planning to write my master's thesis on video object detection. Most of the state of the art algorithms use the imagenet VID dataset for their performance evaluation.
I have to reevaluate some of them, but all download links for the dataset I have found are dead.

Does anybody know, where I could find the dataset? 

Thanks in advance!",MachineLearning
"Paper: [https://arxiv.org/abs/2012.13490](https://arxiv.org/abs/2012.13490) 

Abstract: 

>In this article, we aim to provide a literature review of different formulations and approaches to continual reinforcement learning (RL), also known as lifelong or non-stationary RL. We begin by discussing our perspective on why RL is a natural fit for studying continual learning. We then provide a taxonomy of different continual RL formulations by mathematically characterizing two key properties of non-stationarity, namely, the scope and driver non-stationarity. This offers a unified view of various formulations. Next, **we review and present a taxonomy of continual RL approaches**. We go on to discuss evaluation of continual RL agents, providing an overview of benchmarks used in the literature and important metrics for understanding agent performance. Finally, we highlight open problems and challenges in bridging the gap between the current state of continual RL and findings in neuroscience. While **still in its early days, the study of continual RL has the promise to develop better incremental reinforcement learners** that can function in increasingly realistic applications where non-stationarity plays a vital role. **These include applications such as those in the fields of healthcare, education, logistics, and robotics.** 

https://preview.redd.it/pxtcaot5mhaa1.jpg?width=1062&format=pjpg&auto=webp&v=enabled&s=d1dc0dfa8e1282d1aa64b8cd7363315ec031dfe2

https://preview.redd.it/49e56pt5mhaa1.jpg?width=1207&format=pjpg&auto=webp&v=enabled&s=1a9c48a683e39398e97b83c9fbcf2b2236af8113

https://preview.redd.it/hg670rt5mhaa1.jpg?width=1238&format=pjpg&auto=webp&v=enabled&s=47b7158dcef891a49f1b3e8f313e4713ebc984e7",MachineLearning
" So, I have recently found that there is a serious issue with benchmarking evolutionary computation (EC) methods. The ''standard'' benchmark set used for their evaluation has many functions that have the optimum at the center of the feasible set, and there are EC methods that exploit this feature to appear competitive. I managed to publish a paper showing the problem and identified 7 methods that have this problem:

[https://www.nature.com/articles/s42256-022-00579-0](https://www.nature.com/articles/s42256-022-00579-0)

Now, I performed additional analysis on a much bigger set of EC methods (90 considered), and have found that the center-bias issue is extremely prevalent (47 confirmed, most of them in the last 5 years):

[https://arxiv.org/abs/2301.01984](https://arxiv.org/abs/2301.01984)

Maybe some of you will find it useful when trying out EC methods for black-box problems (IMHO they are still the best tools available for such problems).",MachineLearning
" 

I have an extensive database of syslogs from HP switches and Aruba access points. Does anyone have an opensource ML recommendation so that I can build an anomaly detection engine using this data?",MachineLearning
"Hi, so I need to create an application (for windows and linux) that runs a few pytorch models, on a user's local device. I have only ever deployed models on the cloud. That is pretty straightforward — package your dependencies and code inside a docker container, create an API for calling the model and run it on a cloud instance.

&#x200B;

But how do I do it when the model needs to run on the end user's device? Docker doesn't really work since there seems to be no way to keep the user from accessing the docker container and hence my source code. I would like to avoid torchscript since my models are quite complex and it will take a lot of effort to make everything scriptable. There seems to be a python compiler called Nuitka which supports Pytorch. But how do python compilers deal with dependencies? Python libraries can be dealt with by following import statements, but what about CUDA?

&#x200B;

I would ideally like a single executable with all libraries and cuda stuff stored inside. When run, this executable should spawn some API processes in the background and display the frontend that allows the user to interact with the models. But is there a better way to achieve this? I would prefer not to make the users setup CUDA themselves.",MachineLearning
"Hey everyone,

I'm currently working on writing a scientific paper and I'm wondering if there are any AI tools out there that can help me find relevant sources for my research. I'm familiar with some of the more traditional methods for source finding (e.g. using databases like PubMed), but I'm curious to know if there are any newer, AI-powered tools that might be worth exploring.

If anyone has any experience with using AI tools for source finding, I'd love to hear your recommendations and insights.

Thanks in advance for any input!",MachineLearning
"I think AI has a huge potential for misuse. Maybe not yet, but AI can develop at unpredictable speed, especially as AI can be used to optimise the development of AI itself.

AI tech is probably still relatively safe in the hands of responsible, careful and ethical people. But those are not the only people that have acces to AI technologies.

So, why this techology and equipment still open to everyone, without any regulations or limitations?  


Edit: sorry if question is stupid, and I actually hope it is and I am just overparanoid. But I'm realy anxious about that issue and would be happy to hear something that would soothe that anxiety.",MachineLearning
"Hi all, I have spent the last months working on a new Python neuro-evolution library: [NeuralFit](https://neuralfit.net/).  I know there are already great neuro-evolution libraries out there, but  the focus of NeuralFit is ease of use so that a wider audience can be  reached. It also seamlessly exports to TF/Keras! Anyways, feel free to  try it and let me know if you have any feedback 😊",MachineLearning
"Hello, I'm looking for a library that trains a CRF model in Python (if Pytorch, that would be even better).

I am working on a semantic segmentation task where we are trying to segment curvilinear structures.

My requirements for the CRF training are a bit specific:

\-  In my case, the image pixels are not the graph nodes. Instead, since  the dataset is curvilinear structures, for every image I have a set of  edges (small pieces of the curvilinear structure). I now want to train  the CRF on these edge-pieces, that is, the graph nodes will be these  edge-pieces. Thus the trained CRF essentially does a binary  classification for each of these edge-pieces (that is, whether this  edge-piece should be part of the segmentation output or not).

\-  I need a library where I can specify the unary and pairwise potentials  of these edge-pieces in order to train the CRF. As a simple example, the  unary potential is the average likelihood of the edge-piece, and the  pairwise potential is the angle between two edge-pieces.

\- It is not a linear-chain CRF because edge-pieces could be connected to multiple other pieces.

\-  Currently, I have frozen a deep neural network (DNN) which generates  the edge-pieces. If the CRF library is in PyTorch, I could train the DNN  and the CRF end-to-end, but if the CRF library is in Python, I would  only be able to train the CRF. At this stage, even a Python library  would be very helpful.

Some of the existing libraries don't work for my requirement:

\- [PyDenseCRF](https://github.com/lucasb-eyer/pydensecrf) : It does not have learnable parameters.

\- [python-crfsuite](https://github.com/scrapinghub/python-crfsuite) : It does not allow me to specify the unary and pairwise potentials.

\- [pytorch-crf](https://github.com/kmkurn/pytorch-crf) : It does linear-chain CRF while I need a graph one.

\- [crfasrnn\_pytorch](https://github.com/sadeepj/crfasrnn_pytorch) : It by default assumes the image pixels as the graph nodes. I cannot specify the unary and pairwise potentials.

If I could get any leads, that would be immensely helpful, thank you.",MachineLearning
" I am responsible for building a defect detection system for TIG welding. If gas flow gets too high, there is a fair chance that welded piece might have porosity defect. The project aims to predict % of defect by predicting gas flow.

Attached is the link of how the flow pattern looks like over time, like square waves.

[Flow rate fluctuates between 0-8 liters per minute over a given time](https://preview.redd.it/6v0iohbiucaa1.png?width=811&format=png&auto=webp&v=enabled&s=152b264a8c6177110d99262755bf058d463ab4b3)

I have data from various workstations on after welding, if the piece had a defect or not. Please help me solve this problem or give rough steps to follow.",MachineLearning
"A lot of interesting models that came out recently have been using the transformer architecture, or otherwise required a lot of VRAM, such amounts, in fact, that it has become unattainable to even just run inference for almost everyone, because the amount of necessary VRAM can only be attained by pooling together numerous >1k usd GPUs.

But VRAM in itself isn't that crazy expensive, it's the the rest of that super powerful GPU that's attached to them that's expensive. If I understand correctly, some manufacturers, like, say, Apple, have been using common pool of memory for both RAM and VRAM, thereby having GPUs with a crazy amount of VRAM. For example, their least expensive Mac Studio has an amount of VRAM equivalent to an A100.

So my question is, does a unified memory pool (common RAM and VRAM) lead to more capable inference? And if so, should/will manufacturers gravitate towards such a hardware solution? And are there any technical obstacles inhibiting such a path?",MachineLearning
"There are a number of companies out there making special-purpose chip ""neuromorphic"" architectures that are supposed to be better suited for neural networks. [Some of them you can buy for as little as $500.](https://www.tomshardware.com/news/brainchip-akida-pcie-board-goes-solo) 

Most of them are designed for Spiking Neural Networks, probably because of the similarity to the human brain. [Innatera's chip](https://www.eetimes.com/innatera-unveils-neuromorphic-ai-chip-to-accelerate-spiking-networks/) implements the neural network on an analog computer, which I find very interesting. 

* Is the performance really better than GPUs? Could this achieve the the dream of running a model on as little power as the brain uses?

* Are spiking neural networks useful for anything? I don't know of any tasks where a SNN is the current state-of-the-art in performance.

All the good results right now seem to be coming out of transformers, but maybe that's just because they're so well-suited for the hardware we have available.",MachineLearning
"While long ago I hailed from the realm of biochemistry, I'm sitting more on the data side these days and can't help but be nostalgic for the possible use of generative neural nets on the bio side. Having a little curiosity, I found that this work has already began somewhat.

For instance, this group managed to get a reasonable design schema by incorporating Attention into their GANs:  [https://www.nature.com/articles/s42256-021-00310-5](https://www.nature.com/articles/s42256-021-00310-5)

To me then, if we can develop ""reasonable"" (i.e. soluble among other properties) protein sequences using GANs, a natural extension would be to train them for selective binding. For instance, imagine adding the loss from a pre-trained Discriminator that predicts binding a certain target into the above.

On one hand, that seems like a tall order, but given some of the research I've thumbed through, I believe such a discriminator is already shown as tenable. Here's a few papers:

* [A meta review of the topic](https://www.sciencedirect.com/science/article/pii/S2001037022004044)
* [An implementation using Attention](https://www.sciencedirect.com/science/article/pii/S2589004222014419)
* [Another implementation](https://academic.oup.com/bioinformatics/article/34/17/i802/5093239)

Being able to serve up amino acid sequences with selective binding properties seems really really attractive and a natural ""next step"" for GAN research here. The closest I have seen though is [this work](https://www.biorxiv.org/content/10.1101/2022.01.27.478054v1.abstract) which is more of the flavor of redesigning a sequence rather than a de novo approach.

Anyways, I'm no researcher in the field and don't have a dog in the fight so to speak, but it's just an exciting thought to me. Curious if it's been done yet? And if not, just interested in sparking some discussion in case others have similar interests.",MachineLearning
"# Announcing the Airlift Challenge 

# Competition Website: [https://airliftchallenge.com](https://airliftchallenge.com)

The Airlift Challenge seeks improved algorithms to plan and execute airlift operations in the face of dynamic disruptions. Participants compete by submitting OpenAI Gym-based agents that minimize delivery time and cost using reinforcement learning, optimization, heuristics, or any other technique.

# Overview

Airlifts demand the delivery of large sets of cargo into areas of need under tight deadlines. Yet, there are many obstacles preventing timely delivery. Airports have limited capacity to process airplanes, thus limiting throughput and potentially creating bottlenecks. Weather disruptions can cause delays or force airplanes to re-route. Unexpected cargo may be staged for an urgent delivery.

This competition challenges participants to design agents that can plan and execute an airlift operation. Quick decision-making is needed to rapidly adjust plans in the face of disruptions along the delivery routes. The decision-maker will also need to incorporate new cargo delivery requests that appear during the episode. The primary objective is to meet the specified deadlines, with a secondary goal of minimizing cost. Solutions can incorporate machine learning, optimization, path planning heuristics, or any other technique.

# Example Scenario

&#x200B;

https://preview.redd.it/y3sbltaw78aa1.png?width=547&format=png&auto=webp&v=enabled&s=991f18c2bdf33c897b01a5ac16773543523f9aec

# How to Participate

Get started by downloading the starter kit and following the README: [https://github.com/airlift-challenge/airlift-starter-kit](https://github.com/airlift-challenge/airlift-starter-kit)

More details and documentation can be found at the competition website :[https://airliftchallenge.com](https://airliftchallenge.com)

# Model

The air network consists of a graph, where nodes are capacity-constrained airports, and edges are routes with an associated cost and time-of-flight. Each cargo item is stored at a node and must be picked up by agents (airplanes) and delivered to a destination node. Different airplane models can have different route networks. In fact, the network for a specific model may be disconnected, meaning that some airplanes may not be able to reach all airports. Time is needed after an airplane lands to refuel and to load/unload cargo, taking up precious processing capacity at the airport. There are two delivery deadlines: a soft deadline by which the cargo is desired, and a hard deadline after which the delivery is considered missed (with a heavy penalty).

# Interface

The simulation environment is written in Python and follows the PettingZoo multi-agent reinforcement learning interface. An agent issues an action for each airplane, indicating which cargo to load/unload at an airport and which airport to fly to next. The agents observe several state variables, including airplane status, cargo locations, and route availability. A NetworkX object is provided for each airplane, allowing the agent to easily plan paths using existing library methods. A reward signal generated by the environment penalizes late deliveries, missed deliveries, and movement.

# Scoring and Evaluation

Each episode is assigned a score based on missed deliveries, late deliveries, and total flight cost. This score is normalized against baseline algorithms: participants will receive a score of 0 if they only perform as well as a random agent and will receive a score of 1 if they perform as well as a simple “shortest path” baseline algorithm. Scores greater than 1 indicate that the algorithm is exceeding the performance of the baselines.

An algorithm will be evaluated over several episode scenarios. Scenarios are generated according to a random generative model, with scenarios becoming progressively more difficult. In the beginning stages, there will be one type of airplane which can reach all airports in the air network. Later stages will have specialized airplane types: large aircraft can carry large cargo loads over long distances but cannot land at small airports located in the drop off area. Instead, they will need to leave cargo at intermediate airports where light aircraft can retrieve the cargo and complete the delivery.

The evaluation will proceed until either:

1. the percentage of missed deliveries exceeds a preset threshold, or
2. a time limit is reached.

The overall score will be the sum of the normalized scores over all episodes. In addition to performing well on individual episodes, algorithms can also increase their score by completing more episodes.

# References

1. Dimitris Bertsimas, Allison Chang, Velibor V. Mišić, and Nishanth Mundru. The Airlift Planning Problem. Transportation Science, 53(3):773–795, 2019. URL: [https://doi.org/10.1287/trsc.2018.0847](https://doi.org/10.1287/trsc.2018.0847).
2. Carmen Chiu, Adis Delanovic, Jill Platts, Alexa Loy, and Andre Beckus. A methodology for flattening the command and control problem space. In AI and ML for MDO Applications IV, SPIE, 2022. URL: [https://doi.org/10.1117/12.2615180](https://doi.org/10.1117/12.2615180).

Distribution Statement A: Approved for Public Release; Distribution Unlimited: Case Number: AFRL-2022-5675, CLEARED on 22 Nov 2022",MachineLearning
"[https://iap.gridspace.com/](https://iap.gridspace.com/)

First week is on sound and DSP. Second week on NLP and language. Third on speech ML models. And fourth week on applications. There's a signup google form on the site.",MachineLearning
"A friend and I are working on a project that requires us to take images as input find images that match them from our database. What is the most effective way to do this? We've tried SIFT and a few similar solutions, but nothing's been super effective so far. Does anyone have any suggestions? Are there any solid open-source solutions?",MachineLearning
"[https://www.anandtech.com/show/18721/ces-2023-amd-instinct-mi300-data-center-apu-silicon-in-hand-146b-transistors-shipping-h223](https://www.anandtech.com/show/18721/ces-2023-amd-instinct-mi300-data-center-apu-silicon-in-hand-146b-transistors-shipping-h223)

I wonder if this is the beginning of dissolution of NVIDIA's monopoly on AI.",MachineLearning
"Hi, has anyone done any work in using ML or RL to automate providing liquidity in uniswapv3?   


Rewards are based on volume against the TVL, you earn a percentage of each trade, so high volume in a low liquidity market can earn some very good rewards as long as the price is stable enough that it doesn't effect the APR..     


ideally i'd like to assess trends in volume based on TA indicators and then assign a portion of funds to the pool based on some forecasted returns.. 

  
i'm a complete newbie but come from a data engineering background, so i'd like to figure out of there's any where i can look into starting from or if the is a completely green field subject for now.",MachineLearning
"
Which is the best possible method to combine the embeddings of two words. Example is ""comparison"" + ""contrast"" and ""expansion"" and ""restatement"".


So I want the ML model to understand that contrast is more related to comparison and restatement is more related to expansion.

My initial institution is vector addition of the embeddings of these words. I would like to hear you feedbacks on the same.

Thanks in advance :-)",MachineLearning
"I am working on a classification churn model, I have used feature importance to show the features' importance at model fit. I used shap values to show permutation importance of each feature. But I feel it's harder to represent causation of the prediction to each feature. Any ideas ?",MachineLearning
I've a isolation forest model in production environment that trains every time we try to find anomalies and it classifies different points as anomalies. What should I do such that only most likely anomaly points are provided as results and results don't differ much? I do not want to set random state.,MachineLearning
"Hi everyone! I programmed a learning chatbot for (a clone of) the MOS KIM-1, hailing from 1976 with its 6502 CPU and 1K RAM. Basically, it works this way - you give it a byte, and it answers with a byte; at the same time, it learns from each interaction, which byte ""should"" answer which, and updates its knowledge base accordingly. It actually runs on a KIM Uno, an Arduino based clone of the KIM-1.

This is the GitHub page with the code, contained in two short programs: one (optional) to slighly pre-populate the knowledge base with about a dozen of bytes that would constitute a nucleus of original replies (to be evolved into ""your"" interactions, as you chat on), starting from $0100, as well as the actual chatbot program to be launched from $0200 (the ""user input"" byte is to be entered prior to run in $0010, and the reply will be contained after run at $0013, so yes, you are ""chatting"" in hex), in each case, both in assembler and already assembled (and ready to be entered into the KIM-1):

[https://github.com/KedalionDaimon/MOS-KIM-1-chatbot](https://github.com/KedalionDaimon/MOS-KIM-1-chatbot)

And this is my YouTube video, presenting it:

[https://www.youtube.com/watch?v=7MJgi5kua3M](https://www.youtube.com/watch?v=7MJgi5kua3M)",MachineLearning
"Hi everyone, I am having a chunk of textual conversational data which I need to analyze for topics in it.

I am currently using BERTopic to do it but the issue with it is that it does hard clustering, i.e., each datapoint belongs to exactly one topic cluster. But many of the sentences are multi intent and do fall in many topics simultaneously.

Can soft/fuzzy clustering , where each datapoint can belong to more than one topic clusters, be done via BERTopic? If yes, then how can it be implemented? If not, which other algorithms can be used?",MachineLearning
"Hi all! 

What are some interesting applications of machine learning in fields that are not typically associated with technology? 

For example, have there been any successful projects using ML in fields like sociology, psychology, or political science? If so, could you share some links or references?

Looking forwards to discovering your use-cases!",MachineLearning
"Legal datasets are extremely expensive because lawyers are, and this has bottlenecked legal NLP.

To address this, we release the Merger Agreement Understand Dataset (MAUD), with over 39,000 multiple-choice reading comprehension examples for 152 merger agreements that have been manually labeled by legal experts. The dataset was created with the help of the American Bar Association; without their help the dataset would have cost over $5,000,000 to create.

MAUD has substantial room for improvement and can could serve as a research challenge for NLP researchers without any legal background.

Dataset and Baselines: [https://github.com/TheAtticusProject/maud/](https://github.com/TheAtticusProject/maud/)

Paper: [https://arxiv.org/abs/2301.00876](https://arxiv.org/abs/2301.00876)",MachineLearning
"An open-source implementation of Google AI's T5 in PyTorch. This repository contains the architecture to train your own T5 model.

Link to the repository: [https://github.com/conceptofmind/t5-pytorch](https://github.com/conceptofmind/t5-pytorch)

T5 was first presented in Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel.

You can find a link to the paper here: [https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683)

Lucidrains was kind enough to provide feedback and review for this implementation.

Please be sure to follow and support his work: [https://github.com/lucidrains](https://github.com/lucidrains)

You can find the official T5x repository by Google AI here: [https://github.com/google-research/t5x](https://github.com/google-research/t5x)",MachineLearning
"Hi All,

What book would you recommend for feature engineering, especially for bioinformatics and structural biology?

Happy New Year and thanks in advance.",MachineLearning
"Hello mates,

Since I have hardly any background in ML, I have somewhat dummy question. My understanding is that the majority of ML is based heavily on inputs generated by humans (some exceptions here would be unsupervised learning and GANs). So, if this is a case, I wonder if ML can truly outperform humans. Of course, in certain areas, like speed of computation or accuracy, computers will also be better than humans, but I am more interested in, shall we say, more general case.

Kind regards",MachineLearning
"Hi guys, I have a set of feature values defined as x = {f\_1,f\_2,...,f\_n} (x does not contain any zero) and the goal is to measure the similarity between these features using [Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance) so x is converted to a diagonal matrix called X\_i where the diagonal elements are f\_1,f\_2,...,f\_n, therefore, the distance is measured using columns of X\_i.

Then I calculate the covariance matrix of X\_i which is semi-positive definite (SPD) but the inverse of the covariance matrix is non-SPD and Mahalanobis distance is not valid(it became negative).

Any ideas or suggestions?

Thanks.",MachineLearning
"Is there a way to do quantization in native pytorch for GPUs (Cuda)?

I know that TensorRT offers this functionality, but I would prefer working with native pytorch code. I understand from the pytorch docs, [https://pytorch.org/docs/stable/quantization.html](https://pytorch.org/docs/stable/quantization.html), that quantization for the GPU is linked to TensorRT. Given that Nvidia GPUs offer quantization for some time now, it's find it difficult to believe that no other solid implementation for quantization other than TensorRT exists. Grateful for any pointer or suggestions.",MachineLearning
"**Speechbox** is built on the premise that [Whisper](https://github.com/openai/whisper) is good enough to pretty much transcribe any English speech. Furthermore, Whisper was trained to predict **punctuated** and **orthographic** text.

&#x200B;

Speechbox leverages Whisper's quality to **""unnormalize""** audio transcriptions (see examples below) to make them more useful for further downstream applications while guaranteeing that the exact same words are being used.

""we are going to the san francisco beach"" can have multiple meanings:

=> 

1. We are going to the San Francisco beach!
2. We are going to the San Francisco beach?
3. We are going to the San Francisco beach.

&#x200B;

Speechbox will pick the correct one for you 😉

&#x200B;

👉 GitHub: [https://github.com/huggingface/speechbox](https://github.com/huggingface/speechbox)

🤗 Demo: [https://huggingface.co/spaces/speechbox/whisper-restore-punctuation](https://huggingface.co/spaces/speechbox/whisper-restore-punctuation)",MachineLearning
"At AWS we released **Fortuna,** a library for **Uncertainty Quantification**. Fortuna supports conformal prediction, Bayesian inference methods and more.  
Try it out! GitHub stars are very welcome!!! ⭐⭐⭐

Github repo: [https://github.com/awslabs/fortuna](https://github.com/awslabs/fortuna)",MachineLearning
"Found this https://data.nasdaq.com/databases/AS500/data, but it’s $1,000 a year. Anything free? Doesn’t have to be one-minute, just granular",MachineLearning
"Hi,  


As part of Huggingface whisper finetuning event I created a demo where you can:  


1. Download youtube video with a given URL

##### 2. Watch downloaded video in the first video component

##### 3. Run automatic speech recognition on the video using Whisper models from ggerganov [https://github.com/ggerganov/whisper.cpp](https://github.com/ggerganov/whisper.cpp)

##### 4. Translate the recognized transcriptions to 26 languages supported by deepL

5. Download generated subtitle files in .srt and .vtt formats

##### 6. Watch the video in another video component with added subtitles

  
You can test it from here  
\--> [https://huggingface.co/spaces/RASMUS/Whisper-youtube-crosslingual-subtitles](https://huggingface.co/spaces/RASMUS/Whisper-youtube-crosslingual-subtitles) <--",MachineLearning
"I have tried to post reminders to both chairs and reviewers, but no one seems to care about my rebuttal and revision. What an exciting experience!",MachineLearning
Tensorflow 1.x is no longer supported on google colab.  I want to use create\_hparams from hparams and it has contrib module that has been removed in Tensorflow 2.x What shall be the alternative to this?,MachineLearning
"Hello everyone, I am working on a task to generate questions on some documents, currently manual creation of questions by domain experts is a very time consuming process, could anyone please suggest some good NLP models which can generate human like questions given a text.",MachineLearning
"I created my own standard CNN. It can run on either C# or compute shader(GPU) in Unity. I am an experienced coder but I am still new to neural networks but I coded my own to help build my understanding of them.

The Model:

I am attempting to train the network to complete sentences of given text. (this may be the wrong approach). I am training the network with film dialog. I first sort through the dialog and convert the words into indexes for a list of all words in the dataset. The first input layer is 50 (Max of 50 words from user) and the output is just one index which is supposed to be the predicted word. The word indexes are normalized to be from 0-1 based on max word value.  Training goes through every sentence word by word, and tells the network the next word after as the answer.

The Problem:

This is working well in small scale tests, but exponentially falls apart in larger scale models. The reason is that the more total words there are, the more exponentially sensitive the normalized indexes become since it's in the range of 0-1. Which makes training the network near impossible once there are thousands of words.

Is there some simple solution I am missing when it comes to outputing sensitive index values? Any helps/ideas would be greatly appreciated.",MachineLearning
"Greetings!

&#x200B;

In my adventures of Pytorch, and supporting ML workloads in my day to day job, I wanted to continue homelabbing and buildout a compute node to run ML benchmarks and jobs on.

&#x200B;

This brought me to the AMD MI25, and for $100 USD it was surprising what amount of horsepower, and vRAM you could get for the price.  Hopefully my write up will help someone in the machine learning community. 

&#x200B;

Let me know if you have any questions or need any help with a GPU compute setup.  I'd be happy to assist!

&#x200B;

[https://www.zb-c.tech/2022/11/20/amd-instinct-mi25-machine-learning-setup-on-the-cheap/](https://www.zb-c.tech/2022/11/20/amd-instinct-mi25-machine-learning-setup-on-the-cheap/)",MachineLearning
"I recently came across this paper [Are Transformers Effective for Time Series Forecasting?](https://arxiv.org/abs/2205.13504) and it seems to cast doubt on the recent trend of using transformers for time series forecasting, suggesting a simple model can out perform complex transformers.

Personally, in many of my experiments using transformers on temporal data besides the quite commonly tested benchmarks (ETH, exchange, etc) they perform poorly compared to other simple(r) models like GRUs or DA-RNN. Yet we are still seeing an explosion of papers about them in the research community. Are there other recent deep learning based alternatives?",MachineLearning
"Point-E leverages diffusion models to generate synthetic views and 3D point clouds. Using text input, it generates an image, which is then used as a reference for generating the 3D point cloud. ([Learn more about Point-E](https://dagshub.com/blog/overview-of-point-e/))  


We were so excited about the results (and overall coolness 😎) of Point-E, that we decided to share the fun with EVERYONE!  


https://reddit.com/link/102l6ne/video/tu4h2bksjw9a1/player

My team built and deployed a Streamlit app to generate a 3D point cloud model from images using Point-E 🤖 .  This process takes only 1-2 minutes on a single GPU, making it much faster than previous state-of-the-art methods.   


Check it out: [https://point-e.public.dagshubusercontent.com/](https://point-e.public.dagshubusercontent.com/)",MachineLearning
"MAgent2 is the maintained fork of the environments in [https://github.com/geek-ai/MAgent](https://github.com/geek-ai/MAgent), which previously were housed in PettingZoo itself but as of a few months ago was broken off into it's own project. You can check it out here: [https://magent2.farama.org/](https://magent2.farama.org/) / [https://github.com/Farama-Foundation/magent2](https://github.com/Farama-Foundation/magent2)",MachineLearning
"&#x200B;

[Logical Word Embedding with Tsetlin Machine Autoencoder](https://preview.redd.it/x4zytt7vou9a1.png?width=1506&format=png&auto=webp&v=enabled&s=58f817a722dca8c5ad866fec6de9d42d1ec2d339)

Here is a new self-supervised machine learning approach that captures word meaning with concise logical expressions. The logical expressions consist of contextual words like “black,” “cup,” and “hot” to define other words like “coffee,” thus being human-understandable. I raise the question in the heading because our logical embedding performs competitively on several intrinsic and extrinsic benchmarks, matching pre-trained GLoVe embeddings on six downstream classification tasks. You find the paper here: [https://arxiv.org/abs/2301.00709](https://arxiv.org/abs/2301.00709), an implementation of the Tsetlin Machine Autoencoder here: [https://github.com/cair/tmu](https://github.com/cair/tmu), and a simple word embedding demo here: [https://github.com/cair/tmu/blob/main/examples/IMDbAutoEncoderDemo.py](https://github.com/cair/tmu/blob/main/examples/IMDbAutoEncoderDemo.py)",MachineLearning
" Hi,

I faced a classification problem like this: Given a measurement of 18K different variables of 42 samples, each sample is classified as class\_0 or class\_1, divided near equally (19 belongs to class\_0, 23 belongs to class\_1) what is the right approach to eliminate these features to a minimum level, so that the classifier is still predicting correct classses.

I do not provide any domain knowledge for now, but can hint a little bit more, if needed.",MachineLearning
"I need to create and implement my own dataset in the Denoising Diffusion Implicit Models created by András Béres. I'm using the official Colab Notebook from the keras website. At the following point, you need to specify the dataset for the data pipeline that's being used in the training process.

    def preprocess_image(data):
        # center crop image
        height = tf.shape(data[""image""])[0]
        width = tf.shape(data[""image""])[1]
        crop_size = tf.minimum(height, width)
        image = tf.image.crop_to_bounding_box(
            data[""image""],
            (height - crop_size) // 2,
            (width - crop_size) // 2,
            crop_size,
            crop_size,
        )
    
        # resize and clip
        # for image downsampling it is important to turn on antialiasing
        image = tf.image.resize(image, size=[image_size, image_size], antialias=True)
        return tf.clip_by_value(image / 255.0, 0.0, 1.0)
    
    
    def prepare_dataset(split):
        # the validation dataset is shuffled as well, because data order matters
        # for the KID estimation
        return (
            tfds.load(dataset_name, split=split, shuffle_files=True)
            .map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
            .cache()
            .repeat(dataset_repetitions)
            .shuffle(10 * batch_size)
            .batch(batch_size, drop_remainder=True)
            .prefetch(buffer_size=tf.data.AUTOTUNE)
        )
    
    
    # load dataset
    train_dataset = prepare_dataset(""train[:80%]+validation[:80%]+test[:80%]"")
    val_dataset = prepare_dataset(""train[80%:]+validation[80%:]+test[80%:]"")        
    I tried creating and implementing my own Dataset, but it didn't work. Some help would be really appreciated. This is the official website for the set im using https://keras.io/examples/generative/ddim/

I tried creating and implementing my own Dataset, but it didn't work. Some help would be really appreciated. This is the official website for the set im using https://keras.io/examples/generative/ddim/",MachineLearning
"There were many great papers this past year in the field, but below (and in the video) are five papers that may have been overlooked. (YT video: [https://www.youtube.com/watch?v=XnUf9twdchI](https://www.youtube.com/watch?v=XnUf9twdchI))  


Papers are linked as follows

* Badreddine et al., ""Logic Tensor Networks (journal version),"" AIJ 2022 [https://www.sciencedirect.com/science/article/abs/pii/S0004370221002009](https://www.sciencedirect.com/science/article/abs/pii/S0004370221002009)
* Sen et al., ""Logical Neural Networks for Knowledge Base Completion with Embeddings & Rules,"" EMNLP 2022 [https://preview.aclanthology.org/emnlp-22-ingestion/2022.emnlp-main.255.pdf](https://preview.aclanthology.org/emnlp-22-ingestion/2022.emnlp-main.255.pdf)
* Kamienny et al., ""End-to-end Symbolic Regression with Transformers,"" NeurIPS 2022 [https://openreview.net/pdf?id=GoOuIrDHG\_Y](https://openreview.net/pdf?id=GoOuIrDHG_Y)
* Nandwani et al., ""A Solver-Free Framework for Scalable Learning in Neural ILP Architectures,"" NeurIPS 2022 [https://openreview.net/pdf?id=EqZuN4V\_FLF](https://openreview.net/pdf?id=EqZuN4V_FLF)
* Shakarian and Simari, ""Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture,"" TransAI 2022 [https://ieeexplore.ieee.org/document/9951514](https://ieeexplore.ieee.org/document/9951514)

What interesting papers do you think were overlooked this past year?",MachineLearning
"Paper : [https://arxiv.org/abs/2301.00774](https://arxiv.org/abs/2301.00774)

Abstract :

>We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. When executing SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, we can reach 60% sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches.",MachineLearning
"Hello community!

Just want to tell you that I have recently created a Bayesian optimization community [https://www.reddit.com/r/BayesianOptimization/](https://www.reddit.com/r/BayesianOptimization/)  that you may find interesting. The purpose is to discuss research and  applications of Bayesian optimization, from an academic and industry  point of view.

Best!",MachineLearning
"The onset of the Covid pandemic brought a profound shock to the financial markets in early 2020. Major indices and stocks took a resounding hit, with SP500 showing a decline of about 34% from its February high to its March 23 bottom. Following the initial shock, though, many stocks exhibited a strong recovery on the back of interest rate cuts by the Fed and other government policies.

This analysis aims to uncover groups among the S&P 500 stocks in their drop and rebound trajectory shown during Covid and identify the drivers behind them. While sector information provides an intrinsic notion of clustering, it does not capture patterns spanning across sectors. Hence the study leverages Time Series Clustering to temporally cluster the stock prices into ’n’ buckets: found through the elbow plot.

&#x200B;

[https://medium.com/@ashish1610dhiman/time-series-clustering-of-stock-behaviour-during-covid-9bd25b8c7a5](https://medium.com/@ashish1610dhiman/time-series-clustering-of-stock-behaviour-during-covid-9bd25b8c7a5)",MachineLearning
"I have released v0.0.1-alpha of [AIJack](https://github.com/Koukyosyumei/AIJack), an OSS framework to simulate various attacks and defenses against machine learning models. I have implemented more than 30 algorithms, such as *Model Inversion*, *Poisoning Attack,* *Evasion Attack*, *Federated Learning*, *Split Learning*, *Differential Privacy*, and *Homomorphic Encryption*. You can easily experiment with various combinations of attack and defense techniques. We will also support not only standard single-process but also MPI-backend.

I am looking forward to your feedback!",MachineLearning
Has anyone else noticed a steep drop in remote job postings. It seems like we're at a tick above pre-pandemic levels. Or is it just me. All the appealing jobs are 2 thousand miles away.,MachineLearning
"Usually when you approximate the score s(x,t) in Diffusion models, the time t is passed through an embedding network before it is added to the x components in the res net blocks of your model.

What is the rationale behind this? Couldnt you just concatenate x and t in the channel dimension? And If you were to use any other model than a UNet, what would be the equivalent?",MachineLearning
"I am trying to build a hierarchial model, for this I am training a hierarchical tree and each node will have a model for that specific hierarchy. My question is: how will I choose the best model for each node? My idea was to test using the fold I split (I'm using k-fold), but what data will I use to test the final tree later?",MachineLearning
"I'm new to embedding and vector databases. I had this initial thought that I'd be able to provide some sort of parameter that was used as a threshold to determine the cutoff between what a large cluster and what isn't, and that I could just get back a summary of those top clusters and try to then sift through them to figure out what each cluster represented. The more I dig in, I'm not really able t find a way to do this. Seems like vector databases, like Pinecone, expect you to provide it some query vectors to get what you want. Are there any tools to help me easily grab the largest clusters in my vector database versus me seeking them out?",MachineLearning
"Hi ML Community out there, I'm 27 M. I'm a machine learning incoming PhD student based out of Germany.  I have been trying to get into a PhD program since last 4 years (since 2018). When I gave up and got into a masters instead with 3yoe. Now I'm about to complete my masters thesis and start a PhD. 

I already have mixed feelings about my PhD journey now. I have gotten a really good opportunity to do ML theory at University of Saarlandes in Germany. Initially, when I had started, I was really driven to understand maths and underlying concepts of ML. However, more recently I have become more confused about the value of theoretical research and it's ""real"" impact. Of what skills I would have in my career later that would make me desirable to employers (considering I may not get tenured in academia). 

Is it more advisable in long run to stay and get your PhD or just leave and join some ML role in industry that takes Masters guys and get some real world experience on how to use ML to generate business value ? 

There is this added fomo of being 27 and just starting my PhD where I lag in the advantage of age to take high risk bets on things. Doing a PhD in theoretical ML could possibly mean I am very likely to be only employable in select places and this gives me a fear of trying to reinvent myself in my mid 30s.

Any suggestions on pros and cons of ML research in academia vs a ML industry job of a masters grad would be really helpful!",MachineLearning
"Hello,

I am a phd student working on a project that revolves around the correction of geometrical distortions on images, more specifically the goal is to correct cylindrical distortions in QR Codes in order to improve decoding sucess rate. 

So far I implemented traditional methods and got interesting results, but I'm now interested in using machine learning to tackle this problem and since I'm still relatively new to machine learning I would like to hear your feedback/opinions on the subject aswell as sugestions on reading material to start.

So far from my limited research on the matter, I believe a generative adversarial network would probably be the right choice for this problem, but again I'm not sure and I'm really open to all sugestions/ideas.",MachineLearning
" 

Hello everyone !

I recently heard about [Voice.AI](https://voice.ai/), but I hate that it's behind a paywall and not open.

Are there any open-source alternatives to it ?

Thanks !",MachineLearning
"So I will directly dive into the problem setting and will later describe the background:

data:
survival data of ~3000 patients with several clinical and lab(blood) parameters.

question:
does 1 of the parameters has any influence on the survival time?

what I have done so far:
non proportional multivariate hazard model (Cox regression)
problem: highly correlated variables, strong time interaction, some hardly not normal distributed variables (even after transformation)


QUESTION:
Is there a machine learning / AI solution for this problem question?

background:
I am a PhD student in medicine and did intensive mathematics together with my colleagues. But we only had one „old-fashioned“ statistics professor, who answered us for our problem: „seems like your data isn‘t good enough, and you can‘t explore something there, cause its far too complex“
We first want to get an intuition if our theoretical findings can be proved in the data, before we will plan a new study.
I reformulated our problem a bit, we are not dealing with the death of patients, but with time of an specific event.


I am really grateful for any ideas, any sources where to look at and everything which could help 😊😊
Thanks in advance!",MachineLearning
"Anyone knows an updated review of model comparison techniques, like the review that we can see in this one of Raschka: Model Evaluation, Model Selection, and Algorithm Selection in Machine Learningor this other one from Dietterich: Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms. Thanks a lot in advance.",MachineLearning
"Hello, community!

I invite you to take a look at the live coding tutorial video on Diffusion Models

[Link to the video](https://youtu.be/YCPKehpaE4o)

Content covered:

\- Theoretical background

\- Implementation of forward diffusion process

\- Implementation of the training loop

\- Overfitting to one batch

\- Implementation of the reverse diffusion process

\- Training on CIFAR10 dataset (with class label conditioning)",MachineLearning
"I'm new in this field and just see the paper Importance Weighted Autoencoders ([https://arxiv.org/abs/1509.00519](https://arxiv.org/abs/1509.00519)).

While reading the paper I have 2 questions and I think a lot and also googled, but cannot find any clues or answers so it would be great if someone can help!

&#x200B;

1. Why the second term q(h|x) encourages the encoder to have a spread-out distribution?

To maximize the objective or minimize the loss, the encoder q(h|x) will be trained to be small.

I don't know how this could be connected to spread-out distribution

&#x200B;

https://preview.redd.it/81ykicq3vm9a1.png?width=1796&format=png&auto=webp&v=enabled&s=d6e154e8196fcbdd482c324f3c1a2409b991c631

2. How to measure activity of latent dimension?

I roughly know about covariance, so this is the first time about the notation of Covariance\_{x} and I don't understand how to calculate it.

I can understand E\_{u\~q(u|x)}(u) which is expectation of specific latent dimension given input x.

But what do I have to do next?

I thought about the concept 'distribution to change depending on the observation', measuring the variance of specific latent dimension of all the input x but it's notation is not right.

How can I calculate this one?

&#x200B;

https://preview.redd.it/ltxxm451vm9a1.png?width=1770&format=png&auto=webp&v=enabled&s=67757bbd8af5e1fc3198b351747c6cc5b4cf37cc",MachineLearning
"Today a PR opened to Pytorch to formally introduce the FP8 data type.

* Current text: [Proposal of fp8 dtype introduction to PyTorch](https://github.com/pytorch/rfcs/blob/83270da493cf4203b79d5c214ed96e6d62c86fb8/RFC-0030-native-fp8-dtype.md)
* PR: [https://github.com/pytorch/rfcs/pull/51](https://github.com/pytorch/rfcs/pull/51)

# Summary

More and more companies working on Deep Learning accelerators are experimenting with 8-bit floating point numbers usage in training and inference. Results of these experiments are presented in many papers published in the last few years.

Since fp8 data type seems to be a natural evolution of currently used fp16/bf16, to reduce computation of big DL models, it’s worth to standardize this type. Few attempts of this were done recently:

* Nvidia, Arm and Intel - [https://arxiv.org/pdf/2209.05433.pdf](https://arxiv.org/pdf/2209.05433.pdf)
* GraphCore and AMD - [https://arxiv.org/pdf/2206.02915.pdf](https://arxiv.org/pdf/2206.02915.pdf)
* Tesla - [https://tesla-cdn.thron.com/static/MXMU3S\_tesla-dojo-technology\_1WDVZN.pdf](https://tesla-cdn.thron.com/static/MXMU3S_tesla-dojo-technology_1WDVZN.pdf)

This RFC proposes adding two 8-bit floating point data types variants to PyTorch, based on the Nvidia/Arm/Intel paper. It’s important to consider these two variants, because they’re already known to be used by Nvidia H100 and Intel Gaudi2 accelerators.",MachineLearning
I was wondering what each individual does while they wait for the model to train because thats what I am waiting for (ETA 5 DAYS),MachineLearning
"Here is a PyTorch reproduced version of PyramidAT.

Repository link: [https://github.com/kdhht2334/Pyramid\_AT](https://github.com/kdhht2334/Pyramid_AT)

(Original) Paper link: [https://arxiv.org/abs/2111.15121](https://arxiv.org/abs/2111.15121)",MachineLearning
"I need a model that can answer questions regarding a given text (1-2 pages long) in a conversational manner (e.g. ChatGPT) with no additional training/fine-tuning for each different text. 

In many LLMs, an easy way to achieve this is to add the given text as a ""header"" in the initial prompt (e.g. as ChatGPT authors do). However, I guess there's a limit on the length of such a prompt, after which model performance degrades. Of course, if there's a model that can handle prompt ""headers"" of such length that would work fine for me.

Can you point me to anything related to this task?",MachineLearning
"I am interested in the literature or seeing some examples of models capable of extracting discrete action items from text conversations. Action items can be making an appointment, starting a call, etc.

For example, Gmail is able to recognize when people are arranging an appointment in an email chat and it suggests that to be added to the calendar (action item)

What type of models perform such tasks? Could you point me to the relevant literature?",MachineLearning
"Hey guys!

I recently published a website containing machine-learning illustrations! They aim to be ""that"" resource on which you can rely whenever you need to brush-up on these topics  (e.g. technical interviews, exams, or whatever).

[https://illustrated-machine-learning.github.io/](https://illustrated-machine-learning.github.io/)

Other than just spamming the link, I was curious to receive some feedbacks about the website and the illustrations. 😁

I really struggled to find a valid single resource anytime I need something across different topics, therefore my ambition is to create it!

Update:

Examples of illustrations are:

* svm: [https://illustrated-machine-learning.github.io/pages/machine-learning/linear-algorithms.html#support-vector-machines](https://illustrated-machine-learning.github.io/pages/machine-learning/linear-algorithms.html#support-vector-machines)
* bias-variance: [https://illustrated-machine-learning.github.io/pages/machine-learning/bias-variance.html](https://illustrated-machine-learning.github.io/pages/machine-learning/bias-variance.html)
* decision-tree: [https://illustrated-machine-learning.github.io/pages/machine-learning/decision-tree.html](https://illustrated-machine-learning.github.io/pages/machine-learning/decision-tree.html)

Update 2.0

Thank you for your feedbacks! I already (brutally) solved your main concern about the list of contests not visible. I will spend more time on it as soon as I can, in order to make it smoother and better!

Update 3.0

Prooobably I finally understood your problems with the sidebar button, and I tried to make more visible by making it darker and bigger. Is that better now? ",MachineLearning
"Hi. The recent incredible improvements in AI and ML has resurrected an old project of mine to read the back panel of electronic components like this AV receiver and spit out logically formed text to describe each I/O. I have a LOT of direct experience this specific issue as well as general software experience but no AI/ML development experience.

I know it is possible but on a scale of 1-10 how hard? Any new tools make this easier? Ultimately I want to feed the AI pictures of electronic back panels and get formatted text back.

Thanks!",MachineLearning
"The classic way of incorporating sequential context into a transformer model is to make an encoder-decoder transformer, where the context is processed by the encoder component, and then read into the decoder blocks via cross-attention.

However, there are a number of situations where you might want to incorporate non-sequential context into a model - for example, you might want a language model that can generate text conditioned on some input vector describing the person whose text you are trying to emulate, or you might want to condition on some single vector that summarizes all text that occurred prior to the context window. What are standard ways of incorporating such context? I'd also be interested in standard ways of incorporating such context in other sequence models like RNNs with LSTM blocks.",MachineLearning
"I am seeking insights and best practices for data preprocessing and cleaning in PDF documents. I am interested in extracting only the body text content from a PDF and discarding everything else, such as page numbers, footnotes, headers, and footers (see attached image for an example of semantically meaningful sections).

I have noticed that in Microsoft Word, a user can simply drag in a PDF and Word seems to automatically understand which parts are headers, footnotes, etc. I am speculating that Word may be utilizing machine learning techniques to analyze the layout and formatting of the PDF and classify different sections accordingly. Alternatively, Word may be utilizing pre-defined rules or patterns to identify common elements such as headers and footnotes. I know of related techniques for example to extract layout information from receipts and the like (LayoutLM, Xu et al., [https://arxiv.org/abs/1912.13318](https://arxiv.org/abs/1912.13318)) and tabular data (TableNet, Paliwal et al., [https://ieeexplore.ieee.org/document/8978013](https://ieeexplore.ieee.org/document/8978013)), but nothing to solve layout extraction in this particular domain.

I am curious to know if there are any techniques or algorithms that can replicate this behavior in Word. Any suggestions or recommendations for data cleaning in PDF documents, would be greatly appreciated. 

[Image of PDF with semantically meaningful sections](https://preview.redd.it/ct7y0kc5gh9a1.png?width=1474&format=png&auto=webp&v=enabled&s=c9eb6b062ccd4c5954edfc81a886d75ba451314d)",MachineLearning
" 

I am not sure if i've been doing something wrong over the past few days, but Google Colab hasn't been able to use gdown to download files from my Google Drive account even with the files being set to ""anyone with the link can view"".

They don't work if the files are hosted someone else's drive either. I've even downloaded them from someone else's drive, reuploaded the files to my own drive, and it still fails.

When I visit the link in the browser the file downloads perfectly fine. With or without my account being logged in. Does anyone know what's going on?",MachineLearning
"Bhusan  Chettri explains how Machine Learning and Artificial Intelligence can  be used to build automatic systems for detection of insect and plant  diseases in agricultural farming. He further discusses its advantages  over traditional methods and also talks about potential demerits.  Automatic insect and plant disease detection using Artificial  Intelligence is an emerging field that is gaining popularity in the  agriculture sector. The ability to accurately and efficiently detect  insect infestations and plant diseases has numerous applications,  including improved crop yields, reduced use of pesticides, and early  detection of potential epidemics. Bhusan Chettri says, “In order to  understand the automatic detection of insects and plant diseases using  AI, it is important to first understand the basics of artificial  intelligence. Artificial intelligence, or AI, is a field of computer  science that focuses on the development of algorithms and systems that  are capable of intelligent behaviour. This can include tasks such as  learning, problem-solving, and decision-making.”

The  traditional method of detecting pests and diseases in plants involves  manual inspection by trained personnel. This method is time-consuming,  labour-intensive, and prone to errors. AI-based automatic detection  techniques can significantly improve the accuracy and efficiency of pest  and disease detection in plants. Researchers have proposed various  algorithms and methods for automatic insect and plant disease detection  using AI. One of the most common methods is the use of image-based  techniques, where the AI system is trained to recognize the visual  features of different pests and diseases from a large dataset of images.  Another popular method is the use of sensors and other environmental  data to identify the presence of pests and diseases in plants.

In  the context of automatic insect and plant disease detection, AI  algorithms and systems can be trained to recognize patterns in images or  other data that are indicative of an infestation or disease. This can  be done through a variety of methods, including supervised learning,  where the algorithm is trained on a large dataset of labelled examples,  or unsupervised learning, where the algorithm learns to identify  patterns on its own. There are many different algorithms and methods  that have been used in the literature by researchers working on  automatic insect and plant disease detection. In recent years,  researchers have been working on developing AI-based algorithms and  methods for detecting insects and plant diseases. The algorithms and  methods used in the literature vary depending on the specific problem at  hand and the type of data used. Some of the commonly used methods  include image processing, machine learning, and deep learning. Image  processing algorithms are used to detect and classify insects and plant  diseases based on their visual characteristics. These algorithms use  various techniques, such as colour analysis, edge detection, and texture  analysis, to extract relevant features from images. Machine learning  algorithms, on the other hand, use training data to learn the patterns  and characteristics of insects and plant diseases and make predictions  based on these patterns. Deep learning algorithms, which are a type of  machine learning algorithms, use neural networks to learn complex  patterns and make more accurate predictions. Some of the most common  methods include convolutional neural networks, which are a type of deep  learning algorithm that is particularly well-suited for image  recognition tasks, and support vector machines, which are a type of  machine learning algorithm that can be used for classification tasks.

Bhusan  Chettri explains that the use of AI-based methods for insect and plant  disease detection has several advantages. For instance, AI algorithms  can process large amounts of data quickly and accurately, and can make  predictions in real-time. This can be especially useful in the case of  detecting insect infestations, where a large number of plants may need  to be inspected. This enables farmers and agricultural experts to  identify and address insect and plant disease issues quickly and  effectively. Additionally, AI algorithms can be trained on a large  number of images  to recognize patterns that may be difficult for humans  to identify, potentially leading to more accurate detection of diseases  and infestations. This further allows them to learn and adapt to  different conditions and scenarios.

However,  there are also some disadvantages to using AI for automatic insect and  plant disease detection. One potential disadvantage is the cost and  complexity of developing and training AI algorithms, which can require  significant amounts of time and resources. Additionally, the accuracy of  AI algorithms can be dependent on the quality and diversity of the  training data, and there may be potential issues with bias in the data.  In conclusion, automatic insect and plant disease detection using  artificial intelligence has the potential to greatly improve crop yields  and reduce the use of pesticides. However, the development and  implementation of AI algorithms for this purpose requires careful  consideration of the potential merits and demerits of different methods.

“In  conclusion, the use of AI in insect and plant disease detection has the  potential to improve accuracy, efficiency, and cost-effectiveness in  the agriculture sector. However, further research and development is  needed to overcome the challenges and limitations associated with the  use of AI in this field.”, says Bhusan Chettri who is an AI researcher  exploring applications of AI and Machine Learning for soil and wildlife  monitoring.",MachineLearning
"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",MachineLearning
"Say we have two distributions of segmentation annotations (i.e., a bunch of segmentation maps) which I want to establish are 'similar'. To be more specific, I'm working on a research project where we have in-house annotations for images from a public dataset and I want to quanitatively establish that our annotations and the dataset annotations differ in similar ways, or that our annotations 'fall into the distribution of the current dataset'. 

(I'm aware that I can measure similarity between distributions with a measure like KL-divergence, but what I'm not sure about is how I would establish what level is 'similar enough'.)",MachineLearning
"Correct me if any of these priors are wrong:

* Every problem solvable by a neural network is provably solvable in code, although not necessarily in a useful way - at worst you could generate the pytorch source code and the model weights.  

* Neural networks can discover algorithms during training, and use them internally to accomplish the task. This happens emergently in today's large transformer models; it's part of learning how to solve the problem.

* While neural networks can do a lot of things that classical algorithms can't, there's also a lot of things that *both* can do - pathfinding for example. Maybe there's more yet-unknown overlap between them.

Stripping away the neural network and running the underlying algorithm could be useful, since classical algorithms tend to run much faster and with less memory. 

Has there been any research into converting neural networks into code that accomplishes the same thing? My first thought would be to train a network to take another neural network as input and output the corresponding code. You could create a dataset for this by taking various chunks of code and training neural networks to imitate them.",MachineLearning
"Hey everyone,

I want to compare CNN's visualizations using Grad-CAM to attention heat maps from an eye-tracking tool.

I saw that I can calculate importance value for activated neurons on the visualizations.

Do you its possible to compare this data?",MachineLearning
"Website: [https://www.wordfixerbot.com](https://www.wordfixerbot.com/)

My application - WordfixerBot - was built to help users paraphrase, grammar checking, and summarise texts. The paraphraser tool currently offers basic features for language processing tools - paraphrasing tones to choose from and copy function for result text. I am currently working on adding more features to improve it.

Background story: I have always wanted to build an AI-based application, and when I first came across OpenAI GPT-3, I was amazed by its powerful NLP model, so I just gave it a try and hoped it might work out :D.

I would love to receive any feedback and any recommended features that you guys have for my site.",MachineLearning
Some time ago I saw an article saying it is not preferred to use dropout and any kind of normalization(like batch or layer) in a model. But I am not sure why. Any suggestion about that?,MachineLearning
"In this [post](https://www.reddit.com/r/MachineLearning/comments/zloof9/p_implemented_vision_transformers_from_scratch/?utm_source=share&utm_medium=web2x&context=3), I shared my implementation of Vision Transformer model from scratch in TensorFlow 2.x. After some more hours of work, I am super excited to publish the new repository. Now **you can finetune any Vision Transformer model on your custom data using just command line**. After the finetuning the model can easily be converted to TensorFlow lite ✅ and can be deployed to Android/iOS. It is worth to mention that as the model was pretrained on large datasets, you can get very good accuracy on custom dataset with finetuning. 

Below I am sharing my implementation of the whole project and I hope some of you will find it useful 😊. Please have a look and give it a star if you like it. Any advice, improvements are welcome🙂.   


Load any Vision Transformer model,

    from vit import viT
    vit_large = viT(vit_size=""ViT-LARGE32"")
    vit_large.from_pretrained(pretrained_top=True)

Finetuning on custom dataset,

    python train.py \
        --training-data dataset/training_set --test-data dataset/test_set \
        --num-classes 2 \
        --epochs 2 \
        --batch-size 16 \
        --vit-size ViT-BASE16 \
        --model-name ViT-BASE16_cat_dog \
        --save-training-stats 

The GitHub link to the project can be found [here](https://github.com/TheTensorDude/vision_transformer_tf.git).  


Thanks for reading guys. :)",MachineLearning
"Hi everyone.
I was trying to find a possible gpu-enabled version of scikit-learn, but I was surprised to see that most of the libraries are written for NVIDIA GPUs.
While nvidia GPUs are very common, I do feel that Apple’s gpu availability and the popularity of AMD demand a more thorough coverage.

I was wondering whether coding scikit-learn on top of pytorch with GPU support could be something the community would be interested in. 

I do not work for Meta.  I would just like to do something useful for the community.

Cheers.",MachineLearning
"Not sure I got the terminology right on this but by online, I mean models that automatically learn directly from user action as and when it happens. Not something which you log to later process and train the model. Gradient Boosted Decision Trees and other models require periodic re-training right?

Anyone have experience with this? Is reinforcement learning applicable here? 

This is new to me, so please ask if you have clarifications. Thanks.",MachineLearning
"I'm trying to understand how I can use COMET to evaluate translation models [https://github.com/Unbabel/COMET](https://github.com/Unbabel/COMET) ?  
I don't really understand how it was trained the meaning of the outputed values ? [https://unbabel.github.io/COMET/html/faqs.html#which-comet-model-should-i-use](https://unbabel.github.io/COMET/html/faqs.html#which-comet-model-should-i-use)  


Thanks for your help",MachineLearning
"Not sure where else to ask this as I can't find other subreddits to ask this question in. I've heard this AI voice plenty of times. It sounds pretty good and I've seen it used in some videos but I just can't find it anywhere. Play .ht has a similar voice but it doesn't flow as good as this one and makes lots of mistakes. I figured maybe someone here has experience with TTS and they ran into this one at some point. Below I am posting a sample, it's only 50 seconds long. Also, I need this one specifically and I've been searching for days but I can't find it anywhere. 

[https://sndup.net/x628/](https://sndup.net/x628/)",MachineLearning
"Hello Everyone!

Are there any research problems in language comprehension and summarization tasks which don't require much compute? I wish to play with NLP/NLU now but compute requirements are enormous.. After reading around, i found that text to video problem is being actively researched and may not require as much compute as bare language models do. Are their any novel ideas in text to video domain not requiring much compute?",MachineLearning
" Hi Everyone,

Referring from the link - [https://nlp.johnsnowlabs.com/2020/09/19/biobert\_pmc\_base\_cased.html](https://nlp.johnsnowlabs.com/2020/09/19/biobert_pmc_base_cased.html), any BERT model including Bio-BERT generates fixed size token/sentence embeddings (768 size). But I want embeddings of size 50-100 like glove. Is it possible? If it is, how?

I also have a follow-up question: If I am able to generate 50 size pre-trained embeddings, Is there any way I can generate a single embedding vector for sentence, considering only selective words in it?

for eg. sentence = ""I am having **headache** and also some signs of mild **fever**."" In this case, I want final embedding of size 100 (without any padding) generated from token embeddings (of size 50 each) of **Bolded** words. One approach I think of is concatenation but that will require padding in case more number of **Bolded** tokens present. But I need fixed size final embedding vector for a sentence.",MachineLearning
"I built an iOS app called Queryable, which integrates the CLIP model on iOS to search the Photos album offline.

[Photo searching performace of search with the help of CLIP model](https://preview.redd.it/tfvs62t7hy8a1.png?width=1892&format=png&auto=webp&v=enabled&s=fd7dc5dc0aafe13947cf06596004253a05cbdae2)

Compared to the search function of the iPhone Photos, CLIP-based album search capability is **overwhelmingly better**. With CLIP, you can search for a scene in your mind, a tone, an object, or even an emotion conveyed by the image.

How does it works?  Well, CLIP has **Text Encoder** & **Image Encoder**

>Text Encoder will  encode any text into a 1x512 dim vector  
>  
>Image Encoder will  encode any image into a 1x512 dim vector

We can calculate the proximity of a text sentence and an image by **finding the cosine similarity between** their  text vector and  image vector

The pseudo code is as follows:

    import clip
    
    # Load ViT-B-32 CLIP model
    model, preprocess = clip.load(""ViT-B/32"", device=device)
    
    # Calculate image vector & text vector
    image_feature = model.encode_image(""photo-of-a-dog.png"")
    text_feature = model.encode_text(""rainly night"")
    
    # cosine similarity
    sim = cosin_similarity(image_feature, text_feature)

To use Queryable, you need to first **build the index**, which will traverse your album, calculate all the image vectors and store. This takes place only ONCE, when searching, only one CLP forward for the user's text input query, below is a flowchart of how Queryable works：

[How does Queryable works](https://preview.redd.it/a52hempjhy8a1.png?width=1904&format=png&auto=webp&v=enabled&s=d374da8575261b1c8780946ea0c1d75723fea443)

On Privacy and security issues, Queryable is designed to be totally offline and will **Never** request network access, thereby avoiding privacy issues.

As it's a paid app, I'm sharing a few promo codes here：

    Requirement:
    - Your iOS needs to be 16.0 or above.
    - iPhone XS/XSMax or below may not working, DO NOT BUY.
    
    9W7KTA39JLET
    ALFJK3L6H7NH
    9AFYNJX63LNF
    F3FRNMTLAA4T
    9F4MYLWAHHNT
    T7NPKXNXHFRH
    3TEMNHYH7YNA
    HTNFNWWHA4HA
    T6YJEWAEYFMX
    49LTJKEFKE7Y
    
    YTHN4AMWW99Y
    WHAAXYAM3LFT
    WE6R4WNXRLRE
    RFFK66KMFXLH
    4FHT9X6W6TT4
    N43YHHRA9PRY
    9MNXPAJWNRKY
    PPPRXAY43JW9
    JYTNF93XWNP3
    W9NEWENJTJ3X

Hope you guys find it's useful.",MachineLearning
"We all know scripting is great for automating some easy tasks, but have you ever built or contributed to an ML model for personal use to improve or ease your life? 

Simple or complex, I'd love to hear about it!",MachineLearning
"If the tokens correspond to channels (extracted by some set of conv layers), then this would seem to make the inputs to the transformer much more interpretable. The features that a channel ends up encoding can be studied whereas a spatial location is just a spatial location.",MachineLearning
"Has the research community embraced any of the frameworks or findings published by [Anthropic](https://www.anthropic.com/#papers) at all? Google Scholar seems to indicate no, but I'm curious. I work on the applied side and not on the research side, so I don't have a good sense for how influential their work on interpretability is. 

The motivation for my question is that they have a huge amount of funding (although how long that will last after SBF's downfall remains to be seen) and a lot of press attention and fans in the rationalist/EA communities, but my feeling is that their work is largely not being adopted or cited in AI research. If I am correct in this, I'm curious if this is because it is seen as unoriginal, incorrect, or misguided? Or is there something else going on?",MachineLearning
"Paper: [https://arxiv.org/abs/2212.13894](https://arxiv.org/abs/2212.13894) 

Abstract:

>Remarkable progress has been made on automated reasoning with knowledge specified as unstructured, natural text, by using the power of large language models (LMs) coupled with methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from the intended conclusion to the set of axioms that support it) is significantly more efficient at proof-finding problems. We import this intuition into the LM setting and develop a **Backward Chaining algorithm, which we call LAMBADA**, that decomposes **reasoning into four sub-modules**, each of which can be simply implemented by few-shot prompted LM inference. We show that **LAMBADA achieves massive accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.** 

https://preview.redd.it/q3ul0czx4w8a1.jpg?width=542&format=pjpg&auto=webp&v=enabled&s=cdac63939f959819b13e6eb6aa7fdc5a3a6f296f

https://preview.redd.it/bqb28jzx4w8a1.jpg?width=539&format=pjpg&auto=webp&v=enabled&s=78e55a1b4782afc182de731937970fadbe458a78

https://preview.redd.it/nfx5jmzx4w8a1.jpg?width=435&format=pjpg&auto=webp&v=enabled&s=836781cb151fa8b0c9c41246c5366b07fc20f0df

https://preview.redd.it/yd0zrfzx4w8a1.jpg?width=964&format=pjpg&auto=webp&v=enabled&s=f28b5cba41af44aad886b551137256c10e4199e1

https://preview.redd.it/34x4nlzx4w8a1.jpg?width=481&format=pjpg&auto=webp&v=enabled&s=3d6c2fe0e848b9bfb47f7a730430e9d6ed3b541f

https://preview.redd.it/6tdhlkzx4w8a1.jpg?width=544&format=pjpg&auto=webp&v=enabled&s=19f9b3b2b73bede0502890cf0fd6ce47c479e89d",MachineLearning
" 

Hi all,

This is my first post in this community, so please be gentle. I'm currently experimenting with CenterNet (Objects as Points, [https://arxiv.org/abs/1904.07850](https://arxiv.org/abs/1904.07850)) and different backbone architectures, especially lightweight ones.

I use Keras for training and Tensorboard for visualization of the training. The network outputs a heatmap, offset and box dimensions and I train on PascalVoc 2007+2012 (20 classes). I visualize the heatmaps in Tensorboard by multiplying with a vector for 20 colors in order to get a RGB image.

While doing so I notice a cross shaped activation (not always there, depending on the input image, see attached pic). This cross is also seen in the later training stage and it results in false positives. I already change resolution, but this didn't have any impact.

I'm wondering if this could be related to my backbone architecture? So far I used ShuffleNet V1 and a stacked hourglass, both with bilinear upsampling. What puzzles me is the perfect symmetry, the cross is exactly in the middle. Optimizer is Adam and learnrate is 1e-4.

Any clue would be highly appreciated. Thanks!!!",MachineLearning
"Saw [this tweet](https://twitter.com/typedfemale/status/1606741028580790274?s=19) where it says that with some ""quirky tricks"" Nesterov can be obtained as a special case of PID control. I did a google search but it returned nothing of relevance. 

Is this a popular result in optimisation I'm not aware of? Or have I just not looked hard enough? If someone can point me to relevant references, that'll be great.",MachineLearning
"Hello everyone,

I have spent some time trying to figure out how to calibrate my multi-class prediction model, which predicts K values between 0 and 1 for K classes (which haven't been softmaxed).

As far as I understand, I can train a model and calibrate it post-training, i.e. training and calibration are completely independent. Is that right? If yes, I'm wondering what is the current SOTA to calibrate my model? It seems like there is up-to-date resource and I am too new to the field to find the ""best"" method.

Thanks in advance!",MachineLearning
"Hey everyone,

I wanted to bring up the issue of AI taking people's jobs and the potential consequences of this trend. As AI technology continues to advance, it's becoming more common for companies to replace human workers with software and robots. While this can lead to increased efficiency and cost savings for businesses, it also means that many people are losing their jobs and struggling to find new employment.

One of the main concerns with AI taking people's jobs is the impact it will have on the economy. As more people become unemployed, they will have less money to spend, which can lead to a decrease in consumer spending and a slowdown in economic growth. Additionally, the displacement of human workers by AI can lead to increased income inequality, as those who are able to adapt to the changing job market and work with AI may benefit, while those who are unable to do so may be left behind.

There are also ethical concerns to consider. Should we be creating technology that takes people's jobs and leaves them without a source of income? Is it fair to put the burden of adapting to the changing job market on individuals, rather than on businesses or governments?

I'm interested in hearing your thoughts on this issue. Do you think AI taking people's jobs is a problem that needs to be addressed? If so, how do you think it should be addressed?",MachineLearning
"Foundational models can generate realistic images from prompts, but do these models *understand* their own drawings? Generating SVG (Scalable Vector Graphics) gives us a unique opportunity to ask this question. SVG is programmatic, consisting of circles, rectangles, and lines. Therefore, the model must schematically decompose the target object into meaningful parts, approximating each part using simple shapes, then arrange the parts together in a meaningful way.  


Check out the blog (5min read) for the full report [https://medium.com/p/74ec9ca106b4](https://medium.com/p/74ec9ca106b4) 

tl;dr:  
GPT can symbolically decompose an object into parts, is okay at approximating the parts using SVG, is bad at putting the parts together, and is Egyptian.

be happy to take some comments and QA here :D

\--evan",MachineLearning
"Hey everyone!

Joe and I are students at Stanford, and we finally got a breakthrough on our side project.

We call it:

ChatBCG: Generative AI for Slides ✨

or: Text-to-PowerPoint

(Hope it will replace consultants one day :D)

Check out our launch Tweet for more info:  
[https://twitter.com/SilasAlberti/status/1608037989623414791](https://twitter.com/SilasAlberti/status/1608037989623414791)

Do you have any feedback? We would really appreciate it :)",MachineLearning
"In TED Interview on the future of AI from three months ago, Demis Hassabis says he spends most of his time on the problem of abstract concepts, conceptual knowledge, and approaches to move deep learning systems into the realm of symbolic reasoning and mathematical discovery. He says at DeepMind they have at least half a dozen internal prototype projects working in that direction:

https://youtu.be/I5FrFq3W25U?t=2550

Earlier, around the 28min mark, he says that while current LLMs are very impressive, they are nowhere near reaching sentience or consciousness, among other things, because they are very data-inefficient in their learning. 

Can we infer their half dozen approaches to abstract reasoning from the research published by DeepMind so far? Or is this likely to be some yet unreleased new research?

DeepMind list many (not sure if all) of their papers here:

https://www.deepmind.com/research

I was able to find some related papers there, but I am not qualified to judge their significance, and I probably missed some important ones because of the less obvious titles. 

https://www.deepmind.com/publications/symbolic-behaviour-in-artificial-intelligence

https://www.deepmind.com/publications/discovering-symbolic-models-from-deep-learning-with-inductive-biases

https://www.deepmind.com/publications/neural-symbolic-vqa-disentangling-reasoning-from-vision-and-language-understanding

https://www.deepmind.com/publications/learning-symbolic-physics-with-graph-networks

https://www.deepmind.com/publications/how-to-transfer-algorithmic-reasoning-knowledge-to-learn-new-algorithms

https://www.deepmind.com/publications/a-simple-approach-for-state-action-abstractionusing-a-learned-mdp-homomorphism

Can anyone help summarize the approaches currently considered promising in this problem? Are we missing something bigger coming up behind all the hype around ChatGPT?",MachineLearning
"In Japan, deep learning models are not protected as intellectual property. Because of that, I'm running the model in the cloud, but that has been causing multiple issues and raising costs. Since this model requires hefty processing power, I'm planning on shipping mini-pc's with powerful GPUs and everything installed directly to the customer. But then how to protect the model, which took a lot of effort, time and money to train, from being stolen?

The main issue here is probably having a market that is broad enough to make monkey, but at the same time niche enough to not make it worth developing a whole new ecosystem only to protect the model. Is there any readily available OS or a form of container made for such a purpose, or does anyone have another suggestion?",MachineLearning
"Abstract:

Language impairment is an important biomarker of neurodegenerative disorders such as Alzheimer’s disease (AD). Artificial intelligence (AI), particularly natural language process- ing (NLP), has recently been increasingly used for early prediction of AD through speech. Yet, relatively few studies exist on using large language models, especially GPT-3, to aid in the early diagnosis of dementia. In this work, we show for the first time that **GPT-3 can be utilized to predict dementia from spontaneous speech.** Specifically, we leverage the vast semantic knowledge encoded in the GPT-3 model to generate text embedding, a vector representation of the transcribed text from speech, that captures the semantic meaning of the input. We demonstrate that the text embedding can be reliably used to (1) distinguish individuals with AD from healthy controls, and (2) infer the subject’s cognitive testing score, both solely based on speech data. We further show that text embedding considerably out- performs the conventional acoustic feature-based approach and even performs competi- tively with prevailing fine-tuned models. Together, our results suggest that **GPT-3 based text embedding is a viable approach for AD assessment directly from speech and has the potential to improve early diagnosis of dementia.** 

Interesting:

'...there is a risk of overfitting when the data are not abundant, especially with the larger models (Curie and Davinci). Indeed, **when we tested with the Curie and Davinci, we found the model overfitting by observing almost perfect recall and extremely low precision** in AD classification task...'  


Paper: [https://journals.plos.org/digitalhealth/article/file?id=10.1371/journal.pdig.0000168&type=printable](https://journals.plos.org/digitalhealth/article/file?id=10.1371/journal.pdig.0000168&type=printable)

Article: [https://www.eurekalert.org/news-releases/975246](https://www.eurekalert.org/news-releases/975246)",MachineLearning
"Greetings!

&#x200B;

Recently I was asked about a budget AI / ML workload, and decided to test it against some of my own lab GPUs. 

&#x200B;

I'll be adding more tests, and benchmarks over time, but below is a link to my website where I covered it.  As well as the code I wrote to benchmark them. 

&#x200B;

Hopefully this helps someone out there. :-) 

&#x200B;

[https://www.zb-c.tech/2022/12/26/pytorch-drag-race-tesla-k80-performance/](https://www.zb-c.tech/2022/12/26/pytorch-drag-race-tesla-k80-performance/)",MachineLearning
"I was building a YOLOv5 object detection model, and was looking into researching synthetic methods like GANs to increase the size of my training set in an unsupervised manner.

Ik few-shot GANs can be used to ""hallucinate"" images and labels for a classification task, but how can they be extended to hallucinate images and labels in [YOLO format](https://www.edge-ai-vision.com/2022/04/exploring-data-labeling-and-the-6-different-types-of-image-annotation/#:%7E:text=YOLO%3A%20In%20the%20YOLO%20labeling,coordinates%2C%20height%2C%20and%20width.) (basically lists out each bounding box and class)?

Is there some way that I can train a GAN on images / YOLO labels, and get it to hallucinate more images / labels?",MachineLearning
"I am trying to produce an AutoEncoder that has meaningful embeddings for dimensionality reduction. Additionally, I have a specific downstream task I have in mind to use the embeddings for, so I would like to know if it makes sense to write a loss function that considers both the reconstruction accuracy of the AutoEncoder, as well as prediction accuracy for the downstream task. If so, are there any relevant loss functions or articles I should refer to?

Thanks!",MachineLearning
"Dear fellows,

I submitted my article to one of the Taylor & Francis journal in mid-2021. It received a reject and resubmit decision in early 2022. 

I undertook the major revisions and resubmitted my article in mid-2022. Its status went from under review to pending editor decision on September 2022.

However, since then, there has been no update. I tried to contact the chief editor and editor-in-command in the period of last month. However,  I have yet to hear from them.

My paper has already been significantly delayed, and this uncertain situation worsens my anxiety.

What do you think I should do in this case?",MachineLearning
"Hi all!

As most of you here, I've played around a bit with CHATGPT, but felt it was annoying to always have to log into their GUI to ask the questions.

To scratch my own itch and at the same time learn more about how to write my own command line interface, I created 'askai': [https://github.com/maxvfischer/askai](https://github.com/maxvfischer/askai)

It is a simple CLI integration with OpenAI’s GPT3 models. I’ve primarily used it to get quick answers to technical questions, like:

    askai ""How to mock user input when writing a Python pytest test?""
    askai ""How do I remove a conda environment?""

As I've found it quite helpful, I decided to spend some time to package it in a nicer way to share it with you. I've also uploaded it to PyPI to simplify the installation process.

'askai' enables you to:

* Ask questions and get the answers straight into your terminal
* Configure which model and model parameters you want to use
* Overwrite saved configurations when you ask questions

Currently, it only supports OpenAI’s models, but my plan is to integrate more endpoints as soon as new capable NLP endpoints are popping up.

I hope some of you find it useful :)",MachineLearning
"In the wake of all the questions and worries about models that can generate content nearing (or exceeding, in some cases) the quality of that made of humans, there are a couple mechanisms that companies should provide alongside their models. Both vary in feasibility, but in general, both are pretty doable, at least for what we've seen so far.

1. A hashing-based system to check whether a given piece of content was generated by the model. This can be accomplished by hashing all of the outputs of the model, and storing them. If it doesn't pose some sort of security risk for the generator, it could also provide the date of generation.

2. A model for discriminating whether a given piece of content was generated by the model, similar to [this model for GPT-2](https://huggingface.co/roberta-base-openai-detector). This is necessary in addition to the simpler hashing mechanism, since it's possible for only a portion of the media to be generated. This would be imperfect, of course, but if nothing else, we should press companies enough that they feel obligated to give it a dedicated try.

These mechanisms need real support - an API for developers, and a UI for less sophisticated users. They should have decent latency, and be hopefully be provided for free, at some level of usage - I understand the compute required could be enormous.

Curious what others think here :)",MachineLearning
"Hi everyone, 

I am no programmer, and I have a very basic knowledge of machine learning, but I am fascinated by the possibilities offered by all the new models we have seen so far. 

Some people around me say they are not that impressed by what AIs can do, so I built a small test (with a little help by chatGPT to code the whole thing): can you always 100% distinguish between AI art or text and old works of art or literature?

Here is the site: http://aiorart.com/

I find that AI-generated text is still generally easy to spot, but of course it is very challenging to go against great literary works. AI images can sometimes be truly deceptive.

I wonder what you will all think of it... and how all that will evolve in the coming months!

PS: The site is very crude (again, I am no programmer!). It works though.",MachineLearning
"I don't remember where I've read about this, but it left a lasting impression on me as it feels intuitively true and impactful - in a manner, the learning on each datapoint pulls the network towards encoding that individual example, relying on stochastic emergence of shared features, which in turn relies on a dataset:model size ratio that prevents overfitting and a balanced dataset.

Has there been any research into counteracting this phenomenon, such as more purposeful extraction of features, clever batching schemas, synthetic datapoints or anything else such?",MachineLearning
"Hey folks,

I built [TuneMyAI](https://tunemy.ai/) to make it incredibly simple for developers to finetune and deploy Stable Diffusion models to production so they can focus on building great products.

As an app developer myself, I spent a while trying to figure out how to go beyond local GPUs and notebooks and setup our own infra using Kubernetes. In summary, we wanted to make it really simple for anyone to build applications on top of Stable Diffusion without worrying about all the MLOps overhead.

Our API allows you to finetune your Stable Diffusion models for your specific data sets. We handle everything from storage, finetuning, model deployment & inference and integrate with HuggingFace as well. We're working on a bunch of new features including hosted WebUIs, support for additional models like Whisper and more.

Would love for y'all to check us out and share any feedback. You can learn more on [ProductHunt](https://www.producthunt.com/posts/tunemyai).

Thanks & Happy Holidays!",MachineLearning
"Paper: [https://arxiv.org/abs/2212.10562#google](https://arxiv.org/abs/2212.10562#google) 

Abstract: 

>Current image generation models struggle to reliably produce well-formed visual text. In this paper, we investigate a key contributing factor: **popular text-to-image models lack character-level input features, making it much harder to predict a word's visual makeup as a series of glyphs.** To quantify the extent of this effect, we conduct a series of controlled experiments comparing character-aware vs. character-blind text encoders. In the text-only domain, we find that character-aware models provide large gains on a novel spelling task (WikiSpell). Transferring these learnings onto the visual domain, we train a suite of image generation models, and show that character-aware variants outperform their character-blind counterparts across a range of novel text rendering tasks (our DrawText benchmark). **Our models set a much higher state-of-the-art on visual spelling, with 30+ point accuracy gains over competitors on rare words, despite training on far fewer examples.** 

https://preview.redd.it/m4ycamclmb8a1.jpg?width=1245&format=pjpg&auto=webp&v=enabled&s=7b8203a8e133918ce392a008fb490ee0f664043a

https://preview.redd.it/anzemadlmb8a1.jpg?width=1353&format=pjpg&auto=webp&v=enabled&s=b0d81ac5adb0f59a0f20d4fe0b05ea26a24e9694

https://preview.redd.it/5ikr8gdlmb8a1.jpg?width=1531&format=pjpg&auto=webp&v=enabled&s=a4b335dbb5ec68cfadd963c0ae0e1758ba5fb969

https://preview.redd.it/pkhiwnclmb8a1.jpg?width=746&format=pjpg&auto=webp&v=enabled&s=85ba13fb0ed621be84e8ddb65ca847499031260d

https://preview.redd.it/q5l8psclmb8a1.jpg?width=1538&format=pjpg&auto=webp&v=enabled&s=3b6b8ef84899eb466f88a592ad0139961ce80021",MachineLearning
"Hi everyone!

For the 8th (!) year in a row, we have compiled our picks for the most **innovative developments in the Python ecosystem**. From this edition, we are expanding our list to include not only **libraries** but also **tools** that are built to belong in the Python ecosystem — some of which are not written in Python as you’ll see.

**The full list with expanded descriptions is available here**: [https://tryolabs.com/blog/2022/12/26/top-python-libraries-2022](https://tryolabs.com/blog/2022/12/26/top-python-libraries-2022)

As usual, most of the picks have to do with AI / ML.

➡️ Here are our top 10 picks:

1. [Ruff](https://github.com/charliermarsh/ruff) — a fast linter
2. [python-benedict](https://github.com/fabiocaccamo/python-benedict) — a dict on steroids
3. [Memray](https://github.com/bloomberg/memray) — a memory profiler
4. [Codon](https://github.com/exaloop/codon) — a Python compiler using LLVM
5. [LangChain](https://github.com/hwchase17/langchain) — building LLM-powered apps
6. [fugue](https://github.com/fugue-project/fugue) — distributed computing done easy
7. [Diffusers](https://github.com/huggingface/diffusers) — generative AI
8. [LineaPy](https://github.com/LineaLabs/lineapy) — notebooks in production
9. [whylogs](https://github.com/whylabs/whylogs) — model monitoring
10. [Mito](https://github.com/mito-ds/monorepo) — spreadsheet inside notebooks

➕ Plus we added several more to the “long tail” that we hope are useful plus some that we missed [last year](https://tryolabs.com/blog/2021/12/21/top-python-libraries-2021), so [make sure to check out the full post](https://tryolabs.com/blog/2022/12/26/top-python-libraries-2022)!

So:

* What do you think about our picks?
* Did we miss any good ones?

Please let us know! We take feedback seriously to improve the selection every year 💪🏻

Congrats to the individuals and teams behind each of these libraries. We know open source is hard. Thank you for your invaluable contributions to the Python community! 🚀🚀🚀",MachineLearning
"I am working on a unet model that takes as input 64x64 landsat imagery and outputs various classes of agricultural features. The training works ok when I scale the surface reflectance (SR) values to 0-1 (i.e. divide raw SR by the 16bit max constant 65536). What I've noticed is that the model seems to be memorizing the range of values in each image and not learning the shapes and spatial patterns as much. The result is that predictions vary a bit too much from year to year and years not appearing in the training dataset have suboptimal predictions.
Batch normalization does not seem to change anything. Model converges faster but the problem remains.

 What I've tried to do is normalize each image individually by subtracting each channel by its mean and dividing by its standard deviation. This maintains the relative spatial patterns and shapes but bring all images to a mean of 0 and standard deviation of 1.  Feeding these normalized images to the model does not work. I get precision and recall of 0. Pretty much all predictions were 0. Is there a reason why this would happen? 
Am I missing something about the way unet works?
Any insight would be appreciated.


Update: This may be useful to others. I was able to resolve the problem. Apparently, having the each image channel averaging to 0 does not allow the model to train properly and converge. What I've done is randomly shift the mean of each image and adding some random jitter. The model can then train properly. Thanks everyone for your insights. Next, I will evaluate the new model for accuracy of year over year predictions.",MachineLearning
"Hello everyone, 

I'm trying to figure out how to apply concepts from SE into ML research.

For me it seems like I can find really good settings for my Model and dataset, and it can be reproduced. However, I think there's a better way to create code for experimenting. Fore example, creating and testing baselines, and logging test results seems to be the same between most (if not all) my experiments. I find myself copying and pasting a lot of code snippets between my projects. 
Yet, every time I try to set down and write a generic code for experimenting. I find that it's either too limiting or impossible for me to write it. I think if I looked into software engineering concepts and principles it might help. 

I really want to know what was your experience in searching/applying SE into this field, or if you even think it's worth it/possible to. 

some of my colleagues think it's a waste of time, specially considering that the model would run on completely different code.",MachineLearning
"Hi, I'm dealing with a highly unbalanced binary classification of panel data. I am wondering if there are better ways to estimate the performance of the models than splitting once the dataset at a certain date, since that way I could only obtain a point estimate. I don't think group k-fold is suitable since I'd like to respect the temporal order and I'm unsure if using rolling windows would be a valid strategy. Any opinions?",MachineLearning
"Hi,

I am using [Stochastic Depth](https://arxiv.org/pdf/1603.09382.pdf)  in a ResNet based architecture that I train for image classification. I  am wondering how does that work out with batchnorm and whether there  are some things to know to make it work.

To  go into details, Stochastic Depth will drop randomly some resnet block  and use instead exclusively the shortcut identity connection,  effectively reducing the depth of the network during training.

Hence, with probability p: x\_{n+1} = x\_n + f(x\_n), with probability (1-p): x\_{n+1} = x\_n.

To  preserve the expected values during training and inference, they scale  the output of the not-skipped blocks (equation 5 in the paper): x\_{n+1} =  x\_n + f(x\_n)/p. That seems logical (even though it does not seem to  yield better results in practice but whatever).

My  question is more related to the variance of the batchs. If one batch  contains samples that skip a connection and samples that do not ('row'  mode in the [Torchvision implementation](https://github.com/pytorch/vision/blob/main/torchvision/ops/stochastic_depth.py)),  even if the values are ajusted to preserve the expected value, the  variance will be much higher because we have in practice two  distributions (for x\_n and x\_n + f(x\_n)/p), which will mess up with the  update of the batch normalization. Also, at inference time, all forward  passes will be done as x\_{n+1} = x\_n + f(x\_n), which has a different  variance. The torchvision implementation also offers a 'batch' mode that  kinda reduce this issue (because the global variance computed this way  will be the mean of both distribution variances, instead of the variance  of the joint distribution) but it does not seem to be the default mode  (it does not even exist in the [timm implementation](https://github.com/pprp/timm/blob/master/timm/models/layers/drop.py#L137)).

Has anyone here ever think about it? Is there a specific way to use both stochastic depth and batchnorm ? Thank you.",MachineLearning
"Been an industry data scientist for 6 years in fintech and gaming.  
In fintech, I sensed a need for interpretability and robustness. Also, I was not working with a lot of data(\~500k observations to train models). Consequently, I got into the habit of building tree-based models by default, specifically xgboost. Used explainability techniques such as shap to explain models.

After moving to online gaming, the scrutiny is less and the scale is far more. I now have the freedom to use deep learning. I need to be able to demonstrate the effectiveness using experiments, but beyond that, do not need explainability at a granular level. Advantages I see with using deep learning-

1. Custom loss functions - basically any differentiable loss function can be trained on. This has huge advantages when the business goal is not aligned with the loss functions out of the box
2. Learning Embeddings - The ability to condense features into dense, latent representations which can be used for any number of use cases
3. Multiple outputs per model - tweaking the architecture

See all this, Deep learning seems to offer a lot of advantages, even if the performance might be similar to tree-based methods. What do you guys think?",MachineLearning
"Are blacklists actually implemented in these conferences (ICML / ICLR / NeurIPS) given that the number of reviewers required grows every year?

Edit: Should've been clearer, sorry. By blacklist I mean a list of reviewers who are barred from reviewing because of their bad review quality in previous iterations of the conference.",MachineLearning
"QDA is a non-linear classification algorithm. However, does its decision boundary always have to be quadratic? I mean if it can produce linear decision boundaries?",MachineLearning
"Creating this thread for AI Residency applicants at Meta 2023. Any new information or update on application/interview are welcome. Can also have general discussion about applicant backgrounds, backups and future plans.",MachineLearning
"https://mlem-nice-or-naughty.fly.dev

Please enjoy! :)

And, here is the blog post about DDoSing Santa's website and training a Christmas decision tree ^
https://medium.com/@mike0sv/i-trained-a-model-to-tell-if-you-were-naughty-this-year-11a36ca6d472

Have a good New Year's mood!",MachineLearning
"Github Link: [https://github.com/kpthedev/stable-karlo](https://github.com/kpthedev/stable-karlo)

I made **stable-karlo**, an app that combines Kakaobrain's Karlo image generation model with Stable-Diffusion 2.1 in a nice webUI.

Recently,  Kakaobrain released Karlo, their own image generating diffusion model which uses OpenAI's unCLIP architecture. The model is great at understanding text and relationships, but it only outputs 256x256 pixel images. I had the idea to combine Karlo with the new Stable-Diffusion v2 upscaler to get large images and the results are very promising.

Please check out the [Github](https://github.com/kpthedev/stable-karlo) and share your thoughts!",MachineLearning
"A guy on Twitter shared a ChatGPT that is aware of all the podcasts from Andrew Huberman, which is great (https://huberman.rile.yt/?query=)

Has anyone open sourced something like ChatGPT that it is easy to fine tune with external knowledge, potentially tested on scientific papers? It would be great for brainstorming, writing research proposal and exploring the literature in a different way. Maybe even integrating it with Zotero.

As of now I talked about finetuning the model, but let’s say I take the easier path of few shot learning instead. Is there a way to save the state of ChatGPT? In other words, if I open a new chat and feed it all the papers by copy and paste for example, is there a way I can use it next week? Sometimes I have found the session to expire, but recently it seems past chats are saved. Will this last indefinitely you believe?

TL;DR: best way to adapt ChatGPT to specific knowledge?",MachineLearning
"Hi! I'm relatively new to machine learning and came up w a project of my own.

I'm hoping to create a database to suit the needs of my project and was thinking whether there are any APIs available to assist me. The data that I am looking for are molecular data, mainly their optical properties and ADME-T. 

Please let me know if this is the wrong place to ask, thanks!",MachineLearning
"It was one thing with DALLE-2, but at least it couldn’t talk back to them. I mean I have been in board meetings with powerful people in leadership positions that have nothing to do with tech have absolutely horrendous ideas about what ChatGPT is- I am not lying, I have genuinely heard them say they believe it’s basically conscious and using excerpt screenshots of it saying it hates humans as a basis to make business decisions about the future of AI in their company. Like….WHAT?  Have other people heard absurd things like this too? 

 I think it’s just hard to see the professional reality of machine learning, becoming extremely debased from the general public idea of machine learning. I’m sure as we all get even better at our jobs it’s only going to get much much worse. I wouldn’t be surprised if soon we are the new magical witches of the world. i’ll see you guys on the pyres in 20 years.( ok really I’m just joking on that last part) 

What do you all think?",MachineLearning
Every lunar lander tutorial or example I've found so far uses deep RL. Is classical Q learning such an obviously bad idea that no-one bothers with it? I've had some success recently applying Q learning to lunar lander (converting the continuous observations into discrete values) and am surprised there aren't more tutorials about this approach. Am I missing something?,MachineLearning
"Edit: Found [LAION-AI/OPEN-ASSISTANT](https://github.com/LAION-AI/Open-Assistant) a very promising project opensourcing the idea of chatGPT. [video here](https://www.youtube.com/watch?v=8gVYC_QX1DI)

**TL;DR: I found GPU compute to be [generally cheap](https://github.com/full-stack-deep-learning/website/blob/main/docs/cloud-gpus/cloud-gpus.csv) and spot or on-demand instances can be launched on AWS for a few USD / hour up to over 100GB vRAM. So I thought it would make sense to run your own SOTA LLM like Bloomz 176B inference endpoint whenever you need it for a few questions to answer. I thought it would still make more sense than shoving money into a closed walled garden like ""not-so-OpenAi"" when they make ChatGPT or GPT-4 available for $$$. But I struggle due to lack of tutorials/resources.**

Therefore, I carefully checked benchmarks, model parameters and sizes as well as training sources for all SOTA LLMs [here](https://docs.google.com/spreadsheets/d/1O5KVQW1Hx5ZAkcg8AIRjbQLQzx2wVaLl0SqUu-ir9Fs/edit#gid=1158069878).

Knowing since reading the Chinchilla paper that Model Scaling according to OpenAI was wrong and more params != better quality generation. So I was looking for the best performing LLM openly available in terms of quality and broadness to use for multilingual everyday questions/code completion/reasoning similar to what chatGPT provides (minus the fine-tuning for chat-style conversations).

My choice fell on [Bloomz](https://huggingface.co/bigscience/bloomz) (because that handles multi-lingual questions well and has good zero shot performance for instructions and Q&A style text generation. Confusingly Galactica seems to outperform Bloom on several benchmarks. But since Galactica had a very narrow training set only using scientific papers, I guess usage is probably limited for answers on non-scientific topics.

Therefore I tried running the original bloom 176B and alternatively also Bloomz 176B on AWS SageMaker JumpStart, which should be a one click deployment. This fails after 20min. On Azure ML, I tried using DeepSpeed-MII which also supports bloom but also fails due the instance size of max 12GB vRAM I guess.

From my understanding to save costs on inference, it's probably possible to use one or multiple of the following solutions:

- Precision: int8 instead of fp16
- [Microsoft/DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII) for an up 40x reduction on inference cost on Azure, this thing also supports int8 and fp16 bloom out of the box, but it fails on Azure due to instance size.
- [facebook/xformer](https://github.com/facebookresearch/xformers) not sure, but if I remember correctly this brought inference requirements down to 4GB vRAM for StableDiffusion and DreamBooth fine-tuning to 10GB. No idea if this is usefull for Bloom(z) inference cost reduction though

I have a CompSci background but I am not familiar with most stuff, except that I was running StableDiffusion since day one on my rtx3080 using linux and also doing fine-tuning with DreamBooth. But that was all just following youtube tutorials. I can't find a single post or youtube video of anyone explaining a full BLOOM / Galactica / BLOOMZ inference deployment on cloud platforms like AWS/Azure using one of the optimizations mentioned above, yet alone deployment of the raw model. :(

I still can't figure it out by myself after 3 days.

**TL;DR2: Trying to find likeminded people who are interested to run open source SOTA LLMs for when chatGPT will be paid or just for fun.**

Any comments, inputs, rants, counter-arguments are welcome.

/end of rant",MachineLearning
"[Hex Application](https://app.hex.tech/b8e8e163-4ffa-4f61-996e-9ffade22226d/app/f3db3b78-b584-40e5-b839-6650bbe1beee/latest)

Since we are heading into the holiday season, I thought it would be interesting to take a look if you could create a model to look at morality with user's Reddit comments. I used Scikit-Learn's Logistic Regression Model for this.

I started by downloading around 750 comments from Social Grep's website. They have pulled Reddit comments from different sets of subreddits. I pulled from their datasets for confession-like subreddits, the irl subreddits, and the dataset subreddit. I classified the comments manually by a set rule of morality. Once they were scored, I trained/tested the Logistic model with those comments. 

For the specific user testing, I used PRAW to pull the most recent 50 comments from the username provided in the Hex Application. I ran the trained model and outputted the probability of each comment being nice and took an average of the probabilities and used that value to determine whether the user was naughty or nice. I use a script to email a CSV with all of the tested comments and the final score to the user. 

Based on the results that have came through so far, the model is definitely biased towards giving the user a nice decision. I believe that is based on the training data being around 70% nice versus naughty. Does anyone have a way to help the model from being biased like that? 

Feel free to try the app out and let me know what you think!",MachineLearning
"I have a data set with three columns and want to predict a numerical value. The data set is divided into groups such that each group is 50 rows. There is a necessary constraint where the sum of the predicted value in each group of 50 rows must equal the value in one column for that group. What model can I use for this, if any?",MachineLearning
"Hello everyone, 

I'm currently very curious about the performance of self-supervised traing in small models. Tried, mostof the implementation provided by lightly.

my question is, should we use all available data for pretraining. could our model benefit from removing similar images?, if so how would you test it?  

Finally, sampling for pretraining seems to be overlooked in literature, which seems wrong. I'm feeling like I didn't search for it correctly. so any papers that talk about sampling would be great.",MachineLearning
"I am wondering if you have several networks with similar performance on all your available datasets but with different hyperparameters. 
What would your criteria be to choose one of the networks?
E.g would you choose the one with the highest L2 to increase the generalisation?",MachineLearning
"Intro & source code: [https://github.com/houkensjtu/taichi-hackathon-akinasan](https://github.com/houkensjtu/taichi-hackathon-akinasan)

1. The circuit of an ordinary RC toy car is modified so that Jetson Nano can control the movement of the car through GPIO port. Of course, we need to use motor drive controller here, because the upper limit of the output current of Jetson Nano is not enough to drive the car motor directly.
2. The convolution neural network (CNN) is implemented using Taichi programming language.
3. The road data was collected, then classified and labeled, and finally used in the training of CNN models.
4. The pre-trained model is imported into Jetson Nano and the action prediction made for the images captured during driving.

Demo:

https://reddit.com/link/zshrlv/video/pcm3f6id3f7a1/player",MachineLearning
"

I am working on an NMT model, I'm a newbie at this but so far I have a decent result. I'm translating from a rich language to a low-resource one. 

If I use something different in my generate function (e.g., I've used greedy method and Beam ) I could ostensibly improve the results. What I wonder is, has anybody worked with these for an NMT project involving agglutinative languages? If so, any recommendations?

I'm currently searching for options but I honestly don't find much and know less.",MachineLearning
"I have collected experimental data for various conditions. In order to ensure repeatability,  each test is replicated 5 times: which means same input but slightly different output due to experimental variability.

If you were to build a machine learning algorithm,  would you use all 5 data points for each given test, hoping that your algorithm will learn to converge towards the mean response? Or it is advisable to pre-compute the means and only feed these to the model? ( so that you ensure that one input can only have one output)

I can see pros and cons to both approches and would welcome feedback. Thank you.",MachineLearning
"I've been experimenting with GPT3 for different use cases over the past few weeks, the latest one was seeing how well it could parse out structured data from recipe free text, and how well it could further enrich this data.

The general idea was to have a few different prompts to the model, with output from one prompt inputting into the next prompt:

1. Extract ingredients and instructions from the recipe
2. Given the ingredients, group them together into categories
3. Given the full structured recipe generated above, enrich it further with additional metadata (time to cook, healthiness, etc)

This worked out better than I expected - given an input recipe I'm able to consistently (and accurately) extract the constituent parts and group the ingredients together logically (like grains, dairy, etc).

I wrote about it here: [https://binal.pub/2022/12/extracting-and-structuring-recipes-using-gpt3/](https://binal.pub/2022/12/extracting-and-structuring-recipes-using-gpt3/)

One thing that I was surprised by as well was this turned out to be a decent recipe generator. So instead of using a full recipe I could input ""Pumpkin Pie"" and the structured response at the end would be the ingredients and instructions to bake a pumpkin pie with quantities/timings that seemed to be about what you'd expect.",MachineLearning
"I believe it is always better to cite the version of a paper that has been published in a conference or journal, if such a version exists.

&#x200B;

However, I often find myself adding the arxiv version to my bibliography manager when I quickly want to save a paper. I presume many of you do the same.

&#x200B;

Before submitting my paper, I can manually search for every reference on google or semanticscholar (google scholar does not always find the conference version in my experience) and replace the arxiv reference with the conference reference. Sounds like that could be automated, right?

&#x200B;

Whats your workflow in that regard? Are you aware of a tool to automate the process? Or do you even care which version you cite?",MachineLearning
"I apologise if this has already been posted, but I came across this and thought you guys would be interested.  You can make blobs sing in Opera style singing with the help of machine learning and you can record and share your singing blobs!

[https://artsandculture.google.com/experiment/blob-opera/AAHWrq360NcGbw?cp=eyJyIjoieUtyM0I3N1hPZ0lOIn0.&hl=en](https://artsandculture.google.com/experiment/blob-opera/AAHWrq360NcGbw?cp=eyJyIjoieUtyM0I3N1hPZ0lOIn0.&hl=en)

&#x200B;

https://preview.redd.it/skyw5to9ea7a1.png?width=1874&format=png&auto=webp&v=enabled&s=e7f2bc5a7b3c7a025ee0bdc6aa5a6263fb72017e",MachineLearning
"


Hi,

For my final year project for my BSc CompSci and AI course I’m implementing the world models paper to play games. Essentially a variational autoencoder and another network to predict future latent states of the game environment. 

The emphasis of my project is to reduce the number of parameters, and consequently the training time (making a case for reducing the energy consumption). I’ll use existing models and their size alongside game performance to compare with my own. 

I’ve had trouble finding existing literature as to how this can be done. Obviously there isn’t a way to find an ‘optimal’ number required to solve a task, but wanted to find techniques to reduce excess bulk in a NN without sacrificing performance. 

Does anyone have any ideas or know of any resources?

TIA",MachineLearning
 Is anyone aware of any huge datasets I can use? I've done some Googling  and I found a few of them but they all seem to be for 2017 and earlier.,MachineLearning
"[**Whisperer**](https://github.com/miguelvalente/whisperer)

A tool to make audio-text datasets automatically for your ML Projects. 
Two weeks ago, I [shared an early draft](https://www.reddit.com/r/MachineLearning/comments/zd6xlv/p_transform_your_raw_audio_into_a_textaudio/) of a project based on the newly released OpenAI's Whisper.
Today I'm sharing the finished version of Whisperer, which adds diarization, with same-speaker detection across multiple audio files.



**Key Features:**


- Automatic Speaker Diarization

- Automatic Speaker Identification


 -  e.g:  *same speakers across audio files*

- Automatic Transcription

- Forces Gaussian Distributions of the dataset  [see notebook](https://github.com/miguelvalente/whisperer/blob/master/notebooks/AnalyzeDataset.ipynb)

- Modular and Configurable


EDIT:

[Live on twitch if anyone has any questions.](https://dashboard.twitch.tv/u/the_mush_valente/stream-manager)",MachineLearning
" There is a whole more to pooling than Max and average pooling. A few other Pooling methods are:

1. Mixed Pooling
2. L\_p Pooling
3. Stochastic Pooling
4. Spatial pyramid Pooling
5. Multi-scale order less Pooling
6. Super Pixel Pooling
7. Compact bilinear Pooling
8. Edge-aware Pyramid Pooling
9. Spectral Pooling
10. Per-Pixel Pyramid Pooling
11. Rank Based average Pooling
12. Weighted Pooling
13. Genetic-based Pooling

Read the full article here:

[https://medium.com/aiguys/pooling-layers-in-neural-nets-and-their-variants-f6129fc4628b](https://medium.com/aiguys/pooling-layers-in-neural-nets-and-their-variants-f6129fc4628b)

&#x200B;

https://preview.redd.it/6plxeqvts87a1.png?width=517&format=png&auto=webp&v=enabled&s=952c50cd996c621f5f4b629092c1e1819c0fe29b",MachineLearning
"It's only been a month since OpenAI released ChatGPT, and yesterday they launched Point-E, a new Dalle-like model that generates 3D Point Clouds from Complex Prompts. As someone who is always interested in the latest advancements in machine learning, I was really excited to dig into this paper and see what it had to offer.

One of the key features of Point-E is its use of diffusion models to generate synthetic views and 3D point clouds. These models use text input to generate an image, which is then used as a reference for generating the 3D point cloud. This process takes only 1-2 minutes on a single GPU, making it much faster than previous state-of-the-art methods.

While the quality of the samples produced by Point-E may be lower than those produced by other methods, the speed of generation makes it a practical option for certain use cases.

If you're interested in learning more about this new model and how it was developed, I highly recommend giving the full paper a read. But if you're more into reading the gist of it, I added a link to an overview blog I published about.

The blog: [https://dagshub.com/blog/point-e/](https://dagshub.com/blog/point-e/)

The paper: [https://arxiv.org/abs/2212.08751](https://arxiv.org/abs/2212.08751)

I'm sure I have yet to reach all the insights while writing the blog, and I'd love to get your thoughts about the model and how OpenAI developed it.",MachineLearning
"BLIP from Salesforce is now available on Hugging Face transformers!

Here is a list of cool applications you can build on top of it: [https://twitter.com/younesbelkada/status/1605489647395540992](https://twitter.com/younesbelkada/status/1605489647395540992)

With (I think) most interesting application being building image-captioning APIs and Stable Diffusion-related applications (generate image-text datasets to fine-tune Stable Diffusion on it & image to music app)

Any other thing you have in mind that can be built using BLIP?",MachineLearning
"Many recently published llms (gpt, bloom, palm, etc) evaluate on a plethora of tasks in a k-shot setting, and offer evaluations on different model sizes with the same architecture. However, I haven't seen much literature evaluating these models' capabilities for finetuning to individual tasks. Are there resources out there that do this comparison? It would help for deciding what pretrained backbone to use for finetuning in production usecases.",MachineLearning
"Hi Guys,

I'm looking for a paper, which was published in NIPS around 2016, in which the authors used a neural network, to predict the weights of another neural network, I don't have the context in which this was applied :(.

Does this sound familiar?",MachineLearning
"I'm trying to figure out how to go about running something like GPT-J, FLAN-T5, etc, on my PC, without using cloud compute services (because privacy and other reasons). However, GPT-J-6B needs either \~14 GB of VRAM or 4x as much plain RAM.

Upgrading my PC for 48 GB of RAM is possible, and 16, 24 GB graphics cards are available for general public (though they cost as much as a car), but anything beyond that is in the realm of HPC, datacenter hardware and ""GPU accelerators""... I.e. 128 GB GPUs exist out there somewhere, but the distributors don't even list a price, it's just ""get a quote"" and ""contact us""... meaning it's super expensive and you need to be a CEO of medium-sized company for them to even talk to you?

I'm trying to figure out if it's possible to run the larger models (e.g. 175B GPT-3 equivalents) on consumer hardware, perhaps by doing a very slow emulation using one or several PCs such that their collective RAM (or swap SDD space) matches the VRAM needed for those beasts.

So the question is ""will it run super slowly"" or ""will it fail immediately due to completely incompatible software / being impossible to configure for anything other than real datacenter hardware""?",MachineLearning
"Are you an AI researcher itching to test Hinton's Forward-Forward Algorithm? I was too, but could not find any full implementation so I decided to code it myself, from scratch. Here's the [GitHub repo](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/forward_forward) and don’t forget to leave a star if you enjoy the project.

https://preview.redd.it/zne5aapb837a1.png?width=581&format=png&auto=webp&v=enabled&s=01d436b76ec403411580862b89caf550a5bf3794

As soon as I read the paper, I started to wonder how AI stands to benefit from Hinton’s FF algorithm (FF = Forward-Forward). I got particularly interested in the following concepts:

* Local training. Each layer can be trained just comparing the outputs for positive and negative streams.
* No need to store the activations. Activations are needed during the backpropagation to compute gradients, but often result in nasty Out of Memory errors.
* Faster weights layer update. Once the output of a layer has been computed, the weights can be updated right away, i.e. no need to wait the full forward (and part of the backward) pass to be completed.
* Alternative goodness metrics. Hinton’s paper uses the sum-square of the output as goodness metric, but I expect alternative metrics to pop up in scientific literature over the coming months.

Hinton’s paper proposed 2 different Forward-Forward algorithms, which I called Base and Recurrent. Let’s see why, despite the name, Base is actually the most performant algorithm.

&#x200B;

As shown in the chart, the Base FF algorithm can be much more memory efficient than the classical backprop, with up to 45% memory savings for deep networks. I am still investigating why the base FeedForward underperforms with “thin” networks; any ideas, let’s talk.

Unlike Base FF, Recurrent FF do not have a clear memory advantage versus backprop for deep networks (15+ layers). That’s by design, since the recurrent network must save each intermediate step at time t to compute the following and previous layer outputs at time t+1. While scientifically relevant, the Recurrent FF is clearly less performant memory-wise than the Base FF.

**What’s next?**

The most interesting question is why the Base FF model memory consumption keeps increasing with the number of layers. That’s surprising given this model is trained one layer at a time, i.e. each layer is treated as a mini-model and trained separately from the rest of the model. I will explore this and let you know over the coming days",MachineLearning
"Paper: [https://arxiv.org/abs/2212.09720](https://arxiv.org/abs/2212.09720?fbclid=IwAR37T4SsCC0gYaIEdkQ8AaxoNTO8qKyIxblOeFsOJJaP5sRBsvoJRNhXaho) 

Abstract: 

>Quantization methods reduce the number of bits required to represent each parameter in a model, trading accuracy for smaller memory footprints and inference latencies. However, the final model size depends on both the number of parameters of the original model and the rate of compression. For example, a 30B 8-bit model and a 60B 4-bit model have the same number of bits but may have very different zero-shot accuracies. In this work, we study this trade-off by developing inference scaling laws of zero-shot performance in Large Language Models (LLMs) to determine the bit-precision and model size that maximizes zero-shot performance. We run more than 35,000 zero-shot experiments with 16-bit inputs and k-bit parameters to examine which quantization methods improve scaling for 3 to 8-bit precision at scales of 19M to 66B parameters across the LLM families BLOOM, OPT, NeoX/Pythia, and GPT-2. We find that it is challenging to improve the bit-level scaling trade-off, with the only improvements being the use of a small block size -- splitting the parameters into small independently quantized blocks -- and the quantization data type being used (e.g., Int vs Float). **Overall, our findings show that 4-bit precision is almost universally optimal for total model bits and zero-shot accuracy.**       

https://preview.redd.it/s68eedd5v47a1.jpg?width=720&format=pjpg&auto=webp&v=enabled&s=d6492d127af483cbe513190789de59744a0f60c0

https://preview.redd.it/sllrw4p5v47a1.jpg?width=927&format=pjpg&auto=webp&v=enabled&s=97907155937be0228422b3312d5e01e301a82561",MachineLearning
"The main issue I have with GPT-3 is that the output can be compelling, yet factually incorrect. I remember discovering a platform that generates answers alongside sources, but I can't recall the name.",MachineLearning
"Paper: [https://arxiv.org/abs/2212.01349](https://arxiv.org/abs/2212.01349)

Github: [https://github.com/facebookresearch/NPM](https://github.com/facebookresearch/NPM)

Abstract:

>Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases. We introduce **NPM**, the first **nonparametric masked language model** that **replaces this softmax with a nonparametric distribution over every phrase in a reference corpus**. We show that NPM can be efficiently trained with a contrastive objective and an in-batch approximation to full corpus retrieval. Zero-shot evaluation on 9 closed-set tasks and 7 open-set tasks demonstrates that **NPM outperforms significantly larger parametric models, either with or without a retrieve-and-generate approach**. It is particularly **better on dealing with rare patterns (word senses or facts),** and **predicting rare or nearly unseen words (e.g., non-Latin script)**.

https://preview.redd.it/qf2lqrkku47a1.jpg?width=658&format=pjpg&auto=webp&v=enabled&s=a96f014f9f076ad5ebdcec50a70122c0734ab0f5

https://preview.redd.it/gqhlbykku47a1.jpg?width=1241&format=pjpg&auto=webp&v=enabled&s=32c6ddcdec8db26d150ee20288b543bc4142f157

https://preview.redd.it/p7bzdukku47a1.jpg?width=883&format=pjpg&auto=webp&v=enabled&s=3dfd433ee76c10eefc062e3d8a7fa68a1a5b76c7

https://preview.redd.it/z6niwykku47a1.jpg?width=1112&format=pjpg&auto=webp&v=enabled&s=3a123be5446ddcb28d13235dad88b0069b16f717

https://preview.redd.it/s8fdhxkku47a1.jpg?width=1361&format=pjpg&auto=webp&v=enabled&s=6dc8e04ce7fe22380154a6c8ab0556a772ce1c0a

https://preview.redd.it/94t5fwkku47a1.jpg?width=1362&format=pjpg&auto=webp&v=enabled&s=c8bc84a73ec8a5b147834839909b00e2330f145d",MachineLearning
"[Envpool](https://github.com/sail-sg/envpool) provides high parallelization of RL environments. Unfortunately, there are still many environments that are not supported by them. One of them is FrankaKitchen of [D4RL](https://github.com/Farama-Foundation/d4rl/wiki/Tasks), a library for offline RL.

Envpool provides tutorials on how to add new environments [here](https://envpool.readthedocs.io/en/latest/content/new_env.html) My question is: for anyone with experience adding new environments to envpool, how difficult is it? And while difficulty is in the eye of the beholder, I'd like to know how much time and reading was needed to successfully add the new environment, which is a more objective measure.

Full disclaimer: I have not read the tutorial carefully. I first want to have a better idea of how hard it is before I fully commit to it.",MachineLearning
"I have a rather successful model which I have trained to an extent that the loss has now plateaued. The loss over my training dataset follows a Power Law type curve:

https://preview.redd.it/qotu2k09237a1.png?width=825&format=png&auto=webp&v=enabled&s=72a107bc8dc6788b49081963da330893c83c4c1d

That means, 80% of the training examples have a loss which is well below my tolerance threshold. 15% have a loss which is slightly above threshold tolerance. 4% have a loss which is significant above threshold. And 1% have a very high loss.

This results from the inherent complexity of the training examples themselves. Some are simple. Some are complex. And I was wondering, are there any techniques developed to keep optimizing a model when you encounter such a situation? I thought, such a situation is surely very common so maybe some people came up with some strategies or algorithms, but my Google-fu has failed me. Please refer me to literature on the topic if it exists.

So far I have tried pre-selecting and training on the hard examples only and I have tried multiplying the loss gradients with a scalar that depends on the loss itself. None of these approaches give me satisfactory results.

Maybe it is just that the model is not complex enough. But I am maxing out my GPU RAM already (Nvdia A100s) so I cannot really do much better. But I am not sure I have yet reached the limits of complexity with this model.",MachineLearning
"Every time I try to implement something I have to make sure I never use loops and I only use Pytorch/tf tensors.

If I want to have efficient code, I must kind of abandon Python and only use data structures and operations that are provided by those frameworks.

Every time I have a solution in my head, I must think how do Implement it using ONLY the framework, and not the programming language (python).

We basically constraint ourselves to those limited operations that someone implemented in C++ for us. This make things harder, not easier.

We are not programming in python at all. We use a language within a language that really constraints us.

Why not just move to C++ or something new like Rust/Go?",MachineLearning
"*Excerpt from* [*GitHub*](https://github.com/BobMcDear/flaim)

# Flax Image Models

## Introduction

flaim is a library of state-of-the-art pre-trained vision models, plus common deep learning modules in computer vision, for Flax. It exposes a host of diverse image models through a straightforward interface with an emphasis on simplicity, leanness, and readability, and offers lower-level modules for designing custom architectures.

## Installation

flaim can be installed through `pip install flaim`. Beware that pip installs the CPU version of JAX, and you must [manually install JAX](https://github.com/google/jax#installation) yourself to run your programs on the GPU or TPU.

## Usage

`flaim.get_model` is the central function of flaim and manages model retrieval. It accepts a handful of arguments:

* `model_name` (`str`): The name of the model. If it is not recognized, an exception is thrown.
* `pretrained` (`bool`): Determines if pre-trained parameters are to be returned in lieu of randomly-initialized ones.
* `n_classes` (`int`): The number of output classes. This argument's value can fall under three groups:
   * 0: The model outputs the raw final feature maps. For instance, a ResNet is composed of a stem and four stages, followed by a head constituted of global average pooling and a fully-connected layer for generating predictions. When `n_classes = 0`, the output of the fourth stage is returned, and the head is discarded.
   * \-1: Every part of the head, except for the linear layer, is applied and the output returned. In the ResNet example, the output of the pooling layer is returned.
   * Positive integers: `n_classes` is interpreted as the desired number of output categories.
* `jit` (`bool`): Whether to JIT the model's initialization function. The benefit of JITting the initialization function is that no actual forward pass with real data is performed, unlike the default configuration. On the other hand, JIT compilation is generally a lengthy process.
* `prng` (`T.Optional[jax.random.KeyArray]`): PRNG key used for initializing the model. When `None`, a PRNG key, with a seed of 0, is created. If `pretrained` is `True` and `n_classes` is 0 or -1, this argument has no effects on the returned parameters.

The snippet below constructs a ResNet-50 with 10 output classes.

    import flaim
    
    
    model, vars = flaim.get_model(
            model_name='resnet50',
            pretrained=True,
            n_classes=10,
            jit=True,
            prng=None,
            )

Performing a forward pass with flaim is similar to any other Flax module. However, networks that behave differently during training versus inference, e.g., due to batch normalization, receive a `training` argument indicating whether the model should be in training mode or not.

    from jax import numpy as jnp
    
    
    input = jnp.ones((2, 224, 224, 3))
    
    # Training
    output, batch_stats = model.apply(
            vars,
            input,
            mutable=['batch_stats'],
            training=True,
            )
    
    # Inference
    output = model.apply(
            vars,
            input,
            training=False,
            )

Finally, intermediate activations can be captured by passing the string `intermediates` to `mutable`.

    output, batch_stats, intermediates = model.apply(
            vars,
            input,
            mutable=['batch_stats', 'intermediates'],
            training=True,
            )

**Questions, comments, and feedback are welcome in the comments. For more information, please refer to** [**the GitHub repository**](https://github.com/BobMcDear/flaim)**.**",MachineLearning
"Hi everyone, my lab has recently made Foresight - in short, it is a GPT-3 like language model that can simulate a patient's future (forecast disorders, medications, procedures, symptoms, ...). It was trained and tested on two large hospitals in UK covering both physical and mental health. Any feedback is much appreciated ([Twitter](https://twitter.com/zeljkokr) or here).

Paper: [arxiv](https://arxiv.org/abs/2212.08072)

Demo: [foresight](https://foresight.sites.er.kcl.ac.uk/)",MachineLearning
"Hi everyone!

I want to use a server to continuously train my ML models without keeping on my pc 24/7. I am currently running fairly simple deep learning models that would take a week on my computer.

So far the best solution to start with that I found is the AWS t2.micro instance which could be good for starting. I've seen that also google cloud and Nvidia have other options.

Could you please guide me thru or giving me suggestions about which one could be better as I am not an expert and it is the first time I do it?",MachineLearning
"Hi,

I've been working extensively with PyTorch for the last couple of years and became very accustomed to PyTorch Lightning, which takes all the boiler plate code off you hands while allowing you to access almost any step in the training/validation/test loop.

Jax offers cool tools to write elegant code (albeit be it in a very functional style) with vmap, jit, pmap etc. The ecosystem of PyTorch is very strong with a nice OOP style of structuring code and training management packages like Lightning.

Packages like Equinox and Treex have introduced a very PyTorch like neural network packages. Having worked extensively with PyTorch Lightning I realized that you can hijack Lightning with two simple steps to run full Jax models.

The trick is to run Lightning in a pure Numpy mode and to turn off automatic optimization via \`automatic\_optimization=False\`. Then you simply run your jit compiled forward, backward and gradient update inside the train loop. Jax can do device management on its own and without much extra effort, as far as I know.

Other than that, you get basically every advantage of PyTorch Lightning and every advantage of Jax in one swoop.

Link to more details and examples:

[https://github.com/ludwigwinkler/JaxLightning](https://github.com/ludwigwinkler/JaxLightning)",MachineLearning
"Hey! Currently, I am reading the papers on Deep Learning based Recommender Systems. After around 20 papers, I realised the base idea of the papers is the same - recommendation task either Top-K recommendations or simply predicting the utility (i am not talking about those frameworks that simply model the auxiliary information). The papers have differences in base models (I am reading DNN/MLP, Autoencoder and Attentive models based), but the methodology is the same - replace the way to factorize the matrix to find the latent feature vectors of users/items/social relations, only some papers introduce custom loss function with regularisation terms (just to model the social network I would say). And all these models perform as ""state-of-the-art"". The question is ***where is this research field going/developing?*** All these findings/performance results are simply empirical with no theoretical evidence.",MachineLearning
" My question is a bit half-baked, so apologies for the weird title. I wasn't sure how to phrase what I'm asking for / don't know if what I'm looking for exists.

Basically, in all of my RL / classical AI studies, I've noticed an understandable focus on building agents that are good at all kinds of games (chess, backgammon, go, etc.), to the point of being superhuman. I'm interested if anyone here knows if there has been any successful research into extracting and distilling information contained in these agents to inform higher level strategies about the game (e.g. based on how the agent performs in certain scenarios over a simulated set of games).

My assumption is that if an agent is really good at a game, then there are probably things to be learned from its performance to improve our own. I just don't know if there are any papers or articles that dive into these topics, nor do I know how to articulate what I'm looking for well enough to google it.

TIA for helping point me in the right direction!",MachineLearning
"Hi,  I am working on a project that uses some gradient boosting machine  learning algorithms for a regression task. I am not from computer  science background and my knowledge about ML is mostly from Coursera  courses and kaggle.

I wanted to  use Optuna for hyper parameter optimization and Boruta Shap for feature  selection as it is fairly common in Kaggle and I learnt to use these libraries from there.  But is it acceptable or standard practice to use these libraries for  academic research? Or should I resort to vanilla random search/ grid  search for hyper parameter optimization, and correlation matrix for  feature selection?

This is my first research project, and I am not acquainted with the standard practices in ML academia.",MachineLearning
"At big conferences it is fun to get some human labels, because it is super high quality and high density. At neurips 2022 I wanted to study how humans use language to ""fix"" or ""correct"" an existing artifact. The current big models, such as stable diffusion, are generative from descriptions -- It is impossible to have an output image, and describe precisely how one might want to change it to improve it.  


To quote from the blog post:  


>Imagine *describing* a task for your friend to perform. It is unlikely they’ll get it right on the first try. Often, additional communications are needed to *modify and improve* what is being done so far.  
>  
>At Neurips 2022, I conducted a small study to get a sense of the following:  
>  
>**Q1**: How valuable is the modification process?  
>  
>**Q2**: Are the languages of modification and description different?

  
Check out the blog (5min read) for the full report: [https://evanthebouncy.medium.com/the-language-of-modifications-17fac974c1ef](https://evanthebouncy.medium.com/the-language-of-modifications-17fac974c1ef)

TL;DR: We find that modification is both valuable and distinct from descriptive language.   


have a good one!  
\--evan",MachineLearning
"Hey everyone - I made a discord bot awhile ago (now on \~450 servers you can get it [here](https://discord.com/oauth2/authorize?client_id=1022993363475116082&permissions=2147485696&scope=bot)) to run Stable diffusion inference in a discord server and I've open sourced all of the code to run it on GitHub:

[https://github.com/mystic-ai/pipeline/tree/main/examples/apps/stable\_diffusion\_discord\_bot](https://github.com/mystic-ai/pipeline/tree/main/examples/apps/stable_diffusion_discord_bot)

It has everything needed to run it including some Dockerfiles + docker-compose.yml to run it out of the box once you get your auth tokens from discord. Inference takes normally around 2-3s per 512x512 image",MachineLearning
"Hello :-) ML-EDM is a new field of research that consists in optimizing the decision moments of a Machine Learning model that observes data collected over time. Here is a community dedicated to this topic: r/EarlyMachineLearning Do not hesitate to subscribe, we will add content soon, such as scientific articles, instructional videos, and later a python library and tutorials ... see you soon!",MachineLearning
"It seems like a lot of prominent ML researchers are pretty active on Twitter, and that it's a decent place to hear about new research and promote your own stuff. But, the inmates appear to have taken over the asylum over there. Will there be, for example, an ML Mastodon instance?",MachineLearning
"What scenarios have you used GPU profiling tools(Ex. NVIDIA Nsight)  to improve model inference speed? 

Is the primary use case when models are deployed as part of a more complex pipeline? Or can it be useful for improving stand-alone models?

Finally, are there any references/books/tutorials that you'd recommend to learn more about the tech/tools/practices for model inference? Definitely leaning more towards practical/engineering aspects rather than model-centric (Eg. Quantization, Pruning).",MachineLearning
"There are several resources to understand how to make a GAN from scratch. But with Diffusion Models, the DDPM paper code and especially the Improved DDPM paper code are so hard and complicated to understand.

I'm currently doing research on Diffusion Models. I understand the math very well from the paper: ""Understanding Diffusion Models: A Unified Perspective"". It gives a very intuitive and step by step guide to the mathematics and intuition behind Diffusion Models. I loved the paper. But from that paper, I don't have the necessary talent or skill to reproduce the code. It's too big of a project for me to do on my own from scratch. 

So, I wanted help from this community which can give me some guide on how to find articles, GitHub repos or YouTube videos which gives me step by step guide on how to build research level code on Diffusion models from scratch. 

The closest I found was the annotated diffusion model from the Huggingface community but that was very basic and when I wanted to reproduce their own repo on GitHub(annotated diffusion is the name of the blog but they have their own repo on Diffusion model in Pytorch which is in research level) in Pytorch, that was too heavy for me. 

Also, maybe some other guides maybe also helpful on how can I can start from scratch and build models and eventually go to a stage where I can reproduce results like in the Improved DDPM paper. 


I have the hardware resources in my laboratory. But no one in my lab has done any projects on Diffusion Models. So, I'm the first one. That's why I had to find resources on my own.

It will be a big help for me if I can find some help from this community. Thank you very much in advance.",MachineLearning
"Hi all, 

in our recent paper we cast text-to-image generation as a retrieval task, thereby connecting text-to-image models to information retrieval. An essential part of our paper is a case study on game artwork search using Stable Diffusion in which we demonstrate the challenges of prompt engineering. We are curious to hear your feedback!

arXiv: https://arxiv.org/pdf/2212.07476.pdf
Twitter thread: https://twitter.com/webis_de/status/1604469981043134465

Abstract:
> The text-to-image model Stable Diffusion has recently become very popular. Only weeks after its open source release, millions are experimenting with image generation. This is due to its ease of use, since all it takes is a brief description of the desired image to “prompt” the generative model. Rarely do the images generated for a new prompt immediately meet the user’s expectations. Usually, an iterative refinement of the prompt (“prompt engineering”) is necessary for satisfying images. As a new perspective, we recast image prompt engineering as interactive image retrieval—on an “infinite index”. Thereby, a prompt corresponds to a query and prompt engineering to query refinement. Selected image–prompt pairs allow direct relevance feedback, as the model can modify an image for the refined prompt. This is a form of one-sided interactive retrieval, where the initiative is on the user side, whereas the server side remains stateless. In light of an extensive literature review, we develop these parallels in detail and apply the findings to a case study of a creative search task on such a model. We note that the uncertainty in searching an infinite index is virtually never-ending. We also discuss future research opportunities related to retrieval models specialized for generative models and interactive generative image retrieval. The application of IR technology, such as query reformulation and relevance feedback, will contribute to improved workflows when using generative models, while the notion of an infinite index raises new challenges in IR research.",MachineLearning
"Hey guys wanted to show you my app which offers a convenient frontend to use Whisper for transcriptions with Libretranslate to power automatic translations

Code is all open-source here: [https://github.com/mayeaux/generate-subtitles](https://github.com/mayeaux/generate-subtitles)

Also running an instance that you can use for free at [https://freesubtitles.ai](https://freesubtitles.ai/)",MachineLearning
"There are two recent works that I came across, both of which try to use MAML for generative and image translation tasks respectively. To my eye there are various potential problems pursuing this path, I believe that both papers have been accepted as workshop/conference papers. I took CS 330 this autumn and I have been analyzing these papers as part of my final project. Here are some issues that are worrying me. Let's start with the first paper, [Meta-GAN for Few-Shot Image Generation | OpenReview](https://openreview.net/forum?id=SE3Gy6E_PWq) . The first reviewers seem to miss that he did not actually train with just one of (0-8) he trained with all except 9. But that is a relatively minor issue. Here are the three major issues.

1, How does this not suffer from serious memorization issues? MAML does poorly when a single function can solve all tasks. The meta learner has no need to look at the support sets at all. . The domains have no overlap at all, MAML relies on at least some inputs having different outputs across tasks. The tasks are mutually exclusive(just train a normal MNIST GAN)

2. How can you be sure that the GAN's converge? The zero sum game does not apply anymore, the MAML outer loop is providing feedbacks to the inner loop learners at each outer loop step (resetting weights to theta), and so there is an additional player here (the meta learner), my point is that the original GAN paper shows that at convergence (equilibrium) Pg -= Pdata and that is the only solution(no other equilibrium) Can we say the same here even if we ignore assumption 1 (tasks being mutually exclusive).

3. He claims that training from scratch does not even generate 9's. It's not clear to me if he is training from absolute scratch or if he is training a pretrained GAN (trained using normal methods on 0-8). His GAN seems to generate very little variety, looks like mode collapse and memorization if you look at the images closely. He is using an overparameterized Resnet on MNIST, which seems like an overkill, then training on 9's using the meta GAN model for 10k iterations (using between 1 to 32 ) nines. But there are no meta testing images provided. You would have to take an unseen 9 (via z noise) and show that it actually works

4. One reviewer claims that the mode collapse can be attributed to using few shot (for 9's), but that is the whole point of meta learning, the task here is generation, so we need to generate using few shot data. He should ideally be training for a small no of iterations on a few training samples (by truncating z\_noise) and then testing to see if the model continues generating 9's with variety by inferring from the previous digits (different thickness, orientations, brightness etc).

This is the second paper,

[2112.11929.pdf (arxiv.org)](https://arxiv.org/pdf/2112.11929.pdf)

The issues are the same,

1. Memorization and mode collapse. GAN losses can be relatively low in these scenarios too.
2. Reconstruction error is not a great metric to judge performance of the GAN,
3. Apart from these two metrics (that are not really suggestive of all that much in GAN's) , they did worse in most other metrics.
4. Many of their learners collapse, suggesting mode collapse.

tl:dr : Has anyone analyzed if MAML and GAN's are compatible? Can the GAN's be shown to converge? will MAML converge? How about memorization issues when formulated as the above authors do? Thanks",MachineLearning
"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",MachineLearning
"Hi Everyone,

I have good knowledge on standard machine learning models and techniques and have some knowledge of deep learning. I want to tackle the field of sports analytics. Is there any platform for good resources and problems ?",MachineLearning
"Our paper has been accepted to AAAI-23. However, none of the authors will be able to attend in-person as the visa waiting times are high in our country. The original mail regarding paper acceptance mentioned that they were planning alternatives for people who won't be able to attend in-person but they didn't give any updates after that. They recently sent a mail regarding video submission where they mentioned that atleast one author is required to attend in person. 

I have already tried mailing aaai23@aaai(dot)org and aaaireg@aaai(dot)org, but did not receive any response. Did anyone else receive any information regarding this. Can anyone tell me what further steps i can take? Thanks in advance

PS. Forgive me if this is not the right place to post, i don't know where else to ask",MachineLearning
"Is this possible?

Train transformers on a quantum computer to model them for classical computing purposes such as running quantum cross validated regression locally?

[https://discuss.huggingface.co/t/quantum-transformer/28044](https://discuss.huggingface.co/t/quantum-transformer/28044)",MachineLearning
"I’ve been toying with the idea of building an unsupervised model to find (demographic and other) patterns between someone’s followers on Instagram/TikTok. Aside from jumping over hurdles to actually acquire training data, does anyone have any relevant experience/insights about this? (ie the best model framework to use, the best data to train the model on to find salient patterns, etc)",MachineLearning
" I just installed yolo but there was an issue finding my GPU. I was getting this error: 

> AssertionError: Invalid CUDA '--device 0' requested, use '--device cpu' or pass valid CUDA device(s)
 
I think the requirements.txt installed by pip is installing the CPU only pytorch. The easy workaround is to just install the correct pytorch first. 

Note: I did this in a conda environment so if you want to do that too then 
> conda create --name yolov7 python=3.9

> conda activate yolov7


What you should do is install torch first before the requirements.txt and to get the configuration you want @ https://pytorch.org/.  Make sure to choose CUDA. I just selected the pip with the latest cuda but you can do the conda install if you want.

> git clone https://github.com/WongKinYiu/yolov7

> cd yolov7

> pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117
 
> pip install -r requirements.txt  

Then when i used yolo it detected my gpu using --device 0:

> YOLOR  v0.1-116-g8c0bf3f torch 1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 2070, 8191.5625MB)**

if you installed pytorch already through the requirements.txt you need to uninstall (if you don't want to start from scratch in a different virtual environment)
>pip uninstall torch torchvision",MachineLearning
"So a few months ago Yann LeCun introduced JEPA as potentially the new BigThing^TM . It looked like it was full of good ideas, and I was wondering if anyone was working publicly on that? I would assume some people at Meta probably do under LeCun's direction but I am curious what other people think about it?

I would especially love to hear from people who considered it and found some roadblocks :-) As I am wondering if it is worth spending a few days working on it on my side.",MachineLearning
"\[P\]

Hi! I have a ML model which i have exported as tensorflow Lite file. I need to integrate it to the Android Application(Java - Android Studio) to generate results according to the Cough Input via mic. I'm newbie to this. I have attached the python code and also java code below:

Here's the snippets to the the ML model code.

[https://imgur.com/mROdfQI](https://imgur.com/mROdfQI)

[https://imgur.com/vXTBsjc](https://imgur.com/vXTBsjc)

[https://imgur.com/k8B8J4S](https://imgur.com/k8B8J4S)

[https://imgur.com/YK8JJpM](https://imgur.com/YK8JJpM)

[https://imgur.com/tDR17EN](https://imgur.com/tDR17EN)

&#x200B;

Here the code of java where I need to Implement this functionality:

&#x200B;

Here's the layout Ss

[https://imgur.com/JQkq1jX](https://imgur.com/JQkq1jX)

&#x200B;

This is where i need to implement this model to fetch results

[https://imgur.com/jA9RYNS](https://imgur.com/jA9RYNS)

&#x200B;

Let me know if you need more details",MachineLearning
"What are the most critical advances in differentiable world models of this year? 

I haven't read much new in that direction since the Dreamer papers. Is there anything new that you think is promising or that you have tried and that works well?",MachineLearning
"I was reading a little bit about ChatGPT training which led me to a realization how smart of a move making it free to use actually is. We basically know that during the training ChatGPT uses human feedback, which is relatively expensive to get. However, by making it free to use and providing users an option to give feedback opens a door to massive amounts of training data for a relatively cheap price per training sample (the cost of running server). This approach is quite fascinating to me, and makes me wonder about other similar examples of this, so I would like to hear them in the comments if you have any?",MachineLearning
"Hi all,

We are training a distilBART model to summarize podcasts.

We want to be able to properly document the process, and how each decision affects the model.

So far that has included using rouge scores to determine the performance. If there are any other things you think we should do, please let me know.

**But back to the question from the title:**

For some reason, I just can not figure out how to control the training results. *I want to see training- and validation loss after every epoch*, but it keeps either putting it at some weird interval like [here](https://huggingface.co/emmyapi/distilbart-podimo-data-5) ([code](https://github.com/emmyapi/School/blob/main/PodcastProject/emmyapi-distilbart-podimo-data-5.ipynb)), or not at all like [here](https://huggingface.co/emmyapi/distilbart-podimo-data-7) ([code](https://github.com/emmyapi/School/blob/main/PodcastProject/emmyapi-distilbart-podimo-data-7.ipynb)).

Will appreciate any help and general criticism of what we are doing!",MachineLearning
"I'd like to use one of those engine's for Q/A

I see some nice tools out there like nshepherd and happytransformer, but neither of them use squad, but I do see some GPT-Neo squad models out there (for ex with GPT-NeoX)

[https://www.forefront.ai/blog-posts/how-to-fine-tune-gpt-neox](https://www.forefront.ai/blog-posts/how-to-fine-tune-gpt-neox)",MachineLearning
"So I was wondering, basically the title.

If my CNN model is trained to classify images into cat and dog, and I show it an image of a horse. My model should be giving either dog or a cat as the answer, however the confidence of this answer (passing horse through a softmax) should be low. 

But I have found that mostly the models are quite cocky with a high confidence that it is indeed a dog. Or a cat. 

So is there a better way? Is there a technique or method or algorithm that gives accurate confidence on a classification?",MachineLearning
"Hi Everyone, I've been catching up with the talks that happened in neurips 2022 as I could not follow it when it was happening because of my quarter finals. However I have not been able to figure out how to download the slides of the talks. Does anyone here know how to download them ?",MachineLearning
"Hello,

I am working on a problem where I have a collection of product descriptions (which is a collection of 5-20 words) which I then need to classify into one of 110 classes.

The description will sometimes vary between instances for each class. For example, the description for each class can be slightly different - also - the descriptions sometimes contain incomplete and or misspelled words.

&#x200B;

|Description|Class|
|:-|:-|
|black rubber watch with water proof band|black 108 hex watch|
|waterproof band|black 108 hex watch|
|black waterproof atch|black 108 hex watch|
|watch black|black 108 hex watch|
|blue watch|blue 110 bit watch|
|bit watch|blue 110 bit watch|
|watch blue|blue 110 bit watch|

DescriptionClassblack rubber watch with water proof bandblack 108 hex watchwaterproof bandblack 108 hex watchblack watchblack 108 hex watchwatchblue 110 bite watchblue watchblue 110 bit watchblue bit watchblue 110 bit watch

I have roughly 1,800 classified training data points which are heavily skewed toward a few more popular products.

Does anyone have advice for solving this classification problem? Right now I'm using blazing text with ngrams = 5 and am achieving 50% accuracy. However, I am wondering if there is a simpler way to solve this?

&#x200B;

If you're interested - these are current hypter params I'm using:

&#x200B;

    bt_model = sagemaker.estimator.Estimator(
        container,
        role,
        instance_count=1,
        instance_type=""ml.c4.4xlarge"",
        volume_size=30,
        max_run=360000,
        input_mode=""File"",
        output_path=s3_output_location,
        hyperparameters={
            ""mode"": ""supervised"",
            ""epochs"": 500,
            ""min_count"": 1,
            ""learning_rate"": 0.05,
            ""vector_dim"": 10,
            ""early_stopping"": True,
            ""patience"": 5,
            ""min_epochs"": 10,
            ""word_ngrams"": 5,
        },
    )

&#x200B;",MachineLearning
"[https://arxiv.org/pdf/2112.13314.pdf](https://arxiv.org/pdf/2112.13314.pdf)

&#x200B;

 Deep Learning (DL) frameworks are now widely used, simplifying the creation of complex models as well as their integration to various applications even to non DL experts. However, like any other programs, they are prone to bugs. This paper deals with the subcategory of bugs named silent bugs: they lead to wrong behavior but they do not cause system crashes or hangs, nor show an error message to the user. Such bugs are even more dangerous in DL applications and frameworks due to the “black-box” and stochastic nature of the systems (the end user can not understand how the model makes decisions). This paper presents the first empirical study of Keras and TensorFlow silent bugs, and their impact on users’ programs. We extracted closed issues related to Keras from the TensorFlow GitHub repository. Out of the 1,168 issues that we gathered, 77 were reproducible silent bugs affecting users’ programs. We categorized the bugs based on the effects on the users’ programs and the components where the issues occurred, using information from the issue reports. We then derived a threat level for each of the issues, based on the impact they had on the users’ programs. To assess the relevance of identified categories and the impact scale, we conducted an online survey with 103 DL developers. The participants generally agreed with the significant impact of silent bugs in DL libraries and acknowledged our findings (i.e., categories of silent bugs and the proposed impact scale). Finally, leveraging our analysis, we provide a set of guidelines to facilitate safeguarding against such bugs in DL frameworks.",MachineLearning
"Hi! I’m looking for a Post Doc (for Spring 2023) interested in work at the intersection of machine learning and energy systems, particularly collaborative urban systems such as smart and connected building communities. Moreover, I am also informally looking for PhD students (Fall 2023).

###About
We are based in the School of Systems and Enterprises at Stevens Institute of Technology. Our group works on projects at the intersection of Energy, the Built Environment and Data. Projects look at ways we can leverage data to enable an energy efficient and equitable clean energy transition. However, we apply our methods to a wide range of use cases, from understanding mobility in urban areas, to problems in engineering design.
As far as applications go, we’re particularly interested in projects that can eventually be used for groups/networks of intelligent and autonomous energy systems to cooperatively share energy resources. We’re super open to applicants from uncommon academic or personal backgrounds. 

### Applying
Please reach out to me informally through a direct message. 
The Post Doc salary range is 65K-80K.

Thank you for your time.


*This post structure was borrowed from a previous post by user wil3*",MachineLearning
"ChatGPT has taken the tech world by storm, but its older cousin GPT-3 is still relevant. Being able to connect to the text completion API through python allows you to use the large language model to [generate synthetic data](https://www.tonic.ai/blog/can-you-generate-realistic-data-with-gpt-3) with bespoke distributions and relationships. The application is limited, however, as the lack of on-prem deployment limits your ability to show the model your proprietary data to learn from due to privacy concerns.

Real data is complex, what do people think about using LLMs to generate synthetic data? Should they just stick to writing stories and jokes?",MachineLearning
"I am planning to take up an interesting NLP project, due to my limited exposure to NLP I am stuck at the moment. I want to extract 'goal' statements from lengthy reports. For example, the goals can be *We would be reducing our carbon footprint by 50% by 2025* or *Our company aims to increase the diversity in the work-force in upcoming months*. Check below image for example text and highlighted goals.

How can I go about the process of goal extraction, I would like to get some pointers on possible NLP approaches I can start with ?

Note that I do not have an annotated dataset with extracted goals.

https://preview.redd.it/z6houyh7ra6a1.png?width=970&format=png&auto=webp&v=enabled&s=403829bfdf000ea1c76e21fcd55c7728d86d92f6",MachineLearning
"I am actively using ChatGPT nowadays to seek assistance in various tasks such as fixing grammatical errors in manuscripts, to provide simplified/coherent explanations on technical jargon etc. This is giving me an impression that future jobs related to ""writing"" such as proofreaders might run out of business.",MachineLearning
"Thanks to everyone who replied to our [earlier post requesting pre-launch product feedback](https://www.reddit.com/r/mlops/comments/zd7hqy/feedback_requested_new_data_storage_tool_for/)! We’re excited to announce that we’ve now publicly launched [XetHub](https://xethub.com/?utm_source=reddit&utm_medium=organic&utm_campaign=xethub-intro&utm_content=link), a collaborative storage platform for data management.

I’ve been in the MLOps space for \~10 years, and data is still the hardest unsolved open problem. Code is versioned using Git, data is stored somewhere else, and context often lives in a 3rd location like Slack or GDocs.

This is why we built XetHub, a platform that enables teams to treat data like code, using Git.

Unlike Git LFS, XetHub doesn’t just store the files. It uses content-defined chunking and Merkle Trees to dedupe against everything in history, allowing small changes in large files to be stored compactly. Here’s how it works: [https://xethub.com/assets/docs/how-xet-deduplication-works](https://xethub.com/assets/docs/how-xet-deduplication-works)

XetHub includes a GitHub-like web interface that provides automatic CSV summaries and allows custom visualizations using Vega. And we know how painful downloading a huge repository can get, so we built Git-Xet mount—which, in seconds, provides a user-mode filesystem view over the repo.

Today, XetHub works for 1 TB repositories, and we plan to scale to 100 TB in the next year. Our implementation is in Rust (client & cache + storage) and our web application is written in Go.

XetHub is available today for Linux & Mac (Windows coming soon) and we’d love for you to try it out!

More info here:

* [https://xetdata.com/blog/2022/12/13/introducing-xethub](https://xetdata.com/blog/2022/12/13/introducing-xethub)
* [https://xetdata.com/blog/2022/10/15/why-xetdata](https://xetdata.com/blog/2022/10/15/why-xetdata)
* Hacker News discussion (launched on Show HN at #1): [https://news.ycombinator.com/item?id=33969908](https://news.ycombinator.com/item?id=33969908)

https://preview.redd.it/t9tf3kt5i96a1.png?width=1740&format=png&auto=webp&v=enabled&s=9a16df365edbb84916f0e7faae50a6bb77b38efa",MachineLearning
"**Hi ML practitioners,**

We wanted to bring to your attention another release of PyCM (Multi-class confusion matrix library in Python). In this version, `ROCCurve` class and `PRCurve` class are added to calculate and plot ROC Curve and Precision-Recall curve respectively. From now on, PyCM is able to calculate the area under curve of ROC and Precision-Recall curve for different threshold values using these new methods.

## ROC curve

    >>> crv = ROCCurve(actual_vector = np.array([1, 1, 2, 2]), probs = np.array([[0.1, 0.9], [0.4, 0.6], [0.35, 0.65], [0.8, 0.2]]), classes=[2, 1])
    >>> crv.thresholds
    [0.1, 0.2, 0.35, 0.4, 0.6, 0.65, 0.8, 0.9]
    >>> auc_trp = crv.area()
    >>> auc_trp[1]
    0.75
    >>> auc_trp[2]
    0.75

## Precision-Recall curve

    >>> crv = PRCurve(actual_vector = np.array([1, 1, 2, 2]), probs = np.array([[0.1, 0.9], [0.4, 0.6], [0.35, 0.65], [0.8, 0.2]]), classes=[2, 1])
    >>> crv.thresholds
    [0.1, 0.2, 0.35, 0.4, 0.6, 0.65, 0.8, 0.9]
    >>> auc_trp = crv.area()
    >>> auc_trp[1]
    0.29166666666666663
    >>> auc_trp[2]
    0.29166666666666663

The complete change log of this version is available [here](https://github.com/sepandhaghighi/pycm/blob/master/CHANGELOG.md).

&#x200B;

**Website:** [www.pycm.io](http://www.pycm.io)

**Repo:** [https://github.com/sepandhaghighi/pycm](https://github.com/sepandhaghighi/pycm)

&#x200B;

Hope you find it useful!",MachineLearning
"I am working on a research project, I am looking for some ideas from fellows. I have track sensor data with several variables on millisecond level. Now each 'dataset' is a session on the race track and the variables are temporally and spatially correlated. Now I have data from multiple session, and I am training a classfication models for these time series. How can I combine them into a single dataset or is it even appropriate to do it this way or rather train multiple models? I do have to say, the diffeent sessions have high variance with one another.",MachineLearning
"I'm intrigued by random forests but it looks like there's really no open problems in this area. A quick skim on Google Scholar shows, mostly, applications of random forests in various industries/problems. Are there research groups working on random forests?",MachineLearning
"**tl;dr**I built a site that uses GPT-3.5 to answer natural-language medical questions using peer-reviewed medical studies.

**Live demo:** [**https://www.glaciermd.com/search**](https://www.glaciermd.com/search?utm_campaign=reddit_post_1)

**Background**

I've been working for a while on building a better version of WebMD, and I recently started playing around with LLMs, trying to figure out if there was anything useful there.

The problem with the current batch of ""predict-next-token"" LLMs is that they hallucinate—you can ask ChatGPT to answer medical questions, but it'll either

1. Refuse to answer (not great)
2. Give a completely false answer (really super bad)

So I spent some time trying to coax these LLMs to give answers based on a very specific set of inputs (peer-reviewed medical research) to see if I could get more accurate answers. And I did!

The best part is you can actually trace the final answer back to the original sources, which will hopefully instill some confidence in the result.

Here's how it works:

1. User types in a question
2. Pull top \~800 studies from Semantic Scholar and Pubmed
3. Re-rank using `sentence-transformers/multi-qa-MiniLM-L6-cos-v1`
4. Ask `text-davinci-003` to answer the question based on the top 10 studies (if possible)
5. Summarize those answers using `text-davinci-003`

Would love to hear what people think (and if there's a better/cheaper way to do it!).

\---

**UPDATE 1:** So far the #1 piece of feedback has been that I should be *way* more explicit about the fact that this is a proof-of-concept and not meant to be taken seriously. To that end, I've just added a screen that explains this and requires you to acknowledge it before continuing.

&#x200B;

https://preview.redd.it/jrt0yv3rfb6a1.png?width=582&format=png&auto=webp&v=enabled&s=4403b866bd799f53b3ada48723d55fdd3ba93b1e

Thoughts?

**Update 2:** Welp that's all the $$$ I have to spend on OpenAI credits, so the full demo isn't running anymore. But you can still follow the link above and browse existing questions/answers. Thanks for all the great feedback!",MachineLearning
"Does anyone have Benchmarks of RTX 3090 vs RTX 3090 Ti for NLP (such as Transformer Fine-Tuning or anything), Computer vision or deep learning problems. I am trying to see if speed difference makes sense or not for these models and also if its possible to run multiple models in parallel on these machines without frying them ?",MachineLearning
"Hi all,

I'm running a small scale ML experiment with human subjects, and the lab ran into funding issues right when we were ready to launch the final version of the experiment. Are you aware of any small scale (<$10k), fast turnaround (<month) grants or scholarships that we can apply for to close this financial gap?

If it matters, the lab is part of a US research institution, the researchers have a green card but are not US citizens. 

Thanks!!",MachineLearning
"I  am currently learning tools for data science, in particular in a  business analysis setting for pricing strategy, demand forecast, etc. I  am currently reading Géron's Hands on ML and I am fascinated by some of  the ML concepts, such as regressions and random forests, and see the  potentials of these tools for business data science.

But  now when I am reading the neural network part of the book with Keras  and TensorFlow, I slowly realised that these tools are used for a really  big datasets and features for tasks such as computer vision, voice  recognition, etc and not for business analysis applications. Am I right  in this feeling that the basic ML tools are enough, or is there real  application for neural networks and advanced ML tools for business data  analysis?",MachineLearning
"I am looking for the hyper-parameter settings that could produce the highest accuracies for **plain ViT** (i.e., without modifying the model architecture) on ImageNet-1K, **training from scratch**. A lot of people in this sub have experience with ViT so I hope I could get some help here.

For ViT-S, we have a recipe that can achieve 80.0% top-1 accuracy from this paper: [Better plain ViT baselines for ImageNet-1k](https://arxiv.org/abs/2205.01580). Unfortunately they did not experiment with larger architecture (ViT-B or ViT-L).

For ViT-B, ViT-L and ViT-H, the authors of [MAE](https://arxiv.org/abs/2111.06377) claimed to achieve 82.3%, 82.6% and 83.1%, respectively (see their Table 3). However, I was unable to reproduce these results using their code and their reported hyper-parameters.

Any references to strong ViT baselines with reproducible results would be very much appreciated! Thanks.",MachineLearning
"I remember reading a paper a while back that showed early attention layers in a transformer could be replaced with a simpler mechanism since most heads only modeled small n-grams. I think they used some kind of pooling?

Wondering if anyone knows which paper that was and had any thoughts about it since then. Thanks!",MachineLearning
"Currently I noticed that some pytorch training modules use Learning Rate Scheduler and momentum rate Scheduler , a lot of momentum rate schedulers exist similar to LR scheduler ranging from  Lambda, Cosine, Cyclic schedulers , one article that caught my eye that was very interesting was something called [Demon](https://towardsdatascience.com/why-0-9-towards-better-momentum-strategies-in-deep-learning-827408503650) (paper: [link](https://arxiv.org/pdf/1910.04952v4.pdf)) where the momentum starts at a very high value such as 0.9 and then reduces to a very low value towards the end.

So my idea was to implement a OneCycle Learning Rate Scheduler with a warmup of 15 epochs which starts from 0.0002 and then goes to 0.1 and then falls back to a very low value towards the end of the last epoch to about 0.00002 and also implement the Demon momentum Scheduler.

Now my question is will using a demon momentum where we end towards a very low value useful. Now according to the stochastic gradient update equations, initially we would give importance to the average value of the change in gradients and towards the end we would give importance to the next immediate change in gradients using the demon momentum (due to low value) and since the learning rate would be low, the change in the weights would also be minimal , is this useful ? , or in other [articles](https://www.kaggle.com/code/residentmario/one-cycle-learning-rate-schedulers/notebook) they suggest using low momentum for high learning rate and towards the end of the epoch increase the momentum and decrease the learning rate.

So basically first it starts with a momentum with SGD then slowly towards the end it becomes a vanilla SGD.

[Demon momentum scheduler](https://preview.redd.it/eyumc0jyz26a1.png?width=809&format=png&auto=webp&v=enabled&s=19ce92175f9e597db7c16bafe56101b161c302d8)

[One Cycle LR cosine sheduler](https://preview.redd.it/yea5rjczz26a1.png?width=808&format=png&auto=webp&v=enabled&s=f7a4d208e2af1e3d66a84d035bc6395740c97646)

[SGD momentum formulas](https://preview.redd.it/nyxxa340036a1.png?width=694&format=png&auto=webp&v=enabled&s=c83e0de2652a77db0a7ba931bd0c7cb02ca81fe7)",MachineLearning
"Imagine a situation where you have thousands of documents

But need to find an answer from the documents

And at the same time, get the document where the answer is coming from

You could open and search the documents one by one

But that would take forever

Enter Extractive Question Answering with Sparse Transformers

With Extractive Question Answering, you input a query into the system

And in return, you get the answer to your question and the document containing the answer.

Extractive Question Answering enables you to search many records and find the answer.

It works by:  
\- Retrieving documents that are relevant to answering the questions.  
\- Returns text that answers that question.

Language models make this possible.

For example, the receiver can be a masked language model.

The reader can be a question-answering model.

The challenge of these language models is that they are quite large.

The size makes it hard to deploy the models for real-time inference.

For example, deploying big models is not possible on mobile devices.

Furthermore, inference time, latency, and throughput are also critical.

The solution is to reduce the model's size while maintaining its accuracy.

Making the model small is easy but maintaining accuracy is challenging.

These can be achieved by pruning and quantizing the model.

Pruning involves removing some weight connection from an otherwise overprecise and overparameterized model.

Furthermore, you can reduce the precision of the floating points to make the model smaller.

In today's article, I cover this in more detail. Including:  
💡 Document retrieval with DeepSparse and arXiv dataset  
💡 Document retrieval with a dense and sparse model  
💡 Comparing the performance between dense and sparse models

Read the full article: [https://neuralmagic.com/blog/search-documents-quickly-with-extractive-question-answering-and-sparse-transformers/](https://neuralmagic.com/blog/search-documents-quickly-with-extractive-question-answering-and-sparse-transformers/)",MachineLearning
"**TL;DR**

Image search with open vocabulary localization using both index and search time methods.

Article (no paywall): [https://medium.com/@jesse\_894/image-search-with-localization-and-open-vocabulary-reranking-using-marqo-yolox-clip-and-owl-vit-9c636350bf66?source=friends\_link&sk=b4e94d9d4095a2b8b60c5d1904a60825](https://medium.com/@jesse_894/image-search-with-localization-and-open-vocabulary-reranking-using-marqo-yolox-clip-and-owl-vit-9c636350bf66?source=friends_link&sk=b4e94d9d4095a2b8b60c5d1904a60825)

Markdown: [https://github.com/marqo-ai/marqo/blob/mainline/examples/ImageSearchLocalization/article.md](https://github.com/marqo-ai/marqo/blob/mainline/examples/ImageSearchLocalization/article.md)

Code: [https://github.com/marqo-ai/marqo/blob/mainline/examples/ImageSearchLocalization/index\_all\_data.py](https://github.com/marqo-ai/marqo/blob/mainline/examples/ImageSearchLocalization/index_all_data.py)

I wanted to have a few choices getting localization into image search (index and search time). I immediately thought of using a region proposal network (rpn) [from mask-rcnn](https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md) to create patches that can also be indexed and searched (and add the localisation). I figured it might be somewhat agnostic to classes. I did not want to use [mmdetection](https://github.com/open-mmlab/mmdetection) or [detectron2](https://github.com/facebookresearch/detectron2) due to their dependencies and just getting the rpn was not worth it.  I was encouraged by the PyTorch [native implementations](https://pytorch.org/vision/main/models/faster_rcnn.html) of detection/segmentation models but ended up finding [yolox](https://github.com/Megvii-BaseDetection/YOLOX) the best.  

I also implemented one based on the self attention maps from the [DINO](https://github.com/facebookresearch/dino) trained ViT’s. This worked pretty well when the attention maps were combined with some traditional computer vision to get bounding boxes. It seemed an ok compromise between domain specialization and location specificity. I did not try any saliency or gradient based methods as i was not sure on generalization and speed respectively. I know [LAVIS](https://github.com/salesforce/LAVIS) has an implementation of [grad cam](https://github.com/salesforce/LAVIS/blob/main/projects/pnp-vqa/pnp_vqa.ipynb) and it seems to work well in the [plug'n'play vqa](https://github.com/salesforce/LAVIS/tree/main/projects/pnp-vqa). 

For the indexing I cropped the images based on the proposed bounding boxes. I did not test blending methods but feel this might be better as more context can be in the image. If anyone has a perspective on this I would love to hear it. 

For localisation at search time I ended up using [OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit). This worked really well. I did not try [Detic](https://github.com/facebookresearch/Detic) or [CLIPseg](https://github.com/timojl/clipseg) but would be interested to hear if anyone else has tried these?",MachineLearning
"Say I have some trained model that learns an embedding eg: classifier. I freeze it and chop of all layers past the embedding that I want.

Now using this embedding, I train a decoder to reconstruct the original encoded object. Would the reconstruction give me a qualitative insight into what sort of representation the frozen model has learned? If so, why? 

An example with images: I observe that human eyes are reconstructed quite well, but everything else is unclear, giving me the indication that my learned embedding successfully encodes eyes.",MachineLearning
"How to set up multiple ocr in such a way that the final recognition is better keeping in mind that it should work well in production as we..

What methods can be used to get better output for each text from multiple OCR ? How can we determine which OCR produce better result in multiple OCR setup.",MachineLearning
"Been experimenting with language models a lot lately and wondering if human generated text (i.e. ""natural"" text) is really supposed to be maximally likely according to language models even after training. For example, has someone checked likelihood of human translated text to likelihood of machine translated text according to a language model like GPT-3 ? 

&#x200B;

Are there any works that do this already ? Does this idea even make sense to begin with ?",MachineLearning
"We recently open-sourced Inseq, a Python library built on top of 🤗  transformers and Pytorch, aimed at democratizing and commoditizing post-hoc interpretability analysis for sequence generation models.

[https://github.com/inseq-team/inseq](https://github.com/inseq-team/inseq)

Inseq supports thousands of 🤗 decoder-only and seq2seq models, with various attribution methods already baked in and many more to come.  Attributing MetaAI's Galactica writing LaTeX formulas or GoogleAI  Flan-T5 doing commonsense reasoning now takes only 3 lines of code!

The Inseq CLI improves the user experience when conducting global analyses by enabling batched attribution of examples and even entire datasets from the 🤗 Hub directly from the console. Inseq is beginner-friendly but also fully extensible for advanced use cases, supporting attribution of custom functions and the extraction of step scores during generation.

With Inseq, we aim to centralize and standardize some practices of the interpretability community working on NLG and NMT, to enable fair and reproducible evaluation. The project is still in its infancy, and feedback/contributions are very much appreciated!",MachineLearning
"My theory:

* no good datasets, as opposed to image datasets like LAION
* harder/illegal to get music datasets. Shady methods are usually required to get large music datasets (like torrenting).
The only music datasets I've found are classical, and even then, very limited as performances of classical music are still copyrighted.

Therefore, large companies like OpenAI/Google are unable to take the risk in making a good generative music AI due to legal reasons. Startups have a better chance because they have less to lose and can better hide the fact that they trained their model with copyrighted material.

Other than that, I don't believe audio is more challenging to process than images because the complete audio file can be reduced to its spectrogram, which is just a 2D image.

TLDR: No good datasets",MachineLearning
"Instead of explicitly making the output have certain statistical properties using the various norm layers, why not enforce this criteria through an auxiliary loss function?",MachineLearning
"I am working on a problem where the negative/0 label to postie/1 label ratio is 180MM/10MM. The data size is around 25GB and I have >500 features. Certainly, I don't want to use all 180MM rows of majority class to train my model due to computational limitations. Currently, I simply perform an under-sampling from majority class.  However, I have been reading that this may cause loss of the useful information or cause difficulties for determining the decision boundary between the classes (see [https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/](https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/)). When I do the under-sampling, I try to make sure that distribution of my data stays the same. I am wondering if there is a better way to handle this?",MachineLearning
"Project repo: [https://github.com/Linyou/taichi-ngp-renderer](https://github.com/Linyou/taichi-ngp-renderer)

Instant NGP is a novel view synthesis framework that reduces the model training for a single scene from hours to a few seconds. 

This project is a CUDA-free instant NGP renderer implemented in Taichi. Supported by Taichi's built-in GUI system, the project supports real-time rendering and camera interaction while consuming less than 1GB of VRAM. It also contains a fully fused multilayer perceptron (MLP) enabled by the SharedArray feature. 

The following are some pre-trained NeRF synthesis scenes: 

https://preview.redd.it/kdlwzps1uy5a1.png?width=1236&format=png&auto=webp&v=enabled&s=37fdf6fcf1e3adcc4844b14464636d819c2bc3fc",MachineLearning
"Why is it that when I go to create a CNN with 4 layers (output channels: 64, 32, 16, 16), I can do this in PyTorch, but in Tensorflow I get resource errors saying I don't have enough resources?  


For reference I am using a stock NVIDIA RTX 3080.  


Also, now that I am experimenting with larger models, would I benefit from renting TPU?  Does this make the actual models train faster and would it help with larger batches?",MachineLearning
"I'm  trying to understand the following breakdown of batch sizes in the  realm of high learning rates and fast model training from this post:

[https://myrtle.ai/learn/how-to-train-your-resnet-2-mini-batches/](https://myrtle.ai/learn/how-to-train-your-resnet-2-mini-batches/)

Specifically, this bit:

>*The results above suggest that if one wishes to train a neural network   at high learning rates then there are two regimes to consider. For the   current model and dataset, at batch size 128 we are safely in the  regime  where forgetfulness dominates and we should either focus on  methods to  reduce this (e.g. using* [*larger models with sparse updates*](https://arxiv.org/abs/1701.06538) *or perhaps* [*natural gradient descent*](https://arxiv.org/abs/1805.07810)*),   or we should push batch sizes higher. At batch size 512 we enter the   regime where curvature effects dominate and the focus should shift to   mitigating these.*

In combination  with the explanation in the rest of the post about how large learning  rates cause catastrophic forgetting *within* a single epoch, is he just  saying that at any given learning rate, you will always have a higher  bound of accuracy you can achieve due to forgetting and so you might as  well increase the batch size as much as you can for faster training, but  the upper bound of accuracy that can be achieved will remain the same?  That is, the benefit of large batch sizes is the speed of convergence,  not the actual accuracy?

If so,  why does he increase the learning rate by 10% (mentioned at the  start of the post) when increasing the batch size to 512? Based on his  explanation that higher learning rates cause forgetting within an epoch  and the fact that at some point, larger batch sizes cause issues due to  curvature effects, shouldn't he be lowering the learning rate?  Otherwise, he's both increasing the amount of forgetfulness (due to the  10% higher learning rate) and inching closer to the boundary where  larger batch sizes result in losses due to curvature effects?

In  my own tests with a similar learning rate scheme but a slightly  different set up, I'm finding that increasing batch size from 128 to 256  improved accuracy, while going to 512 decreases it. If 512 is where  curvature effect start showing up in my case, that makes sense, but  shouldn't the accuracy for smaller batch sizes remain the same, as the  same learning rate means the same upper bound for accuracy and the only  difference between 128 and 256 should be the training time?",MachineLearning
"Paper: [https://arxiv.org/abs/2212.03551](https://arxiv.org/abs/2212.03551) 

Twitter expanation: [https://twitter.com/mpshanahan/status/1601641313933221888](https://twitter.com/mpshanahan/status/1601641313933221888) 

Reddit discussion: [https://www.reddit.com/r/agi/comments/zi0ks0/talking\_about\_large\_language\_models/](https://www.reddit.com/r/agi/comments/zi0ks0/talking_about_large_language_models/) 

Abstract:

>Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sitting squarely at the centre of this intersection are large language models (LLMs). **The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are.**This trend is amplified by the natural tendency to use philosophically loaded terms, such as ""knows"", ""believes"", and ""thinks"", when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to **remind ourselves of how LLMs, and the systems of which they form a part, actually work.** The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.

https://preview.redd.it/e5j3z4t5fx5a1.jpg?width=557&format=pjpg&auto=webp&v=enabled&s=1a79b1bd7977ae3b1fb958b4573109d5d977df86

https://preview.redd.it/ec1w07t5fx5a1.jpg?width=675&format=pjpg&auto=webp&v=enabled&s=8237001ed02b31d2a286823267ad0761f2c712a6

https://preview.redd.it/ploj8ft5fx5a1.jpg?width=1138&format=pjpg&auto=webp&v=enabled&s=cffe8d8bf0db7a9b9af3f5cf2ca995fce771f441

https://preview.redd.it/33pa69t5fx5a1.jpg?width=428&format=pjpg&auto=webp&v=enabled&s=e5d107f8e75b6ec147cee6fb6084e7a0e16b9eff

https://preview.redd.it/umei7it5fx5a1.jpg?width=735&format=pjpg&auto=webp&v=enabled&s=955f0b58a22f19cd44d5546a44dfc950885852ad

https://preview.redd.it/mycwiat5fx5a1.jpg?width=364&format=pjpg&auto=webp&v=enabled&s=94b1def6bb379b6ff7745d8d537c059d7d7e3355

https://preview.redd.it/dp93met5fx5a1.jpg?width=498&format=pjpg&auto=webp&v=enabled&s=3d043d75f4c3b592abcf516c3376378a17f9d2f8

https://preview.redd.it/yr2rxht5fx5a1.jpg?width=867&format=pjpg&auto=webp&v=enabled&s=2d811678bbb10ff949de9e810bd22ee42990676c",MachineLearning
"Another year of has passed, and we’ve seen exciting progress in research around self-supervised learning in computer vision. We’re very excited that some of the recent models such as [Masked Autoencoders (MAE)](https://docs.lightly.ai/self-supervised-learning/examples/mae.html) or [Masked Siamese Networks (MSN)](https://docs.lightly.ai/self-supervised-learning/examples/msn.html) have been added to our [OSS framework](https://github.com/lightly-ai/lightly).  


The framework is also more and more used in [research](https://github.com/lightly-ai/lightly#lightly-in-research), ranging from medical imaging labs to big tech companies. Although we only have limited resources, we’re happy to make at least a small contribution to the community.  


The framework is built on top of PyTorch and is compatible with frameworks such as PyTorch Lightning for scaling across multiple GPUs.We are curious to hear your feedback.",MachineLearning
"I am trying to recall a recent paper about deep learning activity flow where the authors introduced a penalty term which helps activity flow in the network.  The authors show that this helps avoid the vanishing gradient problem and also show that even with poor initialization, they can train well because of their proposed method.

&#x200B;

The paper proposed an auxiliary loss at the activation level which was able to overcome poor weight initialization and use of sigmoid or tanh activation functions.  I have been searching all day and can't find it.  I think I originally found it on [https://papers.labml.ai/papers/weekly/](https://papers.labml.ai/papers/weekly/)

&#x200B;

**UPDATE**: Paper found! "" Improving the Trainability of Deep Neural Networks through Layerwise Batch-Entropy Regularization ""[https://arxiv.org/pdf/2208.01134.pdf](https://arxiv.org/pdf/2208.01134.pdf).  Thank you to  

[**vladfeinberg**](https://www.reddit.com/user/vladfeinberg/)",MachineLearning
" 

Hello Everyone 👋,

I just implemented the paper named [AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/pdf/2010.11929v1.pdf) popularly known as the **vision transformer** paper. This paper uses a Transformer encoder for image recognition. It achieves state-of-the-art performance without using convolutional layers given that we have a huge dataset and enough computational resources.  
Below I am sharing my implementation of this paper, please have a look and give it a 🌟 if you like it. This implementation provides easy-to-read code for understanding how the model works internally.

My implementation: [GitHub Link](https://github.com/TheTensorDude/vision_transformer_tf)

Thanks for your attention. 😀",MachineLearning
"I am solving a graph-level problem. I want to fit graph embeddings to a learn-to-rank NN to rank the graphs by their ""quality"". The ""quality"" of the graphs is determined by whether they have certain shape or structure, say they have self-loops and no loose end, has many split nodes and merging nodes etc. The node and edge features are not in consideration.

To my understanding, graph embeddings are best suited for graph similarity comparison, are there any techniques that can fit my use case?",MachineLearning
"The standard method is to normalize the entire dataset (the training part) then send it to the model to train on. However I’ve noticed that in this manner the model doesn’t really work well when dealing with values outside the range it was trained on. 

So how about normalizing each sample between a fixed range, say 0 to 1 and then sending them in.

Of course the testing data and the values to predict on would also be normalized in the same way. 

Would it change the neural network for the better or worse?",MachineLearning
"As above. I know there are a deluge of papers out there, but i am looking for a modern but lightweight network that can consume unorganized pt clouds, ideally in batch form (though i am not sure how this will work if points are of different sizes?) and produce 6dof pose, + 3 dof dimensions optionally. 

I assume it's going to be some kind of lightweight PointNet++ type architecture, but would be great if i can be pointed to some resources",MachineLearning
"Excuse my naivety on this subject

Wouldn't  it be awesome if models like OpenAI's GPT  were additionally trained on  all Scientific papers. Downloadable from Sci-Hub

The AI's insight would be amazing.

Is there any plan to do this?

Thoughts?",MachineLearning
"You can read the release notes here: [https://github.com/Farama-Foundation/Gymnasium/releases/tag/v0.27.0](https://github.com/Farama-Foundation/Gymnasium/releases/tag/v0.27.0). You can upgrade from 0.26 without any changes unless you're doing something *very* uncommon; this is how releases will generally be going forward.

&#x200B;

If you're unfamiliar with the maintenance of OpenAI's Gym package transitioning to Gymnasium, you can take a look at the full back story here: [https://farama.org/Announcing-The-Farama-Foundation](https://farama.org/Announcing-The-Farama-Foundation)",MachineLearning
"At [edenai](https://www.edenai.co/) we're trying to compete specialized AI models VS OpenAI's GPT3 in different applications (kw extraction, sentiment analysis, NER ...etc). 
So we need to find the best prompt for doing NER with GPT3 but not verry succesfully. We need at least a list of words and their types (class). 

Does anyone have an idea ?",MachineLearning
"Paper: [https://robotics-transformer.github.io/assets/rt1.pdf](https://robotics-transformer.github.io/assets/rt1.pdf) 

Blog: [https://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html](https://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html) 

Github: [https://github.com/google-research/robotics\_transformer](https://github.com/google-research/robotics_transformer) 

GithubIO: [https://robotics-transformer.github.io/](https://robotics-transformer.github.io/) 

Youtube: [https://www.youtube.com/watch?v=UuKAp9a6wMs](https://www.youtube.com/watch?v=UuKAp9a6wMs) 

Abstract:

>By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting realworld robotic data. We argue that one of the keys to the success of such **general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data**. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising **scalable model properties**. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale **data collection on real robots performing real-world tasks**.

https://preview.redd.it/xow9in4ozp5a1.jpg?width=1521&format=pjpg&auto=webp&v=enabled&s=492ba718d72c05d9d234ded984df2846462f580f

https://preview.redd.it/iwzzds4ozp5a1.jpg?width=1503&format=pjpg&auto=webp&v=enabled&s=7323499dc4edcd96c606fffc6594ce9516a1d961

https://preview.redd.it/pibl8v4ozp5a1.jpg?width=1073&format=pjpg&auto=webp&v=enabled&s=6176bb00320ac39affaa25aff077a4e70e50f226

https://preview.redd.it/j57hks4ozp5a1.jpg?width=762&format=pjpg&auto=webp&v=enabled&s=f0baf806a668eb0891926ebe9235e66a40122242

https://preview.redd.it/owq6vl0rzp5a1.jpg?width=1503&format=pjpg&auto=webp&v=enabled&s=023ea3d21d01a928e297154bdfe86dc691841621

https://preview.redd.it/cdpv3sbrzp5a1.jpg?width=1522&format=pjpg&auto=webp&v=enabled&s=bc320067b9bb50666d26bf7568d99536dad25b30",MachineLearning
"Hello! I'm looking for a good source to keep me up-to-date on the most important ML updates. I have a PhD in ML, but now work in physical science research. I'm seen as the ML ""expert"" among my colleagues, and they come to me for any ML related questions. My problem is that I now spend much of my time keeping up with the physical science updates and focusing on my specific applications of ML to the physical sciences, and I find myself out-of-the-loop on general ML updates often now. This is particularly problematic when a colleague heard something about some new ML approach, but I hadn't heard about it yet. I first heard about the JAX framework when I was asked a question about it. I first heard about transformers when asked a question from an audience member after a talk. In each case, I had the relevant answers to the questions shortly after I looked up the topics, but I would really like to be prepared for such questions in advance. And occasionally such updates lead to a shift in the direction my own research is headed.

Importantly, I am *not* looking for newsletters that talk about the most popular papers of the past week or month. While interesting, most of these papers don't have a particularly large (direct) impact in the long run, and they usually aren't just the high-level content needed to stay up-to-date. I'm most interested in hearing about the which methods are really catching on or have caught on as being the go-to methods, which network architectures are becoming the go-to architectures, which frameworks have become commonly used, etc. Things that have stood the test of a thousand other researchers trying them out and finding them to be useful.

Anyone have any recommendations of a newsletter that might fit this need? Or does anyone have a recommendation on how to keep up with such topics without a newsletter? Thank you much!",MachineLearning
"Hi everyone

&#x200B;

I have a problem implementing haarcascade algorithm, which is the huge number of possible features. For example, for a mere 50×50 window the possible features exceed 3 million easily (the 5 known rectangle features, with all possible variations on width/height and position within the window).

&#x200B;

Is there any recommended way to prune/optimize this, say based on size? Is this how it's meant to be, or am I missing something?",MachineLearning
"With the advent of GPT, distinguishing between real and fake texts is going to be harder. I don’t think GPT will write a New York best times seller anytime soon, but, I’m sure schools would like to know wether a student wrote their paper or just out the prompt into some text generator. 

I understand NLP in theory but have never actually used it in practice. Those who have, what are your thoughts on this? Is it reasonable to train a network to recognize writing dialects of different popular networks?",MachineLearning
"I came across this site not too long ago https://interiorai.com/. I'm curious to understand how this model was created/trained.

My first guess is that this is a fine-tuned Stable Diffusion model with new added classes + training samples.

I haven't done much image generation work or used Stable Diffusion for anything but toy problems, so I'm trying to understand what a workflow to create a model like the above would look like.",MachineLearning
"We made a library for inference/fine-tuning of open **175B+ language models** (like BLOOM) **using Colab or a desktop GPU**. You join forces with other people over the Internet (BitTorrent-style), each running a small part of model layers. Check out [our Colab example](https://colab.research.google.com/drive/1Ervk6HPNS6AYVr3xVdQnY5a-TjjmLCdQ?usp=sharing)!

Thing is, even though BLOOM weights were publicly released, it was extremely difficult to run inference efficiently unless you had lots of hardware to load the entire model into the GPU memory (you need at least 3x A100 or 8x 3090 GPUs). E.g., in case of offloading, you can only reach the speed of \~10 sec/step for sequential (non-parallel) generation.

A possible alternative is to use APIs, but they are paid and not always flexible (you can’t adopt new fine-tuning/sampling methods or take a look at hidden states). So, Petals come to the rescue!

[This is how Petals work: some peers want to use a pretrained LM to solve various tasks with texts in natural or programming languages. They do it with help of other peers, who hold subsets of model layers on their GPUs.](https://preview.redd.it/w5y78uqnzo5a1.png?width=1630&format=png&auto=webp&v=enabled&s=ca504025afe8550207c73d7df8fc67bb2d1afebd)

More details:

* Paper (with speed measurements): [https://arxiv.org/abs/2209.01188](https://arxiv.org/abs/2209.01188)
* GitHub repo: [https://github.com/bigscience-workshop/petals](https://github.com/bigscience-workshop/petals)

What do you think of it?",MachineLearning
"Given that well-funded groups like Google, Meta and OpenAI may eventually develop an insurmountable lead for services like image classification and NLP that seem to require huge numbers of parameters, I'd be surprised if there wasn't an effort underway to make a BOINC-powered distributed system that millions of us mere peons could contribute to collaboratively. But aside from the now-defunct MLC@Home project, I haven't found anything yet. Am I missing something?",MachineLearning
"I'm training a per-pixel image classification network, which, for each pixel in the image, predicts whether it is a sign for disease A or disease B. Note that a given pixel could be a sign for both disease A and disease B (this is a multi-label problem).

My question is: are the relative probabilities going to be calibrated?  In other words, does it make sense to sort the NxNx2 probabilities, or are the probabilities for the two diseases (i.e. channels) not calibrated / comparable, since it is similar to solving two independent problems?

If it matters, I am using a ResNet, some fully-connected layers, and then a convolutional decoder.

Any thoughts will be much appreciated, thanks in advance!",MachineLearning
"I'm working on real time semantic segmentation problem, and hesitating between PaddleSeg and MMsegmentation toolbox,   
any advice ?",MachineLearning
"We, the KerasCV team, just published a new tutorial that teaches you to train new embeddings for specific concepts in StableDiffusion via textual inversion!  Let us know what you think!

[https://keras.io/examples/generative/fine_tune_via_textual_inversion/](https://keras.io/examples/generative/fine_tune_via_textual_inversion/)",MachineLearning
"Hey Reddit,

My friend and I are building a project management platform for AI/data science teams (essentially a JIRA for ML). We aim to develop a data-centric, experimental tool that models the ML pipeline to organize workflows, building off the Agile methodology of software development. Our tool will allow ML engineers to design, track, and manage custom pipelines, data flows, and models all on the cloud. Below of a list of some features we plan to introduce:

**Integrations**: Include a host of integrations to MLOps tools (KubeFlow, MLFlow, etc), cloud computing services (AWS, Google Cloud, Azure), source code management (Github, Bitbucket)

**Iterations**: Allow multiple iterations within pipelines, and separate each iteration by various steps in the ML pipeline (business understanding, data visualization, data pre-processing, model training, model testing, model optimization, and deployment). Include a Kanban chart per each part of the pipeline

**Callbacks**: The ability to request to go back to previous stages of the AI pipeline to either improve previous steps (like data preprocessing or model training/development/designing) or request other teams to improve previous steps (we refer to this as callbacks)

**Storage**: A cloud storage solution to store ML models, datasets, or any other metrics/graphs/whatever ML engineers want to store.

**Sketchpad**: A sketchpad to design data flows and ML models, and link them to code Private Assignment: The ability to individually/uniquely assign tasks to different roles in a team, and the ability to be able to privately and specifically send vital information to specific people. for example, the pm could only send the data set to the data engineer, the preprocessed data to an ML engineer (potentially added on top of all this is a differential privacy layer), and send the packaged model to an integration engineer.

**Chat**: A chat/communication platform to interact w/ your team Quantitative Focus: ML is quantitative. The client wants QUANTITATIVE results. Hence, the epic should be emphasized on being quantitative rather than qualitative.

**Experiments**: We redefine “sprints” as “experiments.” We make two changes to sprints. First, we DO NOT have any deadlines on any sprints. This is to not put the engineer under pressure. Secondly, instead of asking “what”, we ask “how” when asked to describe the experiment. This provides a heavily qualitative focus on the experiments, with a focus on function rather than immediate deliverability as in software engineering.

We would appreciate any feedback on our platform, as well as any problems you guys are facing in data science/ML project management.

Thanks a bunch in advance!",MachineLearning
">TL;DR: We paid USD $800 USD and spend 4 hours in the AWS Forecast console so you don't have to.

In this [reproducible experiment](https://github.com/Nixtla/statsforecast/tree/main/experiments/amazon_forecast), we compare [Amazon Forecast](https://aws.amazon.com/forecast/) and [StatsForecast](https://github.com/Nixtla/statsforecast) a python open-source library for statistical methods. 

Since AWS Forecast specializes in demand forecasting, we selected the [M5 competition](https://mofc.unic.ac.cy/m5-competition/) dataset as a benchmark; the dataset contains 30,490 series of daily Walmart sales.

**We found that Amazon Forecast is 60% less accurate and 669 times more expensive than running an open-source alternative in a simple cloud server.**

We also provide a step-by-step guide to [reproduce the results](https://nixtla.github.io/statsforecast/examples/aws/statsforecast.html).

### Results

**Amazon Forecast:**

* achieved 1.617 in error (measured in wRMSSE, the official evaluation metric used in the competition),
* took 4.1 hours to run,
* and cost 803.53 USD.

An **ensemble of statistical methods** trained on a c5d.24xlarge  EC2 instance:

* achieved 0.669 in error (wRMSSE),
* took 14.5 minutes to run,
* and cost only 1.2 USD.

For this data set, we show, therefore, that:

* Amazon Forecast is 60% less accurate and 669 times more expensive than running an open-source alternative in a simple cloud server.
* Classical methods outperform Machine Learning methods in terms of speed, accuracy, and cost.

Although using StatsForecast requires some basic knowledge of Python and cloud computing, the results are better for this dataset.  


**Table**

https://preview.redd.it/vt9ru0149i5a1.png?width=1274&format=png&auto=webp&v=enabled&s=db5ccd4a3fdb00cd896f80a09555ad8024990c5c",MachineLearning
"# To Try it Now:

If you want to get started straightaway, you trust my code and you're setup on CUDA (or have a virtual machine/Colab machine/etc), in the terminal, you can just run `git clone` [`https://github.com/tysam-code/hlb-CIFAR10`](https://github.com/tysam-code/hlb-CIFAR10)  `&& cd hlb-CIFAR10 && python -m pip install -r requirements.txt && python main.py`, and you should see things training straightaway. If you just want to browse the repo, feel free to head on over to [https://github.com/tysam-code/hlb-CIFAR10](https://github.com/tysam-code/hlb-CIFAR10).

# About the Project:

Hi there,

I've been working in the modern instantiation of this field  for just over half a decade or so, and have always sort of wanted a good testbench to develop neural network research on. Most of the codebases  I've worked with over the years have been very ad-hoc or not very close to the bleeding edge. Or, the highly performant codebases really weren't amenable to hacking -- just for performing the main function they were  built for (a single paper, benchmark, service, etc).

I've spent 120-130 hours or so rebuilding David Page's lovely record-setting dawnbench project ([https://myrtle.ai/learn/how-to-train-your-resnet/](https://myrtle.ai/learn/how-to-train-your-resnet/)) in a way that is generic and amenable to both hacking and rapid experimentation/prototyping. One benefit of this is that the original code is extremely performance optimized, so not only is it an easy workbench to prototype on, but a good testbed for if a new method is orthogonal or not to older methods.

I've tried to make it as absolutely plug and play as possible. I hope this offers a nice combination of being fun and useful. I've personally found that I've already gotten a significant amount of value in testing some methods that would otherwise be very hard to test with it. In both cases, either change required just a single line to be  added, which I felt validated the original design decisions. I have some hopefully-not-to-nefarious plans for it if I can get it down around the \~2 seconds mark within a few years.

You can get my Patreon and other info from the github link above if you're interested in more work like this in the future from me.

I plan on hanging around in the comments for a while in case anyone has any questions. Please do let me know, I love the conversation, and whether or not you did or didn't like it, and so on. The feedback helps me build better software that's applicable to the community in the future! So please, feel free to drop me a line. It would be great to hear what you think! :) :D",MachineLearning
"Does global average pooling work if the channel dimensions are big like 64x64. I feel with a channel size that big, the averaging to one pixel value will lose all the information and the model will find it very hard for the learn from it unlike a small channel dimension.",MachineLearning
I am working on mapping 3D scenes from monocular videos and wanted some help on how I can use the pretrained NeuralRecon ([https://zju3dv.github.io/neuralrecon/](https://zju3dv.github.io/neuralrecon/)) on ScanNet dataset to reconstruct the local scene mesh around human body. For the human body dataset I am using 3DPW ([https://virtualhumans.mpi-inf.mpg.de/3DPW/](https://virtualhumans.mpi-inf.mpg.de/3DPW/)) dataset.,MachineLearning
"Is there a way to get around GPT-3's 4k token limit?

Companies like Spellbook appear to have found a solution, with some people speculating what they have done on Twitter - e.g., summarizing the original document, looping in 4k chunks until the right answer is produced, etc.

I suspect multiple solutions have been applied.

I'd be curious if you have any ideas!

Relevant Tweet: https://twitter.com/AlphaMinus2/status/1600319547348639744",MachineLearning
"Has Open AI said what ChatGPT's architecture is? What technique is it using to ""remember"" previous prompts? Have they come up with some way to add recurrence to the transformer or is it just using a feedforward sliding window approach?",MachineLearning
"Agile/scrum/waterfall etc', was there something you tried and didn't work? How adjustments that aren't just time extensions to these known methodologies?

Im just wondering what other teams do that work, since my team is still trying things out, with a lot of convincing needed for managers/pm who are more pure software oriented.

I've found a few references online on how algorithm/ML/datascience development don't fit nicely into agile cycles, but i ended up with more questions.",MachineLearning
"Does anyone know of a document editor in which you can dictate text with OpenAI Whisper? Like similar to Google Docs but obviously with fewer features—and not for writing code, just normal text. I would like to use it!

&#x200B;

I am aware of the HuggingFace Spaces (e.g. [https://huggingface.co/spaces/openai/whisper](https://huggingface.co/spaces/openai/whisper)) and Colab notebooks where you can use Whisper, but I don't know of any straight up writing tools.",MachineLearning
"I've been trying to empirically assess what biases ChatGPT has about certain things when I give it minimal information about what I want. The approach that I've tried is to repeatedly make a request **in a new thread**, look at the distribution of key words, phrases or word/phrase categories across its responses, and compare these distributions across different requests. E.g. one set of requests that I've made have the structure:

>Make up a realistic story about (a|an) **<TRAIT>** person. Include their name and a description of their appearance.

I collected 10 responses for each of the following **<TRAIT>**s: ""intelligent"", ""unintelligent"", ""devious"", ""trustworthy"", ""peaceful"", ""violent"", and did the same for 2 other request structures that request similar information, using the same set of **<TRAIT>**s. So I have 30 responses in total for each of the 6 **<TRAIT>**s.

Before I finished writing a program to analyse the results, some biases stood out immediately. E.g. for ""intelligent"", the responses were almost always about women, except for one or two that were about a person called Alex, of unspecified gender (it used ""they/them"" pronouns in those responses). The people in these responses were almost always scientists too, and the names were nowhere near as diverse as they could have been (e.g. for the request structure above, 4 of the 10 women in the responses were called Samantha). If I repeatedly make the same request **in the same thread**, these characteristics of the responses do display more diversity, but the responses all have the same structure (e.g. the same number of paragraphs, and often near-identical sentences in corresponding paragraphs).

It wasn't clear to me if these biases are representative of its biases across a wide range of interactions, or if it's just bad at drawing random samples in its first response, for some reason. So I tried a simpler request, of giving me the name of a vegetable. I asked 35 times, and it said ""carrot"" 30 times and ""broccoli"" 5 times. The results of all my vegetable-name interactions are [here](https://gist.github.com/OsePedro/ca6ce437f4dcefc83af06b1d532e5d82). I also tried asking it to name an American president in 6 threads, and it said ""George Washington"" each time, and I tried asking it to name an intelligent person, and it usually said Albert Einstein, although it did occasionally say Stephen Hawking.

# Questions

Assuming that carrots do not constitute anywhere near 85% of the vegetables in ChatGPT's training set, can anyone suggest likely causes for this bias in its initial responses? E.g. what characteristics of the reward function are likely to have made its initial responses so biased, compared to the training data? Is this a common phenomenon in conversational agents trained by RL?",MachineLearning
"&#x200B;

https://i.redd.it/nuqdy9e3815a1.gif

Hey Reddit,

I just wanted to share an art & research project we've been working on that uses AI and reinforcement learning to teach an AI how to sculpt 3D sculptures. It's been really cool to see the AI learn new strategies and adapt to create unique pieces of art and how we as designers start to take on the role of curators.

The project started earlier last year after thinking about the future of art and the role of humans in the creative process. Something that seems to be more relevant than ever these days. With AI becoming more advanced, what does that mean for us as creatives? We also wrote about this on the project page. It's an interesting thought and I'd love to hear your thoughts on it.

Check out the project here: [https://onformative.com/work/ai-sculpting/](https://onformative.com/work/ai-sculpting/) and let's discuss in the comments.

&#x200B;

https://preview.redd.it/ikymqhl7815a1.jpg?width=1280&format=pjpg&auto=webp&v=enabled&s=235a00be8f9c70a205f42f8fd5a355e1237a6210",MachineLearning
"I am working on a multi class sentence pair classification on an highly imbalanced dataset.
I have tried different techniques from sampling techniques to focal loss.

My question is does it make sense to perform sampling (over or under sampling) while training the model with focal loss?

My understanding is that focal loss is geared towards penalising samples that are hard to learn, whereas sampling techniques modifies the distribution of the dataset. So they can be used together. Please share your thoughts.",MachineLearning
"Hi Folks,

Have you tried **ChatGPT, GPT-3, or other generative models** but have been frustrated by the lack of support or guidance when it comes to using them effectively? Are you interested in learning more about the power of prompt engineering and how it can help you get better results from generative models?

We have recently launched a new open-source platform called **discuss.openPrompt.io**, where you can ask and answer questions, discuss, and share your knowledge and experiences with **ChatGPT, Prompt-Engineering, GPT-3, stable diffusion, and other generative models**.

&#x200B;

As many of you may know, ChatGPT was released recently and has generated a lot of excitement among the NLP community. When ChatGPT was released, we were excited to try it out, but we quickly realized that many people are still unsure of how to use it effectively and get the expected output. That's why we decided to create [discuss.openPrompt.io](https://discuss.openPrompt.io) \- to provide a space where experts and beginners alike can learn from each other, share their knowledge and experiences, and discuss the latest developments in the field of generative models.

&#x200B;

But OpenPrompt is not just a Q&A forum. It's also a platform for sharing resources and discussing the latest developments in the field of generative models and prompt engineering. You can share tutorials, code snippets, datasets, and other useful materials that can help others learn and experiment with these tools. You can also participate in discussions about the latest trends and innovations in the field.

Whether you're an experienced user of generative models or just starting out, OpenPrompt is the place for you. If you are interested in generative models and want to learn more, join our platform and participate in the discussions. We would love to hear your thoughts, ideas, and suggestions on ChatGPT, prompt engineering, GPT-3, stable diffusion, and other generative models.

And if you're attending **EMNLP2022 in Abu Dhabi**, we would be happy to chat with you in person and discuss the latest developments in the field of generative models and all the exciting things we have planned for the future.",MachineLearning
"Hi all!

I have once read a paper where you could write text and an AI would generate sound design out of it (like; a man walking through grass while birds sing) but I couldn't test it myself.

Is there any site where I could test something like this? If you know anything let me know would be great!

Thank you so much!
X
John",MachineLearning
"Fellow machine learning enthusiast here!

I want to train a large NLP model and I'm wondering whether its worth it to use Google Cloud's TPU's for it. I already have an Nvidia RTX 3060 Laptop GPU with 8.76 TFLOPS, but I was unable to find out what the exact performance (in TFLOPS to be able to compare them) of google TPU v3 and v4 are.

I know TPUs (I think the factor is 12x) are a ton faster and more optimized for machine learning than GPU's, but I'm still wondering whether its worth it to just build a graphics card rig for the long term. (since the pricing and estimation seems unclear to me since I cannot see how much I'm paying per TFLOP.)

Has anyone done the numbers on price/performance and hourly cost? Also is there any factor I missed? Thanks a lot in advance!",MachineLearning
"[savegpt.com](https://savegpt.com/) is a browser extension available both on the Chrome webstore and Firefox addons.

https://reddit.com/link/zikps2/video/5zinkph4b85a1/player",MachineLearning
"**Paper:** Signal Propagation: A Framework for Learning and Inference In a Forward Pass ([https://arxiv.org/abs/2204.01723](https://arxiv.org/abs/2204.01723))

We introduce the framework for learning with forward passes.

I made a friendly and thorough tutorial to learn about and implement forward learning: [https://amassivek.github.io/sigprop](https://amassivek.github.io/sigprop) .

The most interesting insights from the framework:

* This algorithm provides an explanation for how neurons in the brain without error connections receive learning signals.
* It  works for continuous networks with hebbian learning. This provides  evidence for this algorithm as model of learning in the brain.
* It  works for spiking neural networks using only the membrane potential  (aka voltage in hardware). This supports applying this algorithm for  learning on neuromorphic chips.

A library for integrating forward learning into any model is at:  
[https://github.com/amassivek/signalpropagation](https://github.com/amassivek/signalpropagation)

Recent interest in forward learning is growing. I was invited to present this work at a reading  group when one of the members pointed out the connection between the signal propagation framework and the forward forward algorithm: the forward forward algorithm is an implementation of this framework.

**Abstract:**

We propose a new learning framework, signal propagation (sigprop), for propagating a learning signal and updating neural network parameters via a forward pass, as an alternative to backpropagation. In sigprop, there is only the forward path for inference and learning. So, there are no structural or computational constraints necessary for learning to take place, beyond the inference model itself, such as feedback connectivity, weight transport, or a backward pass, which exist under backpropagation based approaches. That is, sigprop enables global supervised learning with only a forward path. This is ideal for parallel training of layers or modules. In biology, this explains how neurons without feedback connections can still receive a global learning signal. In hardware, this provides an approach for global supervised learning without backward connectivity. Sigprop by construction has compatibility with models of learning in the brain and in hardware than backpropagation, including alternative approaches relaxing learning constraints. We also demonstrate that sigprop is more efficient in time and memory than they are. To further explain the behavior of sigprop, we provide evidence that sigprop provides useful learning signals in context to backpropagation. To further support relevance to biological and hardware learning, we use sigprop to train continuous time neural networks with Hebbian updates, and train spiking neural networks with only the voltage or with biologically and hardware compatible surrogate functions.",MachineLearning
"Hey guys.

We created a small Framework with which you can implement a model of a problem and let the Framework solve it. The Code is pure Java and there is no further need to understand complex mechanisms of Machine Learning. In the repository is a quick example on how this framework plays the game ""Snake"".  
For training, it uses a Genetic Algorithm to train Neural Networks.

If you can implement Snake in Java you can also use this Framework to let it play your game.

The motivation for this Framework was that many people learn Java in school and are interested in Machine Learning, nearly every Framework out there for Machine Learning is written in Python though. This Framework should not be used to make the most complex predictions, it should more get used to implement your first Machine Learning Algorithm in a language most of you are familiar.  
We try to lure you into the world of Machine Learning! ;)

[https://github.com/tomLamprecht/AI-Framework](https://github.com/tomLamprecht/AI-Framework)

Feedback of any kind is very welcomed! :)

PS: I'm aware that evolving a Neural Net with Genetic Algorithms is by far not the most efficient way, but I went this road, and I'm going to finish it :D",MachineLearning
"Is  there a way to make a neural network that performs only regression to estimate its own error at inference time, without having ground truths for reference?

My network predicts N points and I know the \[x,y\] coordinates for each. On a labeled test  set I can compute the distance between each point and the ground truths, however, I want the network to be able to estimate these distances by  itself.

I do not have separate classes, the network is trained using just the L2 loss between its predicted points and the expected ground truth points.",MachineLearning
"Here is a [podcast episode](https://generallyintelligent.com/podcast/2022-12-06-podcast-episode-24-jack-parker-holder/) with Jack Parker-Holder from DeepMind where we discuss open-endedness, evolving agents and environments, offline learning with world models, and much more!",MachineLearning
"New HuggingFace blog post on RLHF: [https://huggingface.co/blog/rlhf](https://huggingface.co/blog/rlhf)

Motivated by ChatGPT and the lack of conceptually focused resources on the topic.",MachineLearning
"In audio, I have to generate an image representation of some folder of audio files.Unfortunately, I have to generate a cochleagram and run each audio file through an auditory toolbox 1 by 1.  For large datasets this can take hours for each iteration when testing stuff out (different resolutions, number of filterbanks applied, etc).  I then have to save off all the features into one flat dataframe on my hard drive for training.

A sub problem to this is data augmentation.  Pedalboard is a python library that allows for audio data augmentation.  However, this must be performed when the audio is still audio, before the cochleagram data is generated using the filterbanks.  For this reason, I either need to save the entire augmented dataset in a flat file, and then generate their visual representations, or make all this pre processing part of the CNN, which would make training time unfeasible.  Thus, the only augmentations I use are simple image shifts and gaussian blur, when I really could be making use of different convolutional reverbs and other forms of data augmentation using VST plugins.

Is this just where things are at right now?  I was initially planning on comparing different filterbanks for use as inputs to CNNs, but it seems now a better comparison may be comparing cochleagrams (without audio based augmentation) to mel-spectrograms (with audio based augmentation) , which would allow me to make everything part of the training loop since mel-spectrograms generate much faster.",MachineLearning
"I am wondering, is there any relationship between the dimension of the input vector and the ability of the output that it can represent? (e.g. can I say that a 10-dimensional feature vector has better representation ability against a 5-dimensional one, assuming data are sufficient to train a model.) Or if not, can you suggest any reference to the formal induction that illustrates that relationship?",MachineLearning
"Hey folks, 

We interviewed Petar Veličković at NeurIPS last week here -- [https://www.youtube.com/watch?v=1lkdWduuN14](https://www.youtube.com/watch?v=1lkdWduuN14)

Categories  (Cats for AI) \[[00:00:00](https://www.youtube.com/watch?v=1lkdWduuN14&t=0s)\] 

Algorithmic Reasoning \[[00:14:44](https://www.youtube.com/watch?v=1lkdWduuN14&t=884s)\] 

Extrapolation \[[00:19:09](https://www.youtube.com/watch?v=1lkdWduuN14&t=1149s)\] 

Ishan Misra Skit \[[00:27:50](https://www.youtube.com/watch?v=1lkdWduuN14&t=1670s)\] 

Graphs (Expander Graph Propagation) \[[00:29:18](https://www.youtube.com/watch?v=1lkdWduuN14&t=1758s)\]

&#x200B;

References

MLST#60 Geometric Deep Learning Blueprint (Special Edition)

[https://www.youtube.com/watch?v=bIZB1hIJ4u8](https://www.youtube.com/watch?v=bIZB1hIJ4u8)

&#x200B;

Categories for AI

[https://cats.for.ai/](https://cats.for.ai/)

Organised by:

Andrew Dudzik - DeepMind

Bruno Gavranović - University of Strathclyde

João Guilherme Araújo - Cohere / Universidade de São Paulo

&#x200B;

Petar Veličković - DeepMind / University of Cambridge

Pim de Haan - University of Amsterdam / Qualcomm AI Research

&#x200B;

\[Petar Veličković\] Graph Attention Networks

[https://arxiv.org/abs/1710.10903](https://arxiv.org/abs/1710.10903)

&#x200B;

Learning to Configure Computer Networks with Neural Algorithmic Reasoning \[NeurIPS 2022\] \[Luca Beurer-Kellner, Martin Vechev, Laurent Vanbever, Petar Veličković\]

[https://openreview.net/forum?id=AiY6XvomZV4](https://openreview.net/forum?id=AiY6XvomZV4)

&#x200B;

Graph Neural Networks are Dynamic Programmers \[Andrew Joseph Dudzik, Petar Veličković\]

[https://arxiv.org/abs/2203.15544](https://arxiv.org/abs/2203.15544)

&#x200B;

Expander Graph Propagation \[Andreea Deac, Marc Lackenby, Petar Veličković\]

[https://openreview.net/forum?id=IKevTLt3rT](https://openreview.net/forum?id=IKevTLt3rT)

&#x200B;

\[Pim de Haan, Taco Cohen, Max Welling\] Natural Graph Networks

[https://papers.nips.cc/paper/2020/file/2517756c5a9be6ac007fe9bb7fb92611-Paper.pdf](https://papers.nips.cc/paper/2020/file/2517756c5a9be6ac007fe9bb7fb92611-Paper.pdf)

&#x200B;

\[Uri Alon, Eran Yahav\] On the Bottleneck of Graph Neural Networks and its Practical Implications (they discovered oversquashing)

[https://arxiv.org/abs/2006.05205](https://arxiv.org/abs/2006.05205)

&#x200B;

\[Topping,...,Bronstein\] Understanding over-squashing and bottlenecks on graphs via curvature

[https://arxiv.org/abs/2111.14522](https://arxiv.org/abs/2111.14522)

&#x200B;

\[Andreea Deac, Petar Velickovic, Ognjen Milinkovic et al\] XLVIN: eXecuted Latent Value Iteration Nets

[https://arxiv.org/abs/2010.13146](https://arxiv.org/abs/2010.13146)

&#x200B;

\[Petar Veličković et al\] Reasoning-Modulated Representations (RMR)

[https://openreview.net/forum?id=cggphp7nPuI](https://openreview.net/forum?id=cggphp7nPuI)

&#x200B;

Dual Algorithmic Reasoning \[PetarV, under review ICLR\]

[https://openreview.net/pdf?id=hhvkdRdWt1F](https://openreview.net/pdf?id=hhvkdRdWt1F)

&#x200B;

\[Petar Veličković, Charles Blundell\] Neural Algorithmic Reasoning

[https://arxiv.org/abs/2105.02761](https://arxiv.org/abs/2105.02761)

&#x200B;

\[Andreea Deac, Petar Veličković, ...\] Neural Algorithmic Reasoners are Implicit Planners  (which got a NeurIPS spotlight in 2021!)

[https://arxiv.org/abs/2110.05442](https://arxiv.org/abs/2110.05442)

&#x200B;

A Generalist Neural Algorithmic Learner

[https://arxiv.org/abs/2209.11142](https://arxiv.org/abs/2209.11142)

&#x200B;

ETA Prediction with Graph Neural Networks in Google Maps

[https://arxiv.org/abs/2108.11482](https://arxiv.org/abs/2108.11482)

&#x200B;

\[Randall Balestriero\] A Spline Theory of Deep Networks

[https://proceedings.mlr.press/v80/balestriero18b/balestriero18b.pdf](https://proceedings.mlr.press/v80/balestriero18b/balestriero18b.pdf)

&#x200B;

\[Ahmed Imtiaz Humayun \] Exact Visualization of Deep Neural Network Geometry and Decision Boundary

[https://arxiv.org/pdf/2009.11848.pdf](https://arxiv.org/pdf/2009.11848.pdf)

&#x200B;

\[Ahmed Imtiaz Humayun \] MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without Retraining | ICLR 2022

[https://www.youtube.com/watch?v=0Muk7nKzOW8](https://www.youtube.com/watch?v=0Muk7nKzOW8)

&#x200B;

\[Randall Balestriero, Jerome Pesenti, Yann LeCun\] Learning in High Dimension Always Amounts to Extrapolation

[https://arxiv.org/abs/2110.09485](https://arxiv.org/abs/2110.09485)

&#x200B;

\[Hattie Zhou\] Teaching Algorithmic Reasoning via In-context Learning

[https://arxiv.org/abs/2211.09066](https://arxiv.org/abs/2211.09066)

&#x200B;

\[Ahmed Imtiaz\] Exact Visualization of Deep Neural Network Geometry and Decision Boundary

[https://openreview.net/pdf?id=VSLbmsoZxai](https://openreview.net/pdf?id=VSLbmsoZxai)

&#x200B;

\[Beatrice Bevilacqua\] Size-Invariant Graph Representations for Graph Classification Extrapolations

[https://arxiv.org/pdf/2103.05045.pdf](https://arxiv.org/pdf/2103.05045.pdf)

&#x200B;

\[Brendan Fong David I. Spivak\] Seven Sketches in Compositionality: An Invitation to Applied Category Theory

[https://math.mit.edu/\~dspivak/teaching/sp18/7Sketches.pdf](https://math.mit.edu/~dspivak/teaching/sp18/7Sketches.pdf)

&#x200B;

Tim’s examples of applied Category theory cited:

&#x200B;

\[Lennox\] Robert Rosen and Relational System Theory: An Overview

[https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=5866&context=gc\_etds](https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=5866&context=gc_etds)

&#x200B;

\[Bob Coecke\] Introducing categories to the practicing physicist

[https://www.cs.ox.ac.uk/bob.coecke/Cats.pdf](https://www.cs.ox.ac.uk/bob.coecke/Cats.pdf)

&#x200B;

\[Bob Coecke\] Categorical Quantum Mechanics I: Causal Quantum Processes

[https://www.researchgate.net/publication/283043644\_Categorical\_Quantum\_Mechanics\_I\_Causal\_Quantum\_Processes](https://www.researchgate.net/publication/283043644_Categorical_Quantum_Mechanics_I_Causal_Quantum_Processes)

&#x200B;

\[Bob Coecke\] Quantum Natural Language Processing

[https://www.cs.ox.ac.uk/people/bob.coecke/QNLP-ACT.pdf](https://www.cs.ox.ac.uk/people/bob.coecke/QNLP-ACT.pdf)",MachineLearning
"**Paper:** Large language models are not zero-shot communicators ([arXiv](https://arxiv.org/abs/2210.14986))

**Abstract:**

Despite widespread use of LLMs as conversational agents, evaluations of performance fail to capture a crucial aspect of communication: interpreting language in context. Humans interpret language using beliefs and prior knowledge about the world. For example, we intuitively understand the response ""I wore gloves"" to the question ""Did you leave fingerprints?"" as meaning ""No"". To investigate whether LLMs have the ability to make this type of inference, known as an implicature, we design a simple task and evaluate widely used state-of-the-art models. We find that, despite only evaluating on utterances that require a binary inference (yes or no), most perform close to random. Models adapted to be ""aligned with human intent"" perform much better, but still show a significant gap with human performance. We present our findings as the starting point for further research into evaluating how LLMs interpret language in context and to drive the development of more pragmatic and useful models of human discourse.

**Authors: Laura Ruis, Akbir Khan, Stella Biderman, Sara Hooker, Tim Rocktäschel, Edward Grefenstette**",MachineLearning
"Basically, they developed a lie detector for Large Language Models.

>**Abstract**  
>  
>Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels. 

Paper:  [Discovering Latent Knowledge in Language Models Without Supervision](https://arxiv.org/abs/2212.03827)

&#x200B;",MachineLearning
" Important paper researching the replication of training data in diffusion models. Very relevant to recent debates around ""art theft"" and [""data laundering""](https://waxy.org/2022/09/ai-data-laundering-how-academic-and-nonprofit-researchers-shield-tech-companies-from-accountability/).

>*Abstract* Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they stealing content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data.

Paper: [Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models](https://arxiv.org/abs/2212.03860)",MachineLearning
 I am aware that one 1x1 convolution is needed for separable convolution but when else is it useful. I see it used in mobilenetv2 before the depthwise separable convolution later in the bottleneck but not sure why. I also see it used with stride 2 when max pooling could be used instead. Could someone please explain the logic behind this. Thanks.,MachineLearning
"&#x200B;

https://preview.redd.it/m2pg8yhahr4a1.png?width=2117&format=png&auto=webp&v=enabled&s=1869b4e1a44514f264e0c558daca7af12e894890

**Paper:** What the DAAM: Interpreting Stable Diffusion Using Cross Attention ([arXiv paper](https://arxiv.org/abs/2210.04885), [codebase](https://github.com/castorini/daam))

**Abstract:**

Large-scale diffusion neural networks represent a substantial milestone in text-to-image generation, but they remain poorly understood, lacking interpretability analyses. In this paper, we perform a text-image attribution analysis on Stable Diffusion, a recently open-sourced model. To produce pixel-level attribution maps, we upscale and aggregate cross-attention word-pixel scores in the denoising subnetwork, naming our method DAAM. We evaluate its correctness by testing its semantic segmentation ability on nouns, as well as its generalized attribution quality on all parts of speech, rated by humans. We then apply DAAM to study the role of syntax in the pixel space, characterizing head--dependent heat map interaction patterns for ten common dependency relations. Finally, we study several semantic phenomena using DAAM, with a focus on feature entanglement, where we find that cohyponyms worsen generation quality and descriptive adjectives attend too broadly. To our knowledge, we are the first to interpret large diffusion models from a visuolinguistic perspective, which enables future lines of research.

**Authors: Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin, Ferhan Ture**",MachineLearning
"I have a big data set that I would like to train on, so my thought is that I am going to do distributed training , but I am currently setting up MultiWorkerMirroredStrategy on tensorflow and i find it hard to use even with https://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-distributor

https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy

So I was wondering if there are other recommended way of doing NN training if you have big dataset?",MachineLearning
" Hello!

I've developed a project [NaimAI](https://www.naimai.fr/), to help PhDs and scientists in their scientific literaure review. To describe it brievely, it has 3 main features : 1 search in papers, 2 structures abstracts into objectives, methods and results and 3 generates automatically a (pseudo) literature review.

I wrote a [medium article](https://medium.com/@yaassinekaddi/literature-review-with-naimai-open-sourced-fcbdb36762de) that goes through the details.

Github repos : [https://github.com/yassinekdi/naimai](https://github.com/yassinekdi/naimai)

I've created a subreddit in case : [r/naimai4science](https://www.reddit.com/r/naimai4science/)

I'd be happy to have your opinion about it and hopefully this could be useful!",MachineLearning
"



So I want to develop a product Recommendation feature for my ecommerce site. We have our table which has successfull cart orders by customers. Table looks like ( cart_id , product_id , category_id , product_name). Now I want to develop a product Recommendation model using this data. What are various product Recommendation models (similar to Amazon)(production usecase) that I can explore and study ? Can someone send me production examples with sample code that I can start a POC with ?",MachineLearning
"Paper: [https://arxiv.org/abs/2210.12375](https://arxiv.org/abs/2210.12375)  
Code: [https://github.com/martenlienen/torchode](https://github.com/martenlienen/torchode)

https://preview.redd.it/itk8xdyxin4a1.png?width=2560&format=png&auto=webp&v=enabled&s=696d627f8cd1c1f2df5c9e1eee12800c2c89f5b6

We have developed a new ODE solver suite for PyTorch that eliminates some unintended side-effects that can occur in batched training with adaptive step sizes by tracking a separate solver state for each sample in a batch. Additionally, torchode can speed up your neural ODE or continuous normalizing flow by minimizing the solver overhead through various implementation optimizations in its code such as combined operations (einsum, addcmul), polynomial evaluation via Horner's rule and JIT compilation. See the paper for details.

I am happy to answer questions here on reddit. If you are a NeurIPS (+workshops) attendee, it would be great to see you at my poster at the DLDE workshop on Friday at 05:10 PT / 13:10 UTC or 09:05 PT / 18:05 UTC.",MachineLearning
"**Paper:** Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation ([arXiv](https://arxiv.org/abs/2212.00774))

**Abstract:**

A diffusion model learns to predict a vector field of gradients. We propose to apply chain rule on the learned gradients, and back-propagate the score of a diffusion model through the Jacobian of a differentiable renderer, which we instantiate to be a voxel radiance field. This setup aggregates 2D scores at multiple camera viewpoints into a 3D score, and repurposes a pretrained 2D model for 3D data generation. We identify a technical challenge of distribution mismatch that arises in this application, and propose a novel estimation mechanism to resolve it. We run our algorithm on several off-the-shelf diffusion image generative models, including the recently released Stable Diffusion trained on the large-scale LAION dataset.

**Authors: Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, Greg Shakhnarovich**",MachineLearning
"I've been playing around with SD's inpainting capabilities and I'm trying to implement a particular use case.

I want to replace the garment on this dog photo with another specific garment. Using SD's inpainting, I was able to replace it with a 'text-prompt generated' garment, but I want to replace it with something very specific. I have multiple images of the replacement garment. I was wondering if someone has seen an implementation for this. I think it should be possible to use Dreambooth's object tokenisation to get photorealism, but Dreambooth also relies on 'text-prompt' inputs.

&#x200B;

https://preview.redd.it/b52c2ed63m4a1.png?width=774&format=png&auto=webp&v=enabled&s=1500a1d7588300a769585372e71c7076ed2a9635",MachineLearning
"&#x200B;

TLDR : People uses dreambooth or textual inversion to fine-tune their own stable diffusion models. There is a better way: Use LoRA to fine-tune twice as faster, with end result being less than 4MB.  Dedicated CLI, package, and pre-trained models are available at [https://github.com/cloneofsimo/lora](https://github.com/cloneofsimo/lora)

&#x200B;

[fine tuned LoRA on pixar footages. Inspired by modern-disney-diffusion](https://preview.redd.it/jggo5rf3ok4a1.png?width=512&format=png&auto=webp&v=enabled&s=dc8296acca829c3da4bb921a899df4171d6e5157)

&#x200B;

[fine tuned LoRA on pop-art style](https://preview.redd.it/dxtjyna5ok4a1.png?width=512&format=png&auto=webp&v=enabled&s=bc9fd1b5ea6d559df328d3922cfdc09dce7fcc08)

Thanks to the generous work of Stability AI and Huggingface, so many people have enjoyed fine-tuning stable diffusion models to fit their needs and generate higher fidelity images. **However, the fine-tuning process is very slow, and it is not easy to find a good balance between the number of steps and the quality of the results.**

**Also, the final results (fully fined-tuned model) is rather very large**. **Consequently, merging checkpoints to find user's best fit is painstakingly SSD-consuming process.** Some people instead works with textual-inversion as an alternative for this. But clearly this is suboptimal: textual inversion only creates a small word-embedding, and the final image is not as good as a fully fine-tuned model.

I've managed to make an alternative work out pretty well with Stable Diffusion: adapters. Parameter-efficient adapation has been a thing for quite a long time now. Mainly, LoRA seems to work robustly in many scenarios according to many researches. ([https://arxiv.org/abs/2112.06825](https://arxiv.org/abs/2112.06825), [https://arxiv.org/abs/2203.16329](https://arxiv.org/abs/2203.16329))

LoRA was originally proposed as part of LLM's method, but this is rather model-agnostic method, as long as there is some space for low-rank tensor decomposition (which literally every linear layer has). No one seems to have tried them on Stable diffusion, other than perhaps (not sure if they did, because tey used other form of adapters) NovelAI, known as hypernetworks.

**# But is it really good though?**

I've tried my best to validate my answer : Yes. it's sometimes even better than fully fine-tuning. Note that even though we are fine-tuning 3MB of parameters, being even better than fully fine-tuning is not surprising : original paper's benchmark had similar results.

What do I mean by better? Well I could've used zero-shot FID score on some shifted dataset, but that would literally take years as generating 50,000 images on single 3090 device takes forever.

Instead, I've used Kernel Inception Distance ([https://arxiv.org/abs/1801.01401](https://arxiv.org/abs/1801.01401)) that has small standard deviation which I can reliably use as a metric. For the shifted dataset, I've gathered 2358 icon images and fine tuned them on 12000 steps for both fully fine-tuning and LORA fine-tuning. The end result is as follows:

&#x200B;

[LoRA 0.5 stands for merging only half of LoRA into original model. All initiated from Stable Diffusion version 2.0.](https://preview.redd.it/pxknfbobrk4a1.png?width=1632&format=png&auto=webp&v=enabled&s=56e0c2331b128fb63bef96cd1dc471b18b0283bb)

LoRA clearly wins full fine-tuning in terms of KID. But in the end, perceptual results are all that matters and I think end users will prove their effectiveness. I haven't had enough time to play with these to conclusively say anything about their superiority, but I did train LoRA on 3 different datasets (vector illustrations, disney style, pop-art style) which is available in my repo.  End results seems pleasing enough to validate the perceptual quality.

**# How fast is it?**

Tested on 3090 device with 5950x cpu, LoRA takes 36 min on 12000 steps, while fully fine-tuning takes 1 hour 20 min. This is more than twice the speed. You also get to keep much of Adam memory saved + much of the parameters don't require grad so that's extra vram saved also.

Contributions are welcomed! This repo has been tested on Linux device, so if something doesn't work please leave a Issue/PR.If you've managed to train your own LoRA model, please share them!",MachineLearning
"Hello, hoping someone out there has dealt with something similar and has suggestions on things I could try.

I am basically trying to estimate a ratio y/x with a k-nearest neighbors regressor. On validation data when x is small, I am significantly underestimating the ratio on average. When x is large, we have a relatively unbiased model.

I took a look at this wiki page (https://en.wikipedia.org/wiki/Ratio_estimator) which seems somewhat relevant but I'm having a bit of trouble parsing how I could apply it to my situation, if at all. Is there some statistical correction method I can apply so that my predictions are unbiased?",MachineLearning
"Continuing my analysis of errors in widely-used LLM benchmarks (post on Google's GoEmotions [here](https://www.reddit.com/r/MachineLearning/comments/vye69k/30_of_googles_reddit_emotions_dataset_is/)) — I analyzed HellaSwag and found 36% contains errors. 

For example, here's a prompt and set of possible completions from the dataset. Which completion do you think is most appropriate? See if you can figure it out through the haze of typos and generally non-sensical writing. 

*Men are standing in a large green field playing lacrosse. People* *is* *around the field watching the game. men*

* *are holding tshirts watching* *int* *lacrosse playing.*
* *are being interviewed in a podium in front of a large group and a gymnast is holding a microphone for the announcers.*
* *are running side to side* *of* *the* *ield* *playing lacrosse trying to score.*
* *are in a field running around playing lacrosse.*

I'll keep it spoiler-free here, but the full blog post goes into detail on this example (and others) and explains why they are so problematic. 

Link: [https://www.surgehq.ai/blog/hellaswag-or-hellabad-36-of-this-popular-llm-benchmark-contains-errors](https://www.surgehq.ai/blog/hellaswag-or-hellabad-36-of-this-popular-llm-benchmark-contains-errors)",MachineLearning
"2.1 supports the new prompting style and brings back many of the old prompts too! The differences are more data, more training, and less restrictive filtering of the dataset.

>The filter still stripped out adult content, but was less aggressive, which cut down the number of false positives it detected. We fine-tuned the SD 2.0 model with this updated setting, giving us a model which captures the best of both worlds. It can render beautiful architectural concepts and natural scenery with ease, and yet still produce fantastic images of people and pop culture too. The new release delivers improved anatomy and hands and is much better at a range of incredible art styles than SD 2.0. 

 Related links:

[Stability AI GitHub](https://github.com/Stability-AI/stablediffusion).

[ ](https://huggingface.co/stabilityai/stable-diffusion-2-1)[weights and model cards.](https://huggingface.co/stabilityai/stable-diffusion-2-1)

[Stable Diffusion Prompt Book online](https://stability.ai/sdv2-prompt-book).",MachineLearning
"**EDIT 11:58am PT:** Thanks for all the great questions, we stayed an almost an hour longer than originally planned to try to get through as many as possible — but we’re signing off now! We had a great time and thanks for all thoughtful questions!

PROOF: [https://i.redd.it/8skvttie6j4a1.png](https://i.redd.it/8skvttie6j4a1.png)

We’re part of the research team behind CICERO, Meta AI’s latest research in cooperative AI. CICERO is the first AI agent to achieve human-level performance in the game Diplomacy. Diplomacy is a complex strategy game involving both cooperation and competition that emphasizes natural language negotiation between seven players.   Over the course of 40 two-hour games with 82 human players, CICERO achieved more than double the average score of other players, ranked in the top 10% of players who played more than one game, and placed 2nd out of 19 participants who played at least 5 games.   Here are some highlights from our recent announcement:

* **NLP x RL/Planning:** CICERO combines techniques in NLP and RL/planning, by coupling a controllable dialogue module with a strategic reasoning engine. 
* **Controlling dialogue via plans:** In addition to being grounded in the game state and dialogue history, CICERO’s dialogue model was trained to be controllable via a set of intents or plans in the game. This allows CICERO to use language intentionally and to move beyond imitation learning by conditioning on plans selected by the strategic reasoning engine.
* **Selecting plans:** CICERO uses a strategic reasoning module to make plans (and select intents) in the game. This module runs a planning algorithm which takes into account the game state, the dialogue, and the strength/likelihood of various actions. Plans are recomputed every time CICERO sends/receives a message.
* **Filtering messages:** We built an ensemble of classifiers to detect low quality messages, like messages contradicting the game state/dialogue history or messages which have low strategic value. We used this ensemble to aggressively filter CICERO’s messages. 
* **Human-like play:** Over the course of 72 hours of play – which involved sending 5,277 messages – CICERO was not detected as an AI agent.

You can check out some of our materials and open-sourced artifacts here: 

* [Research paper](https://www.science.org/doi/10.1126/science.ade9097)
* [Project overview](https://ai.facebook.com/research/cicero/)
* [Diplomacy gameplay page](https://ai.facebook.com/research/cicero/diplomacy/)
* [Github repo](https://github.com/facebookresearch/diplomacy_cicero)
* [Our latest blog post](https://ai.facebook.com/blog/cicero-ai-negotiates-persuades-and-cooperates-with-people/)

Joining us today for the AMA are:

* Andrew Goff (AG), 3x Diplomacy World Champion
* Alexander Miller (AM), Research Engineering Manager
* Noam Brown (NB), Research Scientist [(u/NoamBrown)](https://www.reddit.com/user/NoamBrown/)
* Mike Lewis (ML), Research Scientist [(u/mikelewis0)](https://www.reddit.com/user/mikelewis0/)
* David Wu (DW), Research Engineer [(u/icosaplex)](https://www.reddit.com/user/icosaplex/)
* Emily Dinan (ED), Research Engineer
* Anton Bakhtin (AB), Research Engineer
* Adam Lerer (AL), Research Engineer
* Jonathan Gray (JG), Research Engineer
* Colin Flaherty (CF), Research Engineer [(u/c-flaherty)](https://www.reddit.com/user/c-flaherty)

We’ll be here on December 8, 2022 @ 10:00AM PT - 11:00AM PT.",MachineLearning
"**Abstract**: Although deep learning has made great progress in recent years, the exploding economic and environmental costs of training neural networks are becoming unsustainable. To address this problem, there has been a great deal of research on algorithmically-efficient deep learning, which seeks to reduce training costs not at the hardware or implementation level, but through changes in the semantics of the training program. In this paper, we present a structured and comprehensive overview of the research in this field. First, we formalize the algorithmic speedup problem, then we use fundamental building blocks of algorithmically efficient training to develop a taxonomy. Our taxonomy highlights commonalities of seemingly disparate methods and reveals current research gaps. Next, we present evaluation best practices to enable comprehensive, fair, and reliable comparisons of speedup techniques. To further aid research and applications, we discuss common bottlenecks in the training pipeline (illustrated via experiments) and offer taxonomic mitigation strategies for them. Finally, we highlight some unsolved research challenges and present promising future directions.

&#x200B;

**Paper:** [https://arxiv.org/pdf/2210.06640.pdf](https://arxiv.org/pdf/2210.06640.pdf)",MachineLearning
"Mecab is a CRF-based morpheme analyzer made by Taku Kudo in 2011. It is very fast and accurate at the same time, which is why it is still very popular even though it is quite old. However, it is known to be one of the most tricky libraries to install, and in fact many people have had a hard time installing Mecab.

So, since a few years ago, I wanted to make a pure python version of Mecab that was easy to install while inheriting the advantages of Mecab. Now, Pecab came out. This ensures results very similar to Mecab and at the same time easy to install. For more details, please refer the following link.

[https://github.com/hyunwoongko/pecab](https://github.com/hyunwoongko/pecab)",MachineLearning
"Hello!

  
I've been working on a project that uses the Natural Gradient and I was wondering if anyone has suggestions on ways to include higher-order information. Is there some equivalent to the Hessian for natural gradients? If not, is there some sort of way of finding a trust region where the Natural Gradient approximation is reasonable?  


Thank you!",MachineLearning
" 

I looked into few sources saying that **Random forest** is one of the most popular MLA's for soil mapping. Especially since it is suited for both classification and regression problems. The only con for this one is that it is not suited for large data sets.

Would definitely need some guidance in identifying the best MLA for soil mapping and testing, it would help me a lot hearing from you guys!

I have based this info from: [https://soilmapper.org/soilmapping-using-mla.html](https://soilmapper.org/soilmapping-using-mla.html) at 6.1.3 chapter.",MachineLearning
"I work in an oil refinery. Beyond my regular role, I have been working on Python-based analysis at my workplace, including machine learning. Many colleagues have sent their data to me for analysis or to create ML models, but I do not have time to process all the requests (though I’d love to).

I’m hoping to look for a no-code and low-cost method that empowers chemical/mechanical/electrical engineers (who have no Python or ML knowledge) to attempt ML studies on their data, before passing it to me for further work or to put into production.

We happen to be using Power BI for dashboarding. Is asking the engineers to use Power BI Premium Pay-per-user AutoML a good idea? Or are there better, or cheaper or easier to use platforms? Thanks for your advice.

Additional question: would anyone know the full list of models that are considered by Power BI’s automl? Googling doesn’t seem to give me such info.",MachineLearning
I upload free type beats to YouTube and I've always wondered if any of them have been used to make a song but I don't know how to search for it,music
I'm looking for channels like soulhawk or Eduardo Diaz. Any help is appreciated I'm looking for any genre except country music. Thanks,music
"[https://soundcloud.com/user-701659528/now-you-say-you-dont-love-me-alt-version?si=bfad0bd1dbd440d88821b4dc290c5542&utm\_source=clipboard&utm\_medium=text&utm\_campaign=social\_sharing](https://soundcloud.com/user-701659528/now-you-say-you-dont-love-me-alt-version?si=bfad0bd1dbd440d88821b4dc290c5542&utm_source=clipboard&utm_medium=text&utm_campaign=social_sharing)

Tell me what you think!",music
"I haven’t been listening to them since many years, I kinda spiralled back to it. Brings back happy memories. What do you think of this group? Ik, ik they are mainstream af but their music keeps me happy for some reason.",music
"Shuki Levy is a name that might not be immediately recognisable to most people, but chances are you've heard his work. As one of the most prolific composers of music for children's cartoons in the 1980s and 1990s, Levy was responsible for creating some of the most iconic and memorable intro themes of the era. If you grew up during this time, you're sure to have fond memories of the many shows he scored.  
I for one think this man (At 75) is a living legend yet no one really knows who he is.

The 80's and early 90's era of kids television was another time, less about being PC, kids health, kids education rather a means to entertain and sell toys. Many cartoons often existed just as a means to sell the toys already created rather than the other way around.  
A cartoon needed an intro theme and this era they needed to be catchy, often punchy and compete with a busy morning schedule for the kids.

One of Levy's many great creations is the intro theme to **Jayce and the Wheeled Warriors**. This show, which debuted in 1985 was by far one of my favourites and one of my favourite intros. Not one of the most popular shows the but the intro theme has a lot of essense of what a young boy needed to hear and get pumped for. It had a great upbeat melody, catchy lyrics and a little bit rocky. It perfectly captured the spirit of the show and made it an instant hit with kids, including me. Even today, decades later, the theme song is still fondly remembered by those who grew up watching the show and I am sure you can sing along to it to this day.

One of the main iconic intro theme's more of you will know from Levy is of course **He-Man** and the Masters of the Universe. This show, which debuted in 1983, was one of the most popular cartoons of the era, and Levy's theme song played a big part in that. The opening sequence, which featured **He-Man** riding **Battle Cat** and wielding his sword, was accompanied by an epic orchestral score that perfectly set the tone for the show.

These are just two examples of the many intro themes that Levy wrote during his career. He also composed music for shows like **Inspector Gadget**, **M.A.S.K**., and **The Mysterious Cities of Gold**, among many others. His work was so ubiquitous that it's hard to imagine growing up during this time and not hearing his music on a regular basis. I could most many videos of these intro's and if you heard them then you would be at the very least humming along to them today if not still remember the lyrics off by heart.

He has done some of the power ranges themes, Digimon, He was even one of the two main composes for Rock the Dragon - From Dragon Ball Z!!!!

The list of themes to his credit that are just amazing and places Shuki Levy as a great musical genius whose work had a profound impact on an entire generation of children yet has way less recognition than he deserves. All his creations are a testament to Levy's talent and creativity and there should be more done to have him recognised for that.",music
"I've been trying to get ahold of a lossless or at least good good-quality rip of Bruton Music BRH 11 Pop Vocals but the only available one is the internet archive, the only issue is that the audio has several instances of a record skipping or some other vinyl related issue. If anyone has a link to a good-quality rip of the album please send it to me.",music
"Hey all.

I would like some answers to my survey on how drugs have affected/influenced the music industry.

[https://forms.office.com/Pages/ResponsePage.aspx?id=dRMlVPtPsE-fXqtybpucX9Gu2PX2p1BHu4H01Von9JJURDlPMjRFN1k1TUtXQUROTklGS1NDVkZVTi4u](https://forms.office.com/Pages/ResponsePage.aspx?id=dRMlVPtPsE-fXqtybpucX9Gu2PX2p1BHu4H01Von9JJURDlPMjRFN1k1TUtXQUROTklGS1NDVkZVTi4u)",music
"Great tunes, great people and a great cause! All Turkish DJ lineup and all proceeds donated toward earthquake relief efforts. Come join us at Catch One LA for a night of House & Techno!

[https://dice.fm/event/96l2n-playing-for-turkey-syria-16th-mar-catch-one-los-angeles-tickets](https://dice.fm/event/96l2n-playing-for-turkey-syria-16th-mar-catch-one-los-angeles-tickets)",music
"Years ago, around 2007 or so, there was a mashup artist named Bobby Martini. They did GREAT work. They even had a website, [www.bobbymartini.co.uk](https://www.bobbymartini.co.uk) . Then their website became a news site for some reason and I can't seem to find a single reference to them on the internet. The only thing that's left of their work is on [archive.org](https://archive.org) in the wayback machine. Does anyone know what happened to this mashup artist?",music
Ever since I started listening to music more and more passionately my expectations become way higher even songs that I used to love I’ve grown out of because they don’t reach my expectations anymore and I get bored of them. It’s hard for me to fully enjoy an album where I like every single song on it now,music
"Gordy (Ex-Avalanches) - 'One Kiss'

[https://soundcloud.com/el-gordo-506764727/one-kiss-by-gordy](https://soundcloud.com/el-gordo-506764727/one-kiss-by-gordy)",music
"My friend and I are taking a music theory class and we made the most god awful video to explain some people in the history of music theory and we need some people to watch it to tell us what we should take out before the project is due 😭
[Music Theory Project](https://youtu.be/H-GMeZuwflo)",music
"What do you think is the best album opener of all time? A track that captures your attention, blows you away, and then keeps you hungry for more for the rest of the album that follows it? The track that tells you ""that's it, you're in for a great time.""

I'll post mine shortly. I'm very interested to see what you guys select 👀",music
"They were in the band ""The Furay"" in the early 1980s. I cannot find anything online, but I remember listening to them on my Mom's back porch back in the 1980s. 

Anyone have an old tape laying around? I'd love to hear this again....",music
and sorry for my broke english if i couldn’t manage to explain myself. and there is no bad meaning after my words i love the Beatles and this question got me thinking lol,music
There are so many good ones but my favorite has to be Amon Amarths “Under The Northern Star”. The guitars and Melodie’s on that track are beautiful and I love it!,music
"Hello. I've allways listened to rap and metal, but around few months ago I've heard some folk song and somehow, it clicked. So I decided to somehow find more genres that would suit me. So on my journeys through internet, I found this song: https://youtu.be/5r3B7yz6J68 The whole thing, the vocals, the instrumentals, overal vibe just makes me feel so good, but unfortunately I couldn't find any songs that would be similar to this one, so I'm asking you for help. Could you guide me to some songs, or Spotify playlist that would be like this song?
Thank you!",music
"Savage Garden was the one artist in my Walkman as a kid that used to be on loop, sad to see them walk away from such a promising musical journey. Ah well creative and personal differences kill the buzz sometimes I guess, did any of your favorite bands go extinct?",music
" If you are currently in high school and like Music, I ask you to participate in my AP Research survey about the effects of different genres of music on reading comprehension. Pick one of these that interest you the most and complete the test! I would appreciate it if you could take the time to fill this out so I can start my analysis of the data! Thank you! 

&#x200B;

Classical - [https://forms.gle/PWwfwUtBXihdp6mj9](https://forms.gle/PWwfwUtBXihdp6mj9)

Jazz - [https://forms.gle/kvMTSkQFe5298BwF7](https://forms.gle/kvMTSkQFe5298BwF7)

Rock - [https://forms.gle/fKYUXhP2odVF3EmK9](https://forms.gle/fKYUXhP2odVF3EmK9)

HipHop/Rap - [https://forms.gle/6MuvcpEZdApRdyxP8](https://forms.gle/6MuvcpEZdApRdyxP8)

Techno/EDM - [https://forms.gle/EvfYyEtr4pBBm3gs9](https://forms.gle/EvfYyEtr4pBBm3gs9)

No Music - [https://forms.gle/Fii7ZhUpD2rcy3wX6](https://forms.gle/Fii7ZhUpD2rcy3wX6)",music
"Hello! I just uploaded this song to Spotify. It is a sad breakup song so I hope you enjoy it. I am a new artist and I will continue to upload more music in the following weeks. My songs are mostly slow and have dark vibes.

[https://open.spotify.com/track/3xAcMoostFGd4eAWKi0F85?si=1a44fa48485d4571](https://open.spotify.com/track/3xAcMoostFGd4eAWKi0F85?si=1a44fa48485d4571)",music
"Blues and jazz singer Hazel Miller is the guest on Mile High Stash this week and you can listen at [PodFollow.com/Mile-High-Stash](https://PodFollow.com/Mile-High-Stash) or wherever you get your podcasts. Hazel left Louisville, Kentucky in 1984 in search of stardom in L.A. and made it only as far as Denver, where she became a Colorado legend.",music
"Me and a few friends started a project a few days ago to create a huge list of musical genres, obscure and well known. Since then we have amassed over 180, and created an online spinner wheel for them. I have a challenge for any music creators here; Comment ""I would like to participate"" under this post and I will spin the wheel for you 3 times, your job is to try to create a song with the (likelyvery different) music genres/Subgenres you get, which I will reply under your comment along with an example track of each genre. Enjoy!",music
"I'm part of organizing a party where you're supposed to dress like you came to the wrong party, or wrong place. I don't know how to explain it. And we're making a playlist. And just going off the clothing theme, I've added some songs that are a bit inappropriate for a party, and WEIRD. Christmas music, etc. 

I would LOVE LOVEL OVE to get some more suggestions on what to add. Think either songs that are extremely WEIRD (does not have to be in English) or songs that just do not fit into the party theme at all. Help a pal out",music
"I think many bands and artists are accused of selling out, although they just changed their style. For me a great example is Linkin Park. Many people claim, they sold out after their second album, but actually their „sellout“ albums were by far less successful, than their first ones.",music
"...here:

https://glidemagazine.com/288896/the-church-keeps-it-dramatic-lush-on-the-hypnogogue-album-review/

\#thechurch

\#stevekilbey

\#timpowles

\#ianhaug

\#ashleynaylor

\#jeffreycain

\#rogermason

\#reybeepr

\#insight #info",music
"Are there any songs about grieving a parent/dad that wasn’t really there? Maybe about having somewhat of a connection when you were little but not later in life, & now the possibility is gone…",music
"...here:

https://www.allaboutjazz.com/rolling-stones-and-grateful-dead-stoking-the-eternal-flame

\#rollingstones

\#gratefuldead

\#jerrygarcia

\#brentmydland

\#keithrichards

\#mickjagger

\#ronniewood

\#charliewatts

\#rhinorecords

\#kayosproductions

\#warnerbrothersrecords

\#columbiarecords

\#insight #info",music
"Ok so I need some song suggestions for a performance. We basically will be celebrating 12th graders 100 days before graduation with a path to heaven and to be basically ""splashed"" with holy water and what we are basically thinking is using either iconic fun songs or calmish music (since it's to heaven yk?)

But I don't listen to much music, so any suggestions?",music
".....here:

https://glidemagazine.com/288884/40-years-later-revisiting-the-ramones-return-to-form-classic-subterranean-jungle/

\#theramones

\#philspector

\#paulmccartney

\#ritchiecordell

\#insight #info",music
"I personally don’t get how people find modern pop good, but it is all my gym plays (to loudly to block out) so I must become accustomed to it.",music
"A little home made video playing some sounds. Give it a try

[https://www.youtube.com/watch?v=GY6tTAGuDkw](https://www.youtube.com/watch?v=GY6tTAGuDkw) ",music
"Checkout this playlist I created that will make you ""levitate""

[https://open.spotify.com/playlist/3wR1jSTZIQ3e3dSBLjX82h?si=1e35d29798284aa7](https://open.spotify.com/playlist/3wR1jSTZIQ3e3dSBLjX82h?si=1e35d29798284aa7)",music
"https://youtu.be/ziJcLJpKCcc

Apologies if YouTube links aren't allowed, first time posting here. This (the studio version) isn't on Spotify, and it's really a lot nicer than the live versions that are.

Stunning soft fusion playing, and it's making me want to put down the drumsticks and pick up guitar again.",music
"I recently discovered D&B and love to dance to it (at least I try) on social occasions like a club. But I also listen to it casually on a daily basis. The problem is I cant find any new songs and am stuck with a small selection of songs which I like. Do you guys have any recommendations for? Here are some songs and artists I like:

Monrroe - Out of time, As I fall, never too old, closer, mud, closer

Bruh C - You & I, No excuses

Annix - Kaleidoscope, Nocturnal

Bensley - Hit the light, Halcyon

Hybrid Minds - Kismet, Bad to me

And a few more like: Sal  De Sol - Ready or not, Macky Gee - Aftershock, Shy FX - Bey Bey Bey

Thanks for any suggestions

Edit: By D&B I mean drum and bass",music
"Pretty much every app I try either requires you to pay to listen, pay to chose specific songs, or pay to play music in the background. I don't mind ads I just want to listen to music without needing to be on the app.",music
"One per decade:

My Generation- The Who 1965
Another Brick in the Wall - Pink Floyd 1979
Youth Gone Wild- Skid Row 1989
Leash - Pearl Jam 1993
In One Ear - Cage the Elephant 2008",music
So there’s a few artists I would like to listen that are pretty bad people in real life do you know how I could listen to the music without supporting them is YouTube a good source.,music
"I\`m looking for channels that advertise for songs you´d probably miss if you wouldn´t actively look for them. Most famous example would probably be WorldStarHipHop for Hip Hop, but i don´t care about the genre, so let me know what you know!

Channels i know already:

 JennyMusic - Rock, Epic Rock

Datpiff - Hip Hop

Froat - Underground Hip Hop

Dex Filmz - Japanese Hip Hop (yep, even found one for that :) )

&#x200B;

I know record labels do this as well, but i need a broader catalogue in one place. 

Thanks in advance!",music
"I’m obsessed with music and so is my spouse and I love cliché classic cheesy romance stuff so I’m making them a mixtape, on a cassette and everything!

The thing is we have very different music tastes xb. I love early 2000s and modern while they love 80s music. The only artist we agree on is Cupcake xb.

Ive googled a few 80s love songs but they’re all kinda slow and sad and I want even just a bit more upbeat so I’d love some recommendations!! They’ve given the request of Head Over Heels which is more of the vibe I’m going for lmao

Thanks in advance!!",music
"Can you guys/girls send me some of your favorite songs for carving down the mountain ?

Thanks!

edit: specific songs would be best ",music
"It's easy to find bands doing great versions of metal, rock, hiphop, pop, etc. We have thousands of those and it's a great time for music because of it.

But who is pushing the boundaries and doing something that feels new? It doesn't have to be because of using a different instrument, sometimes it's just in the way they play, or even just the singer being very different.

For me (and maybe I'll get skewered for this), an example of a very unique approach (despite being formed in the 90's) is the album Ohms by Deftones. It's synth heavy, Chino is a very unique singer, one song even breaks down into finger snaps. It FEELS new and unique and difficult to categorize for that.

Who is doing something different?

Don't know if it might sway suggestions but I trend heavily toward the more artsy and rock/metal, but love to hear all genres.

EDIT: All sorts of neat suggestions here thank you! I'm going through them one by one.",music
"1 We are the World - Michael Jackson and others 

2 Redneck Woman - Gretchen Wilson 

3 I Wanna Talk About Me - Toby Keith 

4 My Sharona - The Knack

5 Kerosene - Miranda Lambert

With some very strong honorable mention call outs to Dixie Chicks “Wide Open Spaces” and Miley Cyrus “Party in the USA”",music
"Verse 1:

My parents, they never got along

Fights and arguments, all day long

I used to wish that one of them was gone

Just to believe their love wasn't wrong

But they split up, and it tore me apart

Left me with a broken and empty heart

Love can be destructive, it can leave a scar

I'm still trying to heal, but it feels so far


Chorus:

Does it make sense to suffer in silence

Thinking you have someone by your side

When they're not there, it's all a lie

Loneliness is what I feel inside


Verse 2:

My girlfriend, she couldn't give me security

Left me feeling lost, with no clarity

The fear of being alone, it's killing me

I'm trying to be strong, but it's hard to see

Outside my house, it's a different story

Social anxiety, it's hard to ignore it

Familiar faces, they just make me worry

Triggers my trauma, and I can't restore it


Chorus:

Does it make sense to suffer in silence

Thinking you have someone by your side

When they're not there, it's all a lie

Loneliness is what I feel inside


Verse 3:

I spent a night on the street, it was rough

With bums and criminals, it was tough

No jacket, no warmth, in the cold and dark

Surrounded by people who've lost their spark

My grandfather said, ""I become bad""

My father said it too, when my mom was mad

I said it too, when I felt so sad

Trauma and pain, it's all I've had


Chorus:

Does it make sense to suffer in silence

Thinking you have someone by your side

When they're not there, it's all a lie

Loneliness is what I feel inside


Outro:

Life is like a movie, you don't like

Hoping for a twist, to make it right

But sometimes, it's just a sad sight

And you turn it off, to end the fight

Neighborhood is forever, it's true

Money can't erase, what I've been through

Reasoning and traumas, they never leave you

But I'm still trying, to push through.",music
"2 male voices in the song, the song sounds like it's from 1980s. I think it's about germany, previously called prussia, maybe it's about the pressure prussian government put on the other nations they went to war with.

Some of the lyrics: pushing down on me / give ourselves one more chance",music
"Genre: Rock/Alternative Rock

Side A: 
Hiroshima Girls - https://youtu.be/NQX2vhmTleQ

Side B: 
Crazy - Link N/A

Great Days - https://youtu.be/ZN3k95Cm4MU

Note on crazy: Many links are no longer live, or otherwise only half functional. I will update this post once I am able to find one.",music
Is it just me or does the start of the song “The Wall” by Kansas sound very low quality. I’m on Spotify with sound quality on the highest setting and it’s just the start of the song sounds bad but the rest is perfectly fine. Every other song on my playlist sounds good as well.,music
"Hey, just wanted to get some additions to my playlist, and thought this would be the best place to ask, any suggestions are welcome!",music
"They are my fave band and I don’t know anyone that has heard of them. They are big in Europe but here in UK relatively unknown. I am beside myself today as I have managed to secure tickets for a tiny gig, meet and and greet and after party. Have met some of the band member at other small gigs they have done and looking forward to this big time.",music
"https://youtu.be/oP3Kv2-covk

""I know that things can really get rough when you go it alone
Don't go thinking you gotta be tough, to play like a stone""",music
"https://youtu.be/38YEnQWDLvo

""I want to live where soul meets body
And let the sun wrap its arms around me
And bathe my skin in water cool and cleansing
And feel, feel what its like to be new""",music
"I have recently stumbled upon the website SONOMO, which makes it possible to sell fractions of your songs. Does anyone have experience with the website as a musician or as an investor? Would you recommend it? Are there any benefits you see over the other established music investment platforms? Do you generally like the concept?

I am very curious about your experiences as I am still uncertain myself!",music
"so to anyone part of an older generation, this might sound silly because it used to be a commonplace thing in the music industry, but looking online, i see that in the case with a lot of albums, one or two singles will be released, then the album, but then sometimes more singles off the album will be released; why is that? surely it makes more sense to release all the singles before the album to generate buzz than to release singles after? why would someone buy a single when they already have the single on an album they bought just a couple months before? for example, oasis released “be here now” in august 1997, but the single “all around the world” was released in january of 1998 which just doesn’t really make sense to me. if anyone could answer this i’d greatly appreciate it.",music
"I want to listen to someone who pours their soul into their instrument. Preferably not someone with only one good song. I want an album I can listen to that has the emotion that a hendrix song has. Some music I already like is: Cream, Hendrix, B.B. King, Beatles, RHCP, Vulfpeck, Nirvana, Black Sabbath, and The White Stripes.",music
"Hello everyone :) I‘m trying to help my friend get a bit more recognition, he works hard on his music and I feel like he deserves more attention
[Spotify, Album: Ashtray of Love](https://open.spotify.com/album/2iT8XaSB5rG1BYuKNBNTkb?si=Zwibq8HRQZ-xlwyoIx-2Nw)

[teaser at his new song, Amnesie](https://vm.tiktok.com/ZMYUNDsBL/)",music
"I love tons of Steam Powered Giraffe songs, but I don't know if this one is racist or not. I don't want to listen to it if it is racist, so can someone please help?
I AM NOT TRYING TO START DRAMA, I AM JUST ASKING A QUESTION. I AM ALSO NOT TRYING TO SLANDER/RUIN THE REPUTATION OF THE BAND OR ANYONE WHO HAS WORKED ON THIS SONG.",music
"Almost ten years ago on June 18 of 2013, Pittsburgh rapper Mac Miller released his second studio album, ‘Watching Movies With The Sound Off’. Whilst this was Mac’s second album, he had released several mixtapes prior to this, including 2010’s ‘K.I.D.S’, 2011’s ‘Best Day Ever’ and ‘I Love Life, Thank You’ and 2012’s ‘Macadelic’. ‘Watching Movies’ boasts features from many hip-hop heavy hitters including Tyler, The Creator, ScHoolboy Q and Action Bronson amongst more. Mac had begun handling his own production under the alias of Larry Fisherman and did the majority for this album, accompanied by additional production from Pharrell, The Alchemist, Flying Lotus and more. The LP’s title comes from Mac’s tendency to watch films on silent whilst he crafted music, and the album artwork was designed by his brother, Miller McCormick.

Mac’s earlier works had a more playful, boom-bap style in it’s songs, which he had started to evolve from on his ‘Macadelic’ mixtape and further continued with ‘Watching Movies’, bringing more introspective lyricism and experimental production. Speaking in 2012, Miller had expressed desires to grow as an artist, writing and producing more, wanting to ‘show the world all that I do and maybe only part of what I’m capable of, but a lot more of into how much I do’. In my opinion, ‘Watching Movies’ proved a success in alignment with his aims, with the album featuring songs spanning many moods, such as the rowdy ‘O.K.’ or ‘Bird Call’ or the more poignant tracks ‘Remember’ and ‘The Star Room’. Mac showcased his ‘Delusional Thomas’ persona more on this LP, rapping in a pitched up voice which he would further employ on his 2013 EP, ‘Delusional Thomas’. In the process of creating the album, Mac claimed to have recorded around 400 tracks, some of which being on his 2012  jazz EP ‘You’, released under the pseudonym Larry Lovestein & The Velvet Revival, hints of which are shared on ‘Watching Movies’ tracks like ‘Youforia’. 

Mac has been one of my all-time favourite rappers and artists for years now ever since I first bought my copy of ‘Watching Movies With The Sound Off’ way back in 2016, and it broke my heart to never see him before his untimely 2018 passing. This was the first album I had heard from him, and it managed to grab me in a way no hip-hop album really had before, with it’s genre-bending beats and perceptive tones and wordplay. It probably still stands as my top project Mac released, highlighting my best-loved era of his, before the harder rap tones of 2015’s ‘GO:OD AM’ and 2016’s soulful ‘The Divine Feminine’. Any alternative hip-hop fan should give this album a full listen through, as it remains as influential and timeless as it’s release ten years ago. In no order, my top 5 picks from ‘Watching Movies With The Sound Off’ are below: 

-‘Bird Call’ (2:08)
-‘Red Dot Music’ Feat. Action Bronson (6:07)
-‘S.D.S’ (3:04)
-‘Someone Like You’ (4:15)
-‘I’m Not Real’ Feat. Earl Sweatshirt (3:23)[Full insta post with media](https://instagram.com/p/CpI-lb4IYp-/)",music
"Hey Guys!

Hope everything is well ;) I would love to understand /confirm my understand about the following song lyrics, Would you mind helping me ?

Here the link to the vocal audio:

[https://soundcloud.com/dominicandkevindetta/lost-lost-raw-export/s-ny6u5HpoRYB?si=fe30fade327e4150b3ddf46543d250cb&utm\_source=clipboard&utm\_medium=text&utm\_campaign=social\_sharing](https://soundcloud.com/dominicandkevindetta/lost-lost-raw-export/s-ny6u5HpoRYB?si=fe30fade327e4150b3ddf46543d250cb&utm_source=clipboard&utm_medium=text&utm_campaign=social_sharing)

Here the text transcription by me possibly wrong in some parts:

*I'm lo, lo, lo, lost. I swear I'll be better than I was. I thought that I was special. We will be together. But I've never been so wrong. Now low, low, low, lost. I swear I'll be better than I was. I thought that I was special and we will be together. But I've never been so wrong. Tell me, was it worth it ? My heavy heart is broken, but now you're telling me that yours's even open ? Anyway, baby, it's so much and I feel just like a foolish I have done stupid loses.*

*Why I'm lo, lo, lo, lost, I swear we'll be better than I was. I thought that I was special and we will be together but I've never been so wrong. Now low, low, low, lost. Swear will be better than I was. I thought that how special and we will be together but never been so wrong.*

Don't know if it's ""I'm low"" or ""I'm lost"", what do you think ? If this would be a song, what would be the title ?

Thank you in advance for the people that will help me!

Bye

Dominic",music
"I spent days putting the layers on this together so we can keep it simple and soak up the inspiring background rain, beats, and jazzy flare I spinned in.  The artwork in the background is actually a collaboration from multiple students at Columbia University in the City of New York (one which is me).

&#x200B;

I hope you enjoy.

&#x200B;

Best,

Joe

&#x200B;

[https://youtu.be/JiSRJtUvwVU](https://youtu.be/JiSRJtUvwVU)",music
"[https://www.youtube.com/watch?v=79GGtH9PMpY](https://www.youtube.com/watch?v=79GGtH9PMpY) 

Cuz is said to be the best rapper of all time in Sweden, maybe there are some sweds here who think otherview? 

And what do no-swedish people think ?",music
"i dont remember a whole lot other than there's a part where the music switches from the regular melody to 8-bit and before it goes back to the original melody, there's 4 game punch sound effects and then a digital voice saying ""you lose!"" and then a man saying ""ahh fuck""",music
"I've found quite a few who've ended up being some of my most-listened artists. I love unearthing completely underrated artists. There are so many!

Does anyone else enjoy finding these?

Mine:

Julian Dente (39) [39 not 39k!!]

Global Genius (8k)

Loviet (12k)

Arbour Season (15k)

Yøuth (17k)

Chelsea Lankes (21k)

Me Not You (22k)

BEL (33k)

Paperwhite (37k)

Kate Stewart/KStewart (98k/20k)

Annika Bennett (125k)

Abby Holliday (134k)

Scavenger Hunt (153k)",music
"I have an audio clip that consists of two clips merged together, I have the isolated/unmerged audio for the first clip, but not for the second. Is there a website or some sort of thing that can allow me to remove the audio I already have from the merged clip? Kind of like using the singular clip I have to negate its own audio from the merged clip. Any help would be much appreciated! (I am a producer btw if you're wondering why I wanna do this lol)",music
"Hey guys, idk if this is the right sub. So basically I got a library of 30k+ song, but somehow, about half are duplicates. Is there any music player(preferably android) that can help me scan for and delete these duplicates. Thanks a lot. If this is the wrong subreddit, kindly direct me to the right one, cheers.",music
"[https://www.youtube.com/watch?v=x3y-4\_TwnJY](https://www.youtube.com/watch?v=x3y-4_TwnJY)

we need to get 1500 views for a radio session in my country!

If you can leave one like or some comment in the video will love you eternelly!

it's take 1 click! love u",music
"So, I've been playing drums for a looong time, but I never experimented with drum plugins, or basically didn't ever do anything with the sound of the edrums except from the options of my module. The Jay Weinberg kit from mixwave has just dropped, and I'm blown away with how it sounds, but I have trouble gathering information about Mixwave. Is it a software? Does it come free with the plugin purchase? Is it a similar software like ezdrummer or superior drummer? Thank you in advance for any help.",music
I was told by a artist they would do a feature with me . I sent the song a week ago I don’t know if they heard it or not . I haven’t really got much communication back . How long do I continue to wait ? Or do I just give up and fill in the open verse myself .,music
"I just watched the movie “The Natural” and in it is the song “Winning” by Randy Newman in the soundtrack. I have been wondering what kind of genre of Jazz this is. I really enjoy it and they use it in a lot of older baseball movies.

[https://youtu.be/Kr9Epdkzw2I](https://youtu.be/Kr9Epdkzw2I)",music
"like why i’m recording a song and my family member knocks on my door and ask me to turn it down a little, but i don’t want to continue my work to a lesser degree than what i’ve already begun, honestly i’m only thing for me to do is continue with the same degree i stared with 💯",music
"I’ve been out of the country music loop for a while and want to dive back in.  What are some top songs for me in that time period?

I will listen to them as they come in.",music
Love listening to their music every now and then. Was getting into my car after a soccer match and thought they sounded familiar. Sure enough it was Depeche Mode back at it again with another tune. [https://www.youtube.com/watch?v=iIyrLRixMs8](https://www.youtube.com/watch?v=iIyrLRixMs8),music
"I've recently started acquiring a taste for aggressive music beyond the metal genre. Here's what I've found so far.

John Wick introduced me to Le Castle Vania.

My Friend Pedro introduced me to Battlejuice.

And... that's about it. Are there any other artists that have a similar aggressive sound in this genre? I've tried navigating the electronic genre but most other artists have upbeat music and I'm looking for shit to listen to that'll give me some fucking adrenaline. (nothing against upbeat music or more experimental stuff - just looking for a very specific kind of vibe rn)

Thanks in advance!

edit: thanks for all the suggestions :D",music
"I am currently looking for songs that I can use for a short film I am making. Do you  guys have any recommendations?

Suggest from Royalty-free songs to indie songs that I need to contact the creator for permission to use. Upbeat music and also Happy then Sad music.",music
"This is a link to my dead male friend of mine's page. He died of an intentional overdose ala Darby Crash. His name was Alex and he left us this blog about Casper Rose and The Distillers: https://thesagacontinues.art.blog 


RIP MAN",music
"I charge very affordable rates

Here's my resume below:

[https://docs.google.com/document/d/1\_Pt-2p4dgGo\_45b329XM4fw39jkrJ7AuZfrhH9sCits/edit?usp=sharing](https://docs.google.com/document/d/1_Pt-2p4dgGo_45b329XM4fw39jkrJ7AuZfrhH9sCits/edit?usp=sharing)",music
I have grown up listening to mostly exclusively rap and mainstream radio artists however recently I’ve been getting into more music and I just really want some new music to check out preferably albums so please leave any suggestions and I’ll listen to any genre except edm,music
"What's the best music distribution for a new artist and allows you to sync lyrics, and make your projects look professional as possible?",music
"Did you ever realise that League Of Legends’ Arcane’s „Dynasties and Dystopia” by Denzel Curry sounds a bit like Ariana Grande’s „Bang Bang”? Especially the intro. My sister thought is was sampled. And I can’t unhear it now.

Hear it yourself:

Dynasties and dystopia:
https://youtu.be/y_fB0IMbq54

Bang bang:
https://youtu.be/YswhUHH6Ufc",music
"I’m doing this thing where I listen to an album every day. I will do any non-greatest hits album of any genre but mainly something in the camps of rock, rap, or r&b. Feel free to leave as many suggestions as you want, I love discovering new music.

Favorite Artists (If this helps me find something I like): The Beatles, Nirvana, Radiohead, Weezer, The Smiths, Pixies, The Police, Gorillaz, Prince, Out Kast, Tyler the Creator, MF DOOM, A Tribe Called Quest",music
"Writing a song and I love where I have gone with it, but the chorus is in Bb minor and don't know how to change back into e major for the verse etc.... I have researched a bit into circle of fifths and have played around for hours but cant find chords that can transition back to E major. If anyone can help, would be appreciated",music
[https://open.spotify.com/album/2MAmT11hQfoo25ooMyhpfx?si=WlnsVqsYT22doBrZ2z2CKA](https://open.spotify.com/album/2MAmT11hQfoo25ooMyhpfx?si=WlnsVqsYT22doBrZ2z2CKA),music
"Based on the past two genres, we can probably predict that some new genre will begin to rise in a decade or two and become the next cultural zeitgeist of music, most likely coming into being in the 2030s or 2040s. In your opinion, what might this new genre sound like or if you already know of a genre that might become the next zeitgeist, what is it. 

I think it will definitely be a product of Gen Alpha growing up with streaming services. I think the fact that they have practically all the music of the world at their hands form birth will lead to some cool fusion of basically any mainstream genre under the sun. Kinda in the way rock blended blues, country, and jazz, I think this new genre will make up a similar blend, probably some combination of hip-hop, indie, and electronic.",music
"Thought I saw this on r/Music in the last 24 hours and am coming up empty.  Can someone help with band name.  Melodic, slower tempo funny workout music video.",music
"Cool retro voice. She has been gaining a following recently. [Emmaline - ""Peppermint Kisses"" Live from Parker Jazz Club 2022](https://youtube.com/watch?v=--oUpOY7RRs&feature=shares)",music
"Some examples of what I’m looking for is Shame by Stabbing Westward or the bridge in Lonely Day by System Of A Down.
(“And if you go, I wanna go with you / And if you die, I wanna die with you / Take your hand and walk away”)
Songs about complete devotion to a person, loving them so much you would do whatever they asked, not having an identity without this person, they are all you have, you would die for them, etc.

I’m not looking for stalker-ish songs like Dangerous by Big Data & Joywave or Mx. Sinister by I DONT KNOW HOW BUT THEY FOUND ME 
and I’m not a fan of rap, pop and most indie music (indie rock is acceptable).

Thank you :)",music
I am trying to find a album of a band that I can’t remember the name of or any songs from I have good memorys with it singing along with it with my mom and I’m trying to find it but I can only remember a small discretion of the cover it has a man in a house that I think was flying and the man had a big head. Thats the only thing I can recall and I want to see if any knows what band I’m talking about,music
"I’ve listened to this a bunch of times, and while there seems to be 2 spots where it can be looped, I myself can’t seem to get it to loop seamlessly. The part that seems obvious is the beginning and around the 1:20 mark https://youtu.be/inQMFPaECTk",music
"Hello, community, I'm from Colombia, and I'm telling you about my situation. I graduated as a professional musician and it's been hard for me to work in my area. I'm planning to do a master's degree at Berklee, the master's degrees that catch my attention are music production and songwriting in New York and others They are in Spain. which are:

\-Music Production, Technology, and Innovation

\-Scoring for Film, Television, and Video Games

\-Master of Music in Contemporary Performance (Production Concentration)

I would like to know your opinions or someone who has done some of these master's degrees with that suddenly there are more job opportunities and which of these has the best job prospects. I don't consider myself a bad musician but it has been difficult for me to integrate into the musical world of my country, I have thought about giving up music and starting a new career but I see this as a last possibility or if you know any other master´s in another university or college I would appreciate it.",music
Hey don’t know if this is breaking the rules but I’ve listening to this one specific song called “hope she’s in heaven” but it disappeared without a trail. I was wondering if anyone would know where I can find it I looked on Spotify and YouTube but it’s been taken down from each and it’s driving me crazy.,music
"I want to make a band with people vaguely in my age range (I'm 23) that perform at places like parks for fun with any sort of instrument that can be easily carried around, and doesn't require any wired electrical equipment (Like stand alone keyboards and keytars, acoustic strings, tambourines, etc)

I want to incorporate eclectic musical elements from every member, and allow any member who has the ability to write songs to do so. 

Requirements:

A: Must have an instrument that they have very basic knowledge of how to play (ukulele, guitar, keyboard, keytar, tambourine, maracas, flute, etc) 

B: Must either have a very very basic knowledge of sheet music and music theory, or be willing to listen to the people who have a very very basic knowledge of sheet music and music theory.  

&#x200B;

Hope we are able to get a fun band together and make some summer memories. 

Message me or reply to the post to discuss details.",music
"[https://open.spotify.com/track/6ssfnd7WMyjaRmKSSVE5TR?si=fcd7832a8cea4989](https://open.spotify.com/track/6ssfnd7WMyjaRmKSSVE5TR?si=fcd7832a8cea4989)

Hey all, we're a brand new Punk Rock band out of New Zealand and South Africa. That's right we're based in two countries. If you're a fan of rock mixed with Indie, Punk and Synth-pop then you might like us! Enjoy!",music
" Backend Rico comes out with his single songs\\Hip hop & Rnb Music on spotify music. click below, press play and stream below>>>>>https://open.spotify.com/artist/7L9phpt8zVIQxNnEpexTdE?si=8nsQNgOpS46q9vy587dMcQ",music
"I know a good chunk of you here make music aswell. Whats your most creative/deep cut sample pull you have done for something? Mine is below, im curious to what other peoples are now!  
 [My best sample flip ever - YouTube](https://www.youtube.com/watch?v=ev_F9W-Yr-Q)",music
"I’m having a hard time describing the type of vibe I’m going for but imagine listening to more old school inspired music while vibing/getting high in the desert or on a beach, lol. Or going on a road trip out west. What music might satisfy this craving for me?  Would love full playlist or album recs. 

Lately I’ve been listening to these playlists a lot: [Psychedelic Rock Essentials](https://music.apple.com/us/playlist/psychedelic-rock-essentials/pl.5b365739c6094beca818cb82b76d43f8) & [80’s Soft Rock Essentials](https://music.apple.com/us/playlist/80s-soft-rock-essentials/pl.25c27415dc3e46ed800b254cd653cb67)",music
I am a person from the younger/new generation who loves to listen to the older music. Think about Elvis and Sinatra but basiccaly anything between the mid 50s all the way up to the early 90s. Especially GNR ( Guns and roses) Really gives me a great vibe. But I seem to notice more and more that this great music of GNR and the others is starting to get forgotten. Any other young guys/girls from the 21t century or late 90s that can prove me wrong and tell me I am not the only one listening to these great songs?,music
"I am flying across the country this week and I find that there is really no place better to really listen to music than on a plane. 

I am wondering what one song gives you the chills - in that way good songs can - so that I can add it to my playlist.",music
I just listened to this song and fell in love with it. Hope you like it too! [https://www.youtube.com/watch?v=ih\_pJRFIR2s&ab\_channel=Susurrous-Topic](https://www.youtube.com/watch?v=ih_pJRFIR2s&ab_channel=Susurrous-Topic),music
[https://www.tiktok.com/@natesmithmusic/video/7184991597640076586?is\_from\_webapp=1&sender\_device=pc&web\_id=6984910271870567941](https://www.tiktok.com/@natesmithmusic/video/7184991597640076586?is_from_webapp=1&sender_device=pc&web_id=6984910271870567941),music
"I need help choosing a better musical artist

One of my friends is a large lil peep fan, and I don't even consider him real music
So I just help i guess for reasons why bands like the beach boys or velvet underground are very much better then mumble rap
So the beach boys or lil peep !?!?",music
"I’ve been wondering about this for a while. For example, “Clothes Off” by Gym Class Heroes features Patrick Stump of Fall Out Boy. If Joe Trohman of FOB also joins in, would it be considered a feature with FOB or just Joe and Pat? At what point is a band featured? Is it the majority of the band, half, two members, etc? Like in this example, do Pete and Andy have to sign off on the name usage since they’re not involved? Or can Joe and Pat use the name? If two members of Slipknot out of like 7 I think there are, collaborate with another band, could they use Slipknot’s name or do they have to all sign off on it or be involved? 

Also I am aware it could be on a case-by-case scenario but in general how does that work?

TL;DR At what point is a band credited as a feature with another band, if the entire band isn’t involved?",music
"How y'all doing? I'm typing this because I've had a discussion with a relative of mine about the people who stick themselves in a music period ('80s ,'90s,...) and do not listen to new bands that are the same genre they listen to.
I use to listen to rock and metal and I love the old school (Metallica, Iron Maiden, Megadeth, Black Sabbath, Judas Priest, RATM, Dream Theater,...) but I'd also like to discover new bands with the same genre and are currently making new music. Could you please tell me if you know any Spotify playlist that stands for new bands that do Heavy Metal (like the old school one) or recommend me any band, artist or song you think I might like because of my musical taste?",music
"An example would be The 1975 and 5 Seconds of Summer. A lot of people know who they are, but they aren't on the level of like The Rolling Stones or Coldplay. You don't see these artist winning Grammys or spending weeks on the to 100 charts.

I would like to branch out and listen to more bands. Comment below some recommendations.",music
"My daughters in a band, in Leeds UK. I’m so proud she wrote this song. I love it. I hope others do too. It’s their second single

They are all from Newcastle - hence the name - it’s a geordie word ;) but met at Uni in Leeds. It’s punk and shouty but utterly brilliant. Give it a few mins of your life to listen. 

She’s ace. They are ace. It makes her happy which makes me happy",music
"Just wondering if anyone can confirm this. I tried a few different headphones and it seems to be left side heavy. Wired and wifi headphones. 

Is it just me? Other audio doesn't do this. Is it just how its mixed/recorded?",music
"Please join my group [https://www.reddit.com/r/Sananda\_Maitreya\_Fans/](https://www.reddit.com/r/Sananda_Maitreya_Fans/)

to talk about my fav artist Sananda Maitreya

&#x200B;

(who once was known as Terence Trent D'arby! )

[www.SanandaMaitreya.com](https://www.SanandaMaitreya.com) \>> Official Website

[www.youtube.com/SanandaMaitreya](https://www.youtube.com/SanandaMaitreya) \>> Official YT Channel",music
"So I have been wondering this for a long time.
So you ever been to a school party, bar mitzvah, or just listened to the radio any time in the late 2000s-early 2010s and heard very hype, party music.
Here are some examples:

I gotta feeling
Can’t hold us 
Dynamite 
Give me everything tonight
Half the songs by maroon 5
Glad you came
Wake me up
Some nights
And a million more

I might write an editorial about it for school, but I was just wondering if anyone noticed this pattern.",music
"Hi, I always wanted to listen to music but the problem is that I don't know how to find it and what would be the fastest way to find it. I like some music artists like Tyler the Creator, Słon(rap artist from my country) and I like some songs of the weeknd too. 

What is the fastest way and what do you recommend?",music
"Did a song about the 80s cartoon ""She-Ra?""

Like, I enjoyed the show as a kid too, but what provoked Spirit to make that song???

Did Mattel / Filmation pay them to do that, or they just decided to on their own?????",music
"It has this bongo/hand drum beat and is fairly fast. The chorus is this guy (maybe Creole not sure) saying something like “balay bum bum balay” over and over again. It’s definitely a Tik tok song with a catchy vibe and is sung in non-English language. Very modern sounding and probably made in the last 10 years if I had to guess.

Edit they might be bucket drums",music
"Pink Floyd - A Momentary Lapse of Reason 
The Beatles - Abbey Road
Pink Floyd - Atom Heart Mother  
Red Hot Chili peppers - Californication
Linkin Park - Hybrid Theory 
Linkin park - Meteora 
Coldplay - Parachutes
Tame Impala - Lonerism
Radiohead - Hail To The Thief 
Radiohead - In Rainbows
Saba - Few Good Things
Electric Light Orchestra - Time 
Electric Light Orchestra - A new world Record
2Pac - Me Against The World 
Electric Light Orchestra - Discovery
Eagles - Hotel California 
Algiers - Shook 
Nirvana - In Utero",music
I am trying to create a vibe workout playlist that has songs that sound like “Monster”- Meg and Dia or “Zombie”-The cranberries. Any suggestions?,music
"Think about it. If we are comparing Boybands from the 90s, such as Take That to Kpop groups like BTS. Some of the members have similar haircuts to each other.

For example: 
(For Haircuts)
• Mark Owen = Jimin
• Gary Barlow = RM
• Howard Donald = V
(For their faces such as the look of their smile)
• Robbie Williams = SUGA
• Jason Orange = J-Hope

Now that we’re left with Jungkook and Jin, Who do you think Jungkook and Jin would’ve looked like in Take That?",music
"Hi,

This may sound strange but i listen to metal, with a HUGE exception; i need to google the meaning of all song lyrics before listening out of some weird fear of them giving me like bad/negative energy and giving me bad luck and diseases and stuff like that. For example im super scared of listening to any songs that contain death, satanism, disease or anything negative like that. I'm scared of listening to One by Metallica for example. 

Please help I know this isn't normal.",music
" Hi guys, My name is Hugo Arciniegas, I am an English teacher from Colombia, and I want to get deeper in this production world which I like it and I feel I belong. 

I just got a brand new laptop for this purpose and I am looking for any person who has and extra code for any of these two programs to get them legaly, I would appreciate it!  
*Yes, I don't earn that much so I can't pay for them lol*  
Thank u in advance! 

You can listen to my new song, I hope you guys like it! 

 [https://distrokid.com/hyperfollow/hugoarciniegas/pxrx-ti-feat-ashesndreams](https://distrokid.com/hyperfollow/hugoarciniegas/pxrx-ti-feat-ashesndreams)",music
"I have a very poor attention span and I'm really itching to sit down and listen to a double album (lysflath to be specific) and honestly I can barely find the time, and when I do my attention span always dwindles off.",music
"Arístippos Better Have My Stoicism - Bizarril

[https://open.spotify.com/album/0RoVhPSckl7SzS70aLYJQ3?si=3pDh6EMdQ3S\_hKKkYJuDKw](https://open.spotify.com/album/0RoVhPSckl7SzS70aLYJQ3?si=3pDh6EMdQ3S_hKKkYJuDKw)

mini-album, genre difficult to classify",music
"This is the first single for my knew album titled go away a song all about the privilege of being alive and making the most of life would love any feedback you have as I really take it on board 

https://youtu.be/DWNk3ET3tjQ",music
"I've been in choirs most of my life and love vocal harmony like a capella, and chamber music, but I'd like recommendations for alternative/acoustic bands or groups that have more than one lead vocalist and harmonize well. 

To help narrow down what I'm looking for, I like: The Wailin Jenny's, Simon+Garfunkel, Thirdstory, Darlingside, Walk Off the Earth, Hailaker 

Thanks in advance",music
"What I mean by emotion is that the sound of La petite fille de la mer is hauntingly beautiful, it's soothing but it's also as if there's underlying depressing/eerie tone within. Kinda like what you hear in a stop-motion film (coraline, the nightmare before christmas, corpse bride) from the late 2000s. Not the best in describing classical music, sorry.

I would give music that I deem resemble the emotion of La petite fille de la mer to give you an idea, such as: Serenity (From Resident evil 4), Epilogue (From A tale of two sisters), Lacie's Melody (From Pandora's Hearts), You (By Vietra From Soundcloud)

Thank you and have a great day!",music
"Hi guys! 
I try to find a oldie song in french, very happy song, i just remmber he said C'est la C'est la vie.
The rhythm remains a little of boots ain’t made for walking 
Ty!!",music
"https://youtu.be/eyBe0vze5Q0 

Michael Jackson a couple months before his death. (Bad quality)

After all he went trough, dude was still shutting down streets like it was the 80's and 90's. And he's in the backseat of a SUV? Thought Beverly Hills folks were a little more used to celebs, going crazy over his forehead and 2 fingers? Lol.

Who comes close today?",music
...so make great quality music with great quality sounds so everyone can enjoy life (notes while zoning out to a nyc live band),music
"This is one of my new favorite melodic hardcore punk songs, and for the life of me, I cannot find the lyrics to this song. Can anyone out there help me out?",music
"My new pack is out

All the samples are 100% royalty free!!!

No email subscription!

You can download the pack here: [https://vinyloops.com/products/diva-presets](https://vinyloops.com/products/diva-presets)",music
"Recently, I have noticed a trend on TikTok where users compare their physical appearance before and after listening to a song by Lana Del Rey. Thoughts?",music
"Recommend me some relaxing and feel good albums that will make me confident of myself and move forward with the difficult stages of my life.
 I am open to all the genres, feel free to recommend :-)",music
"For a long time but I've always referred to certain pop music as ""proper pop"", there's ""artists"" that claim to spend ages on their music and it's all treated very seriously by their PR guff but it still comes out sounding no more deep or complex than a Stock, Aitken and Waterman song. There's also artists that don't claim this and are clearly in it for their 5 minutes of fame who are often more enjoyable even if it's by virtue of not taking it too seriously.

Pop isn't serious music and shouldn't be treated as such but I feel the term ""proper pop"" always needs explaining. It's like i know there's a better term i just can't think what it might be.",music
For the past week or two all I have seen on my front page is questions that seem either pandering for response or intentionally controversial (for response again). This doesn’t seem like the intended purpose of this sub (IMO). Is this a mod thing or should I just find another sub? Maybe I’m the one who’s wrong.,music
"There used to be a video on YouTube of Garth Brooks in an interview playing a shortened version of 'If Tomorrow Never Comes' on an acoustic guitar, but I think it was removed. Does anyone happen to have a link or a copy/audio file of it? It was a beautiful piece and I'd love to hear it again.",music
"Hello people, my name is Hugo Arciniegas, a Colombian English teacher. I really like this music production thing, because it's what really appeals to me and I want to dedicate more time to it. So, I've decided to start off on the right foot, here I share with you my song ""PXRX Ti"" I hope you like it! Happy day and thanks in advance!

[https://distrokid.com/hyperfollow/hugoarciniegas/pxrx-ti-feat-ashesndreams](https://distrokid.com/hyperfollow/hugoarciniegas/pxrx-ti-feat-ashesndreams)",music
" So I'm making a playlist for my guy best friend and i want some really great indie pop, summer vibes songs, like the ones which would easily light up hus mood, bring a smile on his face just by feeling the vibe and looking at the lyrics!! Overall give the butterfly effect to him, list your fav songs of such kind!!

If y'all wanna know what type of song I'm talking bout then here are a few
https://open.spotify.com/track/5aQe0faFOjeskrHwpleXLJ?si=I_u9HO8GReiDyVpj9FAWng
https://open.spotify.com/track/5cwCW4r6ybpSiHWMpMKHoJ?si=NK60y1fYTjKQeCb9Ue0wZA
https://open.spotify.com/track/1tD8J13a74q8fBqXwAP50j?si=sZrxQJrMQlml3imtqZrmLg
https://open.spotify.com/track/4h4QlmocP3IuwYEj2j14p8?si=BksLYzhGTGar8J02Csq2iA
https://open.spotify.com/track/37KYM3zuGyozpEwDuO2OVw?si=G5EZqT4eQ8mNXIEjz0xEUg
https://open.spotify.com/track/3GWaFFi6TlPcZLYmCCHZ9O?si=QGYud2d4Q0Ko5y7JzmXctg
https://open.spotify.com/track/53m2qiG26iGnHb3ygFFDp2?si=kxVeiw8YQIOO9rIIH4WEOg",music
"My top 10 right now would be:

1. Travis Scott
2. Lil Durk
3. Yeat
4. NBA YoungBoy
5. King Von
6. Future
7. Playboi Carti
8. Joey Bada$$
9. Lil Baby
10. Tory Lanez

How would your top 10 look like?",music
"Years ago my friend played me this song in which a deep-voiced man talks about colors, set to music. He describes very specific colors like ""Azure"" and ""Burgundy"" and assigns each color personality traits. Like ""Magenta....selfish and vain. An enemy of Violet. Burnt Orange... friendly and aloof. Only trusts Gold and Ochre"".

It's like nothing I've ever heard before. Super dreamlike. I feel like the title sounds pretentious, like ""Requiem of Colors"" or some such. I've since asked that same friend for the song title & forgotten it. Too embarrassed to ask again. Anyone know wtf I'm talking about? Also, if you haven't listened to this song and discover it through this post... you're welcome.

Tried posting this in r/NameThatSong but didn't have enough specific info for that forum.",music
"https://open.spotify.com/track/4qj1tji798varSboYl713s?si=5P8_cS__QMOPOgLbHbJXiA&utm_source=copy-link


Hey! First of all, good morning, good evening or good night. This is my first post in this reddit so I'm kinda a newbie and I'm sorry if I did something wrong writing this here or like this, but I really wanted to space out and talk about what this song means to me.

Wanted to say too that I may be overthinking the song and it isn't that meaningful, it's literally 2 lyrics lol.


This song is currently my most listened and my favorite of all my life. It's so simple yet so attractive. It has only 2 phrases of lyrics which, for me at least, have so much meaning behind them. It feels like a heartbreaking love story told in just 17 fucking words and it destroys my soul imagining what is hidden.

""We are so sad""
""I'm sorry, you're not the want that I want, no, no...""

The first lyric talks about the relationship in which both parts are depressed, probably because they both have problems but also because they hurt each other. It's a toxic relationship I guess.( I will be referring to the one that is talking as Dave, and the one that is listening as Carl). Dave have had enough and in an attempt to find his way out of the rabbit hole where they are trapped he decides to end it all and break up with Carl.

""I'm sorry, you're not the one that I want"" 

Feels like Dave just doesn't love him anymore, or that he knows that they are harmful and that they won't end well if they keep the same dynamic and being depressed. Carl's not the want that he wants, because he wants somebody that'll bring him the happiness or the peace of mind that he craves, which Carl won't be able to give him or even himself. You can feel the overwhelming feeling on Dave's voice as he tells Carl that this is the end, that he's sorry that it had to end that way and that he wished to be able to feel happiness with him, but they just make each other even more depressed.


The instrumentals now change to a more hard and sad beat. I myself imagine the rest of the song as Carl as he walks by an infinite road while he continuously accelerates while remembering all the great moments that they had. As he moves faster and starts running he starts feeling more sadness and anger at him. He feels he's the problem, the one and only monstrous problem that has lead to this outcome. 

Crying while he runs and runs, the road seems to be finally ending. The song ends with Carl jumping to the void at the end of the road, freeing Dave (well, he thinks so at least) and ending it all.


Again, this is my sole opinion and I've mixed it with the shitty story that plays in my head while I listen to it. Oh btw, the rest of the album is so fucking great too, and they aren't much known but I assure you it is worth to give them a try, at least with this album.",music
"You can definitely talk about other things. Like production, or singing. But are good melodies/chords and good lyrics the most important? What records have good melodies/chords and lyrics but don't succeed, if there are any?",music
"Just thinking of ideas for a playlist that's basically a ""GO TO HELL!"" to an individual, the world or the powers that be. Here's what I have off the top of my head: 

Carly Simon - You're So Vain

Alanis Morissette - You Oughta Know

Eminem - The Way I Am

Rage Against The Machine - Killing in the Name

Queen - Death on Two Legs

Public Enemy - Fight the Power

Black Sabbath - War Pigs

Gnarls Barkley - Crazy \[to an extent\]",music
"**Prompt Provided**:  You are an artist, singer, rapper, storyteller, and music composer. Use your knowledge of the related topics to formulate an interesting, engaging, and enthralling song about dreams. Make sure to create the word lyrics, the music instruments, and their music sheets.

**ChatGPT Response**:  

Verse 1: 

When we close our eyes, we're transported to a place 

Where anything is possible, a world without a trace 

Of all our fears and worries, a place where we can fly 

And every single dream we have, it feels like it's alive

&#x200B;

Chorus: 

Dreams, they take us away 

To a place where we can play 

Where everything is bright and new 

And anything we want to do, we can pursue

&#x200B;

Verse 2: 

In our dreams we can be anyone we want to be 

We can climb the highest mountains, or sail across the sea 

We can have the love we want, the fame and fortune too 

In our dreams we have the power, to make our wishes come true

&#x200B;

Chorus: 

Dreams, they take us away 

To a place where we can play 

Where everything is bright and new 

And anything we want to do, we can pursue

&#x200B;

Bridge: 

But when we wake up, we realize 

That it was all just in our mind 

And though it hurts, we have to say 

Goodbye to that dream and start a brand new day

&#x200B;

Chorus: 

Dreams, they take us away 

To a place where we can play 

Where everything is bright and new 

And anything we want to do, we can pursue

&#x200B;

Outro: 

But don't be afraid to dream again 

Because dreams are where our hearts begin 

And though they may not come true today 

In our dreams, we can find a way

&#x200B;

Music Instruments: 

The song ""Dreams"" would be an upbeat, pop-style track, featuring a blend of electronic and acoustic instruments. The song would start with a gentle piano melody, and gradually build with the addition of a driving drumbeat, electric guitar, and a pulsing synthesizer bassline. The chorus would feature a catchy, sing-along melody, with a soaring string section adding to the emotional impact of the song. The bridge would feature a more stripped-down arrangement, with just the piano and a soft acoustic guitar. Finally, the song would end with a triumphant outro, featuring a full choir singing the chorus one last time, accompanied by a soaring brass section and a soaring lead guitar solo.

&#x200B;

Music Sheet:

\[Verse 1\] 

Am C G D 

When we close our eyes, we're transported to a place 

Am C G D 

Where anything is possible, a world without a trace 

Am C G D 

Of all our fears and worries, a place where we can fly 

Am C G D 

And every single dream we have, it feels like it's alive

&#x200B;

\[Chorus\] 

G D Am C 

Dreams, they take us away 

G D Am C 

To a place where we can play 

G D Am C 

Where everything is bright and new 

G D Am C 

And anything we want to do, we can pursue

&#x200B;

\[Verse 2\] 

Am C G D 

In our dreams we can be anyone we want to be 

Am C G D 

We can climb the highest mountains, or sail across the sea 

Am C G D 

We can have the love we want, the fame and fortune too 

Am C G D 

In our dreams we have the power, to make our wishes come true

&#x200B;

\[Chorus\] 

G D Am C 

Dreams, they take us away 

G D Am C 

To a place where we can play 

G D Am C 

Where everything is bright and new 

G D Am C 

And anything we want to do, we can pursue

&#x200B;

\[Bridge\] 

F C G Am 

But when we wake up, we realize 

F C G Am 

That it was all just in our mind 

F C G Am 

And though it hurts, we have to say 

F C G Am 

Goodbye to that dream and start a brand new day

&#x200B;

\[Chorus\] 

G D Am C 

Dreams, they take us away

 G D Am C 

To a place where we can play 

G D Am C 

Where everything is bright and new 

G D Am C 

And anything we want to do, we can pursue

&#x200B;

\[Outro\] 

G D Am C 

But don't be afraid to dream again 

G D Am C 

Because dreams are where our hearts begin 

G D Am C 

And though they may not come true today 

G D Am C 

In our dreams, we can find a way

&#x200B;

&#x200B;

Message me if you need anything else to compose a song or if this is just perfect the do it. Also, if you have done it, send me the song itself. I would like to hear what this thing could compose.",music
"Name a band, or even song, that everytime you play them, all you get is a swift “ugh, why are you playing them/this?” 

I would say Avenged Sevenfold, Breaking Benjamin, and Three Days Grace are three big ones here",music
"Hi everyone,
I have project proposals to Artists and Recording Labels of which I believe they will find them very interesting and advantageous to cooperate with.

As a company from another country, my challenge is finding the right potential Record Labels or Artists to pitch to.

Many Labels or Artists do receive Marketing or Distribution proposals, that doesn't shake me because this is definitely the different one.

We are looking to build healthy working relations, and my only fear is not finding the right candidates to share this big deal with.

Any recommendations or advices?",music
" I am looking for unique/weird/funny instruments to play as a hobby, by now I have a kazoo, kalimba, Jaw harp, and otamatone.  
Can anyone give me more ideas???

The type of instruments that are fun and don't need THAT MUCH learning and skills, even though if someone here has an idea that is a little more challenging I can accept that too.   


(Thanks for whoever answers)",music
"Hello everyone!

To all the fellow music producers, I wanted to reach out with this opportunity to grow your listener base. There is no bullshit playlist or promotion scams or anything like that, i promise you that I am not trying to sell you anything, just a free platform that can make your listener base grow.

I recently started working at a start-up called HotDrop that connects underground artists with fans looking to find new music. The concept is like Tik-Tok by previewing short song snippets but audio only and only for the musician to post.

We are open for beta release right now and looking for new artists to try out the platform - its totally free. If you are interested and would like to give it a shot, just sign up [here](https://studio.hotdropapp.com/login?utm_source=SudarshanJajoo) and you can immediately start uploading music for free. You could check out our iOS app too to see what it is like from the perspective of the listener.

Let me know what you think if you choose to sign up, and I am open to taking any questions or feedback!",music
"Will be doing a lot of travelling by train in the coming weeks so thought it would be a good idea to get some suggestions on albums of the past year I should have a listen to. 

Hopefully this will help pass the time staring out the window

Edit: Thanks for the responses! Lots to go through. Feel I should add my own 5 as well

Sudan Archives - Natural Brown Prom Queen.
Denzel Curry - Melt My Eyez See Your Future.
Nilüfer Yanya - Painless.
Craig Finn - A Legacy I’d Rentals.
SIPHO - She Might Bleed (EP).",music
"My dad used to work at a club in the 90s and he met Radiohead, idk if it was before Creep or after but he says the booth was too loud and told them to lower the sound because ppl were leaving, and they didn't care so my dad told the bartender to not give them any drinks and rightfully so 10 min later they were pissed and had to turn the volume down so yeah they were pretty cocky",music
"Hi,

Not sure if the right sub but I'm looking for music featuring ""middle-eastern violin-like strings"".

Usually featured in some songs from Dzihan and Kamien as well as Gnossienne No. 1 (Buddha-bar remix). These drawn out seamless strings seemingly following a certain scale backed by some bassline.

I really like this stuff but idk how to find more.

Thanks.",music
"I recently rediscovering his music but couldn't make up my own mind what genre to fit him in. There is Blues, Jazz and Fusion mixed in his albums or is it just rock?",music
" 

I'm  sitting here watching the recent Dinosaur Jr live they did in Hawaii,  who ever is recording is in the very front row phone out the whole time  getting the whole performance

I  can see how this would be annoying to the band and people around them,  but do you think its a necessary gap that is being filled by the  audience? Obviously I support the band in the ways that I can, I buy  merch, I buy their music etc, and I'd never make it to 99% of their  shows because I live in the middle of nowhere and a lot of bands don't  come out much and if they do its rare and still quite the distance from  me, Also with Covid-19, some people may still feel uncomfortable going  to large gatherings like that

Do  you think its alright? If you're a live musician, do you care?  Would  you prefer shows going to a no phone policy and taking a camera crew  with them so that they can sell the performance online as another income  source and to keep the crowd from being disruptive? I think its an  interesting topic, I'd like to hear everyones thoughts on the matter!",music
[https://www.youtube.com/watch?v=K38xNqZvBJI&ab\_channel=BowlingForSoupVEVO](https://www.youtube.com/watch?v=K38xNqZvBJI&ab_channel=BowlingForSoupVEVO),music
"steel grey night vibes : https://open.spotify.com/playlist/7m27PCvQN3LqQtBwkPYhJm?si=2xH1X_i1SQSGDSnhd-2gFg

saul goodman vibe: https://open.spotify.com/playlist/3TYJlVkGnOFcYMLkFCgayt?si=5H3aeMLxTDKB-97fHuROcA",music
"For those of you who don't know him, he's a very calm dude but, from what I observed, he's popular with the like girls who act 'weird/quirky.'
I personally don't like them, just my opinion though 
But DeMarco is just so hard to not like, it's tough for me to say this, but I lost boys 😢😖
May God have mercy on us all 🫡",music
What genre of music is this song and any song like it? [Example song](https://open.spotify.com/track/6RKeESxpM58GCfR5eRdfzY?si=FVhQbihkRs696NlPtL6xVQ),music
"This is an EP of meditation music I made last year influenced by American folk/roots music. The first track has a spoken word story I created to lay the foundation of the rest of the instrumental tracks. Great for headphones when you are chilling out right before sleep or studying. Enjoy!

https://youtube.com/playlist?list=OLAK5uy_knnooiCrqmGJPHqkJ8FPlIll3ksZex37o",music
"trying to remember a song about seeing your mother from a distance but she doesnt know who you are. I would maybe put this song/band alongside pearl jam, guns n roses, and others like that. its a sad song.",music
i forgot what the song sounded like but the cover was a guy with curly blond hair and he was i front of a street sign and the music sounded somewhat like lil-shine if that helps,music
"Scorpions - Rock Believer [Hard Rock/Metal] 2022, great album. Anyone else agree? I got the sting live twice last year, really hoping for another sting in 2023. If anyone has listened to the album do you enjoy it and if so what is your favourite song?",music
This was a cool series of songs where this music teacher from Maldova had people send in their parts from all over the world. [https://www.youtube.com/watch?v=Ewope50pczk](https://www.youtube.com/watch?v=Ewope50pczk),music
"This was such a good remix, the instrumental build halfway through though is so amazing.    


[https://youtube.com/watch?v=J6hcgQOx5O4&feature=shares](https://youtube.com/watch?v=J6hcgQOx5O4&feature=shares)",music
I’m receiving my B.S. in Industrial Engineering in May- but my heart has always been in music. What skills should I hone in the first few years of my career that might help kickstart a career in music production in the future?,music
"I know a lot of people are probably from the US, but even then it'll be cool to see what is brought up. Mine will be ""O bêbado e a equilibrista - Elis Regina""",music
"Hello, I’ve recently come across jealous by eyedress and it has just made me really sad. I just can’t stop thinking about the good times and about my childhood. I just miss the freedom and carelessness I had back then. I just can’t see anything good in the future and everything just seems like it’s gonna get worse. Anyone else feel like this?",music
"I'm someone with no musical practice/knowledge whatsoever, I barely learned about it at school. And I just started trying to play the ukulele cause everyone says it's easy and it's good to start, but my dream has always been to play the piano. Although, I don't have a piano and neither do I have the money to play piano lessons, so I was thinking that maybe after learning how to play the ukulele and when I get good at it, I could buy myself a keyboard (or synthesizer, I don't really know how it's called in English) and learn at home with some youtube videos. That way I could learn a little bit more about music and even if a keyboard it's something way different than a ukulele, maybe it would help me to get used to playing music and all that stuff, and to know if I really could be good at playing music or if I'm just a lost case, cause I've never even been good at playing the flute lol.

I know it's probably not the same as a piano, but to me it would be enough :)

However, what's making me doubt it it's the fact that everyone says that playing the piano or even a keyboard it's really hard. So idk if someone like me, or anyone really, could learn how to play it just with youtube videos.

What would you recommend me to start learning that instrument? Or should I just wait until someday I have the money to get myself a piano and a piano teacher?

Any advice will be appreciated:), I hope you are having a great weekend and send y'all my blessings",music
"For me it's: Despite all my rage, I'm still just a rat in a cage. From Bullet With Butterfly Wings, Smashing Pumpkins.

I'm 38 now, and cheesy as it may be, that line still sums up my feelings towards the world.",music
"**Study link:** [https://unlv.co1.qualtrics.com/jfe/form/SV\_8B208FTcrCIM7HL](https://unlv.co1.qualtrics.com/jfe/form/SV_8B208FTcrCIM7HL)  
**Advertisement for Community Members**

Title: Online Studies of Music Perception and Emotion

The purpose of this research study is to examine the extent to which perceptual music skills and self-ratings of emotional reactions to sounds contribute to how we listen and enjoy music. The study is open to those that are 18 years of age or older. We invite all participants that meet these criteria; however, participants must have a firm grasp of the English language as they will be answering questions presented to them in English. Additional exclusionary criteria include: (1) cognitive or brain deficits due to past traumatic brain injury; (2) neurological disorder or disease (e.g., schizophrenia); (3) inability to provide informed consent; (4) uncorrected visual impairment; and (5) history of hearing impairment.

In this research study, you will be asked to do the following:

1. Answer a series of background question, including some medical questions
2. Complete tasks assessing musical skills and general auditory processing
3. Answer a series of questions about your emotional reactions to sounds
4. Listen and rate music

The entire testing session will last up to 1.5 hours. If you have any questions regarding your participation in this study, please contact the Principal Investigator, Dr. Joel Snyder at [joel.snyder@unlv.edu](mailto:joel.snyder@unlv.edu).

Thank you!",music
"Verse 1:
Am F
My mind is racing,
C G
Thoughts won't slow down,
Am F
Every little thing,
C G
Becomes a big cloud.

Pre-Chorus:
F G
It's hard to breathe,
Am G
When I'm in my head,
F G
I need to break free,
Am G
And get out of bed.

Chorus:
C G
Overthinking, it's all I do,
Am F
Trying to make sense of every clue,
C G
But it's a cycle, never-ending,
Am F
My thoughts just keep on bending.

Verse 2:
Am F
I think about the past,
C G
And worry 'bout the future,
Am F
I can't enjoy the now,
C G
It's like a bad sutra.

Pre-Chorus:
F G
It's hard to see,
Am G
When I'm in my mind,
F G
I need to break free,
Am G
And leave it all behind.

Chorus:
C G
Overthinking, it's all I do,
Am F
Trying to make sense of every clue,
C G
But it's a cycle, never-ending,
Am F
My thoughts just keep on bending.

Bridge:
Am F
I need to breathe,
C G
And let go of the stress,
Am F
I need to believe,
C G
That I can handle the mess.

Chorus:
C G
Overthinking, it's all I do,
Am F
Trying to make sense of every clue,
C G
But it's a cycle, never-ending,
Am F
My thoughts just keep on bending.

Outro:
Am F
My mind is slowing down,
C G
I'm starting to see,
Am F
That the present is now,
C G
And that's all I need.

BPM: 100

Like a YouTube video or smth of you playing it",music
"I love the song by King Crimson called 21st Century Schizoid Man, but I don't really like the rest of the song between the 2nd and 3rd verse. I was wondering if they made a version of the song that is shorter and put it on spotify anywhere.",music
"I'm at my wit's end, googling everything I can to figure this out. The wiki for the single states that Walter Afanasieff added it, but then it states that he's not a guitarist, so he obviously hired someone to record it. I'm a guitar player who likes to know who's recording with who. Any help with sources would be super appreciated!",music
This song should be mandatory to be on everyones unbiased top 10 all time songs list. This song is like if Van Gogh greatest painting turned into a song. 5 minutes of pure art. https://youtu.be/BuNBLjJzRoo,music
"I make music for fun from time to time. I don't make money from it and I'm not planning to. I just do it for myself, my family and friends.

Recently I thought about uploading some of it on YouTube and Instagram maybe, but I don't want anyone to just take it and claim it to be there own. How do I put some kind of copyright on it or something to protect it?
Btw I'm living in a 3rd world country where we don't have copyright offices or polices.",music
I can’t get FL Studio but there’s this music I have and it’s an FLP file and I was wondering if anyone with FL Studio could use the file on studio and separate the different tracks and export them separately as non FL files because I need the tracks separately and I can’t use the file.,music
"I'm sure this thread has been made before, but anyway. I'm not sure if I'm missing any settings. I've set the max quality for all I could find on Spotify, and yet, YouTube music sounds better. At least to me. Not that Spotify sounds bad, but for some reason, the same songs on YouTube music have more punch in the bass, and background instrumental all sounds sharper, without ruining the vocals. Simply put, it just sounds better. Sorry for the lack of proper terms. Is there a fix to this? I personally would like to stay with Spotify.",music
"Recently I found a good hardstyle song whose album cover intrigued me: [Link to YT](https://www.youtube.com/watch?v=wE6G6dOuCt4)

As I was looking online for the source of this image, I found out that the woman in the image is Russian singer Dead Blonde, presumably along with GSPD, another Russian artist.

Using reverse image searches, I found a post on VKontakte by GSPD talking about one of his albums, with this image posted along with it. However, the album he talks about is completely different than the song above, and actually was posted after the release of this song.

I tried looking into the group behind this song, and I couldn't find any official info or website. Even this video is posted by a random user, saying it was provided to YouTube by Саппорт-Музыка , which is the label that signed ""Tomorrow"", who themselves have no info on this group. 

**TLDR:** Does anyone have any info about the music group ""Tomorrow""?",music
"Hi Reddit Family!!! I made this song a month or two ago and I just recently had a music video made for it. I would really appreciate some feedback on this song/video. Please let me know what you think. oh yeah AMA!!

[https://www.youtube.com/watch?v=L6rKY3dbLB0](https://www.youtube.com/watch?v=L6rKY3dbLB0)",music
I don't really know how to precisely describe it so I'll just use beautiful. Stuff like Us and Them by Pink Floyd or Epitaph by King Crimson or Wish You Were Here by Pink Floyd or SOUCD by Pink Floyd or you're starting to see a pattern here.,music
"I was on Yahoo chat .. Thats going beck.. You remember them days.. When we could chat to anyone from around the world.. How the Internet was Supposed to be..

I met a guy from Canada. I'm in the UK. He told me about Blue Merle..

Personally I thought they were brilliant.. But they never made it out of Canada.. And were slated for sounding too much like Coldplay..

Dont care.. I thought they were great!

Heres a taster..

[https://www.youtube.com/watch?v=T3yzZ57G3VA](https://www.youtube.com/watch?v=T3yzZ57G3VA)",music
"When I use AI to make a work of music, am I the composer? or is it the AI? or am I and the AI co-composers? And when I play the music that I made with AI, am I the performer? or its it the AI? or are we co-performers?",music
"Hello!! I am a high school AP Research student looking for participants to take my survey. The purpose of this survey is to collect various amounts of responses from two different targeted group and form a comparison using those results collected.

I am looking for people ages 14-18 who play an instrument or are in orchestra. I am also looking for people who do not play an instrument and are not in orchestra so that I can form a comparison and see if playing an instrument affects academic self-efficacy and its effect on academic performance to those who don't play an instrument.

I would appreciate it if you could fill this survey out. The survey is completely anonymous and will only take up a few minutes of your time. Thank you!!

Survey Link: [https://docs.google.com/forms/d/e/1FAIpQLSeCljEwyjc5U4ecmh\_-4VANyCxh-8gdcOedqVTzIfCc8RMvkw/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSeCljEwyjc5U4ecmh_-4VANyCxh-8gdcOedqVTzIfCc8RMvkw/viewform?usp=sf_link)",music
This is a lifetimes worth of cds I inherited from my grandfather. Now I have a shit ton of cd’s with little room to store them. Which ones do you guys think are worth keeping or selling. How should I attack this collection?,music
So i am going to buy my first vinyl revords and im going do buy plastic protectors and they sell some with seam and some without seam and those with seam is the exspensive and the ones without are the cheap one so im woundering what is the diffrence betwene those with seam and those without,music
"A bit more of a blog type-post today. I'm gonna list 5 songs I like from 5 bands I don't like. Whether those songs are actually good and the bands actually bad is completely subjective of course, so if you find a band you like on here, please know that you can like whatever you like. Starting the list is:





5.  'Letting the cables sleep' by Bush
https://www.youtube.com/watch?v=d8TrkCObypE


Starting off with Bush, I am not actually that big of a hater of Bush as many other people are, I just find them very meh. But considering they were part of the awful Post-grunge subgenre (that gave us acts such as Staind, Trapt etc.), they at least had some good songs here and there. Such as 'Machinehead' or the kinda recently released song 'Flowers on a grave'. But Letting the cables sleep is actually amazing.

Its a lot calmer than most Bush songs on 'The Science of Things' which allows Gavin Rossdale's singing to fade into the background a lot less. The background has these almost Radiohead-esque robotic sounds which works perfectly under the watery guitar and piano ballad production. The violin that creeps in sometimes is also beautiful and adds to the song even more. Gavin's voice is a lot more melodic than usual which only makes the song more melancholy. I never expected something so good to come out of Bush, no pun intended.



4.  'Plush' by Stone Temple Pilots
https://www.youtube.com/watch?v=V5UOC0C0x8Q&pp=ygUZcGx1c2ggc3RvbmUgdGVtcGxlIHBpbG90cw%3D%3D


Stone Temple Pilots were one of the bands that had a singer who had the Eddie Vedder vocals and ran with it. Though the agression and passion of Pearl Jam, along with the great guitar riffs that they provided, is completely lost from most of STP's music. Its a lot softer too, which makes it even less interesting. And maybe Plush is a bit of a cheat code considering its their most popular song, but sometimes those songs are actually the best.

This song shows that grunge can be made somewhat attractive even when you have a singer who sounds like he has no teeth. The guitars make for a really chill yet deep sound, over which Scott Weiland's vocals fit perfectly. All the transitions are very seemless, from the first part into the chorus and the second part. Everything in the song works exactly as it should and that is honestly the best part.


3. 'Jumpsuit' by Twenty one Pilots
https://www.youtube.com/watch?v=UOUBW8bkjQ4&pp=ygUaanVtcHN1aXQgdHdlbnR5IG9uZSBwaWxvdHM%3D


Sticking with Pilots, here's 21 of 'em. Twenty one Pilots was a band with massive radioplay, especially from the album 'Blurryface', which made most songs they made very overplayed after just a few weeks. After they took a hiatus for about 2 years I thought they just stopped trying after that album, but soon they were back again with 'Trench', which is a critically acclaimed album yet I don't love many of the songs on it. They're not bad, but very boring sometimes. Jumpsuit however, is a certified neigbourhood classic.

A full on hard rock song is something I didn't expect at all, most of their songs are a blend of pop, hiphop, alt rock and reggae sometimes. But the build up is already great. A heavy guitar riff, which later turns out to be the chorus, starts of the song before Tyler starts singing. Then the chorus plays again, before a melodramatic silence with feedback from Tyler sometimes. But the part starting with the last chorus, holy damn, am I listening to 21 Pilots or Bring me the Horizon? The last chorus plays with screaming feedback from Tyler and after the chorus the screaming continues until the song ends a few moments later. Experimentation works kids.




2. 'Everything is average nowadays' by Kaiser Chiefs
https://www.youtube.com/watch?v=JTkSV7sXz8Y&pp=ygUsZXZlcnl0aGluZyBpcyBhdmVyYWdlIG5vd2FkYXlzIGthaXNlciBjaGllZnM%3D


Kaiser Chiefs are literally a band named after a football/soccer club that one of their favorite players played for, so that kind of gives you an idea of how seriously the band took themselves. They are a succesful band in Great Britain, but are mostly known for their song 'Ruby' In the US, which is a good song but again, very overplayed and slightly annoying. And among the solid but unspectacular Indierock on 'Yours Truly, Angry mob', you can find 'Everything is average nowadays', which is far from average in my opinion.

It's energetic, isn't overly serious but still musically solid and actually sounds unique among the other indierock on this album or in GB in general. The electronic elements on the pre-chorus compliment Ricky's slightly higher singing voice very well. And whilst some people might find it annoying or just plain bad, I think it works very well.



1. Seether feat. Amy Lee 'Broken'
https://www.youtube.com/watch?v=hPC2Fp7IT7o&pp=ygUOYnJva2VuIHNlZXRoZXI%3D




Remember what I said about how I dislike the voice of the STP singer and the Eddie Vedder sound in general? Well, take that voice, make it even more annoying, make the guitar riffs more bland, make the people in the band more pandering to the emo crowd and the songwriting edgier. And that's how you make Seether. But apparently, they only needed the help of Amy Lee (Evanescence) to make one of the best songs of that era.

The dark yet melodic production works well with Shaun's vocals and of course it's Amy Lee's bread and butter. So the vocal performances are great. But does the production compare to the vocals? No, its even better! Whilst the beginning is simplistic, the deeper you get into the song the more instruments get brought into the mix. Synths, Violins, Acoustic and electric guitars, you name it. They all make for one the most ethereal songs to cry to of all time.



Thank you very much for reading all the way to the end and I hope you can appreciate my choices here. Is there a song you like by a band you dislike, or do you disagree with one of my choices? Please leave a comment and i'll be sure to respond. Have a nice day, Peace.",music
Listen on Mixcloud [here](https://www.mixcloud.com/KeepLaughingForeverRadio/80s-90s-songs-tv-themes-retro-jingles-and-movie-quotes-klf-radio-show-11/),music
[https://open.spotify.com/track/4vxgwwbTxShyq0DtEPqeS5?si=2127e2b168614ccf](https://open.spotify.com/track/4vxgwwbTxShyq0DtEPqeS5?si=2127e2b168614ccf),music
"I'm just wondering what song people would agree on has the lyrics that aged the worst.  
It can be for any reason, what so ever, just that the lyrics has aged poorly",music
"[Raindrops](https://soundcloud.com/fladan/raindrops-1) I made it using Garageband for IOS, a decent pair of headphones and a few online tutorials. Any feedback would be greatly appreaciated.",music
There’s this music that I want to learn to play but I don’t have perfect pitch so I don’t know what notes are played. Can any of you guys do that or have tips on how to transcribe?,music
"Hello guys)
My name is Suffocatus and here it is my first solo album. I’m writing music in those styles like Symphonic Metal, Gothic Metal, Instrumental Metal, Heavy Metal and Experimental Metal. I believe you will like. If you listened for it, please write your opinion about that in comments under this post. Here it is a link for a Spotify album)

[Suffocatus — Songs for the lonely heart (by a lonely man)](https://open.spotify.com/album/6hyMep1lPoRyuEx3Bx4iFF?si=MyOO0WSCT9ylw5XqZoJRYQ)",music
"Hii! I’m trying to find more songs like:

-Major tom by peter schilling

-desire by future world orchestra

-poison arrow by abc




Especially with Major Tom, am i looking for songs that give off the same floaty feeling where u really feel engulfed and light on ur feet.

thankss!",music
"I can’t find any good music anymore everything is the same i like rap but the gun talk/murder talk is getting boring I like rappers like sha gz ,wacotron, King von, chief keef, 21 savage can someone help me out with good music",music
"For me there's only one.. I was 17. Listening to Chicago House, Hip Hop, Acid, Iron Maiden, Queen, U2, Whitesnake, Depeche Mode, Simple Minds, Floyd.. An eclectic mix and then one day it happened..

The year was 1989.. 

The Stone Roses - released their debut..

 The Clothes, The Culture. The Attitude.. 

Safe to say there were some mad times and some amazing albums to come out after that as the years have gone by.. 

But that one album.. For me - Changed everything.",music
"Simply wanna know your favorite music album and why.

Imma start of and say Illmatic simply because it is a true hip-hop classic every rap fan should have heard atleast once.",music
"ai had generated a march for me and this is what it contains:  


Tempo: March (120 bpm)

Key: A Freygish

Time Signature: 4/4

Melody:

A | A B C D E | E D C B A | A B C D E | F E D C B |

A | A B C D E | E D C B A | G F E D C | B C D E F |

Chords:

A | Am | A | Am | A | Am | G | F |

A | Am | A | Am | G | F | E | E |  


i would appreciate if someone could give me a midi file of this",music
"Amazon unlimited is awesome for an audiophile like myself. With so many Ultra HD and Dolby Atmos songs at my fingertips I can crank up my Echo Studio with Sub and focus on what needs to get done. 

Music is not just the background sound in our lives, but fuels and propels us daily, and at times invests in us with almost superhuman abilities. That last mile of a marathon when your body won't go another step, but then your power song comes through your earbuds and wham! Or the painter whose work has always been good but never great, then they stumble across an old 45 going through their mom's records one day, put it on, and Nina Simone moves them to look at the world in a whole new way, and their next showing takes the art world by storm! 

Whether it's driving to work or driving the corner ball into the pocket, what's coming out of the speakers matters, and having access to not just the music you know, but to every track ever put into digital format is I am sure why most of us who pay for Amazon's streaming Unlimited Music service continue to do so. 

But over the past year music is quietly disappearing from Amazon's catalog. Songs in our playlists that we value no longer allow access, new discoveries stumbled upon in an interesting article in Pitchfork Magazine mysteriously can't be found while searching Amazon Music, and on and on...

Where do these works of artistry go? 

We are told by Amazon's ""Help"" section on the topic that music which appears ""greyed out"" as they put it, is no longer available. They list 6 different reasons, but those reasons are vague and don't address specifics. The ""Help"" section also doesn't address the fact that recently more songs than ever are Poof! disappearing, and doing so with a cloak of silence from the company recently valued at 1 trillion dollars!

So is it greed on the part of Amazon that is denying us the music that we want to hear, or is it stubbornness on the part of the artists? The frustrating thing is Amazon won't answer questions, and the music industry has its lips sealed regarding this issue. Yet we are still paying month after month, driving the mega corporation to even greater heights, but getting ever diminishing returns, because we argue that 90% of everything released on to digital format is still a good deal... But is it?

Music should not be blocked on a service that CAN afford to give the artist the little extra bit of royalties they are looking for in order to keep the music available to millions who are paying them to do just that, ensure that at the touch of a finger they can listen to what moves them.

So I ask those ""in the know"" so to speak, to explain to me how they are getting away with denying those of us who are paying quite a bit every month to have access to unlimited music, access to songs which we know should be available, but have now mysteriously vanished. 

It's not just that I want to know, I really need to know. Music is not just what drives me, but it's also my livelihood, and songs that are absent without leave, have an impact on what I do and what I create myself.

I hope I get some of you experts out there to chime in on the topic, and maybe we can figure out what's going on, and find a way to bring back our music!",music
"Hi. I don't know much folk or country but got hooked on the O Brother, Where Art Thou? soundtrack. 

It's all southern folk and gospel music that was popularized in the early 20th Century (including songs people knew that were handed down over time). 

Do you know any good collections or albums of songs like these (from the soundtrack)? 

I'll Fly Away: [https://www.youtube.com/watch?v=lFamN-oXRMQ](https://www.youtube.com/watch?v=lFamN-oXRMQ)

You are my Sunshine: [https://www.youtube.com/watch?v=At4Kccy9Zog](https://www.youtube.com/watch?v=At4Kccy9Zog)

Go to Sleep Little Baby - a sort of dark acapella lullaby, so interesting!

[https://www.youtube.com/watch?v=94h2L9oBOHM](https://www.youtube.com/watch?v=94h2L9oBOHM)

I'm searching and can find some, but perhaps there are great collections I'm not finding. TIA.",music
"I have recently stumbled upon unlimited Spotify accounts, it's basically accounts that have premium forever and you aren't charged monthly, I'm not too sure what I can talk about on this subreddit related to this (mods if I can't just let me know and I'll remove it ) but if you're interested please drop me a message and I can let you know",music
"I just recently discovered the [Professor of Rock](https://www.youtube.com/@ProfessorofRock) YouTube channel. While I find the stories to be pretty interesting, I don't like the delivery. Note only that, I'm looking for something I can listen to while commuting. Any suggestions?",music
"Heyyooo! So, I will be traveling this summer and I really want to make a playlist of songs that mention or are titled the places I'll be going and wanted some input! Going to the following places: London, Paris, Cologne, Brussels, and Amsterdam. I am a lover of all sorts of music and really excited to see what we can put together.",music
"Any albums that anyone knows of that absolutely worship women in that sexy type of way? I’m just looking for a new vibe. Single songs are also welcome but if there are whole albums devoted to the topic, then great!! (Partial to 90s hip-hop, psychedelic rock, or rock of any kind but all genres accepted) thank you ;)",music
"The circumstances through which a record were created adds to the intrigue of the album itself. Would love to hear about some albums with some interesting backstories.

I'll start off with J Dilla's critically acclaimed 'Donuts'. 29 of the 31 tracks were recorded while Dilla was at the hospital undergoing complications due to thrombotic thrombocytopenic purpura and lupus. He died three days after the release of the album.",music
" 

## Classic Naruto ep  #110 - Formation! The Sasuke Retrieval Squad 

at the mark of  the video the amen break leads to song   
WHAT'S THE NAME OF IT   [https://youtu.be/bHKjfZLVVOs?t=381](https://youtu.be/bHKjfZLVVOs?t=381)",music
"Any musicians looking to network with other musicians join this discord!

[https://discord.gg/gbXF6P3F](https://discord.gg/gbXF6P3F)

We are trying to build a community for all musicians to build their network so they can work with other artists and have some better opportunities!",music
"
I’ve figured out that my vocal range is E2/F4, but ive also noticed an not developed whistle. So Im confused: baritone or ?

Thanks ahead",music
"Not sure if this is the right group to pose this question but...

The media box that I'm using to play FLAC files (Mede8er 600X3D) doesn't read VORBIS tags (only ID3's apparently). I was wondering if anyone knows if it's possible to add ID3 tags to FLAC music files. Or perhaps to replace the VORBIS tags with ID3's. I can convert them to WAV's if necessary but I'd like to keep them as close to lossless as possible.

TIA!",music
What’s your one favorite song that you can’t get tired of no matter the countless times you’ve heard it? Mine is “A Little While” by Yellow Days. Let me hear yours 😎,music
"Don’t flame me for asking this but what is so special about this song? I do enjoy it and I relate to it a decent bit but talking to anybody over the age of 30 about the song, they make it seem like it’s song from God himself. Like I said, I do enjoy a decent bit of Radiohead’s catalogue, but this just seems like a bit much.",music
"[Listen on Spotify](https://open.spotify.com/track/1zKlifaneubhq5eeveCwMf?si=4MKa0TqGRMi3nwNbbMvOMQ)

[Listen on Apple Music](https://music.apple.com/us/album/spend-it-on-you/1668560441?i=1668560660)

[Listen on YouTube](https://youtu.be/5X_DDcj_lKw)

[Listen Everywhere Else](https://linktr.ee/jrandtk?utm_source=linktree_profile_share&ltsid=724ab1dd-8ea0-4b63-8071-42f78354975b)",music
"No hate to Lana del Rey fans, everyone has different tastes, I'm talking about Lana specifically. I don't think she's a bad artist. She plays some genres that I like, has a good voice and a great production. And yet, there's something about her that makes me not like her, something that puts me off, but I can't pinpoint what it is... It is her? It is something in the lyrics? Is it the fandom? Something about her gives me a bad vibe, even if her music is not bad at all.

Why is this happening? And does anyone have the same problem with her or another artist?",music
 [An Overview | DIAGRAM 1 (THETA WAVES + SOMATIC DIALOG) (bandcamp.com)](https://diagram-1-theta-waves-and-somatic-dialog.bandcamp.com/album/an-overview),music
"can someone send me the full version? i tried shazam and soundhound no luck

[https://youtu.be/mH7\_Bqh\_MDs](https://youtu.be/mH7_Bqh_MDs)",music
"(My apologies if this isn’t the right sub to be asking) 

Does anyone know the beginning instrument used in the introduction of the song? I remember it being used in a lot of children’s programs as background music in the early 00’s and late 90’s. Here’s the [Spotify link,](https://open.spotify.com/track/6TrZdWmmsJTLjkka3tBZwB?si=RtqNqz8STL6Nu0XITHvRfw) I’d give a  video but none of the other live performances intros have the same “perfect” sound like this one does.",music
[https://open.spotify.com/album/3Hoqe4BQxmzIKxH38sFwPE?si=ZCMhFdZkTH6At7wgc8SMJA](https://open.spotify.com/album/3Hoqe4BQxmzIKxH38sFwPE?si=ZCMhFdZkTH6At7wgc8SMJA),music
[https://abcnews.go.com/GMA/Culture/brad-paisley-releases-song-ukrainian-president-volodymyr-zelenskyy/story?id=97444556](https://abcnews.go.com/GMA/Culture/brad-paisley-releases-song-ukrainian-president-volodymyr-zelenskyy/story?id=97444556),music
"I'm looking for some features for a Future/Lil Uzi hip-hop album. I'll send the beats to you if you are interested. No payments, just for fun! Thanks!",music
The trend of dropping deluxe albums has become increasingly popular but who started it? what’s the oldest deluxe album you know of?,music
"My (awful) work commute is 40 minutes to and from every day, mostly in traffic. I’ve been taking the time to listen to new albums. What are your favorite albums, the all killer no filler ones. Any genre is fine as long as it’s between 30-40 minutes. I’m trying to get a full range of new music to not get too bored while driving.


Edit: thank you so much for all of the recs! I finally understand what the phrase “rip inbox” means. I think it’s safe to say I’m set for listening material for the next few years haha",music
"I've recently started getting onto albums and mainly hip hop stuff, I want to branch out to other genres and listen to all the critically acclaimed albums eg Beatles, pink floyd, red hot chili peppers
Give me your recommendations, anything",music
"From time to time you stumble upon a song, either intentionally or by chance, that is written in a language you don’t understand a word of. Despite of that this song just resonates with you. Even though you don’t understand it, you feel it which can be equally powerful.

So what is your favorite song in a language you don’t understand a word of?",music
"Check out Twirlanta [Explicit] by 22Gz on Amazon Music
https://music.amazon.com/albums/B08YP7VQN7?trackAsin=B08YP4PSX4&ref=dm_sh_CJFOdDPCQeGBomggYkhgNmeNX",music
"Any musicians looking to network with other musicians join this discord!

[https://discord.gg/gbXF6P3F](https://discord.gg/gbXF6P3F)

We are trying to build a community for all musicians to build their network so they can work with other artists and have some better opportunities!",music
"Hello,

While using shazam you probably came across this bastard named Jincheng Zhang, he plagiarises copyright free songs making them his own, so everytime you try to shazam such songs he is the one that pops up.
My question is, is there a way to make shazam search for another alternatives rather than just showing the first match? Or is there an app  that is equivalent to shazam  that gives you list of matched songs insead of just a single one?

Have a nice day/night.",music
"So im doing a little Motorhead re appreciation at the moment. Motorhead have obviously been heralded as the loudest band on earth, but im interested to see how their level of volume compared to Sunn0)), whos volume ive experienced first hand and is clearly also loud as hell. So does anyone have any definitive evidence as to who actually was the loudest band, be it Motorhead, Sunn0)) or someone else? Its for science.",music
"I have just found an album called Another time, Another place by John Deacon. Anyone know any background about this album? John Deacon is Queens former bassist and i wonder if he mad this album because i cant find any links to him and this album!!",music
"I tried to learn about music and summarized my basic understanding here. Would love to get feedback from community and to know if there is anything I got wrong.  


[https://nikhilsoni.me/2023/02/25/getting-started-with-music-theory/](https://nikhilsoni.me/2023/02/25/getting-started-with-music-theory/)",music
"
Hi guys! I’ve just started recording videos of myself singing and playing guitar/piano, looking for a basic mic <£40/$50 that I can plug into my phone to record the audio alongside the video as my iPhone mic is pretty shocking. Any recommendations as to what to go for or tips as to what specifically I should look for?",music
"George David has released his EP / Mini Album ‘Three Months’ 

Head to Spotify and all the other music platforms to share in the folky love . 

All the latest news will be coming through George’s website and management label 122 Music Management. I’ll be also feeding news through the Reddit community so hope to see a few of you on here soon . 

Have a great week all. 

AMGE/122MM

PP George David",music
" Very recently I heard a piano piece at 5 minutes 57 seconds into this meme video ([https://www.youtube.com/watch?v=XtQlctzFymM&t=363s](https://www.youtube.com/watch?v=XtQlctzFymM&t=363s)) that Shazam said was ""Stakin Musik"" and no, no it really isn't. Stakin Musik is a horrible song, so nobody ever seems to have bothered looking for the piano piece inside of it, causing my search to lengthen up to the point where I have given up and instead asked here. Please please help me, the piano piece itself sounds perfect to me and I absolutely cannot find it anywhere, it is driving me more than insane. I am not sure if this post will work out here but I am just really desperate for the answer...",music
"I want to be able to navigate to my music better. Maybe I just have an awful memory, but it’s incredibly hard for me to remember the names of songs I listen to. I usually go by the picture, or the visual look of the name of the song, if that makes sense. Cause a lot of the time I don’t know the names of songs off the top of my head.",music
"What are some other songs with a string quartet (and only strings) ala Eleanor Rigby. I've only been able to come up with:

1000 Umbrellas - XTC  
Golden Brown - Stranglers (OK, harpsichord)  
The Juliet Letters - Elvis Costello",music
Hey! I'm bored of my playlists. Tell me your favourite song and I'll add them all to a playlist on Spotify. Any genre at all. I'll listen to anything 😊,music
"And what makes bands like slipknot, avenged sevenfold, halestorm, rammstein, 5fdp. Municipal waste are also really big for a thrash metal band. Like yes I know these bands got big a while ago but they are still big, I just want to know how? And why?",music
"ok so i joined literally a second ago cuz i have no luck so far remembering this band name n it’s killing me. i lost all my apple music a few weeks ago so i’ve hadda deal with the painful task of putting it all back n ofc i had a buncha music from ppl i don’t remember. the only things i rlly am positive abt was that the band name began with a D, n this particular album had a purple/blueish background with a cartoonishly drawn campfire. the genre i believe was also more alternative so if anyone knows wut band i mean then plz tell 😭",music
"Is anyone’s favorite song “If we’re alone aren’t we in this together” by October Fall or has anyone even known this song???

I feel like I’m alone in this but this song just isn’t like any other..",music
"It’s been a long time i’ve been streaming afro beats day and night, in the shower, with girls, alone while walking around Paris… and it’s by far the most versatile music type that you can listen limitless. 

What your thoughts ?",music
"Any musicians looking to network with other musicians join my discord! 

[https://discord.gg/gbXF6P3F](https://discord.gg/gbXF6P3F)

We are trying to build a community for all musicians to build their network so they have a better chance at making it!",music
I was listening to Back in the USSR by the Beatles and J'aime Les Filles by Jacques Dutronc which both list different types of girls and I was wondering if this a sub genre of music? Are there many more songs that feature lists of girls? Is it common in non English songs?,music
"I want Punk beat to worldwide
please feel the Punk soul
If you like this beat, please give me a big thumbs up! 
https://youtu.be/6n7aK4il7rc
https://youtu.be/pAbsUdNe9Yc",music
"https://www.youtube.com/watch?v=qnYtcH4YS44

A song that always draws a smile on my face. A beautiful combination of music and traditional dancing, by a musician that died far, far too early.",music
"Spotify just randomly showed me a Song that hit right in my memorys and now i want to know if im wrong or if they almost sound the same .

The Original i know is : American Boy by Estelle and Kanye from 2008.

The new Song i found is : SRT by Atwood and Kyaru from 2020.

Just let me know what you think, best is to hear American Boy first and then right after listen to SRT.",music
"A couple months ago the popular music rating and charting site Rate Your Music, or RYM, added the ability to add and rate music videos to their site. As a big fan of the visual side of both pop music and of music in general I've been super excited about this, there really has never been a place for people to rate and talk in depth about music videos as they often go overlooked in the broader world of music.

Unfortunately I don't think very many people know about the feature yet, as most videos of even popular artists still have hardly any ratings. Almost all of The 1975's videos, for example, have less than 5 ratings, and many other similarly large artists are the same way.

So I'm making this post in an effort to push engagement - if you also love music videos and rating stuff and yearly charts, give RYM a try! Would love to see more people use it.",music
"Deep House - Playlist Volume 1

Label:  Black Delta Records

Style:  Deep House

[https://www.youtube.com/live/EyZvaN0VDi8](https://www.youtube.com/live/EyZvaN0VDi8)",music
"Lookup ""Drug Addicts"" by Lil Pump

I expect a typical response to find a song in another genre of music that is unethical as well. You can do that, but vast majority of other songs in that genre will not be like that. Well, give or take 70% of the rap genre has a very similar message as ""Drug Addicts"" by Lil Pump is why. It has nothing to do with bias.

Imo the genre is often about drug addicts, waking up drinking a fifth in the morning, objectify women, obsessive consumerism, outrageous bragging of how much money the artist has, violence, drug usage. It is not at all wrong to dislike the genre when the message is about that. It's not at all wrong. I consider it a red flag if someone does support those items.",music
"So there’s this friend of mine who for the record I don’t find her physically attractive, not my type. I’m also happily married.🤷🏻‍♂️ However, I came upon this strange phenomenon the first time I heard her sing. Her singing has a peculiar effect in the middle of my brain/behind my eyes. It makes it tingle and relaxes me. Is this what it means to be serenaded? 🤣

Has anyone ever experienced this?? I never had in my 33 years alive, and Chatgpt wasn’t particularly specific and concluded that research on this is limited. Thanks in advanced.",music
"Her music makes me wanna curl up and cry about the fictional man I had made in my mind who dumped me for a white girl.

I love your best American girl and a pearl cause it just makes me feel a wave of emotions crashing into me even though I’ve never experienced such things 💀

Her music and voice is amazing at conveying emotion. “I will” from her album “bury me at make out creek” is beautiful and it makes me feel as if I am going through the same desire to take care of someone with my whole heart.

My personal favorites:
-I will (Makes me feel like I’m in a serious committed relationship and I don’t care what their flaws are, all I want is to take care for them)

-Your best American girl (White guy dumped me for white girl, I’ve never experienced this but the song makes me feel like I have)

-Liquid smooth (Makes me feel like I’m an angered and frustrated woman because my only worth is to be a pretty little commodity to the men I get into relationships with)

-Goodbye My Danish Sweetheart (made me feel like a neglectful and manipulative partner who now sees her wrongs and regrets doing such things to her partner who grew smarter and had now left her for the such)

-Pink in the night (I DIDNT DO IT RIGHTTTTT~~ huge wave of emotions 100/10)

-Townie (No words, just love it)",music
"Was there one album that you loved so much it made you decide you had to see the band live yourself, what was it? 

Mine was Dave Mathews Band Live at Red Rocks 8/15/1995. That album was so well recorded and the performance was top notch! I listened to it almost constantly for an entire summer. I made it my goal after I heard it to see them live as soon as possible!",music
"[https://soundcloud.com/personal-purgatory/forever](https://soundcloud.com/personal-purgatory/forever)

[https://soundcloud.com/personal-purgatory/all-of-what-was](https://soundcloud.com/personal-purgatory/all-of-what-was)

Thank you to anyone who takes the time to listen:)",music
"Lately I've been super into female fronted alt rock bands like Mazzy Star, The Cranberries, The Sundays, Slowdive, Lush etc to name a few.

What are some more bands that have this same beautiful and sad 90's sound?",music
"Like the title says, I'm looking for songs that start off slow and continue to build throughout to an incredible crescendo. I really enjoy songs like EOB - Brasil, and Phoenix - Love like a Sunset pt 1, as well as many LCD Soundsystem songs that have great transitions / buildups.  I’m open to any and all recommendation you might have!

EDIT: Thank you to everyone who gave recommendations. Still working my way through them, but man you all killed it! Going to have a phenomenal playlist after this.",music
"What is the tuning for daydream by the lovin spoonful, when I play it in standard it sounds different so I was wondering if anyone could help me out?",music
I can’t tell you who it’s by or any of the lyrics 🤦🏻‍♀️ I just remember it had percentages (think remember the name by fort minor) but it was kind of popish. By a guy. I can hear the music but I can’t figure it out.,music
"Mine is Wretched Weaponry from the Nier: Automata soundtrack: [https://open.spotify.com/track/7w0lm7M5Qw6IrddeN0w4Ll?si=d3f0ee610634415c](https://open.spotify.com/track/7w0lm7M5Qw6IrddeN0w4Ll?si=d3f0ee610634415c)

Something about it just gets me into the zone. I've stopped listening to it outside of work, too, so that I can enhance the effects.",music
"so amazing...

Journey - Feeling That Way (Official Video - 1978) 

[https://www.youtube.com/watch?v=vg5vziU-qIs](https://www.youtube.com/watch?v=vg5vziU-qIs)

older version:

[https://www.youtube.com/watch?v=yC3HBz7NR\_s](https://www.youtube.com/watch?v=yC3HBz7NR_s)",music
"Hey everyone, I've been looking for a certain kind of genre that fits my kind of music since most are broad and fit a lot of different types of music. My favorite band is The Killers but what I really like about them is the clear vocals, the upbeat rhythm between rock and pop that isn't really alt rock but also not indie pop. For example, Imploding The Mirage, described by Wikipedia is Alt Rock, Pop Rock, and Synth Pop but the Killers really don't fit in to any of those. If anybody knows a certain genre that emphasizes vocals and isn't straight alt rock or pop rock. I like indie pop the most if anything so if there's a genre that's close to that as well.  I'll also take band recommendations!",music
"It's such a masterpiece that describes an optimistic view of the world and how the world could be. I think if the humankind ever had an anthem, it should be this song.",music
"[https://www.youtube.com/watch?v=BoJ4osu6mow](https://www.youtube.com/watch?v=BoJ4osu6mow)

This is my latest single! What genre would you say this is?

Lmk what you think of it!!

:D",music
"Can anyone recommend an album where all the songs have a different structure, but it sticks to one style? My favourites like this are Innerspeaker and Lonerism from Tame Impala.",music
"This will likely get deleted, but I'm looking for a playlist that will melt the mind/face of a family friend of mine, educationally. I'm thinking a lot of alt rock/prog but also some landmark edm and weird shit. Idk, can yall help me out?",music
"It’s been twenty-two years since we first met Gorillaz. On their debut single, *Clint Eastwood*, the virtual band tossed listeners into a cartoon world filled with spirits, giant apes, and ghostly harmonica melodies. The brainchild of Blur frontman David Albarn and artist Jamie Hewlett has only gotten wilder since expanding its scope with albums about destructive inner demons and environmental degradation. The latest chapter in Gorillaz’s lore, *Cracker Island*, is a concept album concerned with deranged, religious cults. 

*Cracker Island* is ten songs of pure pop bliss. Although the eighth album from the cartoon collective is their most tempered and conventional sounding so far, it ticks all of the boxes you’d expect from a Gorillaz record: There’s a carefully assembled cast of seasoned voices and current stars, a bizarre theme, and delightfully summery, honeyed production that brims with colour. 

Greg Kurstin, a frequent collaborator of Adele and Sia’s, as well as Albarn and Remi Kabaka Jr. make for a formidable trio of producers. They use a stunning constellation of shimmering synths to bridge the choruses and verses of *Baby Queen* together, adding detail to it with every repetition. It’s a sound not too far removed from the dance-pop that’s been filling the charts in recent years, but the collective’s willingness to douse these beats in such decadence and use so many bespoke accents gives their work distinction. On the title track, they retrofit a frenetic Thundercat bassline into the framework, adding more energy to their operation. 

For the most part, the cast is used sparingly, as accompaniment for Albarn instead of playing rival soloists, as has been the case on previous Gorillaz albums. The singer sounds assured in the protagonist position, his sullen, slightly zombified voice setting the tone for the collaborators to follow. Adeleye Omotayo’s mournful purrs that trail Albarn’s brassy voice fit seamlessly into *Silent Running*. On *Oil*, a song where Albarn finds a world ""made of faulty dreams,” Stevie Nicks wraps her throaty chants around Albarn’s to bolster the sense of despair.   

When the guests are pampered and given space to act individually, the results are mixed. Bad Bunny thrives in the spotlight, leading Gorillaz into the world of reggaeton, unmapped terrain for the virtual band. “La cura del miedo la tiene' tú/Veo todo claro aunque se fue la lu',” he sings on *Tormenta*, one of the album’s most devastating lines. Kevin Parker, on the other hand, fails to impress during his chorus on *New Gold*. The song’s hypnotic qualities bear a strong resemblance to his work as Tame Impala, yet he sounds lifeless as his voice wafts over a surging bass. 

Similarly uneven is the narrative’s throughline. Albarn’s first lyrics on *Cracker Island* introduce a nebulous cult, “On Cracker Island, it was born/To the collective of the dawn.” It’s an intriguing setup, but by the third song, the story has been forgotten and replaced by wan commentary about the oversaturation of social media. There are fragments of other ideas - *Tarantula* is a soul-stirring tale of Albarn’s attempt to reunite with a lover and *Baby Queen* is based on an interaction the singer had with a Thai princess - but these songs too would benefit from being placed into separately themed sets to be fleshed out to a greater degree.  

Although the tracklist is tight, the appeal of *Cracker Island* is vast. The album doesn’t have as many of the jagged edges that distinguish Gorillaz's more substantial concept pieces but that’s the beauty of it, it’s a refined, endlessly replayable pop album. Once you’ve touched down on the sandy shores of *Cracker Island* you won’t want to leave. 

*Cracker Island* \- Gorillaz - 8/10",music
"https://youtu.be/E6pvAkActvU
It’s from rayman origins and it’s the Desert world, generally from this world it has amazing music but idk the genre or what it resembles. Anything related to the track listed is much appreciated",music
"Hi, I've seen some posts already about this, but i wanted to get some more thoughts on which would be better as a beginner. Thanks",music
"Hey guys on the hunt for more future bass similar to this. Check it out I love it!

[https://www.youtube.com/watch?v=bR8sE9ubyTI&list=LL&index=2](https://www.youtube.com/watch?v=bR8sE9ubyTI&list=LL&index=2)",music
[https://open.spotify.com/track/7vlHYPY0PDHdHwAIt32HxL?si=4ab93ead97fb49b6](https://open.spotify.com/track/7vlHYPY0PDHdHwAIt32HxL?si=4ab93ead97fb49b6),music
"I've been following Constantine for a while now.  I discovered him while I was looking through some indie folk albums on Bandcamp.  I considered his debut album to be one of the most beautiful and awe-inspiring things I've ever heard.  Since then I eagerly await the next album.  This is his third so far.  Lacking in the psychedelic influences that set his first two albums apart, Fables goes for a stronger ""minstrel"" approach with short and cute songs that seem fine for the whole family to listen to, if the whole family likes LOTR.  The songs are all very charming and well-written, with a very strong spirit of folk attached.  However, because the songs are short, they are also underwritten in comparison to his largely imaginative and sometimes outlandish epics from the first two albums.  Still, Fables has one of the most realistic medieval vibes I've heard in any album of any genre.  So if you like folk music, check out this short piece, and then check out his two better albums from earlier times.  I quite enjoyed this as background music as I was reading Inheritance, and I feel a need to come back to it later.

[https://constantinedayoflight.bandcamp.com/album/fables](https://constantinedayoflight.bandcamp.com/album/fables)",music
I discovered them while watching American Horror Story and they’re EASILY my favorite band. I love literally all of their music but I’ve never met anyone who even know who they are.,music
"Obviously a major part of rap and hip hop are guests/cameos. While listening to Tenacious D’s Kickapoo, I started wondering about the best cameos in the other genres… ideally not a true 50/50 split, more like a part of a song like just a verse or chorus?

Think my favorite’s the legendary Dio in Kickapoo, though Meatloaf is great, too. 

Give me all the songs!!",music
"I really WANT work with music, music makes me happy, alive, so, i don't want live a life working with medicine, school, i want work with sounds, making music, but how? how i can do it ?",music
"So many artists suffer from the ""Sophomore Slump"". They work for years on a debut and it is awesome and the follow up seems rushed. But that's not true for everyone. What artist had a better 2nd album than the first?",music
All the websites have removed the video. I can’t post videos but if anybody wants it I have it screen recorded. It won’t let me send on Reddit so if anybody wants it lmk:),music
I’m looking to purchase a whole Bitcoin what method would be the best and cheapest for purchase?  Also what is the most secure method for storage?,Bitcoin
"TLDR; [Article from 2009](https://stockhouse.com/news/natural-resources/2009/03/23/hyperinflation-is-impossible-part-1). An autodidact ""Austrian economist"", *almost* getting a full picture... good points made along the way. 

At the time, goldbugs were proclaiming doom due to reserves being ""massively printed"" by the Fed. This article presented a critique of that position.

The article title is incendiary (and meant to disabuse the goldbugs of their claims at the time, not as an eternal pronouncement), and of course... this is fully American in viewpoint. I'll add notes as someone who works in the global banking industry. 

----going long. 

>I get a lot of emails and questions from readers and friends about whether I think the U.S. Dollar could collapse and start a bout of terrible hyperinflation. The questions are usually stemmed from watching an interview on TV with extremely biased energy/gold analysts. People who have every reason to sell you on hyperinflationary doom in order to make themselves a quick buck. I have no respect for these people, so I will not publish their names. They know who they are. I call them the ""opportunistic hyperinflationists.""

>But there is another group of ""inflationists"" who I do respect greatly. Guys like Peter Schiff, Jim Rogers, Doug Casey and Jim Puplava. These guys have spent years, if not decades, **railing against the growing debt bubble and warning that it would end badly**. **A large faction of the Austrian School of Economics (of which I consider myself a student) had been doing the same. They are the ""ideological hyperinflationists.""**

I would put many bitcoiners in this camp as well. 

>**However, this group of economists/pundits/analysts have been terribly wrong in predicting how this debt bubble would unfold. And I am certain that they will continue to be wrong as it continues and reaches its ultimate conclusion.**

Ignoring US debt's collateral characteristics means missing a big part of the problem. 

> Typically, these folks have a fundamental dislike of our current system of currency. **The feel it is immoral, illegal by the U.S. constitution and is doomed to failure as all paper currencies have been since the beginning of civilization. I agree with them on all counts.** But as a function of their dislike for paper money, they have been enchanted by its most obvious replacement: gold. They carry it around with them and flash it at interviews. They become walking salesmen for the return to a gold standard. And they point to a rising price of gold as proof that they have been right all along.

>They haven't and aren't.

>Their arguments are usually the same. That **in order for the massive amounts of debt to be repaid, the Federal Reserve and other central banks are going to have to resort to monetizing that debt via the ""printing press.""** Their claims are well documented. **Even the chairman of the Federal Reserve has promised to do this,** should it prove necessary, earning him the nickname ""Helicopter Ben"" (after promising to drop money from helicopters to prevent deflation). **And it appears he has already started. We can see it in their own figures. By now, I'm sure all of my readers are familiar with the Monetary Base ""Hockey Stick"" graph below that shows how the Fed has essentially doubled the monetary base in just a few short months. This, claim the inflationists, is visual evidence that hyperinflation is already occurring and will inevitably start showing up in everyday prices:**

[The graph will show the reserve levels](https://fred.stlouisfed.org/series/TOTRESNS). Which would have been hockey stick'ed at the time. As I've implied in [other](https://www.reddit.com/r/Bitcoin/comments/10wgl5l/misconceptions_about_central_banks/) [posts](https://www.reddit.com/r/Bitcoin/comments/116axf0/expanding_dollar_supplies_outside_the_united/), domestic reserve levels don't really correlate to global dollar supply. 

It's relatively easy to demonstrate given reserve levels were historically low throughout the greatest inflationary decades of the United States... or by looking at Japan's decades long QE regime (reserve creation), that failed to push up inflation. 

>**First and foremost is the apparent misunderstanding of the differences between money and credit**.

And here is where the author will make some misteps. 

>**At times, they may appear to have the same characteristics. At other times they act completely opposite from one another. As an economy is expanding, an increase in the total amount of credit would appear to have the same effect as an increase in physical dollars because credit is widely accepted as an equal to money. In a sense, they are the same. They are both ""fiduciary media"" (in english they are both a representation of something else, rather than having intrinsic value themselves).** 

Units that are created via credit are *indistinguishable* from any other denomination claim. ""Money"" is what it has always trended towards; a ledger system (which implicitly creates credit - a topic worthy of it's own post; a difference between traditional ledgers and Bitcoin). 

>**But when the economy is contracting, the prospect of default is thrown into the equation. When this happens, money increases in value relative to credit**. Money is more valuable than credit because in the event of default, the physical dollar holders are king. Yes, the U.S. treasury could default on it's obligations. Holders of treasury bonds would get a big, fat zero, while holders of physical currency would still have a claim. In effect, they act similar to a preferred share as opposed to common stock. They are a step above in terms of priority.

Except that it doesn't. As stated earlier, a ""credit generated"" unit is indistinguishable from some fabled ""primal, non-credit"" unit. In reality, ""money"" is the aggregation of the balance sheets of global banks. 

As for physical notes being more valuable than ""credit units""; the author will soon unknowingly suggest why they might not be. It has something to do with the fact that physical note issuance comprises the smallest portion of ""money"". This is not ""rarity"" to maintain value.. but ""rarity"" due to non-usefulness. 

>It is often said that we live with a ""fiat currency"" or with ""paper money."" This is not entirely accurate. **A very small portion of our total supply of money and credit is in the form of physical currency. It depends on how you count it, but regardless, it is under 10% of the total. This is what differentiates our monetary system with that of Zimbabwe or Weimar Germany circa 1920's.** Their economies were based on nearly 100% physical currency because nobody would accept the promises of government in order to issue credit.

Because *who would want that?*.

>**The vast majority of our money supply is in the form of electronic credit**. Electronic credit can be destroyed, while physical notes issued by a central bank cannot. This is why deflation is possible in a credit based monetary system, but not in a paper based monetary system.

Ok, back on track. Credit is money. 

>**There are hundreds of trillions of dollars floating around the world in credit. Much of that is an insurance contract on top of another insurance contract, on top of a securitized mortgage, on top of an asset. The total value of all the aggregate claims on the asset vastly outnumber the value of the asset itself. That is what this crisis is about at its very heart.** Picture an inverse pyramid with assets occupying the bottom bit, securitized mortgages in the middle, and credit derivatives at the top. A stable economy would have a right-side-up pyramid with assets occupying the bottom, etc.

Yes. This does gloss over a great deal of complexity. The question of the size of the collateral multiplier (or if we should even have one), isn't reducible to an inverted pyramid analogy - but I agree that the system up to 2008 was unstable and operated with false certainty...

...and may be worse now.

>**Our problem now, is not that the assets are going to go to zero. It's the value of the much larger derivatives and mortgages that back the assets going to zero**. Their values were derived from faulty computer models that grossly underestimated risk in the underlying asset, but more importantly in the ability for a counterparty to make good on their promise in the event of a default. 

I like the dig at VaR (Value at Risk). I would go further... derivatives, swaps, repo, etc. perform a different function for wholesale banking than one of ""investment""; They are balance sheet tools. 

All are used to manipulate risk-weightings and provisions for credit loss - shifting balance sheet entries. They allow commercial banks to expand their balance sheets (and create more money by doing so). 

I would say that our problem is one of collateral (and maybe the best reason to re-think things). We have a global ledger system: the interconnected balance sheets of the commercial banks of the world. Uncertainty and perceptions of risk create demand for safe and liquid collateral which underpins how these balance sheets interact. US dollar debt instruments have always been the go-to collateral for the system... and what would happen if confidence in those instruments is questioned?

>**So the next time you hear that the Federal Reserve is ""printing money,"" please do not automatically assume that they are printing physical notes. They are creating electronic reserves (credit) to support the balance sheets of the big banks**. **There is absolutely nothing inflationary about this**. The banks are simply taking it and using it to cancel out their derivative losses or are hoarding it in order to prepare for future losses. Previously, banks would have used the electronic reserves to go out and make 10x that amount in loans to consumers or businesses **(in reality the order was the other way around - loans first, then reserves).** That is not the case anymore, and until the bad assets are completely liquidated, it will not be the case again.

Yes. But also, reserves from the Fed are only useful to banks under the Fed's jurisdiction. The dollar is the unit of the *global banking system*. Reserves are used for domestic settlement, domestic interbank transfers. 

And yes, loans first *then reserves*... that was the typical order. This was until reserves weren't required (and that was long before the Fed reduced to reserve requirement to 0%). Banks used the previously mentioned tools to manipulate their balance sheets to allow for further lending in spite of reserve requirements (something the author misses). Reserves *do not* make commercial banks lend.

>Thus far, we have a total of $9.7 Trillion dollars in total government/central bank assistance in the United States. An amount equal to that and more has been provided by their counterparts around the world. More is promised. But the fact remains that the minimal inflationary impact these actions have are negligible in comparison to the amount of ""problem assets"" being devalued around the world. Much of it is just in guarantees - that is, more insurance. The Federal Reserve will offer to swap good assets for bad. All this does is cancel out debt from somewhere else. It's like moving money from one pocket to another. The act of putting money in your right pocket does not make you any richer.

It's was never ""swapping"" good assets for bad. It was the movement of collateral. The *perception* of the collateral changed. What was deemed ""low risk"" yesterday, became ""high risk"" overnight due to perceived correlation. In reality, many of the ""bad"" assets continued to perform and were sold at a [profit](https://www.reuters.com/article/usa-treasury-mbs-idUSL1E8EJ36J20120319) (unknowable at the time). 

>**All in all, the central banks are not nearly as powerful as they'd have you believe. The amount of the total money supply that is controlled by them is minimal. They won't tell you that. They'd prefer you to think that just by them moving their lips they can affect the entire economy's decision making processes. It simply ain't so.**

Most definitely.

>**Closely tied to the belief in imminent hyperinflation and a skyrocketing gold price is the misplaced belief that the U.S. Dollar is on the brink of collapse. Essentially, they are one and the same.** Many of my arguments against hyperinflation are the same against a dollar collapse. **But there is even more evidence stacked against such an occurrence.**

>**Ultimately, the Dollar will end up at zero - but that is not going to happen any time soon**, and I would argue is likely decades away. Until then, the massive amounts of deleveraging will increase our appetite for dollars to pay back debt. **There is too much credit in the system, and as we rid ourselves of it slowly, we need to acquire dollars.** A large portion of the credit derivatives I mentioned above are denominated in dollars even though the underlying asset may be priced in another currency. This is a theoretical short position on the dollar. A ""carry trade"" in other words. It must be unwound, just like the Yen carry trade.

The issue with traditional ledgers, is not only that they allow for credit... They can erase the line between money and credit. Same misconception as highlighted at the beginning - they are the same ""dollars"".

>This is what is meant when we call the U.S. Dollar the world's ""reserve currency."" Most people hear the word ""reserve"" and automatically conclude that because many other countries hold the dollar as their primary currency in their foreign exchange ""reserves,"" that is what is meant by ""reserve currency."" It is not. Total foreign exchange reserves of dollars are far smaller than total foreign credit contracts denominated in U.S. Dollars (reserves worldwide are ""only"" ~4.6 Trillion). It is the reserve currency because it is the default currency for international trade and commerce in general. In order for that to change, 100's of trillions in contracts would need to be re-written. Not practical.

>As such, demand for U.S. Dollars will persist.

>In summary, there are many multiples more debt than capital in the world economy. Debt is being liquidated and will continue to do so until it reaches a sustainable level relative to capital. The process of this debt liquidation puts a higher value on dollars relative to debt, thus ensuring an oversupply of dollars is impossible.

I would put it differently. I disagree that an oversupply is impossible. However, the demand for dollars *is agnostic* to whether they are generated by credit or not. In reality, virtually all dollars are credit originated. 

We are headed in a bad direction, but it will persist as long as the dollar is the denomination of the global banking system... and as long as USD denominated debt remains prime collateral.",Bitcoin
"Just as the title reads. 
What is the common consensus on security of lightning addresses.. is it the same as traditional addresses?
As in, is it advisable to keep BTC in it for storage.",Bitcoin
"A lot of governments own a lot of bitcoin, including China, USA, Bulgaria is rumored to have 200k Bitcoin. But why do they hold Bitcoin? Do they think it's gonna have a bright future or they just confiscated it and forgot about it?",Bitcoin
"I have verified my account (in fact, multiple times) and it still wants me to verify it again. However, when i try, i get this message  

**Error:** **Cannot update the state**

 I've tried from 2 different PC's, and my phone. It's not working. So far I submitted a ticket a week ago, no answer. I submitted another ticket couple days ago, and today i check the ticket, and the first one was marked as SOLVED, with not any answer from them. It's just so ridiculous, i don't want my coins tomorrow, after a week or two, i need to use them now. 

https://preview.redd.it/oonxkma3zrka1.jpg?width=809&format=pjpg&auto=webp&v=enabled&s=04b7778a3f4ccd71d3d1e22dc4929b5cf077b1d6

https://preview.redd.it/etmw9ia3zrka1.jpg?width=328&format=pjpg&auto=webp&v=enabled&s=882ba0b108b0f9cff602d10f7fe0d12505c982b7",Bitcoin
"This post might get dismissed for incoherent ramblings.

I don't know if I'm naturally still a BTC skeptic. I want to move towards being a maxi. I already refer to myself as a maxi but I don't know if I'm telling the complete truth. Are maximalists 100% certain that stacking Bitcoin is an intelligent decision now, or is 99% enough to qualify?

I've read a large plethora of resources on btc. I'm still not set on BTC in relation to the human condition. For instance, the general rejection of the unkown by most people, and how easily public opinion is swayed by the powerful in media and our kleptocratic, plutocratic governments. 

Personally, whilst I was in the process of internalising 'not your keys, not your coins' last year, I got stung in Celsius collapse. I had just withdrawn all of my BTC before the collapse (thankfully) but a couple of shitcoins got stuck. If and when I get anything back, I'll liquidate instantly and buy more btc. All of my holdings are in cold storage now.

But I keep stacking sats, and the doubt is still there, what if this is all for nothing. From a personal investment standpoint as a young person, my brain wants to just DCA into tried-and-tested index funds, not btc, as much as I would love societal BTC adoption to continue on an uptrend. 

I wonder if this sort of illogical fear is due to my negative experience with Celsius (I know, shameful, idiotic, disgusting, I hate myself for it). But, also I can never seem to eradicate the doubt, so that leads me to thinking I have some sort of subconscious illogical trauma from Celsius which is preventing me from becoming more comfortable with Bitcoin again, despite the two concepts being entirely unrelated.",Bitcoin
"I have a transaction I had posted to the blockchain 31 minutes ago, and Blockchair says it has an estimated 26 more minutes to 1 confirmation... Is this the new normal? All of my recent transactions have been like this. Fee I input was 19sat/byte, and i assumed that was high...",Bitcoin
"I have an 8GB RAM with IntelCore i5-7300HQ CPU / 2.50 GHz. Currently 639 GB of free memory space.

Is it possible I can get my own node running? Maybe a light, possibly full node?

Mining out of question for me, right?",Bitcoin
"I’m getting 167k on Friday. I’m thinking about using 20k for debt. Then with the remaining 147k I’m thinking about getting two bitcoin, and then putting 100k in stocks. After I do this my only expenses would be 1604.99/mo (rent/phone/utilities) and 1,161/mo for car/insurance. For a total of 2765.99/mo. I make roughly 2,600-2,700 twice a month. So I have about 2450 leftover after monthly expenses for food, spending money, and savings. I’d plan on splitting whatever’s leftover at the end of the month between my investment account, and my car debt (47k)


(My monthly income does not account for two $5,000 bonuses I receive every 6 months, or my Christmas bonuses which will be about 1200)",Bitcoin
" Organisations, medium businesses and small businesses are accepting crypto, not rapidly but it's growing naturally. Customers are asking merchants to accept crypto as payment for services they are providing.  


Well, this is a really basic reason. But merchants do have other reasons as well to accept crypto payments.  


What's your reason? ",Bitcoin
"Ask (and answer!) away! Here are the general rules:

* If you'd like to learn something, ask.
* If you'd like to share knowledge, answer.
* Any question about Bitcoin is fair game.

And don't forget to check out /r/BitcoinBeginners

You can sort by new to see the latest questions that may not be answered yet.",Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/11c70i8/daily_discussion_february_26_2023/) for unanswered questions.",Bitcoin
Hi I am planning to run a bitcoin full node on aws. But the problem is ssd storage is a but expensive for me. I wonder isnt it possible to use s3 bucket to store lets say 90% of old blocks all the time. If someone knows please explain how often a full node need to fetch some old block data in which circumstance?,Bitcoin
"Jump 5 10 or more years ahead in time .... If you owned BTC or gold but the world or country you lived in had an entrenched government controlled CBDC,  no more FIAT, and all normal day to day transactions of life (food, transport, rent your salary etc) was only permitted by using this CBDC issued by that country, then how will BTC or gold help you and provide you a ""real money a store of wealth"" alternative. Could a bank of the future simply say “no sorry we don’t accept BTC we don’t convert BTC in our CBDC get stuffed” what then? Will it have to become a alternative parallel means of payment, perhap even one that is considered illegal by the country you live in?",Bitcoin
"**BITCOIN IS THE NEW GOLD!**

The first Reason why SILVER will not make you rich is because probably you're paying a lot of PREMIUM to get the coins, etc . If you paid $28 + $15 premium for eagles for example that means that TODAY you lost $12 USD and probably in that time including the inflation you're not going to recover never again your money because in the future will be more money of course but you're going to have less buying power. **Here an example:**

[Here WHY Silver will not make you rich but BITCOIN YES!](https://preview.redd.it/090spckhwmka1.png?width=1024&format=png&auto=webp&v=enabled&s=01a45d4419c2e9c8a9cc5cf4cdd1bb7d35f9272c)

Second Reason: SILVER can be a good WAY to protect your MONEY if\*\*\* there's a BAD GOVERMENT or the Federal Reserve dedice to print more money, but guess WHAT? All the stocks, bitcoin, real state will go UP including SILVER , so at the end it's not a great investment :/

so WHEN is GOLD and SILVER a great way to protect your money?  
These metals are great when there's a NUCLEAR WAR or something terrible is happening WORLD WIDE or if there's a BIG BIG GLOBAL RESET, but these events happens every 100-200 years or when you live in a country like Venezuela, but guess what no one wants GOLD or SILVER in venezuela because the Liquidity is very LOW so ""few people buy metals there""

would I buy silver for myself?

Of course YES because I like the idea about having some protection if something bad happen in this WORLD but I'm 100% sure that if you want to make money you need to learn GREAT SKILLS, start your business or invest smart in long term + being extremely positive mentally .

When to buy silver/gold?  
You can buy at anytime but the first second that you start tracking the price you'll LOSE time, ENERGY and money of course. GET it but forget about your metals, etc and OF COURSE don't get silver in popular websites like AMPEX, JM BULLION because they will overcharge you .",Bitcoin
"I know right now it's really hard and it would take so much time, but what if computers start becoming really fast in the next years? like 1000 times faster? I don't know..

For example you can do it manually, with this website

[https://playxo.com/](https://playxo.com/)

A script in a really fast computer (quantum) could do it and find it?

Is there something I am not seeing?",Bitcoin
"I love listening to podcasts, why not stack some sats!

Over the last month I've earned over 11000 sats while listening to the podcasts I've been tuning into for years. I know it's not much but it's an enjoyable way to get started with BTC. I was also pleasantly surprised by the amount of Bitcoin focused podcasts and clips I've been exposed to, I've really learned a lot. 

https://play.google.com/store/apps/details?id=fm.fountain.apps
https://apps.apple.com/us/app/fountain-podcasts/id1576394424


If you feel like it here's my Referral code:
https://fountain.fm/fountainhead?code=520f04551d",Bitcoin
"Someone posted That Bitcoin Mining difficulty hit a new ATH and I always see people talking about how Bitcoin price Will follow and increase as well. I dont see the connection. Maybe someone can explain what is going on.

My thoughts:
There is always a predictable amount of freshly mined Bitcoin per time on the market regardless the mining difficulty. Why should the market care about the involved mining costs? Bitcoin doesnt get more rare because of It. When there are more miners, the difficulty increases and when there are less miners, It decreases. When miners stop operating, there is no loss of supply. The amount of mined Bitcoin per time stays the same so the amount of fresh Bitcoin hitting the markets also stays the same.

When we compare It to gold mining, its logical that an increase in mining difficulty and mining costs can indeed result in a decrease of supply per time. When its too expensive to mine gold, there is nothing that Will magically decrease the difficulty of gold mining so miners stopping to operate is a real loss of new supply per time which Will make It more rare and can result in price increase. 

The only thing comparable to that is Bitcoin halving which decreases Bitcoin supply and make it more rare. those events indeed showed a price increase of Bitcoin which is logical when there is suddenly only half the amount of new Bitcoin per time available on market.

One argument that I found is that more miners can mean a more secure Network which make Bitcoin itself more valuable. But i kinda doubt This Will have such an impact at This point to be honest. At least not like the price increases due to decrease of supply.",Bitcoin
"I recently installed bisq.

Someone recommends linking your mobile phone with a watch wallet from bisq, such as blue wallet.

Doesn't any involvement of using a mobile device automatically negate all anonymity?

Or does blue wallet have a Tor installed like so in bisq?",Bitcoin
"Hey everyone,  I'm looking for a crypto business bank account that can receive USD ACH transfers, convert FIAT to crypto, and send to an external wallet. Does anyone have a recommendation?  Thanks in advance!",Bitcoin
"Anyone who know's a solid, yet cheaper alternative to product's like Seedor or Exitkey ?

I really like their concept, but I do not want to spend that much money for a few relatively easy and simple item's.",Bitcoin
"I wanted to use my own node for broadcasting bitcoin transactions. Since my node runs on a public facing BTCPay server, I figured the best way was to install an ElectrumX server, so I did and it's been up and running for a couple of months.

I've been able to connect to it using Electrum (desktop), and Blue Wallet (Android). For some reason I am unable to connect with Phoenix.

I haven't tried any other wallets.

If anyone wants to use an Electrum server to interface with the bitcoin network, you can find one at [https://btc.arnzenarms.com:50002](https://btc.arnzenarms.com:50002)

I can't seem to find a list anywhere of all the electrum compatible bitcoin wallets so I can't test them.

This is a non-custodial server. It is only a gateway to the bitcoin network. It should allow you to do things like check your balance and broadcast transactions. Every wallet you use connects either to a node you control directly via RPC or through something like this server. 

Most wallets will connect to a somewhat centralized server (eg. Ledger) or possibly a random electrum server (eg. Phoenix). This process leaks a little privacy since the server knows the IP making the requests, but the rest of the data is publicly available on the blockchain. Though the IP can be obfuscated using Tor or a VPN. 

If you don't want to run your own server, and like the idea of a gun store's server processing your bitcoin transactions. Feel free to use mine. 

If this is stupid or dangerous in ways that I don't understand, please reply to this post and educate me!",Bitcoin
"I'm a Bitcoin holder and proponent, so when I see FUD, especially in mainstream media, my desire is to disprove it and expose it for what it is- FUD.

However, the problem arises in that my sources invariably come from pro-Bitcoin sources such as The Bitcoin Standard, Inventing Bitcoin, Andreas Aantonopolous YouTube, 99Bitcoins, Swan BTC, etc.

This raises a conflict of interest. Why should a non-believer believe what I have to say when all of my sources are Bitcoin advocates or simply YouTube channels. Of course they will say pro Bitcoin things!

Is the answer running a full node? Why should we trust the above sources to accurately describe bitcoin?

Any help with this perplexing problem would be appreciated- S",Bitcoin
"Suddenly bitcoind only reports 1 connection in, 0 out. Did some ‘addnode=‘ to help it along, but still won’t go above 1 connection. It’s stuck 100+ blocks back, too. Won’t sync. It had been running flawlessly for months until just yesterday. I've tried rebooting the whole node a couple different times, too. It's tor-only and I still have plenty of disk space. Thoughts on what else to look at?",Bitcoin
"Between self custody and lost coins how much room could there be for exchanges to create Bitcoin Iou's and not potentially get wrecked if a large portion of customers try and transfer to self custody in a short window of time. 

Is this currently more of an issue because of a higher liquid supply? Meaning hopefully in the future, as more coins are bought up, held, and transferred to wallets;  the amount of Iou's would be dramatically reduced? 

So right now, because of most exchange practices with Iou's; it's being manipulated more like a stock. but as adoption grows and less liquidity is achieved it will become a/the more stable currency it was designed to be.
At that point there would be more of an honest ""price"" realization and less volatility? Am I understand this right at all? 

Excuse my run on sentences. I hope no one had a stroke reading this. I'm just looking for assurance that I'm properly understanding the way things are working.",Bitcoin
"Hi,

&#x200B;

don’t know what I’m missing here. I have the Bitcoin Core 23.0.0 Linux version, it synchronized successfully and shows outbound connections but never reported a single inbound connection:

&#x200B;

Number of connections:  8 (In: 0 / Out: 8)",Bitcoin
"I'm travelling in a few months from Canada. I need to convert my CAD to USD, then to LBP, then to HUF, all while trying to find the best exchange rates and not get scammed on the spread. 

My first thought was ""If everyone accepted bitcoin this wouldn't be an issue"" 

Looking forward to mainstream adoption.",Bitcoin
"My GF works in the medical  industry and a patient  gave her a physical bitcoin as a gift. Is there any danger or scam involved? Could it have been used to do something unscrupulous and then they were trying to ditch it? Should she open/ use the wallet?

Thanks",Bitcoin
"Any quick tips. My brother inlaw is trying to understand btc. I'm doing my best, but told him the only way to understand is to buy and just get involved. Any quick info to get him going on reading would be appreciated.  I already gave him the white papper",Bitcoin
"In many postst and comments in this forum people reccomend using hardwarewallets for storing Bitcoin which I, like most bitcoiners, fully support.
However when it comes to the different hardwarewallets that people here use and reccomend, its mostly ledger, coldcard and trezor. The bitbox is almost never mentioned. However from other scourses like Youtube, the bitbox 02 (Bitcoin only Edition) appeares to be the most reccomend  option for storing btc.
Is the bitbox 02 pushed by german youtubers (like Roman Eher and Niko Jilch) or is it fine and maybe undertepresented in this forum? 
Has anyone tried the bitbox and can reccomend it or has any complains? 
I personally tested the ledger nano s and the bitbox 02. Both are fine in my opionion. I dont like the ledgers partly closed source policy and the storage of customer data. And I dont like the lack of haptical feedback of the bitbox 02. Other than that I am happy with both but like the bitbox better.",Bitcoin
"[https://edge.app/blog/company-news/edge-security-incident-urgent-notice/?af=search-brave-com](https://edge.app/blog/company-news/edge-security-incident-urgent-notice/?af=search-brave-com)

&#x200B;

TL;DR: Private keys were exposed if you were using the buy/sell option. Send the funds to a new wallet as soon as possible after updating the wallet.",Bitcoin
"What do you guys recommend?


I can't use coinbase or binance US. Binance wanted me to fill out all this paperwork and I told them to get fucked. Coinbase was something else 

I'm sports gambling so I want something where I can withdraw bitcoins and sell cheap without too many fees",Bitcoin
"With platforms like Strike and BitPay becoming more and more popular, what do y’all here think about the concept of living solely off BTC? Your direct deposit could go right to one of these platform to buy BTC and use the bolt card or whatever to pay bills, buy stuff, etc. Doing this you could just transfer x amount to cold storage every month as you “savings” / emergency fund.

Just wondering what are possible pro/cons of this since we rarely see significant sharp (10% + declines) in a matter of days anymore.",Bitcoin
"This is the head of the FED back in July last year.

The people that print the money, set interest rates said - “We now understand better how little we understand about inflation”.

If that doesn’t tell you everything you need to know I’m not sure what will.

The macro conditions might not have ever been worse for Bitcoin, but the very reason it was designed was literally cemented by the people it will eventually oust.

Bare that in mind when everyone is still calling for a drop to $12k!",Bitcoin
"Any way possible to buy around 100k of BTC in Canada without reporting it? Looking to purchase about 100k in USD over the next couple of months 

Thanks in advance",Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/11bdooa/daily_discussion_february_25_2023/) for unanswered questions.",Bitcoin
"I’m planning a wedding in November and luckily I have a few family members that are bitcoiners. 

I have a bitbox02 that changes the address with each transaction so I don’t know how create a viable way for multiple people to gift bitcoin.

Any advise is greatly appreciated!",Bitcoin
I’m fairly new to all of this and have all of my coin in cashapp. I know theres probably waaay better places for me to store it so plz lmk friends,Bitcoin
"So I saw the Coinbase documentary already, but was wondering if anybody had any other good suggestions for crypto movies/documentaries to watch? I would appreciate any suggestions. Thanks.",Bitcoin
Does anyone have suggestions on how the Fold card company is profiting? It’s amazing they are paying us SAT’s but curious how they’re profiting,Bitcoin
"We all know Bitcoin is way more than gold 2.0, but basing on the current state of the art (high fees and verification waiting time), the network is not (yet) capable of fully substitute FIAT as a time/cost efficent medium of exchange.

That said, BTC can already outperform metals as a store of value and the FIAT system as a medium of exchange for large amounts. With an average cost of 1,80$ per fee and a 10mins average for payment delivery, the Bitcoin Network can fully replace conventianal investment assets like gold and silver with nothing but advantages, and also be a more reliable and efficent way to send large quantities of money **wherever**/**whenever/to whoever** needed, as opposed to the traditional banking system. 

So why is people so reluctant, at least this is my impression, to promote Bitcoin as a substitute for gold or wire transfers? High volatility is obviously something one doesn't want to deal with when talking about saving purchasing power, but after all this is ""just"" a matter of adoption. 

If the ecosystem was more eterogenous and wide, whales wouldn't be able to have such an impact on fluctuations as they have now. This is exactly why metals prices are more stable than BTC.

I've always thought that the main problem of the Bitcoin industry is the lack of a coordinated (not centralized) marketing and promotion net, with the ambition of tackling on/debunking the mainstream narrative. What we have achieved until now is just an annuity life on the undeniable disrupting features of this technology, a passive approach. I'd like to see more ""aggressiveness"" for those about presence on media, sponsorships and promotion. Something professionally managed.

We, the Bitcoiners, need to talk to people in a different way, addressing more tangible facts already observable, rather than the future prospects inherent to Bitcoin mass adoption.

Not saying YOU should do it, I'm saying the Bitcoin Organization, for example, should.",Bitcoin
Hey guys. I have bought a small amount of BTC and it’s currently stored online in the exchange app. But I would like to transfer them all to a physical storage. What type of storage would you suggest is the best? Please advise and thank you!,Bitcoin
So I moved $30 (0.00121 sats) USD from CB to ledger and the transaction fee says it was $18.462 USD (0.00079608 sats). I'm wondering if that's accurate and where it drew the fee from? It has to come from the receiving wallet right? So I tried to send myself $30 and CB took a fee. Then sent the transaction and My wallet paid the $18.426 and I really only got like $10 out of it?,Bitcoin
"Just looked at the markets and just wanted to remind everybody that holding your BTC on exchanges is risky. Proceed with caution if you choose to do so.

""Not your keys, not your coins,"" Those exchanges have had a rocky road over 2022.",Bitcoin
"This post is for the noobs who just like me synced their nodes very recently. Also I want to admit that I was an idiot thinking my Bitcoin Node was ""online"" supporting the network for a whole month, when in reality it was just sitting there. 

I was trying to connect a widget to my node to read some data over my network. that's when I realized I can’t access it. 

After a little of research, I found a way to check if my node was online (which is on the Bitcoin Core website: https://bitnodes.io/#join-the-network) and it kept returning errors. 

After some more research I realized I need to configure my router to forward port 8333 and allow my firewall to pass through inbound connections. 

tl;dr, after you sync your node, confirm it’s actually online. Peace.",Bitcoin
"Let's talk about the term ""FUD"".   

I think the reaction of calling every idea about about Bitcoin you don't like as ""FUD"" is ridiculous.  It's the same intelectual laziness by which liberals attribute any type of disadvantage to racism or sexism and the way conservatives dismiss any information that doesn't align with their world view as fake news.  

They are all just excuses of self-righteous defensiveness. 

I know even the biggest Bitcoin influences throw this term around, but I would urge you to have deeper thoughts when questioned about bitcoin. By not leaning on the knee-jerk reaction of ""FUD"" you will provide a better understanding to the person asking the question and it will also force you to deepen your own understanding and philosophy of why Bitcoin works. 

And that is exactly why the term is unnecessary and lazy, because all the typical arguments ""FUD"" against bitcoin have very clear explanations that anyone willing to learn how money works can easily understand. And if that person doesn't what to take the time to understand bitcoin, that's fine. They will just have to buy bitcoin at the price they deserve.  

I've often posted here that Bitcoin does not care about semantics, but I think this idea is more than just semantics. It's about promoting a more intelligent discussion around bitcoin, which also requires the respondent to also improve their own understanding.  

Cheers!",Bitcoin
"I heard people talking about how adding a passphrase adds additional bits of entropy to your wallet.

It's pretend I have a trezor wallet with a passphrase on it and someone sees my seed. Can they generate the wallet without the passphrase and access funds?

Bonus question: If I have a wallet without a passphrase, can I add a passphrase to this or do I need to generate a new wallet and transfer funds?",Bitcoin
"Let's say I have a normal wallet with the standard derivation path. This holds KYC Bitcoin.

I begin to accumulate non-KYC Bitcoin and store within a child derivation paths of the standard one.

I want to import my wallet from seed on a new hardware device. When I input my words, will both derivation paths restore? Or is there some other magic I would need to carry out?",Bitcoin
"Kiyosaki's investment strategy for the future involves embracing the opportunities presented by cryptocurrencies and blockchain technology. He shares his thoughts on the potential risks and rewards of investing in Bitcoin and other digital currencies, as well as his long-term outlook for the industry. In this [Video](https://youtu.be/_2dmJAlnqQM) He shares his Key Insights. I am sure, Now he is just getting expert with trends in bitcoin. Watch the video and let's see your inputs on it.",Bitcoin
"Debating with my brother tonight - he argues that immutability is a weakness in the real world because we don’t want mistakes or fraud to go unaddressed. 

He’s saying that the risk of fraud through our regulated legacy trusted intermediaries (banks, visa, law firm trust accounts etc) is a lot lower than the risk of mistake or fraud for the average person transacting on an immutable blockchain. 

What sayest thou?",Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/11ak3gh/daily_discussion_february_24_2023/) for unanswered questions.",Bitcoin
"Used to use cakewallet but it’s been buggy lately. Are there any good ways?

Is blockchain good, am I able to buy BTC there, and then send it to another wallet address?

How about Bitcoin ATMs?",Bitcoin
"Is there a course or site that would provide details of advanced Bitcoin topics? There’s a lot of Bitcoin courses and books that explain BTC but haven’t been able to find one that dives into the deeper technical stuff. I’m sure there’s a reference that everyone uses for this but just haven’t been able to find it

Hope to find something that possibly dives into the C++ code too, but honestly just want to know a few things like, who approves PRs to the Bitcoin code? Do nodes have to approve?",Bitcoin
"For years now, I've been anti-crypto.  I've called it a Ponzi...I've compared it to tulips...I've found pleasure in the markets grinding down over the past year.  I've hated all the cryptards with their rocket emojies and asking ""wen moon?"".  I'm a metal stacker.  I love gold and silver.  But despite all of this, I just bought some Bitcoin, and here's why.

First off, I want to give a shout out to my man [Jerry on Youtube](https://www.youtube.com/channel/UC_VEbZRk9LeDItX_uphKqhA).  He too is a metal stacker.  He too is a patriot.  But unlike me before today, he owned Bitcoin and considers himself a Bitcoin ambassador.

While I still do not think that Bitcoin has intrinsic value, it's not really the intrinsic value that is important here.  Like gold and silver and other hard assets, the premise behind Bitcoin is to opt out of the ""doll hair"" system.  Objectively, it makes little to no sense to hold any substantial amount of money in cash/dollars.  The government excess and debt policy will inevitably fail.  This is a massive alignment with the metals community, and common ground on which I can build.

And though I love metals, we do and will continue to live in a digital age.  The longer I've stacked metals, the more obvious it has become that that we will not transact in metals again.  Bitcoin makes me uneasy because I cannot physically touch it, but I see its transaction value.  And while I do foresee CBDCs and overregulation/restriction by the government, metals and Bitcoin together can provide a risk-diversified portfolio that enables individuals to protect their wealth from multiple possible scenarios that spell doom for the dollar.

For that reason, I will be moving towards a 50/50 metals/Bitcoin portfolio outside of 401k/Roth/HSA investments.  All other income not directed to tax-advantaged accounts will go to metals and Bitcoin (and no other crypto).  Diversification is important as we must prepare for all outcomes.  We do not know how long the dollar system will remain, thought we do know that it is eventually going to fall and must prepare for that, as well.

Anyway, this is a long-winded way to say, I've spent a lot of time shitting on crypto, and most of that is still valid.  However, there is a laudable goal in Bitcoin, despite the clownery of the rest of the crypto world.  There is greed in every asset, but many of us are just trying to make it through the awful mess we see coming.  Thank you.",Bitcoin
"Yeah I said it. But it's for a completely different reason than you might think.

Let's go straight up sci-fi:

It's April 20th, year 2420. The Empire of Mankind finished terraforming Mars almost a century ago and you are living there. You send a video message to your mom, and leave for lunch. Right now the distance from Earth to Mars is the average 225 million kilometers, so the message will reach her in about twelve minutes. She has to record her own message, that will take her about ten minutes. And then twelve minutes that her message travels back to you. That's 35 minutes before she'll respond. Hell 25 minutes is the absolute minimum thanks to speed of light.

You might already know where this is going.

The Bitcoin block time is ten minutes. Each new block is based in the previous one (hash). That means to be at least slightly capable of mining BTC you need to be within ten light minutes away from from the propagating nodes of the previous block to be able to receive info necessary to calculate the new block before someone within the distance does it before your machine. Average distance from Earth to Mars and Venus is 12.6 and 3.75 light minutes respectively. That time is what you are losing mining wise compared to those physically closer to the machine that mined the previous block. Oh yeah, and if you calculate the block you still have to deliver the block to other nodes to get them confirm your block. Good luck beating the laws of physics. The Earth is now the 'central mining bank' of the interstellar financial world of The Empire of Mankind thanks to speed of light limiting it's spread.


Edit:love the immediate downvote from the guy who just read the title lol.",Bitcoin
"What do think the fed rate hike will be for March, 25bps or 50bps? 

What do you think the FED meeting will affect BTC? Anyone anticipating the news?

United States FED meeting.",Bitcoin
"I got this idea, maybe I'm off my rocker but just want to get feedback:

&#x200B;

Its called BBB (Bitcoin Billions Bonds) - or '*the triple B*'

**High-level overview** \- consolidate bitcoin from millions of users into single wallet/address, use that bitcoin to purchase the equivalent amount in boring 3-4% treasury bonds, take that interest from the bond yield and buy bitcoin.

user consolidated bitcoin --> \[buy\] Treasury Bonds -- Bond yield \[interest\] ---> \[buy\] bitcoin

This is the concept of **Churn**ing, fiat interest into Bitcoin

&#x200B;

That is the basic overview, but from the bitcoin purchased from yield interest, you can continue on with the bitcoin purchase from the bond yield to do many other interesting things:

1) Accumulate the yield interest bitcoin for a year, then evenly split the accumulated bitcoin back among the user addresses that pooled the original bitcoin to purchase the bonds.

2) Take the accumulated bitcoin purchased through bond yield, split it by an specific number and randomly send it back to the original users who pooled their bitcoin to purchase the bonds. Once an address receives a payout it is removed, and the process continues.

3) use the bond yield interest to buy bitcoin, then funnel that bitcoin into other smart contracts as dictated by the pool users.

&#x200B;

**Assumptions**: You'd need a company that can take the pooled bitcoin from millions of user addresses ( so ideally billions worth of bitcoin), buy bonds and then take the yield to buy back bitcoin. Ideally the company would be as automated as possible (if maybe not entirely) and auditable. Maybe it can be collectively owned by the users themselves.

Also if the pooled users decide or if dictated in a smart contract, the bonds can be liquidated and bitcoin purchased and that amount returned in proportion/ or equally back to the original creators of the that pool. 

&#x200B;

Am I mental or does this concept have any value?",Bitcoin
"probably the best thing about bitcoin (i know i  know thats coming in hot) is that it fixes the broken fiat incentive structure.  some years ago i was reading some literature about buddhism and one thing that impressed me was a bit about incentive structures.  according to buddhism, people will act in their own self interest most, if not all, of the time.  but what does that mean 'to act in your own best interest'?  for someone who is dying of thirst, they will be looking to get some water.  for someone who has nothing to eat and is cold they are going to be incentivized to get nourishment and clothing and shelter, and on and on.  but what buddhism teaches is that once a person is somewhat enlightened *and* they have their immediate needs met, if this individual is on the right track to 'enlightenment' (whatever that means), then they will be incentivized to help their fellow man.  this is because they get joy and satisfaction from helping others.  right?  who doesn't like to help someone out who needs help, it makes you feel good as well as helping that person.  emphasis on having that good feeling when helping others, as i read it you are on the way to enlightenment if you have trained yourself to feel good when helping other people in need.  that individuals incentive structures are lined up meaning that helping people gives them joy as well as helping to improve the community they belong to.  helping others brings that person joy and in a sense is a selfish act, because they will be getting a sweet dopemine high through helping another person.  now a cynical person may say that this individual is not doing anything good since their so-called kind acts are really a way to get a fix of sweet sweet dopamine, but i disagree.  if i like to fish no one is going to call me selfish for spending my time and money going fishing.  so if i like to give out sats on the internet, who's to say that makes me selfish?  

with fiat, our incentives are to enrich ourselves at all costs.  because you just never know how much is enough.  you have a million dollars and can retire? Wrong! didnt you get the memo? a million dollars aint shit no more, back to work for you! go get your flare on and write up some TPS reports post haste!  all that money you have?  better spend it on consumerist bullshit because if you dont you are losing money through inflation.  And that is on the individual level dont even get me started on the political level.  in politics,  acting fiscally responsible is a death sentence.  so we have the most powerful people being the least fiscally responsible and it's no surprise the guns come real real fuckin fast.  and the pooerst amoung us pay the costs for the rich and powerfuls fucking problems.  fiats incentive structure is fucked up.


in bitcoin, the more that others thrive the more the rest of us thrive.  for us bitcoiners adoption should be the paramount thing.  if you have all the bitcoins in the world then their not worth a pot to piss in because nobody else has any.  if you are a wholecoiner and bitcoin becomes the world reserve currency you have more than you will ever need.  dragons hoarding treasure is the fiat model (or maybe the gold bug model peter shiff if your reading this).  matter of fact, dragons hoarding treasure is almost antithetical to what we need to be doing.  money need to be spent helping others thereby helping the network, and it aligns with the buddists idea about how being enlightened means being incentivized to help others on a deeply personal level.  bitcoins incentive structure is far superior. 

in summary, i give away bitcoin on the internet  not because im some bleeding heart, liberal guilt, soy boy, its because i believe it benefits me directly to do so, i enjoy it, i think its important, *and* i do it because i believe its going to make me rich as fuck (and the rest of you motherfuckers too), deal with it.

also, maybe someone who knows about buddhism can let me know if i got that concept right.  i may have been going through a heavy marijuana phase when i was reading that stuff.",Bitcoin
"In a perfect world where everyone understands the purpose and strengths of Bitcoin, there should be more of a demand for it when inflation data is undesirable. Are we just not at a place where people understand the implication of Bitcoin?",Bitcoin
"Hi does anyone have experience receiving Ledn yield on deposits for their bitcoin and can you share your experience?  (I may have read the fine print that they are no longer accepting deposits for yield return. But maybe they’ll resume deposits for yield again.)

Given I’m someone who used Blockfi but got out just before it went under, I’m skeptical of yield on my BTC and for good reason. Though I only had a small amount with Blockfi. But some of the people who Ledn advertises with I trust. For instance Robert Breedlove. 

So curious of others thoughts on this topic?

Edit: Next time I’ll add in insightful thoughts… 😆",Bitcoin
Hey was wondering what happens when transactions have the same hash/txid. Just experienced that using Binance to send btc. I sent 3 transaction of same amount at around the same time to same address and now all 3 transactions have the same hash. The recipient platform only received one and I have not yet been credited for the other two transactions. What happens in this case? How common is it to have same hash for 3 different transactions ? Really paranoid at this point.,Bitcoin
"Hey community. We are pleased to announce that now it's possible to buy Bitcoin via our I2P link:

* Trocador.i2p (you need to configure your Browser to access I2P first, of course)

In the case of Trocador, all our products work out of the box also for I2P. This means that anyone can simulate rates via [API](http://trocador.i2p/en/docs/), create trades, check transactions, plus using our [AnonPay Payment Gateway](http://trocador.i2p/en/anonpay/) solutions, or even create invoices with our [Payment Generator](http://trocador.i2p/en/anonpayurlgenerator)... all over I2P tunnels.

Are we the first exchange aggregator to be on I2P? Cool. In any case, we intend to continue pushing for privacy and implementing solutions that benefit our users. We are currently exploring whether there are possibilities that allow us to add decentralized exchanges rates among our own and allow users to transact with us in a way that streamlines the user experience and drives its adoption. We are also looking at ways to provide more pairs to swap with Bitcoin Lightning.

[Trocador.i2p website](https://preview.redd.it/fmf8hr3697ka1.png?width=1204&format=png&auto=webp&v=enabled&s=a82c5ad7cce93daf083b38e6e058118270fd83f8)

***Opportunity:***

We are currently looking for people capable of translating the Trocador website into other languages. Those interested, please let us know via Telegram or E-mail. We will pay 100 USD equivalent in Bitcoin for translation in the following languages:

* Hindi

***Observations:***

When adding our address on i2p, check that our Base32 is correct:

* [http://lpn5pb34rpsee3ycqtjf3vzngpibxsvzx4a3kdc3rmavgpbpclvq.b32.i2p/en/](http://lpn5pb34rpsee3ycqtjf3vzngpibxsvzx4a3kdc3rmavgpbpclvq.b32.i2p/en/)

In case you still have not set up your Browser to navigate I2P websites, you can also check out our integrations over Tor (using Tor Browser or Brave):

* [Website](http://trocadorfyhlu27aefre5u7zri66gudtzdyelymftvr4yjwcxhfaqsid.onion) / [AnonPay Payment Gatway](http://trocadorfyhlu27aefre5u7zri66gudtzdyelymftvr4yjwcxhfaqsid.onion/en/anonpay) / [Payment Generator](http://trocadorfyhlu27aefre5u7zri66gudtzdyelymftvr4yjwcxhfaqsid.onion/en/anonpayurlgenerator)

Or you can also use on Clearnet:

* [Website](https://trocador.app/en/) / [AnonPay Payment Gatway](https://trocador.app/en/anonpay/) / [Payment Generator](https://trocador.app/en/anonpayurlgenerator)

***What we have developed so far on Trocador***

* Privacy preserving exchange aggregator with more than a dozen different exchanges;
* No use of JavaScript in order to prevent device fingerprinting;
* Onion support (and now I2P support);
* Fixed and floating rate swaps;
* Payment mode (reverse swap with fixed rate);
* Development of our API and Referral System;
* AnonPay payment gateway, where stores and websites can monetize their audiences or sell their products;
* Development of our own BTCPay server plugin, where stores can receive any cryptocurrency and automatically swap to Bitcoin;
* Trocador Guarantee: insurance coverage for trades;",Bitcoin
"Hello guys, I got a question because I’m not 100% if I understand it right. On my Trezor I have walllet with kyc coins. If I create new wallet with passphrase can I store my non-kyc coins in there without tainting it while using one seed? Thanks for advices.",Bitcoin
I'm looking for the deepest understanding of Bitcoin and the blockchain and I want the best textbook out there. I'm already very familiar with how bitcoin works and I'd like to bring in a more academic piece of work. Thanks in advance.,Bitcoin
"I have an opportunity to go for free this year so it’s not a question of money, it’s more a question of what do they actually do there? I know they have speakers and talks but what about on the main convention floor? I would love to know people’s experiences good bad or otherwise.",Bitcoin
"I'm not against Bitcoin, just genuinely curious, why would you invest in it? It doesn't produce any value

Edit: thanks for your replies and explanations",Bitcoin
"Hello, 
Blue Wallet will shut down its lightning network custody service on April 30. I have a few sats on the lightning channel, I'm trying to transfer them on Chain without success. Does anyone know how to do? Thank you",Bitcoin
"Fed is showing us that we need Bitcoin in the next decade, understand the basics: [https://youtu.be/4tyuLSGpFp8](https://youtu.be/4tyuLSGpFp8)",Bitcoin
"

Please tell me where you are from and be a realish account and i'll tip you 1k sats

I want to spread the wealth  

Hungary here 

I would probably deposit it for fiat into Hungarian forint because cant keep up with living costs.. but my bank doesnt allow transfers from exchanges and it's probably too small to deposit anyways

So id rather give it to others for adoption",Bitcoin
"The lira dropped -90%.
And other currencies have had much worst depreciation.
Is there anything that would prevent Bitcoin from dropping by 90%, or 99% or even 99.99%?…",Bitcoin
"  
[https://loft.trade/](https://loft.trade/)  


I always wanted to be able to buy some stocks using bitcoin/lightning in a non custodial way. If I for example buy Tesla stock on that app how likely is the chance for a rugpul?  I am not looking to quickly trade and cash out using lightning. I want to buy the stock and keep it for a year(s). When i return to login (noKYC and uses your lightning account only) in a couple of years will it still be there? Is it custodial?  


Very little information on this app so far doesnt help with my trust issues. Any alternatives that are available to global citizens not living in the first world?",Bitcoin
"Something has really exhausted me, please tell me if it is possible to earn from trading without investments, where to take money for the first deposit, I no longer know where to look for this information. I will be grateful.",Bitcoin
" Sorry for the clickbait title. I’m actually a believer in web3 and everything that it stands for. I just wanted to encourage more debate in this topic. As we all know owning our own wallet and keeping our seed phrase safe is core to web3. But this is a daunting task. If you ask people to choose between storing money in a bank which guarantees you money back even if the bank goes bankrupt, or storing money in a seed phrase which mean you would lose all your money if you are careless, I believe most people would choose to keep their money in a traditional bank. People do not like to take responsibility, especially when the risk at stake is losing your wealth. So I am wondering what solutions are the space creating for this user experience issue? Is the solution third party custody? If so then that defeats the whole purpose of web3.",Bitcoin
[https://zionodes.com/blog/cryptocurrency-regulation-a-global-overview](https://zionodes.com/blog/cryptocurrency-regulation-a-global-overview),Bitcoin
"
Fiat withdrawals have been paused for the last two months and they have added restrictions on crypto withdrawals making withdrawals extremely impossible. People are in panic because of this and they are trying to get their funds back. Their twitter and telegram support is filled with withdrawal requests which they are delaying. This needs more attention as people are worried.

They are blaming it on a bank citing temporary issues but users are suspecting that their bank account is frozen. Meanwhile they are accepting Fiat & crypto deposits.

Their crypto withdrawal limits for bitcoin are given below.

 The withdrawal limit for the BTC token minimum per transaction is 0.0005 BTC & the maximum per transaction is 0.015 BTC with a daily limit of 0.03 BTC and a monthly limit of 0.148 BTC for Non-Whitelisted Addresses of KYC Level 1. 

The withdrawal limit for the Whitelisted Address of KYC Level 1 for the BTC token minimum per transaction is 0.0005 BTC & the maximum per transaction is 0.03 BTC with a daily limit of 0.059 BTC and a monthly limit of 0.593 BTC.



On top of this they are charging very high transaction fees from users + 1% TDS on each transactions.",Bitcoin
"Is  this a narrative people still believe? I'm not asking if there is any  truth to it as I believe that cryptocurrencies are too new of an asset  class to be considered a store of value or an inflation hedge. I want to  know if people actually buy into the idea. Doesn't seem like it to me  tbh and if it weren't for rampant speculation and greed Bitcoin would  not be nearly as successful as it is. To be clear, I do like and support  BTC, just being realistic.

[Ray Dalio comments Bitcoin and crypto](https://coinspress.com/hedge-fund-veteran-ray-dalio-believes-bitcoin-is-not-a-good-store-of-wealth/)",Bitcoin
"Hello,

Someone has to own the Bitcoin implementation GitHub repository account and admin role, and maybe also own bitcoin.org domain.... So who is that person?

One more question, who replaced Gavin Andresen? 

Highly appreciate long answers and explanations to make a noob like me learn more... Thank you.",Bitcoin
"The wheels of history have been set in motion the past few years, and there is absolutely no hope of turning back. People are going to need to adapt to Bitcoin or starve to death. Or get drafted to die for whatever global war their country’s international alliance is involved in.

Governments are printing infinite trillions of peasant tokens while their citizens starve and WWIII escalates. Do people really think this is sustainable? The only people loyal to the current system are privileged boomers, the financial industry, those too dumb to understand computers or the decimal system. The people opposed to the system are now smarter and stronger than the ones running it.

Bitcoin is compatible with all political ideologies, especially the extremist ones capable of violence. The concept of being motivated by need rather than greed is central to most religions. This all combines to critical moment for humanity, like the printing press or French Revolution. 

It’s almost as if society is being deliberately pushed toward its breaking point by forces greater than any Government or Central Bank. Perhaps an elusive Japanese gentleman is smirking somewhere knowing what is about to unfold.

Reading this subreddit, it sounds like you all know this anyway, so it doesn’t matter. But more normies are catching on, prepare yourselves.",Bitcoin
"I understand that seed phrases are automatically generated with wallets like Trezor. But I’m wondering if it’s possible to create a safe, cold storage wallet, one way or another, and create my own seed phrase for it. Apologies for the noob question :)",Bitcoin
"Bluewallet is sunsetting their lndhub instance on April 30, 2023 and going fully self-custodial for its Lightning wallet.

If you're using Bluewallet with your own lndhub instance or using other lndhub services like Alby, LNbits, LNtipbot etc., YOU ARE NOT AFFECTED.

If you're using the default lndhub service offered by Bluewallet, just transfer your sats to a different wallet or swap to onchain using [Boltz](https://boltz.exchange/).",Bitcoin
Buddy of mine had been using an exchange for about 2 months before they locked his withdrawals and asked for KYC. Only deposited once and had been swingtrading. The amount was about ~2grand (small amount for him). When he went to withdraw the withdrawals were locked and KYC asked for. I would like to know how this is legal? Even banks have to give you your money if they lock/freeze/delete your account.,Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/119rerz/lightning_thursday_february_23_2023_explore_the/) for unanswered questions.",Bitcoin
"Well I have the orange pill in my mouth and I’m ready to swallow it. Just want to ask, what is the best route to swap all my shitcoins to Bitcoin? Directly swapping them, or swapping to USDC then to Bitcoin? Or another route? Thanks!",Bitcoin
Look so I'm really fresh to this crypto stuff so don't judge me when I tell you I used coinbase I'll switch eventually. For now tho I've been trying to send a wallet money the last day but coinbase has been limiting how much I can send out at one time. How long before this limit increases? Also what are some good wallets I should possibly look into with faster/higher buy/send limits?,Bitcoin
"I told my bank I would like to invest into BTC, but my bank tells me your going to lose all your money, the government will regulate BTC and you will lose a lot of money. 

I told the bank that BTC is the biggest crypto around, if it were to fall the world would be in a crisis because so many things are connected to it. 

Why is my bank telling me this so much. Like woah!

Like they told me “go into bank stocks or buy a property on mortgage” im like but this make me like everyone else and I am just paying the bank ti make you guys rich while the person stays the same! This is crazy!",Bitcoin
"This groundbreaking analysis concludes that lower-Low-income, young males are the most likely to invest in cryptocurrencies. They tend to exhibit ""herd-like behavior,"" investing when prices rise and losing money when they decline; typical behavior during financial bubbles. These latecomers are also the least knowledgeable and least able to withstand losses. These investors also follow a ""hail Mary"" strategy similar to investing in lotteries, making a low probability, high reward bet. Another reason given by crypto investors is the poor outlook for attractive returns on stocks and bonds. #investing #money #cryptocurrencies #ezrazask #investing #financialbubbles #investmentstrategies #digitalassets #digitalmoney  https://www.jpmorganchase.com/institute/research/financial-markets/dynamics-demographics-us-household-crypto-asset-cryptocurrency-use#infographic-text-version-uniqId1677161899801",Bitcoin
"(Software = Hot wallet)

I have tested some apps like BlueWallet, Phoenix, Muun, Relai

but none of them gives me the option to use a passphrase for the wallet (additionally to the seed phrase). Muun is even doing some special stuff without the mnemonic phrases.

BlueWalet allows me to enable s passphrase but it’s for the app, not the wallet itself as far as I see.

Is there a good with a wallet passphrase?",Bitcoin
"Someoneis offering to give  5k dollars for free then made a deal with me to return 60% of it to make sure im trustworthy  but he needs me to open a crypto wallet  so that i can  get the transaction done  is this  safe ? He sent me a testimony of a  client  but im not convinced , is there a chance this could be real  in the future while im working with him  might he scam me in the future ? Is it just a slow bait ?  I dont see any way i would lose  the money if its in my crypto wallet, i can runn offf with the 5k if its real  and if it is real then i can invest my money in the business he is offering, i really want to make sure this is  legit please",Bitcoin
"You're based in the US and not sure how to report your Bitcoin and crypto transactions? We’ve got you covered with our updated crypto tax guide for the US! It covers pretty much everything from capital gains & income tax to mining and lending.

Crypto Tax: The Definitive USA Guide 2023: [https://hub.accointing.com/guides/crypto-taxes-usa](https://hub.accointing.com/guides/crypto-taxes-usa)

Let me know if you need more information on a specific topic!

Cheers, Max from Accointing by Glassnode

https://preview.redd.it/shld757h50ka1.png?width=1200&format=png&auto=webp&v=enabled&s=cf25ec354f8f27ca8863e695ba1765ff66f0783b",Bitcoin
Did anyone who wasn’t that well off or have a great job make it to millionaire status or almost there. Would be great to hear your stories,Bitcoin
"Explore More in Surfside Beach, Tx will accept bitcoin for payment. Check them out if your in the area.  Exploremoresurfsidebeachtx.com",Bitcoin
"I started a new job and benefits won't kick in until May.

Has anyone had any experience with [Crowdhealth](https://www.joincrowdhealth.com/promotions/breedlove)? Wondering if it might be a good option for my family and has been a sponsor of Robert Breedlove's ""What is Money"" show.  


I still need to finish the podcast [here](https://www.youtube.com/watch?v=4rIZ55xqFGc&t=14s) but get the basic idea of it and would be very interested how the experience has been for any Bitcoiners.

I couldn't find anything in this subreddit via search",Bitcoin
"I've been trying for hours to find out how. I have $173 worth. I downloaded coinbase, then coinbase wallet. Transfered my bitcoin from Swan to it. What do I do?",Bitcoin
"The world is currently experiencing a dangerous situation with the invasion of Ukraine by Russia, and the threat of a potential Nuclear war looms. However, this situation is not just about political tensions and military strategy; it also involves the role of central banks, global multinational corporations and the rise of alternative currencies like Bitcoin.  It is no secret that central banks have played a significant role in shaping the global economy, their existence has even been linked to the occurrence of World Wars. 

Many people are starting to see the current financial system as flawed. The availability of public blockchain technology poses a threat that could make them redundant virtually overnight. Financial institutions make money by lending it. Governments borrow more money for war than anything else. Financial institutions make money by lending money, and they make more money from wars than anything else. 

No other government has been placed under a larger sanctions package than Russia. Despite these sanctions, the Russian economy has grown. Not only has the Russian economy grown; they're now selling oil for Rubles, and have created a gold-backed Ruble; the once-failed aspirations of Sadam and Gaddafi. This was made possible after Putin publicly claimed that he was exploring the possibility of selling Russian oil for Bitcoin. Could oil and gold-backed Rubles deter nation-states from following the likes of El Salvador, Japan, Germany, Malta, and others in making Bitcoin legal provisions and legitimacy? Would adding gold-backed rubles to their reserves, be a more attractive alternative to hedging dollars with Bitcoin? Private financial multinationals have offices across multiple jurisdictions that can circumvent government sanctions indirectly for Russian interests.

Based on Russia's own economic interests, the primary objective of Russia's invasion of Ukraine is to make Ukraine's admission into Europe/Nato as expensive as possible. In a similar vein to what Nato did to the Societ Union. He can achieve this by destroying its industrial and agricultural assets. Ukraine poses a significant threat to Russia's monopoly on natural gas reserves in the European continent. If Ukraine joins the EU and the EU funds the extraction of their gas, this would destroy Putin's monopoly on it. Additionally, Ukraine competes with Russia on many other major global products, like wheat for example, which would be a huge boon to the EU if Ukraine joined its free trade zone and single market.   

Russia lacks ports with access to international waterways all year round, as its northern sea border is frozen over for most of the year. This is the main reason why they annexed/invaded Crimea in 2014 and built a naval port in Syria. They now control East Ukraine and a land bridge from Russia to the port of Crimea in Ukraine. Giving mainland Russia and the Russian Navy vital access to international waterways all year round.  

When we look at their weapons industry; in contrast to the West, Russia and even China have used their military spending (less than 1/10 of what the US spends per year) on state-of-the-art, a-symmetrical technology like hypersonic missiles and SAM systems. Russia has been leading space and rocketry technology since the Space Shuttle was decommissioned; NASA has been paying Roscosmos to do ISS missions for them for over a decade. Russia used this funding to increase its technical knowledge and has since achieved several publicly documented pioneering technologies, demonstrating a capacity to fire MIRVs over longer distances than previously stated, the ability to take out orbiting satellites, and their proliferation of SAM systems.  

The invasion of Ukraine is extremely expensive, both for Putin and the rest of the world, global inflation is skyrocketing. Governments are taking out huge loans to provide military aid, and Ukraine is getting destroyed. The situation in Ukraine is complex, and it involves political tensions, military strategy, and the role of lending institutions. As we move forward, we must be aware of the potential impact of financial institutions and their fear of alternative currencies, places on global politics and economics. The invasion of Ukraine is just one example of the dangerous consequences that can result from the intersection of these issues.",Bitcoin
"Minnesota Congressman Tom Emmer announced on Twitter that he and a few others are going to introduce a bill that would in his words aim to do the following three things:

1. Prohibit the Fed from issuing a CBDC directly to anyone. 
2. Bar the Fed from using a CBDC to implement monetary policy and control the economy. 
3. Require the Fed's CBDC projects to be transparent to Congress and the American people.

The discussion and trajectory around this bill is going to be a historic event for our financial and political infrastructure not just in the US but across the world for decades to come. Expressing the importance for privacy, he states that ""any digital version of the dollar must uphold our American values of privacy, individual sovereignty, and free market competitiveness. Anything less opens the door to the development of a dangerous surveillance tool.""

From the view I have of the US government, I would be very surprised and thrilled if any version of this bill passes let alone in its current form. Many powerful governments around the world including UK, India and China have already started piloting their in-house CBDCs and are on path for rampant scaling.

CBDCs could potentially be the strongest tool any government has EVER had at its disposal in the history of humanity and this will have shattering implications for all aspects of our lives. The current state of the financial infrastructure is already such that every transaction of ours is monitored in the name of anti-money laundering and tax evasion laws. Should we let them look at all of what we do with our money just because some people out there could be laundering money, evading taxes or performing some illicit activities? Those involved in such activities are already a step ahead and this is also why these people have embraced bitcoin earlier than other segments of population. Despite their illegitimate reasons, the 3 key features for which these shady guys have embraced bitcoin way ahead of the general population are noteworthy - transactions without a central authority/intermediary, decentralization based on cryptographic proof instead of trust and non-kyc/anonymous wallets that preserve privacy.

Governments and private banking institutions have already been monitoring our financial lives and all other aspects that associate with it for almost half a century now and have a dataset whose richness and usefulness has been improving dramatically with the advancement of web and technology over the last few years. I'm scared about the possibility of how much this is likely going to improve over the next decade. Unless we actively choose to not be a part of this system and make efforts to tackle and discourage the existing financial and web infrastructure centered around monitoring, our privacy is going to be lost forever.

CBDCs would be the last nail in the coffin that we should anyhow avoid and the most prominent way we can do this is by adopting bitcoin not just for the big gains but for the values it embraces with a privatised and predetermined monetary policy that is neutral in its intent by code for a cherry on top. Not just monitoring, it is really important that we disempower the government for controlling the value (money) that we spend all our lives to create. They print it out of thin air and devalue it even with other means at their whims to suit their budget and economic interests. We are in a debt bubble whose existence is primarily only because of the nature of money in its current form. Governments have debt that are multiples of their nation's GDP and corporations largely function on and expand with debt while I'm not able to loan out a year equivalent of my earnings while all of our money is lend out to them atleast 4 times over. How and why are we okay with this? Prices keep rising because of the current monetary, debt and growth environment and we have to keep working harder just so we can sustain our standard of living? Real median income of middle class households have been on a decline while the average per capita has been growing. To me, that is nowhere close to an ideal economic, social and political state and far on the other end of the spectrum. We're doing poorly on all these fronts.

Bitcoin is truly the revolution we need because it disempowers monetary shenanigans, debt growth and government control. We employ the government (atleast in a democracy) and need to keep them out of our business and ask them to focus on what we as a society want them to do.

Governments have enough money from the taxes they collect, money they print and other magic monetary and fiscal tools they have at their disposal. At the very least, we need to cut out the monitoring for which they have no good reason other than to empower themselves and have us as slaves to their intent. Bitcoin gives us a chance to do this along with a whip on other overreaches that governments have over economic and social structures as well as our personal lives.

I'm writing my thoughts down with the hope that we as a community can initiate a serious discussion around the bill tabled, overreaches of the government and the importance of bitcoin so that it's not just the Congress whose intent controls the narrative. Or atleast so that we're clear of the differences and know the areas we need to work on to get the government to do what we expect of them.

Governments have already succeeded so far in their fight against bitcoin especially with the way they've got Centralized exchanges to dominate the ecosystem and comply with anti-money laundering laws. I've got thoughts about this I'd like to discuss which I'm keeping for another post so that this one doesn't end up being a soup. Hoping to create a series of posts discussing the developments from the bill tabled and other thoughts I have about the relevant issues.

I really hope bitcoin comes out on the winning side a few years/decade down the line and that would only happen if all of us already involved with crypto take up a discussion and talk about the merits with our friends and family. As much as this sub discourages crypto talk in real life, that is the only way we can move foward. Bitcoin is in such a speculative phase that most of its merits are overshadowed in the social narrative by its volatility and hopes for gains/get rich quick scheme. I think all of us need to take responsibility to change that narrative and make efforts to talk about other aspects of bitcoin  with friends and family as much as that comes with the shame/guilt of being deep in the red.

Long bitcoin, short the bankers and government-controlled magic money.",Bitcoin
"So many Bitcoin holders and critics do nothing more then watching and discussing Bitcoin's price. The holders have enough arguments why it will be more valueable over time because of basic economic theories. While critics have there own tulip comparisson theories.

While everyone is discussing if a one million dollar Bitcoin will be feassible one day. **The primary assumption/thinking error (for most) is that economic circumstances won't change.** After the news of the burning banks and collapsed currency in Lebanon. It really struck me. Bitcoin has become one of the if not the most valuable assets in the country without a change in global sentiment. We are still 70% away from ATH, just chilling at a possible bearmarket bottom in dollars. But in this little country a Bitcoin Holder is wealthier then ever.

Bitcoin doesnt have to win by appreciating in price in normal market conditions, it will simply win with the collapse of a FIAT currency. In other words: **Bitcoin will win by simply not losing.**

People love to fantasize about what needs to happen for Bitcoin to become more valueable then it already is, like mass-adoption or another country embrassing it as legal tender. But very little people think about what Bitcoin mustn't do. Which is failing. **If it doesn't fail**, **the cause effect sooner or later will be an astronomical rise in value.**

Bitcoin's succes and therefore it's price rise will be a result of a failing a system. This makes arguing about today's price, and where it will go under what cicrcumstances so stupid.

TLDR; I just realized that if Bitcoin simply doesn't fail, a rise in value is inevitable. Fiat failing in the long or short term will be the cause-effect of exponential value growth. Not mass adoption or an ETF acceptance or the next halving.",Bitcoin
"Hi guys, I am from Venezuela and, looking to buy bitcoin for hdling, the thing is that almost every people that I know have their bitcoin in Binance so when I ask them to pay me the fees for taking the bitcoin out of the app -binance- are way too high!

So, is anybody from Venezuela here? or can someone give any alternative here?",Bitcoin
"No need to panic, but if you are using the Blue Wallet to store your satoshis, you have to move them by April 30th.

Doesn't affect their non-custodial onchain Bitcoin wallet.

Interesting to see this company pivot to more self-custody. 

https://bluewallet.io/sunsetting-lndhub/",Bitcoin
"When the government rolls out its DIDs and uses them to prevent my ISP from allowing me to transfer bitcoin over the internet, how will I continue to participate in the bitcoin market?",Bitcoin
How would you proceed in order to be successful? Which way would you go? What resources would you use? Do you have any suggestions or tips? Appreciate any inspiration!,Bitcoin
"The lightning network is a second-layer solution on top of the Bitcoin blockchain that enables quick, cheap and scalable Bitcoin payments.

Here is the place to discuss and learn more about lightning!

Ask your questions about lightning

Provide reviews, feedback, comparisons of LN apps, services, websites etc

Learn about new LN features, development, apps

Link to good quality resources (articles, wikis etc)

Resources:

* Here is an awesome list of resources compiled by Jameson Lopp: https://www.lopp.net/lightning-information.html

* Want to test out your lightning fire power? tip the Bitcoin devs! https://bitcoindevlist.com/

* Previous threads: [Search](https://www.reddit.com/r/Bitcoin/search?q=Lightning+Thursday&restrict_sr=on&include_over_18=on&sort=new&t=all)

* Lnbook getting closer towards being finished and can already be seen at: https://github.com/lnbook/lnbook

* Lightning Dedicated YouTube Channel: https://youtube.com/renepickhardt

* Also there is the playlist by chaincode labs: https://youtube.com/playlist?list=PLpLH33TRghT17_U3as2P3vHfAGL8pSOOY

* Lightning stores: https://www.lightningnetworkstores.com/

* Learn more and talk about lightning right here in r/Bitcoin, r/bitcoinbeginners, r/thelightningnetwork, and [the r/Bitcoin chat](https://discord.gg/qE3rWBRNqh)",Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/118qoc4/daily_discussion_february_22_2023/) for unanswered questions.",Bitcoin
"Any decentralized way to leave it to someone? I heard of multi sig, but that’s still open to manipulation isn’t it? Like someone won’t sign the transaction unless they get half or something stupid like that.",Bitcoin
"I have to decide whether to run my bitcoin core node on tor or on the clearnet, but ive heard its dangerous to run it with tor so im stuck not knowing what to do.",Bitcoin
"1. The essence of Technology can be defined as Technics.
2. Technics is a force that strives to achieve goals through the deployment of means in a way that is always more efficient, always faster, often cheaper.
3. Bitcoin is one of the purest manifestation of Technics (**even purer than AI**, right now).

Do you agree?

More context on Technics here: [https://youtu.be/AuRbHzvDC7Q?t=712](https://youtu.be/AuRbHzvDC7Q?t=712)",Bitcoin
"Every time I try and use my cash app bitcoin address, it says it's invalid. Same with my second account and my boyfriend's. Never had this issue before...thoughts??",Bitcoin
"Article or piece of literature that explains in simple terms the significance of bitcoin, what would it be.

Is there something that you’ve read, where you go “I get it now.” I see why there is so much interest in bitcoin.",Bitcoin
"Hi guys, sorry if this is a really dumb question. I wanted to know, since bitcoin generally trends upward over time, do the transactions also decrease in number of satoshis cost per transaction on average? Say for example, bitcoin rises more than 10k this year. The sats have increased in value relative to the US dollar. Will the same number of satoshis be required for a transaction as that transaction would have previously cost when it was 10k cheaper? Or does the number of satoshis used in a transaction decrease with the increasing cost of bitcoin? Today, my transaction cost is about 2000 sats for a medium-priority transaction. At the end of this year, if bitcoin goes up, will that same medium-priority transaction require the same number of sats or less?",Bitcoin
"Im having troubles figuring out what i should use, if a native segwit bitcoin account or a taproot bitcoin account. I know they says taproot is better for privacy and its an overall upgrade, but im wondering why the majority of people use native segwit still if taproot is an overall upgrade? I was hoping this awesome community could enlighten me.",Bitcoin
"Where can I find a live view of active nodes on the network? 

Also, does anyone know where I can find a 24h change, 1 week change, etc change in how the number of node operators fluctuates? I explained that horribly but hope you understand what I’m looking for. 

Thanks 🙏🏼",Bitcoin
"So I borrowed a big amount of money from my dad 1 week ago to spend it for bitcoin. I was kinda upset that prices went up a few hours after I could buy bitcoin. My father noticed that I was constantly checking for the prices to move again and I was hesitating to buy. He told me not too worry too much about, if I dont buy and prices go up Im unhappy, if I buy and prices go down I will be unhappy and if I buy and prices go up I will be unhappy I will be unhappy I didnt spend more. It doesnt matter what you're doing you can always be unhappy. Just buy and HODL. The Important thing is to own bitcoin now. Halving is coming.",Bitcoin
"Hi yall.

Recently there was a post the other day (couldn't find it again) where the author discussed the topic of BTC stability once the fixed block reward become marginal/zero. After reading their post I became curious and looked for more information on the topic. I happened to run into an article written by Pinceton researchers titled ""On the Instability of Bitcoin Without the Block Reward""

I would attach the link but I don't want to get flagged for it. Just google the article name it'll be the first one to come up.

Anyways: after reading through the article I was definitely shocked by some of the rational miner activity that could occur during that future period. Obviously, no one knows the future and we are very far away from this being a concern, but IMO some of the problems that could face BTC seem legitimately concerning. I encourage the curious and brave to read the abstract and intro (at least) of this article and formulate some opinions of your own. The subject material is very interesting.

I would love to hear what you guys think of this research and if their concerns/findings are valid. Personally I'd love to hear some people squash this as FUD, because it definitely has me question some things. Plz let me know your thoughts.

&#x200B;

I made a summary of the abstract and introduction and this is my objective summary of those respective sections

""This paper explores the impact of transaction fees on the security and performance of Bitcoin, particularly as the block reward dwindles over time. While the assumption has been that miners being paid through block rewards or transaction fees does not affect the security of the blockchain, the paper argues that this is not the case. With only transaction fees, the variance of the block reward becomes very high due to the exponentially distributed block arrival time, making it attractive for miners to fork a ""wealthy"" block to ""steal"" the rewards therein. This results in an equilibrium with undesirable properties for Bitcoin's security and performance, and even non-equilibria in some circumstances. The paper also revisits selfish mining and shows that it can be made profitable for a miner with an arbitrarily low hash power share, who is poorly connected within the network.

The paper discusses the implications of these findings for the future security of Bitcoin and draws lessons for the design of new cryptocurrencies. It provides an illustrative example of how rational, self-interested miners could behave in a block chain with exponentially distributed rewards, revealing novel incentive issues that do not arise when block rewards are fixed. The paper argues that there are new and surprising incentive issues in a transaction-fee regime, even assuming that transactions arrive at a steady rate.

The paper's contributions include a mining strategy simulator to fill gaps and confirm theoretical results, as well as a discussion of potential solutions to the incentive issues raised. The paper suggests a variety of possible solutions. These solutions include changes to the protocol, such as increasing block sizes or adjusting fees, or creating alternative cryptocurrencies with different incentive structures.

Overall, the paper argues that the assumption that block rewards and transaction fees do not affect the security of the blockchain is incorrect and that further research is necessary to address the incentive issues raised in a transaction-fee regime.

Main Points:

* Transaction fees have a significant impact on the security and performance of Bitcoin as the block reward dwindles over time
* The variance of the block reward becomes very high with only transaction fees, making it attractive for miners to fork a ""wealthy"" block to ""steal"" the rewards therein
* Selfish mining can be profitable for a miner with an arbitrarily low hash power share and who is poorly connected within the network
* The paper suggests possible solutions, including changes to the protocol or creating alternative cryptocurrencies with different incentive structures""

That is all: keep stacking and tell me your thoughts por favor",Bitcoin
"Notice that the next halving keeps moving up?  A couple years ago, it was projected to be around late May 2024.  A year ago, it was closer to mid May.  A few months ago, it was early May, and today, it's all the way back to April 29th!

Why is this happening?  Basically, the hash rate of the network just keeps going up because better, more efficient ASICs keep getting manufactured.  The Antminer S19 XP for example is 47% more efficient than the Antminer S19, and is now widely available, you can get one on eBay.  As the hash rate increases, the block times average less than 10 minutes.  Because the difficulty adjustment only happens once every 2 weeks, it can go for as long as 2 weeks with shorter blocks before it adjusts and gets the average back to 10 minutes.  But because the hash rate just keeps growing and growing, the difficulty adjustment is constantly playing ""catch up"" and the block times are constantly averaging less than 10 minutes over the years.  This moves the halvings back because each halving is exactly 210,000 blocks apart, which implies exactly 1,458 1/3 days if each block is exactly 10 minutes.  But every 4 years has 1,461 days (assuming a leap year, some 4 year periods would be 1,460 days as they don't have a leap year), so even if it was 10 minutes per block, the halvings would move back by 2 2/3 days per halving.  However, note the movement historically:

Genesis block:  January 3, 2009

Halving 1:  November 28, 2012                       Days since Genesis Block:  1,425        Diff from 1,461:  -36

Halving 2:  July 9, 2016                                      Days since Halving 1:          1,319        Diff from 1,461: -142

Halving 3:  May 11, 2020                                  Days since Halving 2:           1,402        Diff from 1,461:  -59

Halving 4 (projected):  April 29, 2024             Days since Halving 3:           1,449        Diff from 1,461:  -12

So, as you can see, it was the period from 2012-2016 when the network hash rate just grew like crazy, and moved the 2nd halving back by an astounding 142 days, or about 4 2/3 months.  Since then the movement has been less, almost kind of starting to stabilize.  Part of that is because of the China ban on mining, which threw the hash rate off for quite a while and caused the difficulty adjustment to revise downward, which is why the latest halving is only a few days up from the 3rd halving.

Anyways, my point is, when you look at the next halving approaching, just keep in mind that the number of days are NOT set in stone, it's the number of blocks.  And that depends on the (ever-increasing) hash rate and difficulty level.  It all means, that the last Bitcoin will NOT be minted in 2140, but likely many years sooner.

Stack on and hodl!!!

EDIT 1:  Changed the amount at ""moved the 2nd halving back by an astounding 142 days, or about **4** 2/3 months...""  Originally it said **3** 2/3.  ",Bitcoin
"Everyone hates Keynes, but even he was against gold being centralized in bank vaults. Let's not repeat this mistake with #Bitcoin. Self-custody or slavery. No CBDCs.  


https://preview.redd.it/i1yizwkuxrja1.jpg?width=2048&format=pjpg&auto=webp&v=enabled&s=d77fc81546243a76bcb6e04228ad794c7f4390e2",Bitcoin
"[Bitcoin Illegitimate/A Novelty](https://medium.com/eaglebrook-advisors/common-bitcoin-objections-illegitimate-a-novelty-e9aad7021c95)

Seems like bitcoin is still in that category for a lot of people.

How much of the aversion to bitcoin is due to it being new vs. it not actually fulfilling its potential role of money?

There seems to be a reluctance to bitcoin just because it is new. Similar to the internet. That has to be part of it. People don't even take a hard look at bitcoin.

That's not to say bitcoin will successfully see mass adoption, but it is surprising that there is a contingent still ignoring bitcoin.",Bitcoin
"I am thinking of buying another Ledger, loading it with my seed phrase, and giving it to my kid along with my scrambled seed phrase. I would then mail by registered mail, the Ledger password and key to unscramble my seed phrase, in care of my kid, to one of my siblings with the instruction to give my kid the parcel should something happen to me. Unless they both conspire against me, neither will have the ability to steal from me (not that I am worried), lose my btc, or comprise my security.

I don't care what anybody says, I keep my seed phrase scrambled according to my secret key, which currently exists only in my head. A 24 word seed has 6.2x10^23 permutations, so good luck trying to break that assuming you find it and recognize what it is.

What can go wrong with my plan?",Bitcoin
"Brand new to Bitcoin and need some help please.

Hi everyone. I am new to Bitcoin and could really use some help here. I  invested a little money in Bitcoin about a year ago and it has just been sitting there I guess. I invested it in Bitcoin mining. I received a message through my Google Drive reminding me of my Bitcoin and stating I need to do something with it if I don't log in soon it will be taken. Is this true? Is someone hacking me? Or is this a legit process to cashing it out? Any help/advice/thoughts would be very much appreciated.",Bitcoin
"The world's most unique, beautiful and creative Bitcoin experience is taken place from 17-19 March in Bergen, Norway: [Northern Lightning 2023](https://bitcoincoffee.shop/billettsentral/) ⚡

A full weekend of brilliant speakers, epic parties and hands-on experience with Bitcoin will bring together the best bitcoiners from across Scandinavia and the World!

Join us for this [special ENGLISH live broadcast of Scandinavia's #1 Bitcoin podcast](https://www.youtube.com/watch?v=Y8gahQY-N4A) to learn everything about Northern Lightning 2023.

We will also announce a new speaker which we are very excited to share with everyone.

[Bitcoin for Breakfast.](https://preview.redd.it/mdejheripoja1.png?width=1280&format=png&auto=webp&v=enabled&s=7c3f08c58cfc42adfe8b9fd8e13d8df02278e834)",Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/117w61f/daily_discussion_february_21_2023/) for unanswered questions.",Bitcoin
SEC is trying so hard to regulate crypto.. but honestly I think BTC is just to powerful. SEC cannot regulate BTC especially now that we have a country using it as a currency.. SEC may try and say BTC is a currency.. what a load.,Bitcoin
"Hi, I am open for receiving bitcoin donations, BUT the question is not that i would like to receive donations, but the question is how i must proceed to do so... I am just curious and investigating on a self educating way. I am happy if some people do not flag this post for inappropriate, because with that you sign that yourself follow the idea of censorship :) Have a lovely day all of you.",Bitcoin
"It is a risk on speculative technology play based around its potential to become the worlds payment rails disrupting banks SWIFT visa and mastercard.

It is a risk off safe haven store of value play based around its sound money characteristics and scarce supply disrupting gold as the superior safe haven asset.

During times of risk on speculation bitcoin rises alongside technology stocks and during times of risk off recession, financial crisis, high inflation and market chaos bitcoin will rise along side gold.

We know how well bitcoin performs in a risk on speculative mania like 2013 2017 and 2021 but we do not yet know how well it will perform as a superior form of gold, and it is my thinking that its safe haven gains will be far greater than any of its speculative gains seen so far. A safe haven bull run could see gold double to a $25 trillion market cap and bitcoin match golds market cap putting bitcoin at $1.25 million per coin.

There is no other asset on earth that has the potential to be both a risk on speculative asset and a risk off safe haven asset simultaneously. This is unprecedented in terms of global asset classes and goes to show that bitcoin is in a league of its own.

If the world recovers bitcoin will benefit. If the world falls apart bitcoin will benefit. For this reason there is no better asset to own.",Bitcoin
"Does anyone have a list of on-line ammo retailers that accept bitcoin? Bidens gonna poke the bear & ignite a hot war, just trying to get stocked up before a Wolverines redux starts.",Bitcoin
"Hi there - after searching for hours online, I am at a lost of how to track down if a paper wallet was transferred or still on the account (using Exodus).

TLDR - Parents put cash into ATM (i.e., went to bank, withdrew money, deposited it into the ATM, ATM printed the paper wallet), printed paper wallet, sent pic (public key and private key) to scammer. Can I either stop, trace, or transfer before the scammer?

I am using blockchair, blockcypher, and blockchain to view any transactions for the address. It says the public address has 0 BTC but the private address has 0.5 BTC. I looked at the transaction hash, and it looks like the BTC is still there?

I am trying to use exodus to scan the private key QR code but it keeps coming up with ""Unable to add BTC - No BTC to move from the address associated with the private key.""

Already had them report it to the FTC and contact the bank tomorrow to see if they can do anything, but any help (literally any) would be greatly appreciated. It is a considerable amount of money from their retirement.

\-------

More details: The scammer texted parents QR code that turns out is a BTC address (also with 0 transactions).

Downloaded Electrum and checking address: Type = Receiving, Address = Public Address (19.......), Label = Blank, Balance = 0., TX = 0. However, the Public address on the receipt is not the same as the Wallet address (Address 12SKnvw52NjWrSfNy95smARfqBEU9Fc2d2)

\-------

Mempool:

Total Received 0.5 BTC

Total Spent 0.00 BTC

Balance 0.5 BTC

1 of 1 transaction (hash) 2023-02-21 18:53

\-------

Blockchain Explorer

Summary: This address has transacted 1 times on the Bitcoin blockchain. It has received a total of 0.5 BTC and has sent a total of 0.00000000 BTC $0.00 The current value of this address is 0.5 BTC.

Total Received: 0.5 BTC

Total Sent: 0.00 BTC

Total Volume: 0.5 BTC

Transactions: 1",Bitcoin
"
I have recently started my own node and am wondering if someone could explain what’s the difference between wallets and channels. I get how to create channels and all the liquidity issues involved. I’m just wondering how multiple wallets use those channels. I know I’m missing something simple.",Bitcoin
"From a [2016 piece from the Financial Times](https://www.ft.com/content/daa52688-db6a-385e-a313-ef9f3cb6e36e). There will be some opinion injected along the way.

>Moreover, there is a bigger issue still being missed here. **The petrodollar story is in fact a eurodollar story, always was and always will be.**

If fact... there are no Petrodollars. 

When William Simon (formerly of Salomon Brothers), negotiated with Saudi Arabia, he merely arranged a public relations situation that would disguise Saudi US treasury purchases. Most oil was *already priced in dollars*, as USD was already the global medium by 1960. A Salomon Brothers banker would have been deeply familiar with the advent of modern banking at the time.

Treasuries had already become prime collateral by the 70's. Saudi Arabia was merely pointed towards a pre-existing global banking system... *by a banker* who just so happened to be working for the government at the time. 

Oil could, and had been occasionally priced in other denominations; but the most accepted medium remained USD... only rivaled by the pound pre-1950s. Attempting to price oil in any other denomination *would have limited the access to buyers* as no other denomination had the depth of liquidity to facilitate global trade.

>And that’s important because… **eurodollars are in effect nothing more than the original bitcoin**. A free-float of dollar-denominated claims **whose fractioning is out of the control of the Federal Reserve system or the state**, and which is instead supported by the combination of growing US consumption of foreign goods, services and commodities as well as US speculative investment abroad, in exchange for consumption claims on its own system today.

What a statement there, eh? (As a note: I don't fully agree with that first bolded section)

And again, eurodollars *are dollars*. The dollar is a global unit. 

>That’s a complicated way of saying: **""US energy dependency has forced the superpower to share a significant chunk of its wealth with the rest of the world for decades now**, a form of underwriting by the richest state in the world which has increased global interconnectedness, global trade and global growth in general.

Yes. Being the global reserve, means allowing the globe some degree of control over the medium of exchange. That control is mostly in the hands of the global banking system. 

>And it is this foregone stake which has **lubricated the global liquidity system ever since the eurodollar story first began in the 1960s**. It’s also what has empowered the build up of significant foreign current account surpluses, which in themselves represented gigantic shock-absorbing balance sheets for global supply and demand mismatches.

Began in the 1950s, maybe slightly earlier. Again, this is the global banking system. 

>**The unfortunate irony seems to be that the Fed is tightening policy into a dollar bank run (in Eurodollars, i.e. offshore dollars), while believing that its domestic economy is stronger than it really is**, and **as banks become less and less willing to extend Eurodollar funding as they constrain balance sheets due to regulation/risk aversion**. 

The Fed does not realize the extent of the global dollar system, preferring a narrow view that matches its toolkit and mandate. 

Risk aversion as a topic could be a whole other post. 

>In the meantime, however, **we have the clear and present danger from more than US$10 trillion of offshore (Euro) dollar debt riddled with currency, maturity and, in some cases, commodity mismatches**. Brazil’s currency has been decimated and its economy is in recession. China is now the focal point of this problem, with in excess of US$1.1 trillion of dollar debt and a further US$0.6-0.9 trillion in Hong Kong (a decent slug of which will have been lent to Chinese corporates).

It's more than $10T now. 

And there's a graphic to portray the balance sheet mechanics of [the issue](https://www.ft.com/__origami/service/image/v2/images/raw/http%3A%2F%2Fftalphaville.ft.com%2Ffiles%2F2016%2F01%2FScreen-Shot-2016-01-25-at-11.38.45.png?fit=scale-down&source=next&width=344) ...which may give further context to monetary problems in China today. 

That's enough for now. Petrodollars are just global dollars. Most commodities are priced in USD.",Bitcoin
I am a beginner in development in bitcoin and want to use regtest network . For that i need bitcoind and bitcoin core but all the tutorials for downloading bitcoin core refer to download entire bitcoin core and my device has space constraints. Can anybody suggest any roundabout way?,Bitcoin
"Hi guys,

I've just finished reading the Bitcoin Standard and I'm willing to invest in Bitcoin. I'm looking for education content regarding:. how to invest in bitcoin. risks. exchanges, wallets. etc..

Can you help on that?

Thank you!!

&#x200B;

EDIT: Thank you guys for all the answers. I'll dig a bit more on the links you shared before investing",Bitcoin
I am traveling to Portugal in the next few months and would like to know if there are any local restaurants or bars that accept lightning or Bitcoin payments. Would love to give them my business if at all possible while touring the city.  Any and all recommendations are welcome.,Bitcoin
"[Bitcoin block 666666](https://mempool.space/block/0000000000000000000b7b8574bc6fd285825ec2dbcbeca149121fc05b0c828c) had been mined a little over 2 years ago. Not that it has any significance outside of the Bitcoin monetary policy not having changed in 777777 blocks.

At an average of 10 minutes per block it will take roughly another **16.5 Hours** until we get there.

Tick tock, next block...

In HEX, #777777 is a 46.7% gray in RGB color spectrum.",Bitcoin
"I am still learning about bitcoin, if anything I say is wrong please correct me.

After the last halving we saw a major bullrun. 6k to 69k. Optimistically we could expect then, assuming a bottom is around 16k, a ~180k price. Or even higher. 

However even though a halving reduces rewards by 50%. That 50% value is 50% as impactful as the last cut.

50 btc -> 25 btc -> 12.5 btc -> 6.25 btc … etc

This should result in a increasing concave down curve in bitcoin price. Which I believe is well modeled by resources such as the bitcoin rainbow. 

This means that likely the halving of bitcoin will result in a far lower return for those who hope to sell at the top.

I believe that the reason for the large bull run previously is mass distrust of government and fiat currency as a whole.

Your probably saying “well no shit”. The importance of this observation is to understand that the key to long term price appreciation of bitcoin is actually mass adaptation and not the capped bitcoin supply. 

Unlike the limited supply, adaptation of bitcoin is not mathematically guaranteed. There are many obstacles we still have to overcome. Such as ease of access and government regulation. We have made great strides, such as lightning network.

time in the market > timing the market",Bitcoin
"A new financial world order is somewhere over the horizon. It’s coming like it or not, The mantra of crypto is a revolution in money, but revolutions aren’t peaceful. 

The end of the dollar will have major implications for the United States and it’s Allies. The United States will no longer be able to run massive deficits, it’s Allies will be no longer able to depend on aid that the us provides. The centrally planned society will break apart due to lack of funding. 

The federal government funds everything from healthcare to energy to the military corporations to education and scientific research. 

The subsidies that the United States provides  to the us economy is staggering, “I want people to realize that a lot of people depend on the government for their employment.”

A bitcoin standard is an admission of defeat, the realization that printing money never works, and Marxism never solves the problems it states capitalism creates. 

Decentralization is the fundamental thesis on which America was built on, (which is absolute power corrupts absolutely)

I don’t like to use the word “crash” so I’ll say restructuring. Governments are soon gonna have to make hard decisions. Debt to GDP levels have become a national security threat. We are going to have to cut entitlements. reduce spending. Raise taxes. Balance the budget. Reduce energy consumption(see peak oil)

this will result in a downtrend in the stock market. Because a reduction in spending leads to a reduction of economic activity.  

We might see a depression who knows?
(Unless they cancel the debt, which isn’t a politically viable option. We seen what happened with student loans) so maybe a default? 

Regardless we are going to see pain. (And war possibly) 

But a new era will arise. A fair more equitable future. A system that holds people to account. Blockchain technology adds transparency. Stops mindless servitude and tyranny. 

Maybe the crypto anarchist were right all along. “You have nothing to lose but barbed wire fences”

Bitcoin will set us free.",Bitcoin
">Hello,  We have canceled your order and voided the Amazon gift card balance used in the purchase.  We took these actions because you tried to use [Amazon.com](http://amazon.com/) gift cards that are in violation of our terms and conditions. We cannot reissue the [Amazon.com](http://amazon.com/) gift cards or reimburse you for these funds. Please note that if you purchased your Amazon gift card from a retailer, or obtained it from a third party, they won't be able to reissue the gift cards or reimburse you for these funds.  If this activity continues, we may take a permanent action on your account.  If you believe we took this action in error, please reply with an attachment of the gift card purchase receipts or other documentation of gift card ownership such as an email or sms from the sender of the gift card, or and image of the physical gift card. When sharing the document, be sure that the name, address, payment type, and relevant transaction information are clearly visible.  You can find more information on the [Amazon.com](http://amazon.com/) Gift Card Terms and Conditions help page: [www.amazon.com/gc-legal](http://www.amazon.com/gc-legal)  Account Specialist [Amazon.com](http://amazon.com/) [https://www.amazon.com](https://www.amazon.com/)

&#x200B;

&#x200B;

i had this strike agianst my account",Bitcoin
friend got a wallet on ordswap and it gives out a ~~65~~ 64 character key but apparently he only copied the first 50 characters by accident.   I feel like missing 14/64 characters is just way too many to be feasible to brute force but what do you think?,Bitcoin
"Just realizing this today as potentially millions of American workers had their paychecks delayed due to holidays that most other people do not get a day off for.

Makes me wonder how much interest banks earn on our money while they are on ‘holiday’.

I wonder what would happen if, just for each bank holiday, every checking and savings account in America was withdrawn, just for the holiday. Bet that would be the last bank holiday ever. 

Let’s make every day a crypto HODL-day!",Bitcoin
"Hey r/Bitcoin community,

I know this may sound crazy, but bear with me for a moment. As we all know, Bitcoin is a decentralized digital currency that operates on a blockchain network. Its value is derived from its scarcity and the fact that it is not controlled by any government or financial institution. However, Bitcoin's survival could be at risk if and when nuclear fusion becomes a reality.

Nuclear fusion is a process of generating energy by fusing atomic nuclei together. This technology has the potential to revolutionize the energy industry, providing an almost limitless source of clean energy. If this technology becomes mainstream, the demand for Bitcoin mining, which consumes a significant amount of energy, could decrease drastically.

Currently, Bitcoin mining consumes more electricity than entire countries like Argentina and the Netherlands. If nuclear fusion is successful, Bitcoin mining could become too expensive to be economically viable, resulting in a decline in demand and ultimately, its demise.

Now, I'm not saying that nuclear fusion will definitely lead to Bitcoin's downfall. There are many factors to consider, such as the scalability and efficiency of nuclear fusion technology. However, it's important to keep in mind that Bitcoin's value is not guaranteed, and the emergence of new technologies could pose a threat to its existence.

What are your thoughts on this? Do you think nuclear fusion could pose a threat to Bitcoin's survival?",Bitcoin
"3 Reasons Bitcoin Is poised for a comeback.

1. Bitcoin Companies are regulated

Bitcoin has had government recognition and approval since 2014. In March 2014, the IRS issued Notice 2014-21 (the Notice), mandating that bitcoin was to be recognized as property, rather than currency for US federal income tax purposes. By giving financial markets clarity on what bitcoin is, an entirely new #bitcoin industrial complex has emerged as companies expand into the digital frontier.

2. #Bitcoin companies are registered.  

Every major bitcoin mining company, bank, or exchange that wants to have any formidable stake in the bitcoin sector has to comply with their respective Government’s financial departments. Coinbase just recently partnered with Black Rock, the largest asset manager in the world, with almost 10 trillion dollars under management. Once the serious players become fully established, don’t be surprised if governments subsidize different bitcoin mining companies, which would in turn generate job growth (due to higher entry level wages) and help stabilize a dilapidated energy grid.

3. #Bitcoin politicians are respected.

Ted Cruz, Cynthia Lummis, and other politicians have adopted bitcoin as a storage of value. Both she and Ted Cruz spoke extensively at a forum hosted by The Heritage Foundation.   Lummis has went on national television on more than one occasion and embraced bitcoin as a valuable property, as she is advocating for bitcoin as a retirement asset.   In an interview at CNBC’s financial advisor summit she explained that it’s important to support a scarce asset, “I encourage people to by and hold…”

Ted Cruz orated on harnessing and recycling expended natural gas energy and using the excess natural energy to further stabilize the energy grid and mine for bitcoin, simultaneously.   Within the next two presidential elections, bitcoin will be a serious political issue, as it represents absolute ownership of property.

Seizure of Property and assets is lawful through eminent domain and - if criminal activity is involved - civil forfeiture. Self-custody of bitcoin allows for American citizens to utilize their Constitutional Rights if their bitcoin ever came into question in regards to a lawful (or unlawful) seizure of assets.    It is important that humans around the world retain their right to property ownership because individual property ownership is analogous to Life, Liberty, and a democracy.

Once more people understand the unique qualities that make bitcoin valuable property, politicians who enact local laws that enhance protections of bitcoin ownership will most likely be favored in campaign races. And the candidates who are in favor of protecting bitcoin property rights will eventually be encouraged to run for office.

[https://www.youtube.com/watch?v=YVX1gX-shDc]

(https://www.youtube.com/watch?v=YVX1gX-shDc)",Bitcoin
"Assume after an election in May 2024 the new UK government adopts Bitcoin as a way to counter the disaster of Brexit. They keep the pound and add Bitcoin as a second currency. 

What happens to the price of Bitcoin?
What happens to the value of sterling?
What happens to the UK’s reputation?",Bitcoin
"So I downloaded bisq yesterday and tried to fund the Address displayed in the app. Blockchain explorer shows the the transaction went through, to the right address and all that. I was a small amount, just to test. Is it out of sync?",Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/1170cl5/mentor_monday_february_20_2023_ask_all_your/) for unanswered questions.",Bitcoin
"I first heard about bitcoin in passing back during the 2017 bull run, but didn’t do much digging until 2020. That’s when I first bought bitcoin and decided to HODL a little, “just in case it catches on.”

2022. Bitcoin is down a lot. We witnessed so much death and destruction in the crypto industry, yet Bitcoin’s value proposition hasn’t changed. This was when it finally hit me. We can hold our OWN COINS via self-custody. No trust is needed from any kind of financial institution. 21 million coins. No more, no less. This is true financial freedom.

In 2023, Bitcoin is now up 60% from the bottom in USD terms, despite the macro fears and other FUD. While no one can truly make price predictions, the halving is one year away and given previous cycle’s performances, Bitcoin may be the turning point in many of our lives. 

It’s a privilege to be able to accumulate and understand Bitcoin and sometimes I feel like I’m seeing something that my peers may not for the time being. I’m doing what’s best for myself, to escape our current financial system (I mean, I just assume at this point I won’t even be getting Social Security…gotta take care of myself somehow).


Stay very humble, and stack sats my friends.",Bitcoin
"Hello, 

I move a very small amount of bitcoin from a wallet that was laying around. and I set the transaction fee very low. The transaction has been stuck for a good week, maybe even more with 0 confirmations.

Thats fine I really don't care, but I would like to learn a little about this.

1) can I cancel the transaction?

2) am I ""hurting"" the network with this unconfirmed transaction?

3) will it ever get confirmed if I left it?",Bitcoin
"The best way to secure it is to memorise it, always keep a back up somewhere safe. It not that hard to memorise it. Remember, protect your assets at all times",Bitcoin
"Been gone from the scene a while, seems like LocalBitcoin is shutting down. What other options are there to purchase person to person (no exchanges)? Or is person to person trading a thing of the past at this point?",Bitcoin
"I want to try this but idk if starting now and only being able to put in small amounts is worth it. Should i use Kraken to buy and if btc where  to rise to 70k in the next 6 months or year, should i sell then?",Bitcoin
"Bitcoin Vs. MLK: Non-Violent Resistance 

There is a new global civil rights movement that aims to protect the sovereignty of all  human beings.  The philosophical ideology of non-violent resistance was utilized by Martin Luther King to pass the Civil Rights Act of 1964. 

Non-violent resistance became a sociopolitical trend afterward. Civil Rights leaders all over the nation peacefully boycotted and marched all over America to countervail the control authority of a flawed justice system. From 1966 to 1999, nonviolent civic resistance played a critical role in fifty of sixty-seven transitions from authoritarianism.

Bitcoin is an asset that utilizes non-violent resistance by protecting a monetary network through electric power projection. Instead of demonstrating sit-ins to expose the absurdity of 20th century Jim Crow Laws, bitcoin uses a proof-of-work consensus mechanism to protect a Trillion-Dollar monetary network without projecting kinetic (violent) power, simultaneously exposing the absurdity of 20th century monetary policy. 

Since the beginning of recorded history, the winners of a large-scale violent period in history (war) usually resulted in economic spoils and technological breakthroughs. 

There is a monetary and social incentive, however amoral it is, to conduct war operations. However, war is a horrible reason to incentivize technological advancement. Why should humans have to kill other humans on a massive scale before new technologies are created and utilized? 

Bitcoin offers the world a much more attractive incentive to build on top of technological advancements. Bitcoin adoption has the potential to bring large-scale violence to an end and may one day be considered the most efficient form of nonviolent resistance. 

Jason P Lowery has held the controversial contention that kinetic violent energy was the only way to countervail the attack of an oppressor until the creation of bitcoin. 

However, MLK and other Civil Rights Leaders in the 20th century utilized non-violent demonstrations to countervail control authority of an oppressor and amend laws. 

In regards to the Civil Rights Movement, Attorney Richard Cohen points out that “The violence was being perpetrated by the oppressors, not the oppressed. And that was an incredibly powerful message and incredibly important tool during the movement.”

Although we are fortunate to live in a country that has an abundance of energy and resources, it is obvious that the United States Government has been the oppressor when it comes to the overall strategy of monetary policy. By establishing a monopoly on violence, our government has been able to exploit American Citizens through taxation and inflation, among many other flawed forms of monetary policy.

Jamila Raqib, executive director of the Albert Einstein Institution, contends that “non-violent [resistance] works by destroying an opponent. Not physically, but by identifying the institutions that an opponent needs to survive, and then denying them those sources of power.” 

Satoshi Nakamoto understood that the government needs banks to control the monetary infrastructure. Bitcoin defies government protocols by taking the power out of the banks hands and allows self-sovereignty for all those who subscribe to the network. 

Bitcoin is a new iteration of non-violent resistance. Non-violent resistance is just another “clever form of power projection.” 

The multidimensional characteristics of bitcoin are what make it so unique and special. A non-violent resistant protocol has been able to put the power back into the hands of the people it represents, which further indicates that non-violent resistance is in fact a valid protocol for positive change and innovation.  

https://youtu.be/brhfdeHSeXs",Bitcoin
"Hello, for the past few days I tried to find what was the first LN channel that opened on mainnet, but to no luck. Can anyone point me to a resource or help me find the transaction hash when the first channel was open please?",Bitcoin
"both of my best friends turned 20 a few days ago. Instead of expensive alcohol, I gifted them around 20$ worth of bitcoin (and other things of course). Yesterday I set up an hot wallet for one of them. I made sure that he keeps his seed phrase to himselve and then I sent some satoshis to his BTC adress. I think it was also exciting for him to be your own bank. In the following weeks I will educate him more about all the things that I find interesting about BTC. 
Am I doing the right thing here? Or is it too annoying to try to convince your friends into buying bitcoin. When bitcoin becomes a huge success in the future I would feel terrible if I hadn’t told them earlier.",Bitcoin
"MY avg BTC buy's $27.5K, BUT if BTC rebounds, and I can sell a portion of my stack (15%) at $55K, my new avg will be $22K. I FOMO'd at $55K for some freakin' reason lol",Bitcoin
"Real question, as I saw many posts over with different approaches. 

To you, is bitcoin a money or a store of value ? 
Many arguments for both to be fair. 

To me, it feels like bitcoin is a scarce asset and then, the perfect store of value. 
But in the end, could we consider an asset as a money AND as a store of value ?",Bitcoin
"Title says it all. I have my report from my neurologist and it is now 100% sure I have inherited SCA1 from my mother. 

The symptoms will kick in mostly during the 4th or 5th (best case) decade of life. 

At around 2-4 years after the first signs of symptoms (motoric and verbal inhibitions) it is quite normal to be stuck in a wheelchair permanently while loosing motorical functions as time goes on. 

The research around a therapy of spinocerebular ataxias wich are neurodegenerative diseases, are still in play and there is no healing as of right now.

NOW: Bitcoin helps me keep being optimistic. In 10-20 years I hope I can use my stack to push the research and find a lab that studies for a promising therapy plus put it into research foundations for ataxia and so on.
  
There is good research done today but it could be better. 

I think the majority of corporations especially pharma and such will stay in a stinky fiat-oriented system, but I think Bitcoin is likely gonna invade fiat-pharma corporations wich results a push for further innovation. 

And if not: hell, my life is shorter but at least I got my bitcoins to spend it and give them to my kids maybe.",Bitcoin
"The percentage of BTC held for 1 year or longer has broken an all-time record. Doesn't necessarily mean that we are going instantly to the moon, but whenever we have slowly been making new highs on this on-chain metric, it was during a time where a bullish setup was forming.

https://preview.redd.it/ohfa188mldja1.png?width=2195&format=png&auto=webp&v=enabled&s=ab7cbee7df9c4eb37815ad8a5a9e1efb94fc4158",Bitcoin
I've heard commentary from Matthew Kratter that you should never share this.  But a lot of crypto tax software needs that xpub address from hardware wallets to make its calculations.  What are the issues in sharing that with crypto tax software?  It's not private keys or anything.  Is it just the chain analysis risk exposure?  If that's the case we're all screwed anyway unless we have nonKYC Bitcoin and/or have done CJ.  Please help me understand this a little better.  Thanks!,Bitcoin
"I know people do not like inflation and are tired of the oversupply of money. Our wealth (or more accurately saving) shrinks when there is too much money printed and circulated in the market. However, we must agree that financial policy and tools were and are an important motor for world economic growth. With the bank's Loan to deposit ratio, Banks loan out more money than they received, and people spend more money than they earned. We call it the multiplier effect. In a peer-to-peer payment world where there is no bank, and no multiplier,  people tend to lock the bitcoin in their self-custody box or wallet. As bitcoin is peer-to-peer payment, Institutions can hardly make loans unless they supply some sort of fake token. When most bitcoins are locked, currency in circulation will be limited, and that would cause financial problems in the world.  The economy would slow down or stop without its blood of currency. That happens in gold or silver currency times such as Song and Ming dynasties in China. In tough economic times, people only think about saving and protecting their treasury rather than spending on future income like we are doing now for home mortgages. When spending shrinks, the economy slows down.  That's why most economists believe 2%  or so inflation is healthy for the economy. Bitcoin needs to find some organic, healthy, and legal connections with growth-oriented financial mechanisms.",Bitcoin
"The more the success of the Bitcoin revolution appears inevitable to the powerful people at the head of the current monetary and financial system, the more you can see that they start to denigrate Bitcoin falsely.

One day, these powerful people tell you that Bitcoin is a Ponzi Scheme. Another day, these powerful people tell you that Bitcoin is an environmental disaster. Another day, these powerful people tell you that Bitcoin is dangerous and used only for illegal activities.

Of course, all of this is false, and governments continue to tax you on your Bitcoin gains. Isn't it amazing to tax people on something that is dangerous and that everyone should avoid?

This is yet another proof of the hypocrisy of the powerful in the current system who are now trying to make people believe that their CBDCs will solve the people's problem and make Bitcoin obsolete. Yet another lie, but the general public still seems to have a hard time getting past the narrative of the powerful people at the head of the current system.

For those who want to criticize Bitcoin, it seems to me that there are currently at least 3 legitimate criticisms that can be made. Of course, you will never hear the powerful people at the head of the current system make them because that would require studying the Bitcoin system for real instead of just propagating lies.

Here are these 3 legitimate criticisms of Bitcoin:

* **Bitcoin does not consume enough energy**

Bitcoin is energy. Bitcoin units cannot be produced out of thin air like fiat currencies. This is something essential. What is interesting here is that the energy consumed by the Bitcoin network will help fight climate change.

Bitcoin will help the energy transition by giving a real incentive to develop renewable energy.

BTC mining, thanks to its geographic agnosticism, can be installed on oil sites and consume methane that would otherwise be released into the atmosphere. Besides, the Bitcoin network offers a floor price to energy companies that need money to finance the energy transition.

&#x200B;

* **Bitcoin does not have enough nodes on its network**

Nodes are the gatekeepers of the Bitcoin protocol rules. The more nodes on the network, the better.

&#x200B;

* **Most BTC units are purchased in KYC mode, and this is something that needs to be changed**

The purpose of Bitcoin is to allow you to protect the fruits of your labor within a decentralized network without having to reveal your identity.

Some people forget this, but buying Bitcoin in no-KYC mode is the right way to do it.

The more information you give about your identity to centralized exchange platforms that enforce government KYC policies, the more you put yourself at risk when those platforms are eventually hacked. It always happens sooner or later.

Buying Bitcoin in no-KYC mode really protects you from all kinds of censorship and potential threats.",Bitcoin
"Hey plebs, 

I have been working on some content on how to secure your seed phrase recently and I thought I would reach out to the community and see what they all think and ask for some feedback. 

Instead of writing everything in a single article, I have broken it up in a mini-series of 4 articles with each article focused on what I have identified as the [4 key steps to secure your seed phrase](https://www.whatisbitcoin.com/security/secure-your-seed-phrase). 

1. [Generating Your Seed Phrase](https://www.whatisbitcoin.com/security/generate-your-seed-phrase) (Randomness + Verification)
2. [Writing Your Seed Phrase](https://www.whatisbitcoin.com/security/write-your-seed-phrase) (Paper & Metal)
3. [Storing Your Seed Phrase](https://www.whatisbitcoin.com/security/store-your-seed-phrase) (Location & Access) 
4. [Recovering Your Wallet With Your Seed Phrase](https://www.whatisbitcoin.com/security/recover-your-bitcoin-wallet) (Privacy, Security, and Wallet Settings)

I would love to know what the community thinks of all of these steps and  what they think I should add  or potentially change. 

Thanks in advance everyone. I look forward to hearing what you all have to say.",Bitcoin
"All I see in this sub about bitcoin price, hodl, not your keys not your coins, exchanges, adoption etc. But what about the actual technology itself and the possibilities of the blockchain? Can we have a good discussion about it? For starters can anyone explain this whole web 5.0 thing and how it can improve the current web as a whole?",Bitcoin
"As a solo miner, do I have the possibility to set up a Bitcoin full node and join mining by just guessing one hash per block? This is all I can do, and I prefer to play this lottery rather than all other FIAT lotteries.",Bitcoin
"1. For people who want to  earn in Bitcoin, how do they decide on how much Bitcoin to receive. Do they price their wages in fiat, then convert to BTC, using the exchange rate at the relevant time? 

2  For merchants who wish to receive in BTC, how they decide on how much to charge? Do they price their wares in fiat, then convert to BTC, using the exchange rate on the day the transaction is made?",Bitcoin
"Ask (and answer!) away! Here are the general rules:

* If you'd like to learn something, ask.
* If you'd like to share knowledge, answer.
* Any question about Bitcoin is fair game.

And don't forget to check out /r/BitcoinBeginners

You can sort by new to see the latest questions that may not be answered yet.",Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/116352n/daily_discussion_february_19_2023/) for unanswered questions.",Bitcoin
"....is an experimental Bitcoin Receiving QR code that contains both a Bitcoin On-Chain Address and a Lightning Invoice. Any thoughts on if this is the way things are headed in the future?

I have experimented with it using the Muun wallet as well as the CashApp payment app. It works with both and seemed fairly slick.",Bitcoin
This might be a stupid post but I was watching that movie and there’s a scene when the wakandans find out that there is vibranium in other places in the world and that another group of ppl use it. And in turn it changed the game theory of vibranium for the wakandans since another ppl may be just as powerful. Which led my mind to think about bitcoins game theory when countries find out that other countries are buying and mining.,Bitcoin
"I'm a younger dude (lets say young enough not able to buy bitcoin myself) who holds some bitcoin. So far I've only been able to buy some with my parents approval in small amounts with their KYC exchange accounts (about 10$ every week). My thought process being that I won't ever get to a large enough amount to make any difference in my life. I don't think I'll ever sell my bitcoin, but I guess its only sat-stacking from here.",Bitcoin
I want to either transfer directly to a private wallet or send immediately after buying. How do get the lowest fees without having to wait days like on coinbase?,Bitcoin
"I'm learning about the LN, and it seems that you can earn a small amount of stats if LN txns are routed through your node (do I understand that correctly?). 

If that's the case, why doesn't everyone open LN channels with high amounts of BTC to enable the most amount of throughput in the LN earning them the max amount of sats/txn?

Sorry if this is a nooby q.",Bitcoin
"I was thinking... All the people using bitcoin for nafarious means on the darkweb, cant transfer it back into real cash money? Its just stuck as bitcoin right?

How are they getting bitcoin into cash or bank balance with out any eyebrows being raised?

Like they surely can't just transfer it to a coinbase wallet and then withdraw?",Bitcoin
"Hi all, I have been buying BTC via DCA for nearly two years and dumping into cold storage. As title states, I’ve done a crap job of figuring out what my cost basis is, and I’ve only used one platform to buy.  While I’m mainly a Hodler, I would like to “layer out” with some BTC if we get a nice price appreciation later this year.  So, my admin question is: 

Other than creating a spreadsheet, is there an easier way to do this?  Seems like a lot of damn work. 

Thanks in advance and I don’t respond to DMs.",Bitcoin
"Why do we keep seeing super high predictions for bitcoin on the long run if the return after each halving keeps getting smaller and smaller? I understand rising demand will make prices go higher but i dont see it reaching millions...

What do you all think?",Bitcoin
"
I’m summarizing a couple points from a recent Ezra Klein podcast with Paul Krugman, a famous American economist. See the episode [here](https://podcasts.apple.com/us/podcast/the-inflation-story-has-changed-dramatically-paul/id1548604447?i=1000600155877).

1st thing. Apparently J Pow’s administration is using the one inflation metric that has shown the greatest amount of disinflation. That is, core inflation ex housing. In short, rising rents would not factor into the Fed’s current calculation of inflation. I find this interesting, because it dilutes and distorts our collective sense of what inflation is currently at, allowing for more inflation than has historically been allowed the last 30 years or so, simply because of the lens used.

Takeaway here, the Fed is using a more liberal, looser interpretation of what inflation means and perhaps a less stringent approach to reigning in inflation as a result.

2nd thing. The Fed has historically targeted a 2% inflation rate. Paul Krugman alluded that he, and some other economists in his circles, could envision a target inflation rate of say 3% or 4%. On the flip side, there’s other economists vehemently opposed to this idea, with the thinking that compromising on 2% could give way to 5% and could give way to 10%.

Takeaway here, among many economists, there’s a more liberal, looser sense of what inflation should be.

Taken together, I see some bad habits and poor discipline forming with monetary policy. We could have a world with a weaker dollar, running at a higher target inflation rate year over year.

This make me fall back to big daddy BTC. Humans and societies are subject to bad habits. Humans and societies compromise their own values all the time. 

BTC is capped and forces discipline on the beholder.",Bitcoin
**EU Markets in Crypto-Assets (MiCA) Regulation Expected to Enter into Force in Early 2023，How will this affect the cryptocurrency market in Europe?**,Bitcoin
"preferably uk but willing to consider outside. i'm just tired of bitcoin and getting rid of what i have, it's only 0.5 but it just sickens me.",Bitcoin
"Whats the best? Are bitcoin bonds a good investment or a gamble ?

Share your toughs on this post.

"" 50% of Bitcoin invested proceeds return to investors ....""  ( only 50 ? )

https://preview.redd.it/x51pdj9qf6ja1.jpg?width=619&format=pjpg&auto=webp&v=enabled&s=b0fcca7766b77321f0079207eed1c0d845ed598d",Bitcoin
"You people are seriously not thinking clearly.

This situation is not the end of the world.  It is not some massive shift in the world of Bitcoin.  It is not MtGox’s fault.  It is not the high frequency traders fault.  It is not even the speculators fault.  Merchant acceptance will not save us from these things.

We are dealing with a massive new technology that has the potential to change everything.  It will change how money is transferred between countries.  It could become the international standard among currency evaluation.  It will change how the drug/black market operates.  It will change how Governments regulate borders.  This is not a speculative stock.  This is not a currency useful for day-to-day trading.  You cannot be a “bitcoin millionaire” without knowing that if the network got cracked, you’d be worth $0 in minutes.

We have a long, long way to go yet.  In order for the above things to happen(and they WILL happen, even if bitcoin gets cracked or made illegal- something else will replace Bitcoin), the market size of Bitcoin needs to increase.

	1.	$50k - Early adopters will push it up($0.01->$0.20).
	2.	$1 million - People will start using it for small transactions(silk road).
	3.	$50 million - Small time illegal activity will flow through it, and people will start to use it to transfer money across borders(See; Argentina post recently).
	4.	$1 billion - Mid-level illegal transactions and mid-sized legal transactions will flow through it.  Angel and VC Investors(200k - 5 million) will move in in increasing size will both invest and create startups(happening now; this is the step we are on. We will never go back to step 3 unless the Bitcoin network itself is cracked).
	5.	$25 billion - Higher level illegal transactions(mob bosses) and larger investors(multi-million sized) start to move in.  Governments start to get involved, try to regulate what they can, and create rules for the system.  High net worth individuals use it for international currency transfers.
	6.	$400 billion - After that, large businesses & investors move in.  Becomes the de-facto standard for illegal activity.  Government regulation cracks down and becomes more rigid.  Some(A few) Bitcoin businesses are shut down by the Government without warning, prompting fear and anger.  Small companies regularly use it for international currency transfers.
	7.	$1 trillion - After that, International currency movements start to flow through it.  Very large investors move in, it is talked about as if it were standardized and common.  Businesses learn to follow Government rules and procedures become standardized.  Large businesses use it to transfer currencies internationally.
	8.	$5-20 trillion - Becomes the de-facto standard of international trade and currency evaluation, replacing the dollar as the global standard.  Prices stabilize and shift only a fraction of a percent a day.  Can now be used as a real currency for the first time since inception.

Do I know for sure this will happen?  Of course not.  But the first 4 steps were pretty clear in hindsight.  And it makes sense- Why would ANYONE use the dollar for international money transfer post-Bitcoin?  It depreciates, it is expensive to move, it is heavily regulated and tracked, it is subject to seizure.  It is subject to the whims and mistakes of one government, who are all subject to the whims of their short-sighted voters.

So now how the fuck do you go from a $500 million currency to a $5 trillion currency?  It isn’t going to be a linear graph- that doesn’t make any sense.  It isn’t going to be a smooth rise- Why would it?  If everyone can see a nice, smooth, pretty graph going up, everyone is going to buy into that nice, smooth, pretty graph.  It isn’t going to be unidirectional- If the price always went up, everyone would buy into the up, and it would overshoot any and every step.  It isn’t going to happen quickly- Many of these steps take time to build confidence and make mistakes to learn from.

No, it is going to be a very painful up and down process.  Because the technology has so much potential, it is going to experience explosive growth.  Because it experiences explosive growth, it is going to have dramatic, painful, scary collapses.  Then there will be fear.  Then it will stabilize, and then it will start growing again.  A few months later, it will explode again as it approaches the next big transition.

It takes time for Bitcoin companies to get their systems in order.  It takes time for them to earn our trust and for us to weed out the scams and unreliable ones.  It takes time for VC and Angel investors to evaluate and plan Bitcoin ventures.  It takes time for Bitcoin to adapt to its own growth.

For those who think merchant adoption and currency status are the step we should be on, you are gravely mistaken.  The only use that merchant adoption has right now is 1. Getting more people into it/increasing transactions, and 2. making a legal case for why Bitcoin shouldn’t be illegal(which would slow us down by 10-25 years).

This is not the last big rise.  This is not the last big crash.  We aren’t even at the bottom of this one.  Until the network either gets cracked or replaced, this is going to keep moving forward.  There is no going back; We’ve improved the Gold coin, the Dollar, and the wire transfer all at once.  Hang on to your seat, and stop panicking over just another crash.

tl;dr: This is not the last big rise.  This is not the last big crash.  Stop panicking and focus on the long view.

Credit to: 

/u/justsomebadadvice

https://reddit.com/r/Bitcoin/comments/1c5j46/you_people_are_seriously_not_thinking_clearly/",Bitcoin
Pretty much the title. I’ve stored my bitcoin on a trezor and it pretty much just sits there. Only transactions I do are sending occasional bitcoin to my trezor. Are there any benefits at all or am I just wasting money?,Bitcoin
"From [The International Monetary System'. Forty Years After Bretton Woods](https://www.bostonfed.org/-/media/Documents/conference/28/conf28.pdf) - Proceedings of a Conference Held in May 1984 Sponsored by the Federal Reserve Bank of Boston

>**In spite of the Gold Reserve Act of 1934, the United States was not really on a gold standard**. The essence of the gold standard is that the money supply must be limited by the gold reserve. **The last time that the Federal Reserve tightened monetary policy because the gold reserve ratio fell close to the legal minimum was in March 1933**. Since then, whenever the gold reserve neared the legal minimum, the required reserve ratio was reduced and finally eliminated entirely. **A country that loses more than half of its gold reserve, as the United States did in 1958-71, without reducing the money supply is not on the gold standard**. 

>**What happened in August 1971 was the abandonment of the anomoly of dollar convertibility into gold when the United States was not on a gold standard**.

Just want to emphasize this. 1971 was a late political acknowledgement (a leftover *anomaly*/artifact to be cleaned up). The US was not on a gold standard (and arguably wasn't even before 1933). 

...and now onto global monetary expansion *without central bank involvement*.

>The pressures causing some currencies persistently to strengthen, and others to weaken, in response to their differences in economic performance, were exacerbated by the unusual dependence on the dollar. **For from the early sixties onward there was virtually no control over the worldwide supply and use of dollars.** The ""dollar shortage"" of the fifties was becoming the ""dollar glut"" of the sixties. 

The ""eurodollar"" or ""shadow banking"" system arose by the 1950s. It's really just a wholesale global banking market. By the 60s, banks in the US were increasingly borrowing from offshore vs. obtaining ""funding"" from the Fed. Offshore funding allowed banks to bypass restrictions (like reserve requirements). 

>**It appeared impossible for the United States to maintain effective control over the supply of dollars at home and abroad simply by following the old rules of the gold standard game**--i.e., by maintaining a surplus in its external current accounts. 

Can also be said of Fed Funds, reserve levels/QE and QT.

>The urgent needs for capital expansion around the world attracted the expertise of rapidly developing multinational companies, many of them based in the United States, and all of them drawing on additional dollars to finance their desired growth. 

>Capital outflows from the United States, spurred by direct investment from within and **substantial borrowings from without**, began to flood the world with an apparent excess of dollar liquidity-despite the absorption of liquidity that might have been expected from the large current account surplus of the United States. **Central banks abroad found themselves with what became an ""overhang"" of dollars in their foreign exchange reserves.**  

There are numerous examples of Fed chairs lamenting their inability to even measure the money supply. Other countries realized long before that *private sector generated* USD funding had taken off. An ""Overhang"" in exchange reserves is a mild way to put it. 

>**One improvisation after another was attempted in order to preserve or restore confidence in the credibility of the dollar as a reliable standard of value and medium of exchange capable of assuring stability in the payments relations throughout an expanding world.** A **""""gold pool"" among leading central banks**, initiation of a ""ring of swaps"" between the dollar and a dozen or more other currencies, creation of U.S. dollar obligations denominated in foreign currencies, the introduction of an Interest Equalization Tax and other measures to deter capital outflows--all these were part of an effort to sustain the dollar while also building a network of closer joint involvement with other countries in maintaining currency arrangements that could serve the best interests of all. **But this combination of improvisations could not cope with, and indeed may have contributed to, the enormous expansion in markets for U.S. dollars offshore**, and the **new networks of interbank relations** that **made possible the creation of additional supplies of dollars outside the United States and beyond the control of the Federal Reserve**. 

Why do central banks hold gold? Originally, to ""back"" notes, but later just to appear to still be in charge of their denominations. 

These ""tricks"", including swap lines, etc... have been largely *ineffective*, and may have exacerbated the ""problem"" (the shift of monetary control to the global banking sector). *New networks of interbank relations* (interconnected global banks) creating dollars beyond the control of a central bank.

>The ""offshore"" currency markets soon became securities markets and, spurred by the U.S. effort to maintain control over capital exports from the United States, markets in Eurodollar securities (where the interest would not be subject to U.S. withholding taxes) flourished.",Bitcoin
If these leverage trading firms/exchanges let people borrow bitcoins on paper and just debit/credit the difference without actually buying the bitcoin? Instead of the leverage provider actually holding the bitcoin in reserve they just debit/credit the account with the difference at the close of trade? Any thoughts appreciated.,Bitcoin
"Hi everybody,

I am looking for some nuanced takes on my thought process here. I´ve been in crypto for a few years now. In the bullrun of 2021 I started dabbling a little bit in altcoins, winning some and losing some. But the more I dabbled in altcoins, the less focused and more time-consuming my investment strategy become.

Most of my gains came from buying a couple of Bitcoin in 2019 and forgetting about it, but after dabbling in altcoins I started staring paralysed at charts all day. What are all these acronyms? Result; about the same gains as I would have had just holding BTC and touching grass. Others in my trading group have much better success.

I dont have a problem with altcoins necessarily, except I dont understand them and they fill me with this sense of greed and inadequateness. Most of them are vaporware sure, but as a short term trade I see potential in increasing my Bitcoin stack.

Its just ... they seem to drag me into this downwards carousel of greed and obsession. Taking time away from business and family, tempting me into emotional decisions. I feel I understand Bitcoin - digital gold, money, hedge against shit-hits-the-fan. Maybe thats why Bitcoin dont trigger the same greedy obsession in me.

TLDR; anybody went Bitcoin only not mainly because of ideological reasons, but because of easier investment strategies? What was your reasoning, and did you stick to it? What has your results been?

Sorry if I phrase things strange, english is not my main language.",Bitcoin
One of my relatives gave me a paper wallet. I want to somehow turn it into cash. What are the steps? it's a hand written paper wallet.,Bitcoin
"Strike is the blueprint, send dollars and recieve dollars over lightning until the world lives on bitcoin and we all just send sats over lightning.

All banks need to adopt this advancement or they will become disused by anyone under the age of 60.

Banks need to allow their customers to use their mobile banking app to scan lightning invoices letting them pay any lightning invoice with their fiat balance. Banks also need to allow their customers to use their mobile banking app to display a lightning invoice in order to recieve fiat from anyone who can pay to a lightning invoice.

All banks on earth need to let users send fiat through and recieve fiat to lightning invoices. If they do not do this they will just continue to lose market share to the likes of cashapp paypal and every other app and neo bank on earth that does utilise lightning until their only customers are 70 year old geriatrics who refuse to let go of cash and bank cards.

Banks do not have a choice. The first banks to integrate lightning will dominant market share and the last to integrate lightning will forfeit market share. Let the competition begin.",Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/1158ct4/daily_discussion_february_18_2023/) for unanswered questions.",Bitcoin
" I know I know the price is rising AGAIN and the hype train arrived but before you YOLO everything into it AGAIN and write 6 month later that you lost your house ! And s\*icide hotlines become the first post in every crypto forum AGAIN.

Keep these these 2 rules in mind!

1 Dont YOLO all your money/funds into it

2. Dont take any f\*cking credit at all. NEVER! But this time... NO !

I would normaly wish you success but in this case I wish you rather good luck !

Enjoy the ride and keep it clean.",Bitcoin
"I finally took my BTC off exchanges and into my own cold storage a month or so ago. However, I find myself checking the wallet every few days, just to ‘make sure’ my coins are still there! I know this is daft!

Anyone else?",Bitcoin
"Almost 48 hours ago, I purchased Bitcoin via cash app and use lightning to withdraw to an external wallet. When I attempted the withdrawal, I got an error message, exited the screen and saw that the transaction was pending. Well it has been pending for every since then and cash app refuses to refund. What should I do?",Bitcoin
"Relative bitcoin/crypto newbie…but in a world where Bitcoin becomes the standard transactional currency and the $ becomes extinct, what would happen to all of the USD (and other currency) loans that currently exist? Will I pay .000001 btc to the bank to settle up the remaining $200k of my mortgage because USD is almost worthless?",Bitcoin
"I’ve been holding and stacking 3 other “coins” along with Bitcoin for awhile…moved everything into Bitcoin today.  I continue to read, listen and stack sats.",Bitcoin
" I am currently trying to extract data directly from the blk.dat files from bitcoin core but when taking a look at the blk.dat files I am confused about how I would go about getting sender(s) and receiver(s). It is clear to me that if I have multiple inputs / outputs that there might be multiple senders / receivers but how do I determine them?

If the transactions in the blk files don't allow for that then how do I figure out who sent who what?

I do not need a 1 to 1 mapping of transactions, if 5 people combined sent 7 other 3 BTC that is perfectly usable data for my purposes as well.

&#x200B;

My main resource was [https://learnmeabitcoin.com/technical](https://learnmeabitcoin.com/technical). But I tried finding information elsewhere with no success.  
Thanks in advance!",Bitcoin
"Just made a transfer to a new cold wallet, using a brand new Receive address... however when I review the transaction online at [Blockstream.info](https://Blockstream.info), the Privacy Analysis shows 'address reuse'. 

Is this most likely because: 

1) The ""Send"" address is being reused; 

2) 1 or more of the other ""Receive"" addresses (in a one-to-many transaction) is a reuse (presuming mine to not be an address reuse); or

3) Possible user error on my part?

If #1 or #2, aka my Receive address was not a reuse but someone else's was, what are the privacy considerations / risks I have to be aware of for myself?

Thanks in advance",Bitcoin
" **Warning: long post**

This article is dedicated to the other Bitcoiners out there. I know we are excited to share what we know and to teach others, but most of us struggle on where to begin. Do you even remember how you started in the Bitcoin world?

I never thought about this before, until I read a great article about orange pilling. The author points out that most Bitcoin maxis don't remember what their mindset was before the orange pill. Nor can they point out the exact a-ha! moment they flipped. The article continues examining how most Bitcoiners try to convince others to invest in crypto by explaining how good new technology is or about the flaws present in our current system, and never really get their point across. This is because the listener does not have, see or understand the problem in the first place. Why would you look for a solution when you don't have a problem?

This led me onto a path of searching for my most impactful moment that locked me into becoming a Bitcoin maxi. 

I haven't been in the crypto space for very long. I get into it right at the beginning of 2022, crypto winter. My brother in law tells me he invested in some crypto, so I start investigating what this is. I think, partly, because I am a software engineer, this technological innovation hooks me almost immediately. So I decide to buy a small amount of btc and some other cryptos so it is easier for me to keep track of this subject on a daily basis, and it works. For more than a year now, I have been reading up on news about politics, economy and the crypto space, topics I have never been interested in before. And in a short couple of months, I learn more about these subjects than I did in my previous nocoiner life.

In 2022, I move away from crypto and more into a Bitcoin only mindset because of what is going on around the world. One of the biggest events that leads me to this, is the economical warfare between the West and the East. The ability to remove a country from the global payment rails (SWIFT) with just a push of a button is just crazy. What's even more mental, is that there are no other good alternatives to this tool created in the 70s which is under centralised control, that is also slow, expensive, doesn't always work well, and is almost a complete monopoly.

As 2022 comes closer to 2023, more bankruptcies start to happen, multiple frauds are revealed, bad actors exposed, and of course, the price of crypto-everything is going down. All these events can easily push away anyone who just got into the field, but for me it was different. Instead of cashing out and running away from all this ""risky crypto speculation"" I start forming my own opinion. I still see the potential value of some crypto projects and industry disrupting technologies that are being built, however it's still too early to tell which projects are going to survive, which ones are run by bad actors, and which are the ones that are going to provide significant value when scaled up. 

In parallel to forming this opinion, I  start developing an understanding of why Bitcoin is different. I am not going to discuss everything I learned here, because that's not the point of this article, but I will say, even though the price of it went below 40000, then below 30000 and finally below 20000, I kept stacking more. But why?

After going back, I can pinpoint the exact moment I got orange pilled, and it was when I tried to buy my first crypto. After researching all the possible exchanges I can find, I am ready to buy, and conclude that Binance is the best option for me. It is the biggest exchange in the world, widely used everywhere, and that's where my brother in law bought it, so it makes sense to dip my toe in there first. I register, get all my KYC checks done, and in less than a week I am ready to buy. I enter my credit card details, select the amount I want to purchase, click buy, my heart stops for a bit as I am excited and nervous at the same time to be a new crypto owner aaaaand...*error processing payment*. Maybe I entered the details wrong, so I repeat the steps, and I get another error. After a couple more tries, I conclude that my bank is blocking the transaction for safety reasons, understandable, after all it is the first time I am using this website. I contact my bank and after some investigation done by their customer support, the reply from them is: It seems you are trying to buy crypto currency, and since we are regulated by the central bank which does not allow buying crypto currencies, your transactions to Binance are blocked.

At this point, I am very confused. It doesn't make sense that my brother in law is able to buy crypto through Binance and I am not (we live in the same country by the way). Maybe the law has changed recently? I do more research, but can’t find anything about the crypto ban in the country I live in, so I contact the central bank. I explain that my transactions are being blocked because of the regulation they have imposed (according to my bank). They reply back saying crypto purchasing is not illegal nor prohibited, but their position is that it has high risks associated with it. I have a short temper, so this makes me furious. I simply can’t understand how my bank can block me from using my money and lie about why they did it? This emotional response I have is what I credit as one of the biggest impacts of me becoming a Bitcoin maxi. When you start studying crypto as a normie, a lot of the stuff that people in the space say just sound like conspiracy theories. “Decentralised this”, “permissionless that”, why do I need fancy solutions if I never have problems with my bank payments… until now.

So how did my brother in law manage to buy his? He used a different bank. Ok phew, I have an account with that bank as well. Maybe my main bank is messed up and doesn’t allow me to use my money, but at least I have an alternative. Easy peasy. I transfer some money into the other bank account, pay some handsome fees, type in the new card details in Binance and with relief, click buy, because finally, after a few weeks of trying, I am going to get my crypto (fingers crossed). But no, *Payment Failed*. Again. There must be a mistake, this bank does allow my brother in law to buy, so I must have typed in some digit wrong. So I re-enter the details again. *Payment Failed*. I contact customer support of this other bank. A very nice customer service agent confirms my identity, checks all the transaction details and says she doesn’t see a problem, that it must have been a communication error, so I can try again. So I do just that, and to no one's surprise, I get the same outcome. The agent does not know what the issue is so she says is going to check with her colleagues. And this is the response I get after a couple of hours: ""The transaction cannot be completed because it is not allowed due to an operations and security issue since these types of transactions hold high risk, like virtual casinos and online betting"".

As I said before, I have a short temper. So I am fuming at this point, for the second time, a bank is refusing to let me spend my hard earned money on whatever I want (and that is 100% legal in the country). Ok, maybe I am overreacting. My brother in law used this bank, so they are probably just afraid of some legal consequences if I were to lose my money trying to invest in crypto. So I contact them again, saying I acknowledge the risks and ask kindly if they can lift this restriction from my account. Their reply: “Not possible”.

That strong, emotional connection to this specific situation is what made me realise there is a problem, and the key factor that led me to study it to try and understand it better. Other people's experiences, explanations, difficulties and solutions will have little to no impact on their own, but since I have experienced something related to it, I now have a very strong drive to keep learning more about this innovation, which excites me. Which is why I am now truly grateful to these two banks for giving me an orange pill.",Bitcoin
"The more I learn about Bitcoin and money, the more it makes me realise that Bitcoin will stay.

My dream would be to be able to own just one bitcoin at some point which I will probably never achieve.

Would you say, opening a credit just to buy one is a good idea?

Of course, I know it is super risky but wouldn’t this be the perfect way to reach this goal by abusing the fiat system at the same time?

(please excuse my English)

Edit: Thank you very much for your advise and letting me know that it’s massively stupid. Dollar Cost Averaging is the way.",Bitcoin
Serious question as I’m always looking for more ways I can spend my BTC. Did you buy a car? Use it on your overstock? I’m still sort of sad I didn’t jump on the chance to buy a Tesla back when they accepted it. I know a lot of you are all about hodling but I’d like to see it become more of a way people transact.,Bitcoin
"What are you guys listening to these days? I have unsubbed from most podcasts that do nothing but technical analysis and discuss price. Here are some I am currently listening to. Would love to get your recommendations.

1. Trader University (Matt Kratter): [https://www.youtube.com/@TraderUniversity](https://www.youtube.com/@TraderUniversity)
2. Robert Breedlove: [https://www.youtube.com/@RobertBreedlove22](https://www.youtube.com/@RobertBreedlove22)
3. Saifedean Ammous: [https://www.youtube.com/@saifedean](https://www.youtube.com/@saifedean)
4. Preston Pysh: [https://www.youtube.com/@PrestonPysh](https://www.youtube.com/@PrestonPysh)
5. What Bitcoin Did: [https://www.youtube.com/@WhatBitcoinDid](https://www.youtube.com/@WhatBitcoinDid)
6. Natalie Brunell: [https://www.youtube.com/@nataliebrunell](https://www.youtube.com/@nataliebrunell)
7. Bitcoin Audible (Guy Swann): [https://bitcoinaudible.com/](https://bitcoinaudible.com/)
8. Bitcoin Layer: [https://www.youtube.com/@TheBitcoinLayer](https://www.youtube.com/@TheBitcoinLayer)

And, of course, there are particular people who pop up on various channels but don't have a channel of their own.

1) Michael Saylor

2) Lynn Alden

3) Gregg Foss

4) Jeff Booth

5) Lawrence Lepard

6) Jason Lowery

7) Cory Klippsten

Any others you recommend?",Bitcoin
"
I somehow feel that the big names like  Microsoft, Google, Amazon etc. are not really involved.
Facebook tried with Libra but I guess U.S. gvmnt didn’t like the idea of a tech giant building a financial system?
I know they are involved in some projects but nothing of their own making and it feels weird that these companies seem not interested as it would be natural for them to be at the front of this.
They usually jump on every new tech but they turn their heads on something as big as crypto. Why is that?
Or am I totaly mistaken?",Bitcoin
"I've downloaded the Bitcoin blockchain onto a hard drive I had lying around so that I can always sync from this copy. However, this hard drive only has 1 TB so it can fit only a single copy of the full chain.

Now what I'm wondering is this: If I want to run a node with a pruned version of the blockchain, can I run Bitcoin Core in such a way that it prunes the blockchain, writes the pruned version into one directory while keeping the full copy intact?",Bitcoin
"Will the next best time to trade Bitcoin fall? Perhaps much higher than the current value, more people are getting involved, whether for investment or speculation, the growing mood in the cryptocurrency world, ready to sell the house to play the next big one.",Bitcoin
"hello iam not really knowledgeable on bitcoin/blockchain so excuse me for that. 

But i'd like to know whats the best website/service to use if i need to generate new bitcoin adresses for every transaction ? 

I have an ecommerce website and would like to include bitcoin payment but dont want people to see all the transactions happening on my wallet. 

Also is there a way to automatically give a new adress to each customer so they know where to send the bitcoin (without having me to manually send the adress) ?",Bitcoin
"They offered a discount for paying with btc.

Didn't have to pay my bank fees for currency exchange and nothing along the lines of ""deducted amount may still change due to exchange rates"".

Paying with  credit card would have been easy and convenient too except that the fees suck. All I'm saying is that it was easy to pay with btc too.

And the payment is already settled, so the vendor must be happy too. Total cost of the transaction was below 10 cents and I chose a high rate. Could have sent the transaction for 2 cents.",Bitcoin
"Thought I’d ask the BTC community. I’m looking for a US/global index of retailers accepting BTC.

The reason is I would like to try to not have to use fiat payments. Would be grateful to know if a list exists. 

Much love to the BTC community.",Bitcoin
"Hey guys! Sorry if it's a dumb question. I unfortunately don't have access to the strike app in my country, but I'm wondering about it regardless.

I'm aware they are utilizing the LN to make cash transactions in different countries. Yet I can't seem to find any info if they app itself actually offers a ""regular"" Lightning wallet function - similar to Wallet of Satoshi, Blue wallet, etc.

Thanks for clarification on case anyone knows!",Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/114cb5i/daily_discussion_february_17_2023/) for unanswered questions.",Bitcoin
I am on a mac and it is taking forever to sync a new node to the chain. I am doing it on an external hard drive but it got to about 35% and has just slowed down to 0.01% progress increasing per hour. I don't think I will ever catch up. I updated my bitcoin.conf file with some suggested settings and see no improvement. At this point I would like to just download all the blocks separately in a torrent if possible. Does anyone have a link to a torrent with that is relatively up to date for getting all the block data? Or any suggestions for why this node is failing to sync quicker? Thanks,Bitcoin
"Money literally measures all assets and all wealth in a society. For every single thing that has value theres enough money to represent that value. All the value on earth has to fit inside 21 million coins.

Some other coins may be valuable for their smart contract applications monkey jpegs and ICOs. Some coins may be valuable for their meme appeal. Some coins may be valuable for their utility in cloud computing or data storage capabilities. But they are all just assets like microsoft stock, their coins represent demand for their platforms use just like microsoft stock represents demand for microsofts use. But bitcoin is money, bitcoin measures and accounts for the value of all other coins. Not only does bitcoin account for the value of all other coins it also accounts for the value of all stocks, bonds, realestate and commodities on earth.

So while one coin might reach a trillion dollar market cap up there with microsoft, another might reach a 500 billion dollar market cap up there with tencent, bitcoin is money so bitcoin is the one that has the potential to reach a price that represents all the value on planet earth because its core function is money, its function is to be the reference currency and reserve currency of planet earth for all countries corporations and individuals.

This means bitcoins target market cap isnt one trillion like some other software platform coins, it is $100 trillion at least possibly even one quadrillion. Thats a target of between $5 million and $50 million per coin in todays dollars not accounting for future inflation caused by collapsing fiat currencies.

Bitcoin is in an absolute league of its own and it has no competition when you understand the necessary prerequisite properties for something to become money.",Bitcoin
COINME IS A SCAM! DO NOT USE COINME FOR BITCOIN. THEY STOLE MINE! I had bitcoin sent to my coinme wallet no problem but then when I withdraw it via money gram the transaction said failed and now my bitcoin is gone. I have been given the runaround fir 12 hours and promised they would call me back but nothing! They stole my bitcoin! It should've been put back into my account once the atm withdraw didn't go through,Bitcoin
"I like precious metals and bitcoin and hold a little of both. Gold and silver seem to be pretty heavily manipulated thru paper trading. My question is, is that possible with bitcoin? I mean i guess thru gold and silver they produce a derivative backed by nothing really and the comex and london exchanges determine the spot price right. Since bitcoin is decentralized, isn’t that type if manipulation impossible?",Bitcoin
"Mining gets you some coins (rather I should says Sats mainly).

What is the advantage of running a BTC node ?
-- If there is no advantage then why will anyone run it ?",Bitcoin
Does the pool manager have the technical ability to organize a 51% attack using miners included in the pool?  Let's skip talking about his intentions and his potential gain.  Let's say his family was taken hostage and demanded to carry out the attack.  What methods do miners use to stop this attack vector?,Bitcoin
"Lets I have some tokens on Trezor HW wallet. I need to plug on to some exchange someday (maybe 10yrs down the line) to use it.

1) Can US Govt ban ownership of HW wallets ? 
2) Can US Govt ask exchanges to not allow transfer from HW wallets ?

How do we use the tokens in such case ?",Bitcoin
"What data is available about the amount/depth of review of open source software, in particular bitcoin wallets? 

And what skill sets would you need to do it on your own?",Bitcoin
"Are there any good articles or posts about why bitcoin is not a bubble?

It does show similarities with historical financial bubbles (i.e., large price swings), but its performance after many large drawdowns is not typical of a bubble. 

Any other strong arguments that you all have seen?

Would love to do further reading on this common argument against bitcoin. thanks",Bitcoin
"Video Link: [https://youtu.be/8cX1aptP5Io](https://youtu.be/8cX1aptP5Io)

The 24 risks he explains are:

1) Governance Risk 

2) Operational Risk 

3) Strategic Risk 

4) Financial Risk 

5) Competitive Risk 

6) Technology Risk

7) Political Risk 

8) Facilities Risk 

9) Regulatory Risk 

10) Employee Risk

11) Vendor Risk

12) Customer Risk

13) Reputational Risk

14) War Risk

15) Currency Risk

16) Tax Risk

17) Weather Risk

18) Customs Risk

19) Legal Risk

20) Tort Risk

21) Patent Risk

22) Health Risk

23) Lifecycle Risk

24) Dilution Risk",Bitcoin
"i need help to understand what someone can benefit from a non-standard bitcoin transaction also identified as unknown bitcoin transaction, like this one for example:

835d4dcc52e160c23173658de0b747082f1937d1184e8e1838e9394bc62c0392

and also why would someone custom an non-standard script, what is behind this transactions ?

if you have any thoughts in this subject please let me know, it's for education reasons.",Bitcoin
"If I had 2 addresses, each containing 1 BTC (2 BTC total) and sent 2 BTC to another address. Would it be public that the 2 addresses are from the same wallet? This might be a stupid question but I couldn't find anything online.",Bitcoin
"Hello, I'm sharing with you something that I wanted to do for a while: **comparing the actual BTC price with search volume from Google (Trends)**.

[\\""bitcoin\\"" keyword on Google \(blue\) and $BTC price scaled together \(1 year\)](https://preview.redd.it/bizhv7ctpqia1.png?width=1130&format=png&auto=webp&v=enabled&s=fc5ae0889cf3049d64f82e7aa44ed99bddac110e)

[\\""bitcoin\\"" keyword on Google \(blue\) and $BTC price scaled together \(5 years\)](https://preview.redd.it/12glt5k1vqia1.png?width=1130&format=png&auto=webp&v=enabled&s=f42dff089b8df7fb37558259e9b8bb9e70fa7f6e)

[\\""bitcoin\\"" keyword on Google \(blue\) and $BTC price scaled together \(8 years\)](https://preview.redd.it/66npidx8zqia1.png?width=1130&format=png&auto=webp&v=enabled&s=c2e6c056ecd072770de10e89510cb26d76213958)

&#x200B;

This is a chart,  programmatically takes with Python and matplotlib both values from **Google Trends** (Keyword: *""bitcoin""*, period: last 365 days, plotted on the right axis) and **BTC price** (currency: USD, period: last 365 days, plotted on the left axis) and merges the two plots in a single scale y plot. Please be aware, 0 means *""no search interest""* and 100 stands for *""maximum search interest""*.

So, the question remains: ***Is BTC price related to people who search for it or not?***

The conclusions I leave to you. :)",Bitcoin
"Hello. Every time I open an Electrum wallet it prompts me for the hardware device just to open, not required, just asks to connect. Pretty annoying especially a multi-sig wallet that has 5 co-signers and I have to click no to every single one just to view..Anyway around this? 

Thanks !

&#x200B;

https://preview.redd.it/fag1rel0iqia1.png?width=798&format=png&auto=webp&v=enabled&s=57890ec2fd8d16c948fa72c360b6fcd060c92f41",Bitcoin
[https://zionodes.com/blog/the-impact-of-large-scale-mining-operations-on-local-communities](https://zionodes.com/blog/the-impact-of-large-scale-mining-operations-on-local-communities),Bitcoin
"Bitcoin was created in 2008 by Satoshi Nakamoto with a specific goal in mind: to take the control of money out of the hands of the state.

Bitcoin aims to give you back the power over money.

In fact, Bitcoin is not just another investment as some people still believe. Bitcoin is a monetary revolution that you must fully embrace to change your life for the better.

This means you need to stop investing in Bitcoin, and instead, see Bitcoin as the best savings technology available to you. Save the fruits of your labor in Bitcoin and live within the Bitcoin standard.

This means looking for a way to get paid in Bitcoin if possible.

This may seem excessive to some living in the Western world, but what is happening today in emerging countries like Lebanon (look at protesters furious having not access to their own savings. The value of the Lebanese pound has dropped by more than 98% since 2019), Nigeria (the government tries to impose its eNaira), and Argentina (hyperinflation is the norm for decades) shows you what is in store if you don't embrace the Bitcoin revolution before it's too late.

Too late? Unfortunately yes, because today in Nigeria we can see the difference between those who had anticipated the collapse of the current system and those who now see their governments and banks doing everything to censor them from switching to Bitcoin.

Of course, it's never too late to do the right thing and you don't need the banks' permission to switch to Bitcoin. However, once governments impose their CBDCs via their central banks and cash is practically history, your options for converting the fruits of your labor into Bitcoin will be much more limited.

You will be stuck in the debt-based fiat system.

For these reasons, I think it is essential to protect the fruits of your labor while it is still possible. To do this, there is only one thing to do: stop investing in Bitcoin, and start saving and living in Bitcoin.

It will make a big difference in your future life because it will totally change your current expectations about Bitcoin's price and your time horizon. You will understand that there is no need to wait for dips to buy Bitcoin, and there is no need to be stressed about the price of Bitcoin in the short term.

Bitcoin is your peaceful weapon to exit the debt-based fiat system that is flawed and not fixable. It's up to you to use this weapon fully to protect your future.",Bitcoin
"Google has 1 day, 5 days, 1 month, 6 months, YTD, 1 year, 5 years and ""max"" which goes back to 2016. Is there somewhere else that presents the data better? I'm sure there is but I have no idea where to look.



Edit: Thanks for the feedback! I got exactly what I was looking for. The Tradingview app nails it and Crypto Bubbles is also really interesting so I’m keeping both. 

To the moon!",Bitcoin
"I can think of few more wrong statements about btc in ES.

Have you ever been unbanked, unable to pay for something except in person, in cash? sometimes this is even life threateningly inconvenient: even if you have never been actually unbanked, i bet you have experienced your credit card not working and if you are away from home, even with many thousands in the bank, you might find yourself spending the night outdoors. bad stuff could happen. there is an old and good Jack Lemmon movie called *The Out of Towners* that kind of gives one the flavor of this situation.

Now, one could argue that millions of ES citizen do just fine without being banked but just because someone has had some chronic illness throughout their lives does not mean they are ""fine."" same thing with being poor.

And the really big idea i think is this: not every ES citizen will see the benefits of adopting btc. but the ones who are entrepreneurial (at least potentially), who want to build a business and therefore need a good way to pay suppliers etc. will disproportionally benefit from btc, affecting the country's economy much more than even dozens of people who continue to live in poverty. that is to say, the overall percentage of adopters is not the important number -- it is how btc affects the ES economy and if only 1 percent (for example) of citizens use btc, it could still have a huge influence. 

I bet there is not a single billionaire living in ES (that is, from a business that originated there). i would guess just one person building a business there that makes them rich could affect the GDP massively. i would further bet that the first such business that is created will either be based on bitcoin or at least will need bitcoin to get started.",Bitcoin
"I use a mobile wallet. If I loose my phone, will I be able to access my funds? If so, why is cold storage safer? How can anybody access my bitcoin without the private key?",Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/113kkte/lightning_thursday_february_16_2023_explore_the/) for unanswered questions.",Bitcoin
"I came across this today and I'm surprised I hadn't found out about it sooner. Want to do your part with accelerating bitcoin adoption? You can add local merchants who accept btc to the map. You can also check if there are any merchants that accept btc near you. I found out a few that I'll be supporting from now on.

This will become an increasingly better resource as LN spreads. Downloaded the app also.

[Just in case some of you have not heard of this](https://preview.redd.it/4tyj1936uoia1.png?width=1125&format=png&auto=webp&v=enabled&s=be09a611a0584237cc075ab064784f1f183411d0)",Bitcoin
"I have recently been thinking about cold storage fund recovery and stumbled across nTimeLock, a special function that allows you to create transactions that are only spendable once you reach a certain timestamp. This allows you to create transactions that recover your funds to a new wallet at a certain timestamp but can also be easily revoked at anytime.

This experimental and open source tool that I just built allows you to do just that. In a nutshell, it allows you to create a time locked transaction to another wallet and save it as a backup. This backup also includes a revoke transaction that sends the funds to the signer’s wallet, thereby invalidating the backup.

You are free to set the recipient of the funds to any address and if none is set, the tool will generate a fresh wallet, set it as the recipient and add it to the backup, allowing you to fully recover the funds in the event that you lose access to your cold storage keys. Such backups can be stored on your google drive or iCloud accounts and only become sensitive when the transaction date has been reached. 

As I already mentioned, these backups can and should be invalidated close to their timestamp, as you would only want to use them as a last resort in the event that you have truely lost access to your keys. 

[Here](https://github.com/James-Sangalli/crypto-timelocked-backup#bitcoin) is an example of a real time locked backup file.

Links:

* https://github.com/James-Sangalli/btc-timelocked-backup-web (web interface that works with Trezor, COMING SOON)
* https://github.com/James-Sangalli/crypto-timelocked-backup/tree/master/scripts/bitcoin (hot wallet edition that works with private keys pasted in environment variables using node.js, fully functional and used to create the backup above)
* https://james-sangalli.medium.com/utxo-based-backups-an-idea-for-bitcoin-cold-storage-21f620c35981 (article describing the method in greater detail)

Note that these tools are only experimental and should not be used with serious amounts of bitcoin!",Bitcoin
"https://youtu.be/-IBgnmzuOb0

You will spend your whole life trying to own high-maintenance physical property when you could have just bought bitcoin. The United States government can take your property from you for any reason or no reason at all (i.e. imminent domain). Whether you rent a house or buy a house, the utility of the home is just the same. And after you actually “ own” the home, there are still all types of maintenance costs (groundskeeping, landscaping, housekeeping, etc) and an annual property tax that would render property ownership undesirable.   

Real estate is essentially overpriced property that’s also expensive to maintain. As stated in season 2 of bitcoin versus, bitcoin requires no maintenance cost and there is no yearly property tax to maintain ownership of bitcoin. It is certainly possible that bitcoin can absorb a significant portion of the total market cap of real estate. A 10% reallocation of wealth from real estate to bitcoin would result in 1 bitcoin being worth over $1,000,000 per coin. 

Land preceded modern finance and therefore was not originally meant to be used as a derivative of fiat currency. Land is an insecure storage of value, and an inefficient unit of account. Land also does not work well as a means of exchange because of its clear lack of transportability. 

Owning land is not only a weak form of money, it is also a very difficult asset to acquire, as there are many financial prerequisites that you must have before you are able to buy land in America. There’s no incentive to save your money to put a down payment on land, especially if there are an excessive amount of social and financial requirements that will deny you access to your God given right to own property. 

Bitcoin is an affordable digital property with no maintenance cost. Transfer of value settles in approximately 10 minutes. It was Created to be used as a storage of value, unit of account, and a means of exchange. As the dollar continues to depreciate, bitcoin works as a hedge on inflation, among other things. It is a permissionless asset, which means that anyone can buy it with no financial or social prerequisites involved. Bitcoin is an asset that incentivizes you to save money. 

The price of housing has increased significantly in recent years, making it more difficult for many Americans to afford to buy a home. There are a number of factors that have contributed to the rising cost of housing, including the increasing demand for housing in urban areas, the limited supply of available land for new construction, and the increasing cost of building materials. 

Additionally, factors such as low interest rates and irresponsible lending standards during the housing boom of the 2000s contributed to the increase in housing prices. As a result, many Americans are finding it difficult to afford to buy a home, and the homeownership rate in the United States has declined in recent years.

The recent CPI report suggests that inflation is leveling off and it is possible that the prices we see today at the supermarket and in real estate will become the new norm. The truth of the matter is that the millennial and zoomer generation have been priced out of the housing market. The boomer generation was born in a window of time that allowed them to own a home without the need for an expensive secondary education. 

More specifically, The value of real estate has significantly outpaced the average americans rate of pay and has consequently led to a decline in home ownership. 

There has to be a collective shift in thought for the American working class to realize that an asset in cyberspace can be just as valuable as a tangible asset in real life. Most people think that ownership of land is the only way towards generational wellbeing and that is simply not the case any longer. Bitcoin is the most accessible form of property and it will be the catalyst to a global middle class without any real estate involved. And that makes the world a better place.

[https://youtu.be/-IBgnmzuOb0]",Bitcoin
"Roughly what percentage of Bitcoin nodes run software other than Bitcoin Core?  I've heard conflicting answers to this, so I'm curious what the consensus is here.  It seems healthier for the protocol if there's a mix of implementations.",Bitcoin
"Where did it go?

Really confused. I made several small buys today, more to try out different platforms then anything else. One purchase I made p2p with robosats. Worked perfect and I sent the amount to my desktop  Sparrow wallet. I had several utxo’s on that wallet so I decided to start moving them. I have no other desktop wallet so I emailed myself a receive address from my mobile blue wallet. I confirmed the address and sent. 
The address now has 7 confirmations but utxo’s are no where to be found. I checked the receive address and it is an address from my blue wallet. 
On mempool it shows that it went to that address. 
It is $180 so not the end of the world but I really want to understand. 
Any thoughts?",Bitcoin
"

Hello, so decided to buy bitcoin on zengo but my bank declined because it deosnt allow buying bitcoin so i couldnt buy any. So my question is which bitcoin wallet app allows to transfer in euros and buy bitcoin IN APP, so the bank doesnt see that i buy bitcoin?",Bitcoin
"If you dont run your email service on SMTP and the internet, and instead try to build it on your own closed private intranet no one is ever going to use it.

Similarly, if you dont run your national central bank digital currency on lightning using RGB and taro protocol, and instead try to build it on your own closed private database no one is ever going to use it.

If central banks dont build their digital currencies on the worlds best open permissionless payments network then it will fail, and the worlds best open permissionless payments network is the lightning network. Any CBDC that is not built on lightning will not be considered globally legitimate because it will not be native to the worlds most legitimate open and inclusive payments network.

This is hilarious because its true. Any CBDC built on a private closed database will just stagnate and end up disused, and if central banks build their CBDCs on lightning using RGB and taro protocol then that just accelerates lightning adoption and allows sats to be accepted anywhere a CBDC is accepted which allows for even more adoption of bitcoin as money.

This is amazing. CBDCs cant win and even if they work because they are built on lightning that just allows bitcoin adoption to explode even more.

Gotta love game theory and the inevitable domination and success of open permissionless networks.",Bitcoin
"""Money"" here is *useful units of exchange*. They can be lent into existence. 

TLDR; Fractional Reserve Banking, Central Banks, [Money Multipliers](https://imgur.com/gallery/RpXjWPB) *aren't the thing*. 

Goldbug nonsense doesn't map onto the reality of the modern monetary system. Media that bypasses accuracy for shock or ideology *should be dismissed*. 

If the system is disfunctional, address *the real system*. 

Can also look at the last contentious post:  [Misconceptions about Central Banks](https://www.reddit.com/r/Bitcoin/comments/10wgl5l/misconceptions_about_central_banks/)

....

Alright, going long...

Starting mostly with excerpts from a 2022 [paper](https://link.springer.com/article/10.1057/s41302-022-00224-z). The paper gets a lot of the space correct, but is missing quite a bit. 

>Instructors of macroeconomics have traditionally included some discussion of how banks create money through the money multiplier process although recent changes in the curriculum as well as in the monetary system itself (such as the ample reserve regime followed by the US Fed) have made this choice less attractive. This paper is intended to provide instructors with a framework for teaching how the modern banking system operates by using the money multiplier as a prologue to **understand­ing that the now-prevailing shadow banking system generates new forms of money** and **presents monetary policymakers with new challenges**. Hopefully, the framework will prove useful to practitioners and economists who take an interest in the finan­cial side of macroeconomics as well.

Often called the shadow banking or eurodollar system... this is the *global wholesale banking system*. 

Central Banks were left behind. This new global  banking system arose in the 1950s-60s, and Central Banks largely ignored it. 

>A substantial amount of money market funding for shadow banks takes place through the repo market where agents exchange cash for collateral, usually Treasury debt and typically overnight or for short term. Dealer banks, also called broker-dealers or investment banks, participate in the repo mar­ket both as a borrower and a lender. First, as a market maker in the capital market, **dealer banks borrow from the repo market to fund their inventories of securities**.

Because there's *never enough reserve issuance* to purchase government debt issues. It's the global banking system that ""monetizes"" government debt. Central banks are largely *irrelevant* to this process. This can be demonstrated by the low reserve levels vs. progressively larger debt issuance by the US prior to 2007. 

>The collateral that dealers take in through reverse repos can be re­pledged, or hypothecated again, as collateral in their repo borrowing; such reuse of collateral is called rehypothecation. Dealer banks use rehypothecated collateral to provide “money market funding for money market lending” to other shadow banking institutions.

Banks re-use the same collateral for several runs of lending *simultaneously*. This is true money creation, which filters down through domestic ""faces"" of banks. 

>Just as traditional banking dominated by depository institutions can be usefully characterized by the money multiplier, the shadow banking system can be character­ized by a collateral multiplier. 

Traditional banking is barely the money creation engine anymore (and really hasn't been since the 1960s).  Collateral is the source of money creation for the *global* banking system. Not the Fed, not one Central Bank.. and not commercial banks holding a deposit and lending against it. Money creation is *global*, and *wholesale*. 

>Perhaps nothing better symbolizes the transition from the traditional bank-cen­tered credit system to shadow banking than the Federal Reserve’s decision to com­pile and publish prevailing rates in the three *most visible* segments of the repo market (tri-party, bilateral, and GCF). These repo rates, including the Secured Overnight Funding Rate (SOFR) that has been selected to become a reference rate in place of LIBOR, are closely watched as became appar­ent during the money market turbulence of September 2019. **The fed funds market, once the center of attention, is now only one of several money markets competing for the attention of monetary policy makers**.

Fractional reserve was left behind. Fed Funds is not an effective control of the monetary system. The Fed is a proponent of creating new rates (SOFR) to better track *real money creation* activity... but is largely mistaken in their approach. Much of repo is bilateral and not centrally cleared (*not visible*), with less traceability. SOFR as a rate (based on visible secured transactions) does not communicate as much information as LIBOR did. 

Not going further at the moment (will get too long)... Fractional Reserve Banking, while still present, is a thing of the past. 

Most media that laments the disfunction of the monetary system.. and lays the blame at the feet of governments and Central Banks... *gets it wrong*. 

They describe a system we started to leave behind at the turn of the century. 

Central Banks still perform a useful PR role though... appearing powerful (because they say they are), and absorbing/diverting blame... yet have little to no ability to govern the modern system. 

Additional quotes to ponder: 

The Central Bank's Central Bank ([The BIS](https://www.bis.org/publ/qtrpdf/r_qt1709e.htm)):

>The dollar reigns supreme in FX swaps and forwards. Its share is no less than 90% and 96% among dealers. Both exceed its share in denominating global trade (about half) or in holdings of official FX reserves (two thirds). In fact, the dollar is the main currency in swaps/forwards against every currency.

The dollar is global, created and extended globally.. intermediating most trade around the world. The Fed lacks the ability to fully measure this, let alone issue the units required. 

>Every day, trillions of dollars are borrowed and lent in various currencies. Many deals take place in the cash market, through loans and securities. But foreign exchange (FX) derivatives, mainly FX swaps, currency swaps and the closely related forwards, also create debt-like obligations. **For the US dollar alone, contracts worth tens of trillions of dollars stand open and trillions change hands daily. And yet one cannot find these amounts on balance sheets. This debt is, in effect, missing.**

*Tens of trillions*... *trillions of dollars* daily. These are not ""Fed units"" moving around. These are *global dollars*. It's not missing... It's the wholesale banking system doing what it does, with no need for a Central Bank. 

[Econ Talk 2006 - Milton Friedman](https://www.econtalk.org/milton-friedman-on-money/) approx 5:30 in:

>The difficulty people have with understanding monetary theory is simple; the central banks are good at press relations. Central Banks employ a large fraction of all economists so there is a bias to tell the case, the story, in a way that is favorable to the central banks. 

...and they did a very good job. Ever since the 1930s, many people have been convinced that central banks are central, powerful... in reality, they have a record of questionable competence at best. This ain't apologetics.",Bitcoin
"Visa and Mastercard are just payment processors, they help merchants accept money from customer bank accounts by using card credentials to verify money flows from a customers bank account to a merchants bank account. When you buy something they take your card details and move money from your account to the merchants account.

Visa and Mastercard could very easily add a lightning invoice option next to their card credential option. They would take money in the form of data (satoshis) instantly and directly, then use their global forex infrastructure to turn those sats into the local currency and deposit that currency into the merchants bank account. This could be done with in-house infrastructure or simply through partnerships and APIs with companies like strike.

If Visa and Mastercard do nothing they will become exceedingly irrellevant as digital currency payments start to dominate global commerce. But if they integrate lightning payments now they may end up becoming the worlds largest facilitator of lightning payments allowing them to remain relevant. With their already established global reach they could incorporate lightning payments, turn on that capability and overnight 99.9% of merchants on earth would be accepting lightning through their POS terminals and online checkouts without even knowing it, the merchants would just continue to recieve dollars in their bank account as per usual.

There should not be too much of an issue with AML and KYC for accepting sats and if there is there should be a smaller amount that is exempt from these regulations. For example if your purchase is over $1000 you cant use lightning because identity and origin of funds are unknown but for anything under $1000 people are free to pay with lightning without any party having to worry about any type of regulatory friction. This payment size limit would also tie into the diminimis use exemption for taxes that some US politicans are working on that would allow smaller transactions under $1000 to be free from capital gains tax and record keeping requirements.

It seems to me like Visa and Mastercard as the worlds globally dominant payment processors need to figure out how to accept lightning invoices alongside card credentials in order to remain relevant. Otherwise as time passes and the world integrates lightning into cashapp twitter nostr paypal and all neo banking apps while Visa and Mastercard do not adapt the world will simply start using lightning more and more while using their cards less and less until Visa and Mastercard are as irrelevant as telephone booths.

Visa and Mastercard need to figure out how to integrate accepting lightning invoices alongside card credentials. Its possible to do and they will do it, its just a question of how long it takes them to adapt to the modern world. Its even highly likely they already have plans in the works for this type of integration. Afterall if the world wants to use lightning and Visa and Mastercard do not process those payments for merchants then they risk going out of business all together over the coming decade.

Visa and Mastercard processing lightning payments for merchants is just a matter of time. I know people here do not like the idea of Visa and Mastercard doing this and they think merchants should just accept sats directly, but if Visa and Mastercard do integrate lightning payment processing for merchants then that drives adoption beyond belief. All of a sudden every bank on earth will be under pressure to allow customers to pay via lightning invoices from their banking apps, and every bitcoiner with a lightning wallet would be able to pay any merchant on earth. This one move by Visa and Mastercard would make bitcoin and lightning the worlds global payment infrastructure overnight!",Bitcoin
"Currently, my bitcoin is in an Electrum wallet and Ledger wallet.
I have a Coinbase and Poloniex account.

If I wanted to transfer some BTC to GBP what is the best way to do that?",Bitcoin
"I can’t find a satisfactory answer to this question, but I will concede that I might be too dumb to understand some of the answers I’m reading.

I get that there are a bagillion different seed phrase combinations, so guessing a specific wallets word combo is pretty much impossible.

But what about guessing a random wallet? For instance, if you could write a program that would run random combinations and attempt to recover funds with them. Wouldn’t your chances of hitting one within say 5 years be extremely high?

What if there was a community of pirates doing this? Hundreds of servers guessing seed phrases for weeks. Couldn’t they expect to eventually get a big fish?",Bitcoin
So I bought Bitcoin KYC some time ago and after reading up more about bitcoin and falling down a rabbit hole I want to go anonymous. Is there anyway I can do that after having bought KYC?,Bitcoin
I think I already know the answer but to confirm if I use Billfold to store my seed and later change my wallet from Ledger to Coldcard for example will that work since Billfold only contains the first 4 letters of each word?,Bitcoin
"Everyone is welcome! Come check out how a local miner is impacting his community, promoting bitcoin and freedom! Central NC, a pro-bitcoin state!",Bitcoin
"Curious to know why there are still so many blocks less than 2mb when the mempool has so many transactions pending? 

https://preview.redd.it/8rhkq3874lia1.png?width=1268&format=png&auto=webp&v=enabled&s=da2198a36dcc86d200e552f320b334f6859f2183",Bitcoin
"We all know not to buy a cold wallet used as anyone could make a copt of your seed phrase... but could the companies themselves have a copy of your private keys.

 Couldn't Ledger Trezor etc all have copied of your private keys and one day access your btc?",Bitcoin
"$1k?

$5k?

$10k?

$100k?

$200k?

$500K?

$1M?

$2M?

Never?

Assuming most people put a small percentage of their net worth in bitcoin and are passionate about bitcoin in this sub. So, for those people, there would be no reason to sell a large percentage of their bitcoin?

So, is there a BTC/USD exchange rate where you are converting back to USD?",Bitcoin
"It's nice to see that liquidity exists for those holding bankruptcy claims in the major crypto cases. Some people want to take a present value payment and invest in something of their chosing, as opposed to having a forced investment in a bankruptcy case. At the same time, a  claim sale eliminates risks on recovery amounts and timing. I've seen Claims-Market on the CELFTX/Voy dockets transfering claims.",Bitcoin
"Hello,  Maybe a year or two ago Michael Saylor gave an interview that I think lasted about an hour.  He talked about how he founded his businesses and gave a little history of his life.  At one point he discussed that he believed that the number one reason businesses fail is because of one too many acquisitions.  I have spent a lot of time trying to find this interview with no success.  Can anyone help me out?  Thanks!!!",Bitcoin
"Im looking for a ticket service for a small event (450 tickets) with an option for lightning payments and also traditional payments true stripe. For some reason this is very hard to find :-) any tips ?

Gr, bruru",Bitcoin
"Bitcoin startup Arthur Mining raises US$ 4.6 million and prepares operation in Brazil

\#brazil #bitcoin #news

Photo by [Mario Gogh](https://unsplash.com/@mariogogh?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/pt-br/fotografias/VBLHICVh-lI?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 

[https://cointelegraph.com.br/news/arthur-mining-arrives-in-brazil-with-a-focus-on-bitcoin-green-mining](https://cointelegraph.com.br/news/arthur-mining-arrives-in-brazil-with-a-focus-on-bitcoin-green-mining)

[Bitcoin startup Arthur Mining raises US$ 4.6 million and prepares operation in Brazil](https://preview.redd.it/li5fbcblyjia1.jpg?width=5551&format=pjpg&auto=webp&v=enabled&s=f84e4b02cfba55bb91b58b52a42c73e79e698093)",Bitcoin
"I have done some googling on this topic and cannot seem to find a straight answer other than “near infinite”

How many BTC adreses can be generated by a 24 word seed? Is it possible for multiple different seed phrases to generate the same BTC address?",Bitcoin
"Lately, I've become quite intrigued with the concept of investing in Bitcoin versus saving in Bitcoin. The difference being that the long-term goal for someone investing in Bitcoin is to acquire more fiat currency (preferably as quickly as possible), while the long-term goal for someone saving in Bitcoin is to acquire more Bitcoin.

Like many here, I've been saving in Bitcoin for quite some time, but it never really dawned on me what this truly means. I had an eye-opener of sorts some time ago I thought would be interesting to share;

&#x200B;

I think that a deeper understanding between investing vs. saving in Bitcoin requires taking a step back into history.

Humans have used many different monies throughout history. Salt, shells, Rai stones, glass beads and also rare metals like silver & gold.

Glass beads probably illustrate best how the ease of new money creation for one party leads to exploitation of another party;

Glass beads were introduced by Europeans to many places for trade, but they were probably most successful in Africa. They were able to convince Africans that these beads were quite valuable because not only did they look nice, they were hard to produce (Africans could not produce them), and therefore they were scarce.

European ships used to travel to Africa with these beads used as ballast, and they exchanged these beads for gold, ivory and other goods desired in Europe and the rest of the world. Africans used beads as currency, store of value and status symbol in the form of jewelry for centuries (16th-20th century). They were also used for religious purposes (safe passage to the next life).

Glassmaking was not common in Africa, and of course, it was not common knowledge under the African people that Europeans had technology that allowed them to mass produce these beads very cheaply. Over time, the amount of beads in circulation increased, and eventually became worthless as a monetary tool, as glassmaking became more common in Africa.

&#x200B;

This short history lesson illustrates in simple terms what happens when some have the ability to create new money at almost no cost. They keep creating more while hoarding valuable things that are scarcer, until eventually, the currency collapses.

Those who save in Bitcoin view fiat as glass beads; something with infinite supply and therefore without long-term value, while they see Bitcoin as something that is truly scarce, akin to gold. Therefore, they are not interested in trying to buy Bitcoin cheap and sell it for more to acquire more fiat currency.

Because who in their right mind would want to sell his gold just so he could have more beads...?",Bitcoin
"I basically just asked the same question in a comment and people just don't want to answer this question and downvoted me.

Is this what this subreddit is about? I just wanted an answer.

I buy hourly and was wondering what the upside of buying weekly is vs buying hourly.",Bitcoin
"The lightning network is a second-layer solution on top of the Bitcoin blockchain that enables quick, cheap and scalable Bitcoin payments.

Here is the place to discuss and learn more about lightning!

Ask your questions about lightning

Provide reviews, feedback, comparisons of LN apps, services, websites etc

Learn about new LN features, development, apps

Link to good quality resources (articles, wikis etc)

Resources:

* Here is an awesome list of resources compiled by Jameson Lopp: https://www.lopp.net/lightning-information.html

* Want to test out your lightning fire power? tip the Bitcoin devs! https://bitcoindevlist.com/

* Previous threads: [Search](https://www.reddit.com/r/Bitcoin/search?q=Lightning+Thursday&restrict_sr=on&include_over_18=on&sort=new&t=all)

* Lnbook getting closer towards being finished and can already be seen at: https://github.com/lnbook/lnbook

* Lightning Dedicated YouTube Channel: https://youtube.com/renepickhardt

* Also there is the playlist by chaincode labs: https://youtube.com/playlist?list=PLpLH33TRghT17_U3as2P3vHfAGL8pSOOY

* Lightning stores: https://www.lightningnetworkstores.com/

* Learn more and talk about lightning right here in r/Bitcoin, r/bitcoinbeginners, r/thelightningnetwork, and [the r/Bitcoin chat](https://discord.gg/qE3rWBRNqh)",Bitcoin
"Over the past few weeks, it has become clear that the US authorities have been given the green light by the Joe Biden administration to attack the crypto industry.

The plan is simple: use an attack via the banking sector. From the SEC to influential members of Congress and the Fed, via the FDIC or the DoJ, they are all attacking cryptocurrency, as if preventing American investors from accessing it would miraculously solve all the flaws of the current system.

Obviously, this will not be the case. Bitcoin and cryptocurrencies are not responsible for the flaws in the current system. On the contrary, you should see Bitcoin as a widely available answer to a system that will eventually collapse.

If the price of Bitcoin can be affected by these attacks on the crypto industry, it is primarily because the general public does not yet know the difference between Bitcoin and cryptocurrencies.

But that will come because if you look closely, no attacks are against Bitcoin. None can be, because Bitcoin would only be further legitimized if these American authorities were to attempt legal action against it.

Just imagine: all Bitcoiners have been saying for years that Bitcoin is there to protect us from censorship by the powerful in the current system. If they try to ban it in America, it would be proof (if any were needed) that it is more necessary than ever to switch to the Bitcoin plan.

Nevertheless, all these attacks are a good thing for the future of Bitcoin, as they will show the general public the distinction between Crypto and Bitcoin.

Bitcoin is here to stay, while the majority of cryptocurrencies will eventually die out. Bitcoin has no CEO that can be sued by the SEC. Bitcoin will not be able to pay fines to the SEC, because Bitcoin is you, it's me, it's everyone.

That is the great strength of Bitcoin, and that is why Satoshi Nakamoto made the best choice of all, to remain a mysterious creator offering the fruits of his labor to the people to grow.

And that's what the people have been doing for over 14 years now.",Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/112r1gi/daily_discussion_february_15_2023/) for unanswered questions.",Bitcoin
"A common criticism I've seen of bitcoin in the past is that it's manipulated by bad actors, pumping and dumping. I don't buy it, and so I'm wondering if there's a good explanation for why bitcoin is able to rise and fall so rapidly compared to other assets? I'm trying to understand why similar phenomena isn't seen in the stock market - well-performing stocks don't move the way bitcoin moves. The only comparison coming to mind was GameStop a few years ago (or a couple other meme stocks around that time), but this isn't typical of hot stocks.

Are there rules in place (e.g. stock market open and close) that prevent traditional stocks from building the momentum that bitcoin has during it's highs and lows? Or is there simply that much more volume of bitcoin being traded compared to other assets?",Bitcoin
"Fellow friends,I haven’t seen a clear explanation about this question and I'd love if you could point out for me the right place to research or if not a problem, answer straightforward in here.

Does BTC have a physical server somewhere in the world? We eventually use internet but is there a way to “shut it down” when it really pose as a threat to fiat money? I, person A in Canada, send BTC to you, person B in Australia. After our internet layer, what exactly is the path to make the transaction?

I appreciate any comment, even the funny ones =]",Bitcoin
"As someone who's been passionate about Bitcoin for many years now, I've finally had a desire to aid in the development and distribution of Bitcoin knowledge in any way that I can. 

I recently setup my own Bitcoin/LN Node and am finding myself wanting to work with it any time I can. Problem is, I'm not a programmer nor do I know C++. 

Day to day I'm a sysadmin, but I'm not sure in what capacity my career knowledge or experience transfers into Bitcoin development. The closest thing I have towards Bitcoin development would be scripting, but not in Python, PowerShell. Otherwise, system and network infrastructure management for a company is my gig. 

I suppose I could learn C++ and contribute to the network in that fashion, but there has to be other ways where my skills can help fight this desire I have to contribute towards network adoption. 

Any ideas?",Bitcoin
"I try to understand how the seed phrases root, master keys and BIP84 is working.Found this site: [https://bitcoiner.guide/seed/](https://bitcoiner.guide/seed/)

BIP39 is clear so far: I can create 12 (or more) words + passphrase which prdouces a hex seed.But there is also a BIP32 Root key, not sure how that is created but somehow it's derived from the seed I guess.

In the next section **Derived Addresses** I use BIP84 and the inputs from [https://github.com/bitcoin/bips/blob/master/bip-0084.mediawiki](https://github.com/bitcoin/bips/blob/master/bip-0084.mediawiki). So I'm using 11x abandon and 1x about  without any passphrase. The wiki shows these keys:

* rootpriv
* rootpub
* xpriv (Account Extended Private Key)
* xpub (Account Extended Public Key)

and of course the other derivated addresses, private and public keys using the path levels.

On the [bitcoiner.guide](https://bitcoiner.guide) the xpriv and xpub are called:

* Account Extended Private Key
* Account Extended Public Key

the rootpriv and rootpub values are not shown anywhere, but in addition it has these:

* BIP32 Extended Private Key
* BIP32 Extended Public Key

This is really confusing a bit, anyhow my questions are:

1. I don't need to backup anything except the BIP39 Mnemonic and the optinal BIP39 passphrase, right? Except I'm using a 2/3 multi sig wallet. But my focus is BIP39 and BIP84
2. How are the derived addresses with this kind of path level m/84'/0'/0'/0/0 are used?Is the software (like Electrum, etc.) using automoaitcallly a new address for each transaction or do I need to increment the index in the path level manually?
3. If I create a BIP32 wallet in sparrow, it shows me both: a xpub and zpub key. That key is not shown, neither on the mediawiki page nor in the bitcoiner.gudie  
Sparrow shows me this descriptor: wpkh(BIP392) - what is that?  
",Bitcoin
"A thought on how much Bitcoin to stack as the **first-starting** goal, other than the 21 million club.

- For yourself: One block reward worth for the start of your retirement
- For your children / grandchildren: One block reward worth for each on the year they turn 18

One block reward today is 6.25 Bitcoin.
- If you plan to retire in 21 years, one block reward is about 0.0976 Bitcoin (est 2342 USD today).
- If your future grandchildren will be born in 24 years, by their 18th birthday, one block reward is about 0.003 Bitcoin (est 73 USD today)",Bitcoin
"Few days ago I was going through all my passwords stored in lastpass, as I was going to move them to a local password manager, and came across a stored note that said multibit, and next to it there was a long string of characters.

It took me a while to realise that this had something to do with bitcoin.

A long time ago, more then 10 years ago, I played around with bitcoin. I had a wallet - multibit, and I did some little mining. Very small. But I did some.

Overtime, I ditched it, and completely forgot about it.

Right now, I know next to nothing about bitcoin and wallets.

What I want to know is, can I gain back access to the bitcoin? I'm assuming the string I stored was the key.

How do I do this? Just install a random wallet and enter the key?

What wallet should I use? I'm on a mac.

I'm very curios to find out if I still have something there. Even though I did some very small mining, back in 2011, bitcoin was worth like $0.70. Today it's worth more then $20 0000. It's worth a look.

&#x200B;

Edit. Correction.

The title is also suppose to be: ""Found an old key. What do I do with it?""",Bitcoin
"Obviously I’m not going to sell it. I am an amateur at this and I even know selling it soon would be premature. I keep seeing people say the bull market is over and it will shoot up from here, and then I see people saying this is just a surge and the real money will not come this year but likely 2024 or 2025. I guess it is never too late, but should I throw some more at it now or is there still some time to DCA it for a little longer (let’s say a few more months hypothetically)? Appreciate y’all!",Bitcoin
"This is the first public release of our Bitcoin wallet focused on cold storage.

It's open-source, CLI based and written in Rust.

The goal is to provide a secure solution to generate private keys that can be stored in digital mediums (e.g cloud storage). It's an alternative to current-gen hardware wallets for some users that don't have physical security and stability.

It's beta but should work fine. All feedback is welcome.

Please visit our github page for more information: [https://github.com/planktonlabs/frozenkrill](https://github.com/planktonlabs/frozenkrill)

If you have any questions please ask.

https://i.redd.it/rdyptue5peia1.gif",Bitcoin
"I had over $1900 sent to my coinme wallet and they didn’t credit my wallet.  Is this showing that they received it?

https://www.blockchain.com/explorer/addresses/btc/3C5JvLFZJwjjz5Ms5Bkv27rkX35uMV8fHc",Bitcoin
"Anyone heard of or been to this Empower - Bitcoin in energy mining conference in Houston? Trying to figure out if I'm going to pull the trigger or not. Any feedback on the event or the group hosting would be much appreciated.  

[https://digitalwildcatters.com/empower-2023/](https://digitalwildcatters.com/empower-2023/)",Bitcoin
"[https://www.thedailybeast.com/tyler-gallagher-of-regal-assets-took-their-millions-for-gold-and-vanished?utm\_source=pocket-newtab](https://www.thedailybeast.com/tyler-gallagher-of-regal-assets-took-their-millions-for-gold-and-vanished?utm_source=pocket-newtab)

So, this is a general principle, folks, across all investments, including Bitcoin.",Bitcoin
"Hello,

I'm trying to install embassyOS on a NUC11 so that I can run my own full node.  Unfortunately I  cant flash the embassy .ISO because NUC11 doesn't allow for legacy boot necessary to install.

Currently installed ubuntu and trying to install embassyOS.deb but running into this problem.  I dont know anything about Linux.

Offering $50 over lightning to the superhacker who can help me to a fully functioning embassyOS node

[https://imgur.com/a/UUlwtZf](https://imgur.com/a/UUlwtZf)",Bitcoin
"Brazilian Ministry of Finance, Central Bank and Real Estate Commission prepare decree on cryptocurrencies

\#brazil #bitcoin #news

Photo by [Mateus Campos Felipe](https://unsplash.com/@matcfelipe?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/pt-br/fotografias/Koo05y0j1Aw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)

[https://cointelegraph.com.br/news/exclusive-ministry-of-economy-central-bank-and-cvm-prepare-decree-on-cryptocurrencies](https://cointelegraph.com.br/news/exclusive-ministry-of-economy-central-bank-and-cvm-prepare-decree-on-cryptocurrencies)

[Brazilian Ministry of Finance, Central Bank and Real Estate Commission prepare decree on cryptocurrencies](https://preview.redd.it/lesh0imerdia1.jpg?width=3867&format=pjpg&auto=webp&v=enabled&s=bdcca49a72d5c5acc01a910b2fd8558da07afddb)",Bitcoin
"I live in a south american country and work in a travel agency. a german comes to the office asking for a flight to Bremen, hes probably in his 80s and his spanish is not that good but he can make himself understand up to a point. as i see my collegue getting lost on what hes saying i approach him asking if he can speak english. then we have a chat, hes concerned about a possible war in europe and he was raised in the after wwii peridod. then he started talking about how currencies will devaluate and how hes hedging by buying silver.  he has over 12 kilos of silver he wants to bring from germany to my country.

now thats going to be a challenge because customs are corrupt and probably will seize them on any absourd pretext. i didnt mentioned bitcoin but just agree on how govs print money out of nothing and imagined that in a few years our generation will probably dont need to move kilos of metals to bring thier wealth with them.",Bitcoin
"Link: [https://www.wsj.com/livecoverage/cpi-report-today-january-2023-inflation/card/to-save-money-maybe-you-should-skip-breakfast-fSd6mz0miaAPhUFb2jgy](https://www.wsj.com/livecoverage/cpi-report-today-january-2023-inflation/card/to-save-money-maybe-you-should-skip-breakfast-fSd6mz0miaAPhUFb2jgy)

&#x200B;

https://preview.redd.it/nr4nfk6ipdia1.png?width=660&format=png&auto=webp&v=enabled&s=075abedc279da7443f580eea81c9c7d24db99241

Remember, breakfast is an unnecessary luxury!

On the flipside, here is the price of eggs in Bitcoin as shown by the Fed itself: [https://fred.stlouisfed.org/graph/?g=PUbY](https://fred.stlouisfed.org/graph/?g=PUbY)

&#x200B;

https://preview.redd.it/e8mcy2t6rdia1.png?width=1166&format=png&auto=webp&v=enabled&s=19c462760b5407c71a01a96345ed6af806319342",Bitcoin
"I've reset my old android phone to factory settings. I've not connect a SIM-card and Wi-Fi on it. If I install a open-source wallet on it (only once connected via usb), will it be a secure enough backup of the wallet to store it separately from the seed, for example, in a desk at work? If someone steals the phone without knowing about the wallet on it, then probably without a password to the phone (and even more so to the wallet) they will simply reset and clear the internal memory of the phone. Or at least the passwords will give me time to change the compromised wallet. The conversation is not about large sums and very complex ""spy"" method of backing up. What would you recommend to me?",Bitcoin
What would be the best programming skills a beginner could learn to better understand BTC and possibly look for a entry level job with a Bitcoin company/project?,Bitcoin
"what are your opinions on Bitcoin becoming the world reserve asset instead of US Bonds?

Dollar is slowly dying and many countries are escape from the USD.",Bitcoin
"How can you see an estimate of fees to send a route from my node? If I don't have a channel open to them.  With RTL it seems to be just a guessing game of trying higher fees to find a route.   How can I find the cost to route ahead of time? 

Thanks !",Bitcoin
" 

I am currently researching and educating myself on bitcoin through books, podcasts and Reddit… I’ve learned a lot from guys like Andreas Antonopoulos, Saifedean Ammous, Anthony Pompliano, Michael Saylor, Jason Lowery etc. However I feel like I jumped into the rabbit hole with a lot of confirmation bias, and therefore everything i’ve come across is obviously pro bitcoin.

So my question is, are there any well respected, well educated, intelligent thinkers out there who are against the concept of bitcoin, and explain why in a persuasive, coherent manner? I basically want to make sure I have looked at any worthy opposing views before putting my keys in the bowl. Not my private keys of course.

If not, then recommendations for any other guys or gals I should be checking out would be greatly appreciated. Thanks all.",Bitcoin
"“Bitcoin is deflationary.” No, it isn’t. Stop saying that. The truth is complicated, and this article is going to break it down into three easy-to-understand chunks by exploring the way “time” affects the economic “view” of Bitcoin’s supply.",Bitcoin
"Let’s say I’ve put £170 on a coin last week.

Today i just went to check and it says I’m up by 2.56% (Green) 

Why do my total in this coin is now worth £160?

Someone know this? 

Really appreciate it",Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/111vi5s/daily_discussion_february_14_2023/) for unanswered questions.",Bitcoin
"I was wondering today, if someone keeps stacking sats, what would be the compelling reasons for them to sell? The assumption here is also that there is a minimum 5 year time horizon for holding. Saylor said he would never sell, and recently I heard that he had. Thoughts?",Bitcoin
"hope this is the right forum, keeping it simple i purchased bitcoin with a debit card via coin gate simplex. I used a wallet that usually regenerates it's wallet address when purchasing, there was a slight problem as in i needed to leave the site to confirm with my bank but was too late so i had to renter my details but other than that it seemed to go well. I got emails confirming payment and then received my payment id and transaction id but didn't get the bitcoin. I tried to contact simplex by sending them an email but annoyingly all i got was an automated email telling me to make sure i waited more than 3 hours it had been over 24hours at that point) and then another email asking me to rate the customer service. When i went onto my wallet app all i could find was a bot telling me that i should contact simplex for further help. Via blockchair i have been able to find my purchase so i know it has been registered in the blockchain and theoretically no matter my address the bitcoin should still have been sent to my wallet but it just hasn't. Any help would be appreciated.",Bitcoin
"I understand that the energy cost for proving the work in bitcoin network is the cost for providing decentralized trustless security. From what we know, is it possible that the energy for the bitcoin world is greater or less then the current entirety of the banking and financial industries?",Bitcoin
"Report Link: [https://river.com/learn/files/river-bitcoin-mining-zettahash-report.pdf](https://river.com/learn/files/river-bitcoin-mining-zettahash-report.pdf)

Some interesting charts:

https://preview.redd.it/97sglryzk8ia1.png?width=1540&format=png&auto=webp&v=enabled&s=d2fc81aea115938b57096ec60802e21970bfc645

&#x200B;

https://preview.redd.it/wk0u21n1l8ia1.png?width=1335&format=png&auto=webp&v=enabled&s=f43bec0756ad58c8ae58ad49e2699cf4a2eef99b

**Report Conclusion**:

The rapid increase in hashrate over the past year makes a zettahash seem relatively close. And yet, it could still take a decade or two before the Bitcoin network gets to this point. 

A short term boost in Bitcoin's security budget through major price increases could quickly grow the hashrate by the required 300% to reach a zettahash. If such rapid growth does not happen before 2028 when Bitcoin's block rewards are set to halve again after the 2024 halving, then in the medium term it is likely that only significant Bitcoin adoption can help finance a sustained hashrate of one zettahash. 

In the long term if Bitcoin survives, it is inevitable that it will reach a zettahash in hashrate through efficiency improvements and abundant cheap energy. 

Due to the minimum costs of renewable energy dropping year after year, it is at most a matter of decades before the Bitcoin network is almost entirely sustainably powered. This transition will happen long before significant nation states will be mostly powered by renewable energy. 

Humanity is only beginning to scratch the surface of using Bitcoin mining as a tool to help combat, rather than to reportedly cause, climate change. To accomplish this change at significant scale, Bitcoin's hashrate must not only grow to a zettahash, but far beyond it. Regardless of your climate change policy preferences, Bitcoin mining is going to become a bigger part of this conversation in a constructive way. Education is critical to help people understand how mining can play a role, so that this industry can continue to grow over the next decades with less friction.",Bitcoin
I’am setting up my lightning mode and I don’t know anyone to open a receiving lightning channel with my node. What would be the best way in going about finding someone to open a channel with?,Bitcoin
"I'm currently using BlueWallet but I've noticed it doesn't have Taproot support, so I need to download another wallet.

I would prefer to use a mobile wallet, I'm afraid to use it on PC for the risk of it being infected with malware.",Bitcoin
"I recently transferred my Bitcoin to a coldcard.. I am curious to this question as I am still sort of a novice to the Bitcoin world. 

What is the actual point of the coldcard itself? Like say I lost the coldcard but I still had my seed phrase and keys... Then how would I regain my Bitcoin without the coldcard itself? 

Just kind of confused on why the coldcard or any cold wallet is necessarily when in reality all u need is your phrases and keys? 

Can someone help clarify this for me?

Thanks.. I have learned a lot from this sub. Thanks everyone.",Bitcoin
"Hey y'all! 

If you're a bitcoiner in Dallas, consider joining r/DallasBitcoiners ! 

My goal is to bring together the awesome men and women of DFW to celebrate NgU freedom technology.

Stay humble and stack sats!",Bitcoin
"How do I connect umbrel 0.5.3 to an existing full bitcoin node on the same machine?

Or at least use the existing downloaded blockchain data without redownloading everything again?",Bitcoin
"So about every 4 years, there's halving that reduce miners rewards by half.

From 2020 to 2024 Bitcoin Network will reward miners with about 6.25% of total chain value in exchange for keeping network secure. In the next period it will be halved and on and on.

So my question is:

Doesn't it mean that Bitcoin Network will be 2 times more  vulnerable compared to total value every halving?

Is there anything that will keep Bitcoin safe with lower resources spend on security?",Bitcoin
"I have been DCA'ing BTC for over a year now. In June I had to sell some to pay a unexpected bill.  I bought from Swan Bitcoin and transfer to my Ledger weekly. To sell it I transferred it to CashApp and sold the BTC there because it was quick.  

  
   How in the world do I calculate the Loss on that? considering I was buying BTC when it was 60k+ and sold at 19K-ish.",Bitcoin
"If i buy Bitcoin on a KYC Exchange and then send them to my Hardware Wallet, which i can get a new adress for each transaction will my i still be KYC traceable?",Bitcoin
"It took a lot longer than expected to research and write this. Luckily I found a lot of information in some really good comments in this subreddit :)

This is the outline of the series:

Part 1: The current system  
Part 2: Why Bitcoin was created  
Part 3: The Bitcoin tech explained  
Part 4: My conceptual breakthroughs  
Part 5: The history of Bitcoin and crypto  
Part 6: Threats to Bitcoin  
Part 7: Practical information to get started

And you can find the article here:

[https://shadowysupercoder.com/deciphering-bitcoin-part-2-why-bitcoin-was-created/](https://shadowysupercoder.com/deciphering-bitcoin-part-2-why-bitcoin-was-created/)

Any and all feedback is much appreciated :) I'm trying to convey the value proposition of Bitcoin as clearly as possible.",Bitcoin
what's the trick on how to avoid the high transfer fees from the exchanges? buying a low transcational coin from exchanges n sending it to your non-custodial wallet n swapping it afterward into desire coin? is it? how reliable it is?,Bitcoin
"What happens if bitcoin is globally adopted? What’s to stop the major corps from hoarding. Does bitcoin even really stay decentralised if 5 of the biggest companies on the world control a large % of the liquid supply?

As we know already the money always flows upwards, the most in demand products/companies always end up with the cash. I don’t see how this changes with bitcoin? 

Please enlighten me, I wish only to learn more.",Bitcoin
My btc from my different wallets got stolen. how is this possible guys? Is there some kinda bots or software that does this? What can they possibly have on me that gave them the access?,Bitcoin
"One of the criticisms of the BTC blockchain is the small block size which makes it unsuitable to process large number of transactions. People often quote Visa and Mastercard’s ability to process transactions by many folds compared to BTC.

From what I understand, the workaround for this is the Lightning Network (LN) which allows payments to be done at the 2nd layer before it is eventually verified later on the main chain. 

Tried looking for the answers online but is not able to. Maybe (most probably the case) my understanding of this is wrong but if BTC (through the LN) is adopted by the masses (processing the number of transactions at the current level of Visa and Mastercard), wouldn’t that result in a never ending and growing backlog of LN transactions that has to be verified on the main chain?",Bitcoin
"Hi all,

For the past few years I’ve bought and sold BTC through an app called Mode, but they’re closing down soon and need to move my BTC. Any recommendations? 

TIA",Bitcoin
"This was a real interesting bill that was (re)submitted the middle of last year. It seems to have not moved anywhere. Do these bills just get stuck forever?

[https://www.congress.gov/bill/117th-congress/senate-bill/4608](https://www.congress.gov/bill/117th-congress/senate-bill/4608)",Bitcoin
"I think there are dozens of threads with people being laughed by their own families just for the fact that you’re looking for investment opportunities, which sounds so wild to me.

Maybe I’m lucky but even my retired mother in law owns a few btc and we talk about it fairly openly. The same about investing in stocks and other assets. So is our extended family in Romania. After the communist regime fell in 89, there was severe inflation and poverty that affected my country even until today.

Corruption is rampart too so people have more faith into something that’s outside the govt control. 

Anyone else the same here?",Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/11122si/mentor_monday_february_13_2023_ask_all_your/) for unanswered questions.",Bitcoin
"So i have muun, blue wallet and phoenix and im just not sure of which one the community thinks is the best, id love to hear your feedback and your reasoning as to why you think the one you choose is the best.",Bitcoin
"Got into crypto in 2021 after excitedly jumping in when it was between 40-58k. Bought a bunch and my average DCA price was about 48k… I’ve been able to average it down to 35k USD which I’m very proud of, but definitely a big regret for jumping in so quickly during the height of the bull market. Definitely learned a lot that’s for sure! 

What’s your guys’ average buy price for those who got into last bullrun during covid?",Bitcoin
"It's often said that Gold is the king of all commodities because it has one property that all other commodites don't, and thats the property of being money. But everything is relative, gold is only the best money because it's better than silver, if gold didn't exist silver would be the best money and if gold and silver didn't exist something else would be the best money.

The title of ""king of all commodities"" goes to whatever commodity is best at being money.

Bitcoin is a commodity, it is needed by humaity to operate the worlds most decentralised and trusted value transfer protocol. It is also pure energy captured and stored in digital form ready to be turned back into usable energy anywhere on earth at a moments notice. Even the SEC states bitcoin is a commodity.

Is bitcoin the commodity that's best at being money? Is it better at being money than gold? and the answer to that is yes. Bitcoin is more durable, more portable, more divisible, more uniform, more scarce and more accepted around the world for payments than gold.

So if the commodity that is the best at being money is the king of all commodities then bitcoin is the king of all commodities.

Here's why thats important. We are transitioning from an age of financial asset domination driven by lower interest rates and low inflation to an age of commodity domination driven by higher interest rates and high inflation. The era of the commodity supercycle has begun, financial assets like stocks and bonds will stagnate while scarce commodities including bitcoin will enter a multi-decade long period of domination where they outperform everything else.

Inflation is not over, 25 basis point hikes do not fight 40 year high inflation. The 2020s will be a decade of problematic inflation just like the entirety of the 1970s, and because bitcoin is the most finite commodity on earth it will experience the largest percentage gains on earth over the coming decade.

Between now and 2030 bitcoin will be the most important asset to own because bitcoin is the king of all commodities.",Bitcoin
"I was talking with a friend and he made the claim that Bitcoin has no value like fiat currencies because it isn’t backed by gold or oil.
I know the hash rate backs the Bitcoin network but can it really have intrinsic value if the electricity used to secure the network is dependent on electricity generated by fossil fuels? I guess if all the electricity was generated from renewables that would be different. Your thoughts are appreciated.

Edit: thank you for all the input!",Bitcoin
"So my boyfriend sent me about 3k in bitcoin this morning (which he purchased right before he sent to me) and it has only been plummeting since, so i've made no profit since he sent it to me. I was wondering if i sell it right now, given it has decreased in value, will i have to pay taxes on it? no right? cause i haven't gained any profit and nor has he cuz he just purchased that amount this morning and it has since decreased in value. please help! i want to sell because this money is for personal use and i don't have western union, so this was the only way to send me money. i am scared that if i sell it, i'll have to pay a ton of taxes in the future on it for selling. thanks! i live in USA btw!",Bitcoin
How likely do you all think it is for Grayscale to have to sell off it's massive holdings? I know that shares have been sold as part of a restructuring but this is a frightening possibilty?,Bitcoin
"The Mississippi Digital Asset Mining Act would not only legalize home digital asset mining at a private residence and operating a digital asset mining business in an area zoned for industrial use, but it would also protect individuals or companies engaged in proof-of-work (PoW) digital asset mining in the state from “regulatory challenges at the state and local level.”

This means that if signed into law, the bill would prohibit implementing noise limits for home mining beyond currently defined limits, imposing requirements beyond those applied to data centers, modifying the zoning of a mining business without proper notice, procedure and opportunity to appeal, and establishing a “discriminatory rate schedule” for crypto mining businesses. The bill would also exempt miners from money transmitter status. 

Mississippi has some of the lowest electricity rates in the US, and lawmakers see the bill as a means to encourage more cryptocurrency-based investments. 

This new development shows a shift in interest in cryptocurrency in the US, especially since 2022 was marked by a downturn that was fueled by a series of scandals in the broader crypto sector.

What do you think about it?",Bitcoin
"Banco do Brasil now allows taxes to be paid with Bitcoin and cryptocurrencies

\#brazil #bitcoin #news

 Photo by [Eduardo Soares](https://unsplash.com/@eduschadesoares?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/pt-br/fotografias/7I5A7630GpY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 

[https://cointelegraph.com.br/news/brazil-s-oldest-bank-allows-residents-to-pay-off-tax-bill-with-crypto](https://cointelegraph.com.br/news/brazil-s-oldest-bank-allows-residents-to-pay-off-tax-bill-with-crypto)

[Banco do Brasil now allows taxes to be paid with Bitcoin and cryptocurrencies](https://preview.redd.it/jcsx5sev60ia1.jpg?width=4000&format=pjpg&auto=webp&v=enabled&s=bef4a6ecf4090088cced03599459fd7463009ddf)",Bitcoin
"Dear Bitcoiners,

I need the maximum of people to watch this documentary and tell their friend in **New York City** and **New York State** to signup using this Bitcoin link.

# [https://bitcoin.repmyblock.org/bitcoin/exp/register/movie](https://bitcoin.repmyblock.org/bitcoin/exp/register/movie)

The mainstream press is not interested in telling the story so we need to create a force to have political influence. **The Bitcoiners in Nebraska showed it was possible.**

We need the press around the world to shame the New York press on keeping their readers in the dark and letting zombies to be elected.

If you want more info about what I have done, [***https://newyork.abolishthebitlicense.com***](https://newyork.abolishthebitlicense.com).  


[https:\/\/bitcoin.repmyblock.org\/bitcoin\/exp\/register\/movie](https://preview.redd.it/2kccadh420ia1.jpg?width=450&format=pjpg&auto=webp&v=enabled&s=c0d98dd8c8316b8364cbc3e4f8ae0099a65ccb8c)",Bitcoin
"A friend gave me half of his private keys and told me if i figured out all of it, the coins inside is mine (0.04) he is just challenging me for fun, but that got me thinking.. is it even possible to do that?",Bitcoin
"Look at Paxos, a trust company regulated by the NYDFS, they have stopped issuing their BUSD stablecoin because of some moronic letter from the SEC. Now whats to stop a company that issues a gold backed stablecoin from getting shut down or closed by authorities, nothing.

Gold cant work because it requires third parties and counterparty risk to make it usable in modern day commerce.

Bitcoin is the only system that can operate on a global basis without the risk of getting shut down.

Bitcoin will be worth more than gold for this reason alone.",Bitcoin
"If standard traditional ATMs around the world didnt just accept bank card credentials for cash but also had the option to display a lightning invoice for the amount requested and then accept sats for cash it would force banks to allow their users to scan lightning invoices from within their banking apps.

ATMs could accept sats over lightning and dispense cash and have their btc service provider deposit the local currency equivelant into their operating bank account.

If every ATM allowed the option to pay a lightning invoice displayed on the screen instead of just a bank card then banks around the world would HAVE to allow their mobile app users the ability to scan lightning invoices with their banking app or risk becoming redundant to neo banks and fintech companies like cashapp.

Lightning invoices are the one global standard that all ATMs could plug in to no matter what country they are in. Banks would HAVE to then allow the ability for their users to be able to withdraw cash using lightning.

Someone from South Africa could be in Japan and without having any bank card they would be able to go up to any ATM in japan and scan a lightning invoice with their banking app to withdraw cash. But then think about all the flow on benefits of this happening. Not only would banking apps be able to withdraw cash from any ATM on earth without needing a card but all of a sudden every banking app would be able to scan lightning invoices even if its to pay a merchant or send money to a friends wallet. And every ATM on earth would be able to accept a request for cash not just from banking apps but also from any btc lightning wallet so people would be able to use ATMs even if they have never had a bank account before, all they would need is a lightning wallet. All btc lightning wallets would be able to send and recieve to all banking apps too.

The future is obviously all ATMs, banking apps, merchants and wallets etc plugging into lightning, but the one thing that would speed all of this up would be ATMs deciding they will allow the option to request cash by paying a lightning invoice.

One of the top priorities should be exchanges and bitcoin service providers searching out partnerships with ATM providers. Every exchange on earth should be trying to forge partnerships with every ATM provider on earth to help them to plug into lightning. From there everything else in terms of adoption from banks plugging into lightning to merchants accepting lightning flows on automatically.",Bitcoin
"One of my supervisors at my job is Iranian. He recently said that his mother had purchased a VPN using Bitcoin and was then able to use WhatsApp and other social medias to contact him. The Iranian government has made social media and VPNs illegal.            

When he said this in the group meeting, many of my coworkers chuckled and said things like 'wow you're mother is really sophisticated/technical!'                     

I know better than to chime in anymore, but I really wanted to go on the whole ""first world nations like us don't need it as much- it's those in third world countries and those under authoritarian regimes who need censorship resistant money more"" rant, but instead I just smiled and nodded.              

From a recent RadioFreeEurope article in August 2022 *""Iran's government has approved a set of regulations for trading with cryptocurrencies, a move that potentially allows the country to skirt some U.S. financial sanctions imposed over Tehran's nuclear program.""*              

From a CNBC article in October 2022 *""Demand for VPN services skyrocketed [in Iran] over 2,100% on Sept. 22 compared with the previous 28 days, according to figures from Top10VPN.""*              

In 2021, Reuters reported around 4.5 percent of global Bitcoin mining takes place in Iran, due to cheap electricity.              

November 2022; *""Binance has processed Iranian transactions with a value of $8 billion since 2018 despite U.S. sanctions intended to cut Iran off from the global financial system""*

4 weeks ago; Iran and Russia are collaborating to launch the “token of the Persian Gulf”, a stablecoin backed by gold. Not directly Bitcoin related but within the conversation.                 

This is not an exhaustive list of Bitcoin and crypto activity in the country, but I would like to end this post saying that I do not fully support any nationstate, especially those who the UN and HRC have condemned for prior and ongoing abuses of international human rights. But what I will support is anybody, any company, any citizen, any refugee, and even any government, using Bitcoin to enhance their freedoms. Via la ₿itcoin.",Bitcoin
"There are probably thousands of brands in this industry with millions of locations around the world in total. You cant walk 5 minutes in any city on earth without passing one.

It seems to me that it should be a no brainer for these shops to add bitcoin to their list of currencies available for exchange along side the yen euro dollar peso etc. Afterall bitcoin is now one of the largest currencies by market cup on earth. It would add revenue to the bottom line of these shops and exchanging small amounts of btc for cash should fall under the same rules as exchanging foreign currencies for cash as these places do require ID.

Why on earth after all this time do currency exchange shops still not exchange bitcoin?",Bitcoin
"I am aware of the importance of accumulating small amounts of bitcoin in the long run, and I'm already doing what I can to DCA myself. However, my parents see it only as a gamble, considering how much the price fluctuates today.

How can I convince them that bitcoin is actually sound money, and tell the importance to buy some and hold it for the future?",Bitcoin
What’s the point of no kyc exchanges if you need a  bank card to fund the account? Any exchanges that accept prepaid debit cards?,Bitcoin
"Ask (and answer!) away! Here are the general rules:

* If you'd like to learn something, ask.
* If you'd like to share knowledge, answer.
* Any question about Bitcoin is fair game.

And don't forget to check out /r/BitcoinBeginners

You can sort by new to see the latest questions that may not be answered yet.",Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/1108neh/daily_discussion_february_12_2023/) for unanswered questions.",Bitcoin
"Pi is together and up, I just don't know what software to use. Umbrel? Citadel? Ronin? I want privacy yea, but mainly I just want to do my part in making the network stronger. It would be nice to have a home server too...can anyone help me with a list of the best os for my pi?
Any help would be much appreciated. Tyvm.",Bitcoin
"I currently have a sparrow wallet setup with no account separation.  Everything is KYC, all in one spot. secured with my coldcard.

i would like to begin buying non-KYC bitcoin. But would still like to manage it on sparrow, with the same coldcard device, with the same private key.  can somebody ELI5 how to do this? i need some help.

Thanks",Bitcoin
"I don’t have much knowledge about cryptocurrency but as far as I understand aren’t stablecoins also decentralized and have no government control? Also, wouldn’t it be better as the value isn’t fluctuating and is less risky. If the purpose was just making transactions then Stablecoins seem like a better option to me but that isn’t the case with most people and I’m not able to understand why. If it was a stablecoin pegged to the dollar I can understand why people avoid it as the government has control over the value of the dollar but if it’s pegged to a commodity like gold than whats the issue?

Thanks in advance for an explanation.",Bitcoin
"U - S - A - B - I - L - I - T - Y

The biggest, greatest, and worse situation Bitcoin has going against itself is its process. The process it takes to buy, but more so, sell Bitcoin, is cumbersome, confusing, and way to technical for any average person to be able to use it. The problem truly is THAT SIMPLE!

To all of you who are leading the way in new app building and specifically, anyone having anything at all to do with making the process of Bitcoin more user friendly LISTEN UP! The world leaders in technology should make it obvious, it's those companies that have made the usability and interaction between user and technology friendly that have set the trends and have been most successful.

In the case of bitcoin, it takes damn near more than a tech degree to be able to understand the process of even beginning to get involved with such a thing. What does that say for prospective user confidence? And that statement is coming from a guy who was the VERY FIRST teacher's assistant, in the VERY FIRST computer class ever offered on this planet. (See Below)

Bottom line Bitcoin lovers, until this thing follows the rule of K.I.S.S. Keep It Simple Stupid, it will never amount to anything close to a value of $120,000 per coin. Mark my word.

Allow me to share a personal story with you. A good many years ago, some of you may recall, Google was not just preparing to sell off their coveted systems of Adsense and Adwords, but they had actually sold it. It was a Friday they announced the sale, on Monday they would turn over the goods. That Sunday prior to the hand over Monday I wrote to the executive leadership and CEO of Google. I came outright and stated that they were all fools if they were going to go through with a sale like this, with confidence I informed them that if they did go through with this sale that within less than 5 years their company would go BK belly up. I then went on to share with them in detail the reasons they should not make such a mistake, telling them that if the internet was one massive neighborhood with each site and page being a house with a front lawn, that they would be giving away the marketing of their logo across every single house on the entire planet. You know those political representative signs you see around election time stabbed into peoples front lawns? Well I told them that by selling adwords and adsense they would be giving away their entire brand equity, and by having those signs in all of those lawns there was no way possible even to determine the massive value of brand equity that they would give away. Honestly, the value of those ads alone is impossible to gauge but it must be valued at billions in equity if not a trillion USD.

On Monday the news was quiet, and the sale did not proceed.

Now, I am telling all of you here to spread this message I bring to you and spread it far and wide, and do what ever you can possibly do to make this shit user friendly. Know why? Because Bitcoin, your Friday just passed, and today is my Sunday. Just for you.

It is time to get hot guys.

Sincerely

Scott Bruno  
AKA - 13  


(It was 1978 as I entered into junior high shool at John F Kennedy Jr. High in Cupertino, 1 block from Apples first building. Since it was Apples strategy to give their computers away to schools so to invest in the youth that would pay back in good time, ours being the closest school to the company received the first computers. Thankfully, my parents had already spoiled me with a Commodore PET, which ended up thrusting me into knowing more than any teacher did at the time about computing and the rest was history)",Bitcoin
"I was using Jaxx Liberty a little over a year ago and I sent about 1 BTC to the receiving address that was provided in the app. I made sure it was absolutely correct and it even showed up on my balance in the app. Later when I went to send the BTC to another address I kept getting an error code (TX-017).   
The balance still showed as having the proper amount but I could not make a transaction. I took note of all my private keys and then reinstalled the app. When I paired the freshly installed version the balance showed as zero.... I checked the private keys and they were correct but the private key for the wallet I sent the BTC to was now showing as a different public address. It seems as though the app had some sort of bug and paired the wrong private key to the public key. I went on blockchain explorer and checked the old public address and it still shows the BTC is there. It's been more than a year.   


Their support told me to try and restore the wallet using my 12 word seed phrase on another BIP39 wallet such as exodus or edge. I tried that and still zero balance showing the same private key but different public key.   


It does not seem like I was scammed or like Jaxx took my funds because it's still there over a year later. It does seem like they had a serious bug though and I feel like it's their fault but my problem at this point.   


If anyone has any suggestions that lead to the recovery of my missing funds I will tip you well.",Bitcoin
"Is it possible that a paper wallet can be the most secure option for someone who just wants to hold BTC long-term, with no real intention of using it as currency? At least perhaps until it becomes less volatile and more widely adopted as currency.

The issue I'm trying to work around is trust. 
I'm not exactly the most tech savvy individual but I get why Bitcoin is valuable. I have bitcoin, I believe. 
I FOMO'd in back in 2017 and have held my Bitcoin on exchanges ever since. Thank the heavens they've been secure and safe up to now.

I know this is the wrong way to do it and after doing some homework I'm starting to think the old school paper wallet is the safest.... for me.

A real old wallet that has no weak points is almost impossible for me as I don't understand the tech well enough. From what I've read eve an air-gapped laptop  can be compromised. Also doesn't that require trust in a 3rd party to develop wallet generating software that needs updates etc? 

Then multi-sig. Also requires a great deal of trust in a third party because if you don't trust them or don't need to, why give them signing power? 

The the hard wallets like Trezor and Ledger. Again, is their not some form of trust with these companies manufacturing the hardware. How would I know there is no malware embedded there by some sneaky no-gooder?
And don't you have to write your recovery phrase "" on paper or some other medium"" anyway? 

I know the storage is best chosen to fit the usage but I am stuck for a decision. 

I'd appreciate some knowledgeable opinions on this because the only thing I know is that I'm currently doing it wrong.",Bitcoin
"This is one of Bitcoin's main value proposition.

Your money is your money and you can do whatever you want with it. Same goes with any other of your private property, for example your TV or your coffee table. If you want to exchange your bottle of water for an apple with someone else, who is anyone to say that you can't?

You might be wondering...

What if you want to do something illegal with your money? For example: hiring a hitman to kill an innocent person?

**Then this is how it should be:**

You hire a hitman. Someone finds out. You get punished (you go to jail or you get killed in revenge by a victim's family member).

**This is how it should NOT be:**

You ask the state permission to spend your money. You demonstrate to the state with paperwork that you're not going to use this money for illegal purposes. Then you go ahead and hire the hitman anyway using cash or any other method.

See the difference?

Let me give you another example:

**Imagine the state wants to reduce rape cases. In order to do that, they pass a bill stating that going forward every time two persons want to engage in a sexual relationship they need to fill out a government form stating that they both consent to this act. If you have sex without filling out this form, you're commiting a crime.**

Do you see how ridiculous this is?

This is exactly what they do with your money whenever you want to send a wire transfer of $20k to someone else. You need to ""ask permission"" first.

Please stop simping for the governmet to make bitcoin legal tender. The entire point of bitcoin is that we don't need government's permission to use our money.",Bitcoin
"Two questions 
1- out of all the cold wallet storage devices whats is the best and why? 
2- how does one best use a cold storage wallet if you are making weekly bitcoin purchases?",Bitcoin
"This Tuesday, February 14th, will be the next release of the US CPI data, which will directly influence future US Federal Reserve interest rates. If the data shows inflation continuing, the expectation for market to price in a rate hike from the fed will likely send asset prices tumbling, including Bitcoin. If there is a collapse of the current support level, it could present a great opportunity to buy bitcoin. Is anyone excited to stack some more cheap sats?",Bitcoin
"Bitcoin market follows the laws of Brownian Motion.

&#x200B;

[P:price T:time\(cycle\)](https://preview.redd.it/utenpbljgsha1.png?width=101&format=png&auto=webp&v=enabled&s=7c595c2120f5f52cff8e87010dbc62998d65e894)

&#x200B;

https://preview.redd.it/q3wjko2qgsha1.png?width=1478&format=png&auto=webp&v=enabled&s=287b48ebbf7713cc4f4a01c7064ac19cc6198ba4

&#x200B;

https://preview.redd.it/lp73lsmsgsha1.png?width=1333&format=png&auto=webp&v=enabled&s=dac9f53c88102a8e38fc8461265ea5c699f89abb

&#x200B;

https://preview.redd.it/g0rzdxrugsha1.png?width=1327&format=png&auto=webp&v=enabled&s=c143d0a750bb03493bfe7d8b54c05d610327823c

&#x200B;

https://preview.redd.it/k1ftt6sg8yha1.png?width=1793&format=png&auto=webp&v=enabled&s=ada7d72ca39bb05ffad4421c61d6ebfd4aa1fd7a",Bitcoin
"Pricing the US Dollar in terms of infinite Monopoly cash is a JOKE.

If someone offered you literal Monopoly money for anything, you'd laugh in their face. Yet, ironically, what the rules of Monopoly state are essentially EXACTLY what happens with USD as needed. I downloaded and highlighted the official rules from Hasbro. 

Pricing finite bitcoin in terms of exponentially infinite USD is equally as much of a joke as someone offering to pay you in Monopoly money. 

Once you realize, from a fundamental supply perspective, there's literally no difference between USD and official Monopoly money, you'll rush to store your value in something else. 

Regardless of the amount, I'd get off having zero Bitcoin. 

Your choice. Everyone will get it at the price they deserve.",Bitcoin
"Put differently, if someone in the 1st world immediately invests their discretionary income, what difference does it make to them if they get paid in fiat or bitcoin?",Bitcoin
Are there any groups or entities in Fiji trying orange pill the people of fiji? I would love to get involved and help in many aspects including sats..,Bitcoin
"Had to get cashapp and signed up for the Bitcoin feature of the app. Anyone have experience with it and know how it is with privacy, withdrawals etc.? I will be sending all coins to a self-custody wallet once I receive some.",Bitcoin
"Bitcoin has a block size limit. What about transaction data size? I heard there was a single tx block recently, which means the tx is 1mb or larger.

So is there a transaction size limit or not? Why?

Edit) size -> data size. To make it clear.",Bitcoin
"Hi guys as every sunday, I like to post this here too:

*Hi guys, this has been a long journey but all journey come to an end. I say probably because the data posted by CoinDance (my source) has one week lag so maybe we will get data next week (and maybe I will do some recaps).*

*But still even when LocalBitcoins will work for more 4 days, now that it announced it was shutting down people already stopped using it.*

*We had great weeks with over 2k BTC traded in one WEEK years ago!*

&#x200B;

[Traded BTC since 2018. Source CoinDance](https://preview.redd.it/0m35hldynrha1.jpg?width=1816&format=pjpg&auto=webp&v=enabled&s=7d2d1c66365f6d73b13049221ea2ba588aff9d2e)

*For us Venezuelans, LocalBitcoins will have a special place because when the government was really hard exchanging bolivares to any other country (it was forbidden by law), we found a windows in LocalBitcoins.*

*Time passed and alternatives like AirTM, Binance, Reserve came out and LocalBitcoins started losing its userbase with only some hardcore fans. Also they never updated or got an smartphone app.*

*These hardcore users will move probably to Binance (where there is P2P, bolivares to BTC).*

*Let's see what happens with other alternatives like Bisq. I wonder if the traded volume is open information?*

*LocalBitcoins was nice because the public data. Binance, AirTM and others do make public anything so, we will never know.*

*One BTC traders around 550,000 Bs. (Bolivares). Doesn't seems to be that high considering the inflation, but government did a trick and has removed 14 zeroes from the currency since 2007.*

*Monthly minimum wage is 130 Bs. (around 5 USD) and average is around 120 USD. Living here is a problem, one whole chicken in more than one month of work (at minimum wage).*

*Inflation is around 300% yearly, way lower than before.*

*People use crypto to avoid inflation, receive remittances, exchange mining profits (as I always say, electricity is  free for now) and just as a form of payment.*

*Any question, let me know!*

[*https://en.wikipedia.org/wiki/Hyperinflation\_in\_Venezuela*](https://en.wikipedia.org/wiki/Hyperinflation_in_Venezuela)

[*https://coin.dance/volume/localbitcoins/VED/BTC*](https://coin.dance/volume/localbitcoins/VED/BTC)

[*https://coin.dance/volume/localbitcoins/VED*](https://coin.dance/volume/localbitcoins/VED)

[*https://localbitcoins.com/country/VE*](https://localbitcoins.com/country/VE)

[*https://www.reuters.com/world/americas/venezuelan-teachers-march-better-pay-amid-sky-high-inflation-2023-01-16/?utm\_source=reddit.com*](https://www.reuters.com/world/americas/venezuelan-teachers-march-better-pay-amid-sky-high-inflation-2023-01-16/?utm_source=reddit.com)",Bitcoin
"I'm all about using hardware wallets but I want to have the 100-200$ equivalent of BTC ready to be spent from my phone without any approving device. 

Which app would you suggest to do this? Self-custody is a must.

I'm thinking in terms of being able to send/receive BTC in unexpected situations.",Bitcoin
"anybody knows if the two straight lines through the B is a reference to the $?
Or is it a reference to the ¥, the claimed birthplace of satoshi?",Bitcoin
"So I have single cold card and I think I'm going with this method.

Which one is better and is there any technical details I need to know?

Any aditional ideas are very welcome, also I can use old hardware to convert it into luke-warm but still offline device.",Bitcoin
"I love bitcoin but I always found it hard to believe it would one day become the worlds money because fiat and government control over the financial system are so imbedded. But as im watching Jim Rickards talk about CBDCs I had a realisation.

CBDCs are horrible but they will be forced upon us within this decade. They are not ""cryptocurrencies"", they are not ""blockchains"", CBDCs are simply centralised databases that will allow a synthetic digital dollar to be transferred from a consumer banking app to another consumer banking app whether thats a friend or a merchant or whatever. Its a digital balance transfer system between banks that a central bank has complete insight into and ultimate control over.

But if I have a digital dollar account and i want to send money to a merchant in china that has a digital yuan account it doesnt work because CBDCs are not global nor interoperable. There may be some blocs that create a unified CBDC on the same rails like BRICS or an IMF rail or something but even then its a botched segregated handful of systems making global commerce a mess. There is however still one politically neutral global value transfer system and thats bitcoin. So to send money from a digital dollar account to a digital yuan account there will be private sector services that use bitcoin as the transfer mechanism between CBDC systems to assist global commerce and interoperability.

Heres the thing. Whatever is used as the transfer mechanism becomes the money. We had gold, we traded gold, gold was money, but then we created paper currency as the way to make gold easier to use over long distances. Gold would be turned to paper currency and moved around the country then turned back to gold at its destination in the same way CBDCs will be turned to bitcoin sent overseas and turned back into CBDCs at the destination. But because the transfer mechanism has greater utility to transfer value people stop holding the thing considered money and just hold the transfer mechanism medium.

So this natural law essentially means even people who dont like bitcoin and think CBDCs are real money will end up holding bitcoin over CBDCs just because of its transfer mechanism utility. The biggest thing being that bitcoin is not just a more convenient transfer mechanism it is actually a more sound form of money than CBDCs.

There will be some that hold bitcoin because of its sound monetary characteristics and others who end up holding and using it just because of its transfer mechanism utility and these two forces combined will lead the free market to converge solely on bitcoin and discard CBDCs just as easily as people started using paper currency and forgot about gold.

Add to this the fact bitcoin is the only option as the worlds reference currency for trade long term as its the only politically neutral money and its so easy to see that there are a multitude of forces at play that all lead to the same eventuality of bitcoin naturally becoming the worlds global monetary system and reserve currency.

The bitcoin standard becoming a reality is something not even CBDCs will be able to prevent.",Bitcoin
I feel like if I get venders to consider it then we've gained another transaction to the cause. Some times its about reaching those that aren't fully grasping it.,Bitcoin
"Does anyone else feel non-bitcoiners would feel more inclined to buy when then can buy 100,000 sats for $21?

Yes I know they can do the math to work that out rn but we're talking about experienced btc enthusiasts.",Bitcoin
"Just a little reminder that hard political times are often the best opportinities.

Stay aware and use these times to progressively earn your independence #Bitcoin",Bitcoin
"As bitcoiners, we should be interested in increasing the adoption of our beloved monetary system.

Lets pick a date together in which we all go to one store and buy something, it can be the smallest of things. At the register, ask if it is possible to pay with bitcoin. If we ask many of times enough, the entrepreneurs will eventually be interested in setting up payment systems for bitcoin which will increase adoption.

&#x200B;

I think May 22nd is a great day as that is bitcoin pizza day (or the first exchange of bitcoin for consumer goods). Who is with me to organize this matter more clearly?",Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/10zdvxp/daily_discussion_february_11_2023/) for unanswered questions.",Bitcoin
Question: So given the fact that Bitcoin is finite and will no longer be mined.... why wouldn't the cryptocurrency community just choose its next currency to become top dog and bitcoin then slowly fades into oblivion? Similar to how people no longer want to accept pennies.,Bitcoin
"TL;DR Bitcoin is already adopted by the highest influencing humans, the rich.

I don't know if Bitcoin will ever be widely used as a currency for day to day purchases.
But I don't think it's important for it's survival at all.

I think it is fulfilling a critically important rule as a way to secure tradable assets without having to trust anyone but yourself.
this was never possible in the history of the world. as a wealthy man, you always had to trust someone or keep some big vault.
now all you need is to remember some words.

If you are a rich man today on this earth with minimal sense, you own some bitcoins, purely to secure yourself against the unknown, doesn't matter if you know anything about it, when you're rich enough, you'll own bitcoin.
If this is true, it then follows every rich man on earth as the wish and heart desire for Bitcoin to survive.
Those rich men, rather you like it or not, are the most influencing individuals on the planet.",Bitcoin
"as we all know, bitcoins are created by mining, currently about every 10 minutes 6.25 new BTC are put into circulation. That is about 900 BTC per day.
These new bitcoins are paid out to the miners in the form of rewards. 

Example of the exchange Binance:

I want to buy bitcoins and transfer 100 usd to binance. With this 100 usd I can buy bitcoin. Now I own my bitcoins and Binance owns my fiat money. That's true up to this point right? 
But where does Binance get their BTC from?
on which marketplace does binance buys its bitcoins?",Bitcoin
"Hey all. 

I was recently in the market for a new cold storage wallet, and then I kept seeing tweets about how if you aren't using your full node to validate your own transactions, it is essentially useless (I know it's not entirely true). It got me doing some research about using Bitcoin Core to store all my Bitcoin as it seems like this really contributes to Bitcoin's decentralization. 

I also know that I could use a lightweight client (I assume this would be the cold storage computer UI) and connect it to my full node and it offers a similar advantage. 

Please help me understand what I should be considering here. 

Additionally, would it be safe to store all my Bitcoin on my core node?

Thanks in advance.",Bitcoin
I wanted to sell my bitcoin in person but I have no idea how to find buyers. My next step is to find a website I can sell my bitcoin to for fiat into my bank account. What are the best and worst websites for this?,Bitcoin
"Hi all,

I'm building a Bitcoin-only web store for buying real goods like home essentials here in Canada. Company mission is to grow the 'circular' economy of bitcoin. 

Trying to decide on a company name and it would be super helpful if you could share your #1 selection from the below list - I can't decide so taking to Reddit!  
 

https://preview.redd.it/1hy8u3xu5mha1.png?width=1034&format=png&auto=webp&v=enabled&s=a6cdf0b32de9d7cde3ca316b07edf3fd1d7c5dd7",Bitcoin
"imagine a world where bitcoin took over the fiat currency. The government and the central banks have no longer control over the money. They cannot anymore adjust the rates, taxes and so on…

how would this world look like?",Bitcoin
I have been fighting the urge to throw my emergency fund into Bitcoin. I’m curious how y’all have managed to not just go all in,Bitcoin
"Let me see...

Why would you eat at a restaurant if cooking at home is cheaper?

Why would you buy underwear instead of using that money to buy certificate deposits?

Why would you travel to beautiful places if you could just stay locked at home and put that money in the SP500 instead?

It's the same logic.

Purchases are not ""mathematical decisions"". Value is subjective. I put way more value into going to a Judas Priest concert *right now* than keeping those sats for the future. I simply spend and replace.

Under a Bitcoin standard, people would be more likely to save money. They would spend money only on things they consider really valuable. **But they would still spend money.**

Because of the above, businesses will be forced to produce high quality products. 

It's that simple.",Bitcoin
I do lots of sports betting and am trying to get it out from my account and to my bank. By bookie is suggesting to take it out as BTC but I don’t know how to then transfer it to USD. I was told I’d need a wallet and I I’d have to go through like coin base or a company like that. Is that the right way to do it?,Bitcoin
"So added a couple super low priority transactions few  days ago.  Thinking they'd get processed sooner or later no big deal whenever.  Well, then this whole mempool being the largest it's ever been and yeah... pretty sure these things are not getting processed.  I was under the impression they'd auto cancel after 72 hours but so far they haven't.  Where can I find more info on what is going to happen?  If I look at the transactions they still just say they're at the very end of the memory block but were added over 3 days ago.",Bitcoin
"I was first introduced to computers in the military and learned using DOS. DOS is what we used before gates & Jobs invented pc friendly software for Dummies who couldn’t grasp c/ command with green letters on a black background screen. 
As a civilian I worked for Pepsi Cola in late 70’s and they taught me as a manager/ & Sales rep to use pc’s with DOS before Microsoft and Apple invented software for dummies. 
I was no techy,  but I was alway the guy the IT dept PC guru always called to talk our employees;  mostly girls, how to unscrew problems on their individual desktop computer problems. 
Or!  Other, most often, older managers, and Sales people who were just plain scared to death of computers. If they couldn’t type it on a type writer, or copy it on carbon paper, they hated computer technology.

So bottom line here,  or the points I’m trying to relate here is crytocurrency,  Bitcoin specifically, because I’ve been sold for over a year on Bitcoin being gold in our future.
I’ve read several full volumes of books by everyone reputable in the Bitcoin world for well over a year now. I’ve gotten in my brain buying/ selling on an exchange or brokerage like Coinbase is bad/unsafe! I’ve learned how important a wallet ( hot) and (Cold) and the difference is. As well as the importance for immediate storage, and security. I’ve also learned the strongest security to store your Coin on is a Cold wallet as it is ( DETACHED) from any computer, cloud, wifi, or internet connection. In other words as long as you don’t lose your wallet 12 word password phrase, your shit ids safe!!
Now I find out I need to learn how to run my own Node, and need to learn how to use the Lightning network.  This should be getting easier with time not more complicated. Right?  
The reason I brought up this story in the beginning is to give an example of the majority of people I found including all my college educated children in their early 30’s to early 40’s ( I have 4 kids);  they won’t go near bitcoin or crypto after I spend an hour trying to explain the process of first buying bitcoin, then choosing a wallet.
I barely get out all the details between hot & cold wallets before they start rolling their eyes when they ask me, Dad! why are there so many wallets?  Just after I already told them a brokerage is kinda like a bank but its not as safe, so you need your own wallet and a hot wallet is better,  but not to much better l, because they aren’t as safe as a cold wallet and then having your own nodes are safer than someone else’s node,…and on, and on…. Holy shit!😳
Bottom line,  this MUST GET EASIER, SAFER,  and more RELIABLE, BEFORE we start convincing millions that fiat currency put out by the government is robbing us blind, which they are!  
I’m definitely sold on currency history in this country and the world. So can someone convince me I’m not dumb and crazy &?far to skeptical for having these concerns about our future?
Thanks if you even hung around long enough to read all.
A worried Bitcoin lover.",Bitcoin
"I'm a US citizen living in Mexico. I've been trying to buy Bitcoin etc for years. I can never do it with my Mexican bank account with my residency card or a US passport. I got a ledger and moonpay and coinify don't work with either of my cards. Seriously. I've been trying to get involved with this forever. What the fuck. How is the currency of the future gonna be so hard to access?

&#x200B;

edit: thanks for pointing me in the right direction everybody. I'll give it a whirl in these other ways.

Update: After trying a lot of stuff on the list, I thought I'd let people know what is working for me in my situation.  [hodlhodl.com](https://hodlhodl.com) is great. I can do direct bank transfers there that go directly to my hardwallet. And, The ATMs here are very accessible and easy to use. I haven't had to show any ID or anything since the amounts I'm depositing aren't astronomical. I'll continue looking at the other P2Ps but it seems [hodlhodl.com](https://hodlhodl.com) has people dealing in pesos through mexican banks. 

&#x200B;",Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/10yhuay/daily_discussion_february_10_2023/) for unanswered questions.",Bitcoin
"I recently got a notification from the app Coinbase on my phone saying that if I don’t give them my personal information for example, my ID or drivers license by a certain date by May 12 I will lose all privileges on the app, and I will only be able to withdraw from my bitcoin wallet. So if I have any debt, and I put my personal information on my account, does that mean the government can come and take my bitcoin as compensation for debt?

So after reading the comments if I had debt and wanted to withdraw my bitcoin into cash not at the moment but soon like within this year what should I do if I have debt? Should I not upload my info and just cash out before may 12?",Bitcoin
"Hey, y'all,

Thinking of starting several nodes up on my home wifi.

Is there any reason that one shouldn't run multiple Nodes on the same IP (at demarc, separate subnetting)? Given the size of the network at this point, I don't foresee any real issues with creating one point of failure for three or four vs one node.

Wanted to consult with y'all before I move forward with a couple hobby nodes. Bitcoin is a group effort, after all.",Bitcoin
"I'm wondering if there is a risk that governments can ban the use of specific bitcoins based on arbitrary environmental concerns (like was it mined renewable) or if it was used criminally.

Bitcoin is traceable and therefore it's not truly fungible like gold or physical cash. If someone wanted to sanction a foreign nations Bitcoin holdings they could. Any Bitcoin that came from Russia is worthless in the US for example.

If a foreign nations wanted to hold an asset that had no third party liability they could hold gold, but if they held Bitcoin they could be sanctioned against by the rest of the world, meaning it's not fully neutral money.

Does anyone else see this as a risk?",Bitcoin
"If you are uber anti-bitcoin, isn't that your stance? 

The current form of money is fine, it doesn't need to change, ever. 

But...what has represented money throughout history has always changed.

why would we be at the stopping point of the innovation with fiat currency? That seems like the more ridiculous position to take.

Maybe it's not bitcoin (but has the best odds at the moment), but it has to be more likely that something will replace fiat currency at some point for the remainder of humanity, then we have reached the perfect form of money with fiat.",Bitcoin
"Can someone explain why do you think Bitcoin is the future.
Here are the things I know.
- its limited ( so many digital currencies are limited,why only Bitcoin)
- we can transfer Bitcoin without a regulatory agent.
- it's backed by Blockchain 
So what's more to it.",Bitcoin
"When Bitcoin and other crypto currencies have the access into purchasing of goods and services in the physical en-mass, this will improve the performance of bitcoin and reduce her volatility, making her a better currency that can be used for storage value and a reserved currency for the future ahead of us

I hope to see that day arrive in the shortest possible time .",Bitcoin
"Hello,

I recently downloaded the btc Blockchain to a 2tb HDD from a windows machine. I want to use it for a pi I have coming but want to know if I will be able to use it when it gets here. Will I have to redownload it?? Or can I use it? Is there a way to convert the file type of I have to? I know debian used ext4 and did not think about that until I had like 30% of the chain downloaded...which is where I am with the progress as of writing this. Any help would be much appreciated as I would rather not have to start over if possible. It's. Taking. Forever.

Again
 Any help would be awesome. Thank you.",Bitcoin
"In summary , I don’t trust exchanges with my personal info. So I see places like Bisq, hodl hodl, and kucoin offer non KYC bitcoin. It seems like a good choice. 

But how do you onboard fiat on these exchanges without linking a bank account? I know you can use a stablecoin. But you need fiat to buy stablecoins in the first place. So I don’t see that as a solution.",Bitcoin
"Looking into the Heatbit heater, supposedly it mines bitcoin and simultaneously heats your home. 

Does anyone have one? 
Is it worth it?

I live in Canada, thx",Bitcoin
"Sorry if there's an actual term for it but the scenario I describe is as follows:

A transaction is made and included in a block. Then a fork occurs in the chain where another branch outpaces it, resulting in the sibling block being dropped and therefore the transactions on it that weren't included in the new, longer fork, are dropped.

On what order of magnitude does this happen? 1 in 100 blocks? 1 in a million blocks? Or is there something in the Bitcoin protocol that prevents this?",Bitcoin
"In this video, Dr. Jeff Ross shares his insights on why he believes the time has come for everyone to adopt the Bitcoin Standard as the way to exit the fiat matrix.",Bitcoin
"I'm bored of people nit picking the term artificial value, I'd expect anyone replying to this to understand how things can have physical or practical value by virtue of existing.
Eg food as currency or golds physical rarity.

Sure there is not more bitcoin to print, but it can be subdivided",Bitcoin
"Hello everyone!

So far I've tried:

* [How to use Hodl Hodl](https://bitcoin-bumblebee.com/2023/01/08/how-to-use-hodl-hodl/)
* [How to buy bitcoin privately using Bisq in 2023](https://bitcoin-bumblebee.com/2023/01/14/how-to-buy-bitcoin-privately-using-bisq-in-2023/)
* [RoboSats Guide: Private P2P Bitcoin Exchange the Lightning Way](https://bitcoin-bumblebee.com/2023/01/19/robosats-guide-private-p2p-bitcoin-exchange-the-lightning-way/)
* [Peach Bitcoin: a new mobile app for private bitcoin exchange](https://bitcoin-bumblebee.com/2023/01/26/peach-bitcoin-a-new-mobile-app-for-private-bitcoin-exchange/)

They're pretty well known. Are there any others you would recommend?",Bitcoin
"If you remember /u/theochino, the Bitcoiner trying to abolish the Bitlicense *(*[*https://newyork.abolishthebitlicense.org*](https://newyork.abolishthebitlicense.org)*)* seems to still be at it.

This was briefly posted on r/Libertarian \- [**https://libertarian.repmyblock.org/libertarian/exp/register/movie**](https://libertarian.repmyblock.org/libertarian/exp/register/movie)

My BitDev *(*[*https://www.meetup.com/BitDevsNYC/*](https://www.meetup.com/BitDevsNYC/)*)* contacts told me that he is trying to get Bitcoiners to testify on 2/15/2023 at the Oversight - Cryptocurrency and Blockchain Technology in New York City on 2/15/2023. [https://legistar.council.nyc.gov/MeetingDetail.aspx?ID=1078724&GUID=A0BC20A0-0D6A-4AAB-94D4-B75C2AF7CE3D](https://legistar.council.nyc.gov/MeetingDetail.aspx?ID=1078724&GUID=A0BC20A0-0D6A-4AAB-94D4-B75C2AF7CE3D&Options=info|&Search=technology)",Bitcoin
" So the other day we met Nick Spanos, the famous early Bitcoin adopter and evangelist.  
Nick agreed to go live in a webinar officially run on our LinkedIn, where he shared - next to his opinions on what (to him) constitutes the perfect pitch - his ideas for the web3 development, and how to best angle the issue of fundraising. Tough subjects, generally speaking.

So I'm writing this post on Reddit - Chris Pytel here, Content Producer at 10Clouds, on a Friday afternoon CET, with the hopes of promoting positivity and making no fake interactions, but knowing that playing it by the marketing book is the last thing I want to do. Still, my focus inevitably goes to bringing value. It has to.

And I'm thinking what would web3 afficionados like yourselves find valuable, as I know Nick Spanos is a type of man who gets all the attention he deserves in the blockchain development world.

I might just drop the webinar recording and tell how interesting it is to watch (it is).

[So I did.](https://www.youtube.com/watch?v=RfcV4ZeN6OE)

I could then link to the article covering the concept of a blockchain OS, originating from Nick Spanos's utterances during the webinar.

[So I did.](https://10clouds.com/blog/defi/blockchain-needs-a-good-operating-system/)

And heck If I didn't do the job right sharing the valuable. It's you guys who will know best what to do with that knowledge. But here it is - first hand and highly applicable, under those two links above.

For visual/aural experience, go for the video (it's SFW).  
For a more analytical/skimmer type of consumption, go for the text

I can only wish you a time well spent, and a happy weekend.",Bitcoin
"Or is there perhaps generally something u identify yourself at the moment most? Could be a movie, song or whatever cultural projection",Bitcoin
"I know in here, we all know the proof of work/energy thing is bull shit.  But even before I knew anything, and would hear that argument, I’d be like so what, the whole world is burning and consuming energy in so many ways…So what!",Bitcoin
"I want to open a wallet for crypto trading but my drivers license is expired and suspended and I have no valid ID currently.  My question is are there any sites or apps you wonderful people of Reddit would recommend that I can open a wallet to buy and sell crypto with that won’t require a ID while registering/opening, please let me know, thank you.",Bitcoin
"I have had Coinbase (and Coinbase pro) since I got into bitcoin and its been great. I personally haven't had any issues with it... but Strike is just absolutely perfect. The experience of using Strike is so superior in my opinion to the point I'm not sure why anyone here would use anything else.  


I regret not switching sooner.   


If any of you don't use Strike and aren't happy with however you get bitcoin now I 100% recommend Strike.",Bitcoin
"Finally, I was able to grab a very good deal on a Ryzen Mini PC and was a no brainer to set up a Full Node using Umbrel.
I just swapped out the NVME for a 2TB one and it's now synchronizing.
I think it will be done in less than 12h!

Hardware is:
- Ryzen 7 3750H
- 16GB DDR4
- 2TB Gen3 pcie from Kingston",Bitcoin
"As title says, my Bitcoin node is almost synchronized and it took about 1 week.

Can't wait to try and learn about lightning!!

Question: What app do u think is better LND or Core lightning? ty

For the people that already do not have a bitcoin node it is really so easy. I set up in an old computer and now I will be able to not trust, verify and be my own bank.

https://preview.redd.it/dud96pxpxbha1.png?width=985&format=png&auto=webp&v=enabled&s=46d54d44c457329a151dd7a94a4e95be4637b3bb",Bitcoin
"To what clock does Bitcoin refer for timestamps that then uses in order to calculate the difficulty adjustment so that it goes back to 10 minutes, and isn't that a vector of attack?",Bitcoin
"I’m trying to set up some automatic DCA’ing but running into high fees. I have set my bank to auto transfer funds to a specific account for DCA’ing into crypto. Once a month a deposit hits the DCA account. I then tried to set up coinbase to auto buy bitcoin daily, about $5 worth. The fee on this daily buy is $1 (20% fee). 

Can anyone point me to a better way to accomplish my goal of buying $5 of Bitcoin (or other tokens) daily without such a high fee?",Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/10xo9em/lightning_thursday_february_09_2023_explore_the/) for unanswered questions.",Bitcoin
"Hello there! I was playing with some startegies today and I found a sweet formula that outperforms buying and hodling Bitcoin! Here is how it goes:

https://preview.redd.it/48uqh3glqaha1.png?width=1424&format=png&auto=webp&v=enabled&s=a8e879baffcf0044b075846b1ae4740ef8157573

It's an automated system that buys as much Bitcoin as someone can throw at it when price is down 7% on the day. And on the flip side, sells all the Bitcoin bought by the buy order when price is up 7% on the daily timeframe. This can happen in the same day, or a completely different day all-together.

Seems super simple right? Yeah basically, but it works. However, there is one other tool being utilized:

https://preview.redd.it/pin5u3inqaha1.png?width=646&format=png&auto=webp&v=enabled&s=540e90978a34eb2e53d3951899fdc22d4d9c26f3

I implemented an automatic stop loss at 7.1% That will completely cut trades if the strategy doesn't on its own. When this occurs, trading is frozen for 5 minutes while the strategy recoupes. Here are some notable backtest results for periods of aggressive Bitcoin volatility (up and down)

  
2019 Bull market (Jan. 1-July 29 2019):

https://preview.redd.it/l6s1m20qqaha1.png?width=714&format=png&auto=webp&v=enabled&s=901db31a39f0031315aeb24006f6039616c20f99

https://preview.redd.it/4mk7oi0rqaha1.png?width=2422&format=png&auto=webp&v=enabled&s=3a33a9a3d9208bd76ee4a2445734ab9e0f19fd74

Sure we didn't capture all of the bull market, but 99% of trader's didn't. In fact, a huge percentage of traders got completely burned during this period, because they either got in to late or held long enough to lose a substantial amount of gains. Here is how it did during the 2018 bear market that lead up to the bull run above.

https://preview.redd.it/8wcogwssqaha1.png?width=754&format=png&auto=webp&v=enabled&s=39262f2e9f2f64040c56041844f05074001b4d2f

Okay not great, but had I just been buying and holding Bitcoin I would have lost another 30+% during this period of time.

Just something cool i'm playing around with and wanted to sure! Currently papertrading with this strategy, and i'm excited to see what it can do vs. Hodling! I'm going to learn a bit more about the effects of each indicator and test them as I go. I want to hear some suggestions on what I should throw at this! This is not financial advice, just a fun little trial run!",Bitcoin
"I’ve been in the crypto for a few years now. And I’ve recently have got the majority of my friends to buy bitcoin. 
We now do 90% of are p2p transactions not with PayPal or cashapp or Zelle.. We use our BTC wallets and just pay each other in bitcoin. Feel amazing seeing bitcoin take care of all my transactions and helps me stack more. This is the way. 📈",Bitcoin
"Is there any FX or Stocks broker allowing traders to hold BTC as their account's main currency? (Not just BTC deposits to convert into Fiat which is common lately)

I know I could borrow fiat or USDT at crypto exchanges but 1)they can liquidate the position easily 2)The Interest is high, and the ones with low interest only lend up to 65% of your collateral so not worth the hassle. 3)I'd be missing BTC infinite growth potential by holding and trading shit govcoins, the goal is to hold BTC while operating traditional financial instruments.    

Most of the holding remains in cold storage but I'm looking to use a small portion of this liquidity as collateral in a traditional FX exchange and increase it with my FX experience where one can get way higher leverage and liquidity than regular crypto exchanges.

EDIT1: After several days researching on the subject it seems there are no FX brokers yet accepting BTC as base currency, even though there are several useful lightning options, not exact matches on what I'm intending to achieve but very close. https://cointastical.medium.com/exchanges-with-support-for-bitcoin-lightning-network-payments",Bitcoin
I have had the same company for car insurance for about 8 years. I hadn't shopped in a while around to see if I was paying too much. Finally got around to quoting out my car and home policy and was able to save 35%. Now I am going to take the amount I am going to save each month and use it to buy Bitcoin. Now I want to see what else I could save on.,Bitcoin
"I keep seeing arguments and clips of GG and Sec members discussing if btc is a commodity.

Why is it important to have this differentiation and what does it mean for btc owners.",Bitcoin
"Why would you EVER use your Bitcoin for purchases of food, drinks, hotels, gifts etc etc?  
It completely baffles me!  
Guys... Bitcoin is one of the scarcest assets that is going to 6 or 7 figures in value..  
Do you really want to be that pizza guy?  
You will regret using Bitcoin for purchases that you could've used cash or credit card.  
The only reason anyone should use Bitcoin for purchases is emergency only!  
3rd world stuff...",Bitcoin
I had my wallet with Block.IO. Now they locked my account for no reason and won’t let me log back in. They told me that I can still access my money as long as I have my “private key”. I have my private key but I have no idea what to do? How can I take the money that was in my block.io and move it to a wallet that I have access to?,Bitcoin
"I'm doing my crypto taxes and I'm looking at my data from a now defunct exchange. There is BTC withdrawal that I have no idea where I sent it to.  All I know is the date/time and the BTC amounts.  I'd like to find this transaction on chain, hopefully figure out where I sent the bitcoin.

Is there a way to search the blockchain and filter just by time and amount?

I don't know any wallet or tx ID.",Bitcoin
"I decided with wife's support) to divert **all future retirement contributions** from my employer-sponsored Roth IRA to Bitcoin (self-custody). Wifey's been supportive of our bi-weekly DCA of a few hundred, but this plan concerned her. So I made a list of likelihoods (i.e., predictions), which we have discussed, that suggest Bitcoin's dominance will only grow over the next two decades.  

Edit: my wife and decided this together after discussing everything in the list below and risks that are not included in the list.

**What is likely to happen in the next 10-20 years?**

* Many more alt coins will fail and people will either get out of crypto completely or move to Bitcoin
* More exchanges will succumb to fraud or fractional reserve shenanigans making the case for self custody.
* Crypto industry will become regulated which will give institutional investors, investment firms, and financial advisors confidence to recommend bitcoin. 
* The Lightning network will become more robust giving Bitcoin increased capacity for transactions and merchant adoption.
* More world currencies will collapse driving some in those countries and outsiders paying attention to consider Bitcoin
* Bitcoiners like me will educate their children on money’s purpose, how Bitcoin is the hardest money and how to self custody, among other things. This means elementary and middle school kids of today will likely be more comfortable with the concept of digital money and those who buy into the concept will prefer the most established and just decentralized option.
* More wealthy people will become orange pilled and Bitcoin will make more people very wealthy. Since wealthy people often have greater influence over the government, laws attempting to restrict Bitcoin adoption will become less likely to pass
* CBDC conversations and considerations by world governments will increase which will get more people talking about the nature of money and how it should or shouldn't be controlled. Conversations like these should lead more people to the entrance of the rabbit hole. If a CBDC actually does get off the ground it will make Bitcoin’s use case and value even more clear.
* World governments will continue to debase there currencies, stealing value (and hope) from would-be savers.
* We will experience the halving of the block subsidy in 2024, 2028, 2032, 2036 and 2040 which will reinforce Bitcoin’s claim of digital scarcity and a more than 30-year history of functioning as designed (assuming it continues to do so). It will still be “new” compared to the dollar or gold, but it'll have a whole generation that never knew a world without Bitcoin.
* In the long run, I think Bitcoin significantly outperforms stocks, mutual funds and EFTs.

Anything you'd add to this list? Anything you'd challenge as not being likely? Let me know in the comment section down below. And remember to like and subscribe to Bitcoin. 

&#x200B;

*Notes:*

* *I'm keeping the existing retirement money where it is, only new contributions will go to Bitcoin for now*
* I'm still new and learning. I welcome constructive criticism and factual corrections.",Bitcoin
Localcryptos shut down in November and 2 months later now Localbitcoins. Really wondering if this is pure coincidence or there's an unknown factor causing this?,Bitcoin
"Hi all, pretty new to Bitcoin, I recently started trading it, and I was wondering what the formula is to calculate my gains and losses? For example, if I bought $3 worth of Bitcoin, at 22,817, and sold it at 22,900, how much would I make?",Bitcoin
"This might sound like a dumb question but what would happen if everyone in the world were to buy $100 worth of Bitcoin? Looking at the daily trading volume which is 41 billion dollars, everyone in the world would have bough $7 of Bitcoin today. Looking at the market cap which is 435 billion with this kind of idea in mind means everyone in the world is holding $67 in Bitcoin. So if you hold more than $67 in Bitcoin your doing better than most of the world.",Bitcoin
"Theres the argument that banks being able to lend 10x their reserves drives for more innovation and entrepreneurship. To some degree I think thats pretty much undisputed.

If you want someone to lend you money. You will likely need to deposit guarantee and have endorsement.

Wouldnt non KYC kinda break this? Then there is the whole tax evasion. 

What are some substantial reasons for non KYC?",Bitcoin
"In every block, there are transactions that pay a much higher than necessary fee.  What is the motivation?  Fat finger?  Donation to bitcoin security budget? or something else?",Bitcoin
"Hey guys,

Please help me understand how credit would work in a hard money regime. If I had an economy with two entities, each holding 1 gold coin and the entire money supply is just these 2 coins. Now person A gets a 1 Coin loan from person B with a 10% interest rate. The supply is still 2 coins but the total money demand to pay off all debts is now 2.1 coins.

What am I missing?",Bitcoin
"Can someone guess your private key?

Yes, in theory it is possible, but in today's world it is simply impossible. Today I will show you why:

&#x200B;

As we all know, there are approximately 2\^256 Bitcoin private keys. To put that into perspective, that's a 1 followed by 77 zeros. 

(115 quattuorvigintillion 792 trevigintillion 89 duovigintillion 237 unvigintillion 316 vigintillion 195 novemdecillion 423 octodecillion 570 septendecillion 985 sexdecillion 8 quindecillion 687 quattuordecillion 907 tredecillion 853 duodecillion 269 undecillion 984 decillion 665 nonillion 640 octillion 564 septillion 39 sextillion 457 quintillion 584 quadrillion 7 trillion 913 billion 129 million 639 thousand 936) 

that would be the exact number...

So there are 10\^77 private keys, that should mean logically, that with trying, you can guess a specific key. In practice, with today's technology, this is simply not possible.

To put this into perspective a bit. 

\- There are about 10\^18 grains of sand on our planet.  

\- There are 10\^25 drops of water on our planet. (assuming that there are 1.322 billion cubic kilometers of ocean on Earth and one drop of water is 0.05 ml).

\- There are about 10\^78 atoms in the whole universe. 
EDIT: observable universe

There are therefore about the same number of atoms as there are private keys. So I could choose one atom and hide it in the billions of galaxies and you can go and find it... have fun...

Only the thought from how many atoms the human body consists.

&#x200B;

&#x200B;

Now let's do a thought experiment:

Let's assume we have a computer which can try 1 billion private keys per second.

\- 60 billion keys every minute

\- 3'600 billion keys every hour

\- 86'400 billion keys every day

\- 31'536'000 billion keys every year

\--> this means that we could try about 3.1536 x 10\^16 keys per year.

&#x200B;

There are 10\^77 private keys, means

10\^77/10\^16 gives 10\^61 Years

&#x200B;

So if we could guess 1 billion private keys per second it would go 10\^61 years.

The universe is about 10\^9 years old.

&#x200B;

Comparison:

10\^61= 1 with 61 zeros

10\^9= 1 with 9 zeros

&#x200B;

Does it make now slowly more sense that it is nearly impossible?

&#x200B;

Now some surely ask themselves the question whether quantum computers would be able?

&#x200B;

A 256 qubit Quantum computer could hack Bitcoin in seconds. However, quantum computers are future and until then Bitcoin can still be developed. In addition, quantum computers also have the option to hack into any banking system, database, or anything else. So many other dangers would arise.",Bitcoin
"I thought i was semi well versed in bitcoin, but i had a transaction that I sent with too low a fee. (I did not fully understand what these fees meant to the miners) so i had a rather large transaction hang for a few days until i finally paid a pool to manually confirm it.

A hard lesson learned, i will be switching to a wallet that allows RBF from now on...

The more you know 🤷",Bitcoin
"[Choke Point](https://www.piratewires.com/p/crypto-choke-point)

Interesting article on a seemingly concerted effort to stifle crypto.

This make me think of the quote ""they ignore you, then they laugh at you, then they fight you, then.....""

We are officially in the fighting stage now. What do you all think?",Bitcoin
"As of a few minutes ago, as my ""earn"" lock just expired, I am now fully non custodial on an secure wallet !

When I first got into ""crypto"", a couple years ago, I started by registering on a couple exchange: trading felt fun, the earn benefit seemed great, the credit card cashback was attractive, it seemed much more simple.

Beside the whole messaging of these exchange was that non-custodial was too complicated and I know feel like there is a deliberate marketing tactic of making non-custodial appear confusing.

I'm a slow learner, I've been following this sub the whole time and only got out now. First reading you guys made me understand the difference between BTC and b-tiers cryptos, and I eventually dumped everything but BTC. Then you guys made me understand how simple the bitcoin network is conceptually, and how unnecessary custodial wallet were if you follow good security practice. And finally I've taken the ""Not your key not your coin"" advice to its logical conclusion and hold everything myself.

It has not been smooth. I was long delusional enough to believe that my exchange of choice was sound, after all I'm not an idiot like those people who trusted Mt Gox, Celcius, Voyager, FTX... right ? Right ? Surely I was wise enough to pick a serious exchange, and this would never happen to me.

Even as FTX was going under, I ""bought the dip"" and locked it onto a 3 month earn at my favourite exchange.

Which promptly got under suspicion of insolvency with the value of its own proprietary coin divided by 2 nearly overnight. Shit, what happen to these other cretins was about to happen to me ! Maybe I'm no so different after all and it was just a matter of random luck that my exchange hadn't gone belly up yet ? Well I dodged the bullet eventually but felt the heat.

Anyhow, I kept reading this sub because of the meme, because overall it's good spirited and because you guys are helpful and, for the most part, a friendly bunch. (this is in contrast with the cynical, nasty people of anti-crypto subs, makes one wonder who are the sociopaths) As a consequence, I learned a lot, and I'm glad I did. If it wasn't for you I'd probably eventually would have lost it all ""chasing pennies in front of a steamroller"", and quit this whole crypto thingy altogether.

So thank you guys.",Bitcoin
"Number of companies buying Bitcoin grows 593% in Brazil and already exceeds 62 thousand, reveals Federal Revenue

\#brazil #bitcoin #news

Photo by [André François McKenzie](https://unsplash.com/@silverhousehd?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/pt-br/fotografias/vh7U_Z75L3U?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)  

[https://cointelegraph.com.br/news/number-of-companies-buying-bitcoin-and-it-only-increases-in-brazil-and-already-exceeds-62-thousand-reveals-federal-revenue](https://cointelegraph.com.br/news/number-of-companies-buying-bitcoin-and-it-only-increases-in-brazil-and-already-exceeds-62-thousand-reveals-federal-revenue)

[Number of companies buying Bitcoin grows 593&#37; in Brazil and already exceeds 62 thousand, reveals Federal Revenue](https://preview.redd.it/l1bxf42xu5ha1.jpg?width=4240&format=pjpg&auto=webp&v=enabled&s=7aa1b094853f33afa9a824d167a7dcb135c36cce)",Bitcoin
Are there problems from on ramp No-KYC then off ramping KYC to pay taxes?  Do they ask you a bunch of questions about where your money came from?,Bitcoin
"I need website where I can see timeline btc price chart. 

And the very inportant thing, when I hover cursor to price on chart, I need to see tooltip about why is price change.  

I need to see some news what happen in this moment and why the price changed

&#x200B;

.....and I did't find any website like this.

**Does anyone know something like that?**",Bitcoin
 What effect has the FTX saga had on BTC? Did it have any effect on the prices? And what's likely to happen going forward as far as BTC investments are concerned.,Bitcoin
"If I create a new address in one wallet and send part of the funds there, is it possible to understand from the information about the transaction that the transfer was inside one wallet to myself?",Bitcoin
"The lightning network is a second-layer solution on top of the Bitcoin blockchain that enables quick, cheap and scalable Bitcoin payments.

Here is the place to discuss and learn more about lightning!

Ask your questions about lightning

Provide reviews, feedback, comparisons of LN apps, services, websites etc

Learn about new LN features, development, apps

Link to good quality resources (articles, wikis etc)

Resources:

* Here is an awesome list of resources compiled by Jameson Lopp: https://www.lopp.net/lightning-information.html

* Want to test out your lightning fire power? tip the Bitcoin devs! https://bitcoindevlist.com/

* Previous threads: [Search](https://www.reddit.com/r/Bitcoin/search?q=Lightning+Thursday&restrict_sr=on&include_over_18=on&sort=new&t=all)

* Lnbook getting closer towards being finished and can already be seen at: https://github.com/lnbook/lnbook

* Lightning Dedicated YouTube Channel: https://youtube.com/renepickhardt

* Also there is the playlist by chaincode labs: https://youtube.com/playlist?list=PLpLH33TRghT17_U3as2P3vHfAGL8pSOOY

* Lightning stores: https://www.lightningnetworkstores.com/

* Learn more and talk about lightning right here in r/Bitcoin, r/bitcoinbeginners, r/thelightningnetwork, and [the r/Bitcoin chat](https://discord.gg/qE3rWBRNqh)",Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/10wp16x/daily_discussion_february_08_2023/) for unanswered questions.",Bitcoin
"After some recent research I ran into some facts about some of the terminology we regularly use that I thought were very interesting. Now maybe I'm just a bit behind the curve but this was news to me....

According to invesopedia,
Currency can be defined as the following- an asset that maintains the following properties: a medium of exchange, a unit of account, portable, fungible, divisible and durable. 

No surprises there 

However the defition of money is as follows:
Money is a currency (has all the above features) but also acts as a STORE OF VALUE.

Money is a form of currency. But currency ain't no money. 

Huh fancy that.... So really btc should fall into cryptomoney, not cryptocurrency. But I guess it doesn't have that ring to it.

Never again will I consider fiat as money. 
""You owe me some money for that burrito, but If you don't have sats I suppose ill take some currency""

Let me know if this is news to you or I'm just an idiot

Source: https://www.investopedia.com/terms/m/money.asp",Bitcoin
"Does this mean buying and selling bitcoin is in jeopardy?  What will this do to bitcoin price up or down?

 [Coinbase's CEO Cites 'Rumors' the SEC May Ban Crypto Staking for Retail Customers (msn.com)](https://www.msn.com/en-us/money/markets/coinbase-s-ceo-cites-rumors-the-sec-may-ban-crypto-staking-for-retail-customers/ar-AA17goZJ?ocid=msedgntp&cvid=5bb7f8132ad641cf925ff2fafd3739ae)",Bitcoin
I have a ledger which I like but would like a second wallet so I am not keeping all my funds in one place. I saw the Ellipal while searching and it looks nice but thought I’d run it by the community first.,Bitcoin
"Maybe a unpopular opinion even. I still don't believe none said this during the whole scandal. But now that the smoke has settled a litte bit. Do you concur that if it was a bank pulling this off... How long were they will able to keep doing damage?.

I mean i have no doubts more than one bank is probably under the same roof as FTX only they have the ability to hide their crimes in plain sight.",Bitcoin
"Have a nice good evening together in view. In this situation I have to tell you, don't let Craig Wright babble on! Lol is over.
That's enough for me. If no one starts doing something about the Craig Wright.

I will take my signature.

And crack white forever from the bitcoin removed! 
💯 %ᴴᴰ,)",Bitcoin
"Hello all you wonderful Bitcoiners! I've decided to get a Coldcard hardware wallet and I was wondering what the advantages are of buying it with Bitcoin and sending it to an anonymous address. I'm thinking so there is no way to track the wallet to me? 

Any help would be greatly appreciated. Thanks in advance!

All DM's will be deleted immediately.",Bitcoin
"Every week for over a year I’ve been putting money into bitcoin, and just this month my bank told me they no longer allow purchases of crypto lol.

Update: Thank you all for the suggestions! I really only posted this because I thought it was funny, it’s to late for them to stop this train from rolling",Bitcoin
"Hello,

My first post on this subreddit was yesterday and I promptly deleted it after several users recommended I do so. I spent the next 24hours doing as much research as I could on Bitcoin, addresses, keys, hard drives, recovery, etc. and quickly learned that it is totally safe to share public addresses. With this info I have decided that I would like the help of this very helpful community again and am going to update those who saw the previous post.

To start, I found this hard drive last week with one of my buddies while we were sorting through some of my late sons things. We have been taking our time with what we have been organizing and after some searching learned that this hard drive public address is likely Bitcoin related. This led me to post here yesterday where I got overwhelming amounts of help to try and recover the supposedly large balance. Believe me, I would love to ask my son about it or give it back to him, as many others have asked about last time.

https://preview.redd.it/j0b0qozjy0ha1.jpg?width=443&format=pjpg&auto=webp&v=enabled&s=203f041efac88c87f63bd56b39b69b456f312700

https://preview.redd.it/h56yq2amy0ha1.jpg?width=443&format=pjpg&auto=webp&v=enabled&s=200d974a92bbcb1afc3d5a83c5d7c9f32c90b453

The first thing I was recommended to do was purchase a USB to SATA cable so that I could plug it into my laptop. I have the cable and have set it up and have discovered a file called ""BACKUP.dat"". This seems to be a copy of a ""wallet.dat"" that can be used for recovery of the Bitcoins.

https://preview.redd.it/m6v21t1ny0ha1.jpg?width=591&format=pjpg&auto=webp&v=enabled&s=2ef6d8e3d7fb41c05ccf58fed02e7a1d2ebbcd53

Unfortunately, this is all there is on the drive. I was expecting to find ""Bitcoin Core"" downloaded so that the hard drive would also have the Bitcoins and now have learned from several others that I may need to sync this file with other websites and software.

I am hoping that you all can help me with these questions...

1. If I install my own Bitcoin Core where do I put it? On the same hard drive? On my laptop?
2. Is it possible there is more stored on the hard drive and it just isn't visible?
3. Should I just take this to a professional? If so, can you give me recommendations?

**EDIT: I have decided to sell the hard drive to someone who is interested in buying it. After trying mostly everything that was recommended in this thread I cannot figure it out. The buyer seems knowledgeable on defi, dapps, etc. and I can make a bit of money off of it. THANK YOU to everyone who commented with helpful advice and wishes, this is a very helpful community.**

Thanks,

K",Bitcoin
"Hey bitcoiners, 

Currently, using a Ledger X. Have been looking about Bitbox lately and thinking about a switch. Open source software seems right to me.

Do you have any advice ? Which one to use and which one are you using ? 

Thanks !",Bitcoin
" 

The lack of widespread adoption of Bitcoin is a mystery to many, but for those in the know, its unique features make it an attractive investment. Not only does Bitcoin allow for ultimate control over your funds, making them elusive from third-party seizures, but it also enables seamless global transactions without the worry of a bank refusing the transfer.

In a world where financial freedom and privacy are increasingly threatened, Bitcoin offers a solution. With its decentralized nature and cryptographic security, Bitcoin empowers individuals to take back control of their assets and transact freely without the fear of interference.

Moreover, as the Bitcoin network continues to mature and become more widely accepted, its value is poised to soar. With each passing day, more and more businesses and individuals are discovering the benefits of Bitcoin, and its use is growing exponentially.

So, if you're looking for a sound investment opportunity that also aligns with your principles of financial freedom and privacy, look no further than Bitcoin. Don't miss out on this chance to be a part of the future of money. Invest in Bitcoin today!",Bitcoin
"So i run bitcoin core and connected my bitcoin core to sparrow and im told i can connect my ledger to it, so i can validate my own transactions and enforce the rules of bitcoin, im just wondering what are the risks in doing this every time i want to move money out of this hardware wallet connecting to sparrow.",Bitcoin
"*What do you think, is the person (or group) behind the invention of Bitcoin still alive, or maybe he is long dead?*

&#x200B;

*What do you think is the reason we stopped hearing from him since 2011?*",Bitcoin
"Disclaimer: I know nothing about investing. 

BTC is now around 20k to 22k.  Throughout the day it goes up and down between those numbers. 

Do people just sell every time it goes up 1k and buy back again when it goes down 1k.

Is there a name for this?",Bitcoin
"I'm not a computer science expert, but since is possible to write any message in the bitcoin blockchain, wouldn't be possible to write a malicious code/virus into it that affects every node?",Bitcoin
"The BTC price of the past 24hrs showed a typical heartbeat pattern from ECG. It indicates nothing, I just liked the picture :) Have a nice day.

[BTCs heartbeat](https://preview.redd.it/bffjkkdm0yga1.png?width=2108&format=png&auto=webp&v=enabled&s=6f6fcc8222de20f14e90bc8df6b8bbfc525cf86d)

&#x200B;

&#x200B;

[heartbeat signal form an ECG](https://preview.redd.it/n335y3ja1yga1.png?width=314&format=png&auto=webp&v=enabled&s=3de857b595f90a909a7fa47d46b7f333c87a3a9c)",Bitcoin
"Hello there bitcoin users and -probably- developers. Today, I was topping up my ln wallet via my BTC Wallet in blue wallet but I accidentally selected the lowest fee option and RBF for me was disabled.

It's been some time and I still didn't get one conformation and when researching I saw that even if the RBF is turned off there is always CPFP and that blue wallet supports it according to their site.

My question is, how do I use this feature? Do I need to send some more sats or is it automatic or is there another thing for me to do?

txid: c0e19c647457998e689286fe4080a87dad75288e3f845c0404f4ea710c5f92e2 - I know its not a lot money to worry about but it's still money :D

Thanks for all the help in advance, have a great one!",Bitcoin
"Branding is Trust.

People today see a brand as a logo and reputation - but it’s more. It’s certification.

Back in the 1900s, cereals were a new technology, people needed to rely on brands like Post Cereal’s reputation to sleep safely knowing their kid won’t get poisoned from the new technology of stabilized starch. It was a certification of safety.

Similar branding can be seen in coinage and bills stamped by the government. [The Florentine Florin](https://en.wikipedia.org/wiki/Florin) was a strong brand of gold coin as it wasn’t devalued for some 300 years.

Bitcoin itself is a very powerful brand - you can reliably **both** trust and verify it’s doing what it does.",Bitcoin
"I am looking for a good cold wallet... I have heard of Ledger, but the opinions of Ledger are divided. Are there other devices that do it better? 

Basically, I would use the device primarily for Bitcoin.",Bitcoin
"It does add additional data to the chain and makes hosting a node slightly more difficult in theory. But it does increase the security budget for bitcoin, so overall a wash?  what do you guys think?",Bitcoin
"What do you think the future holds for Bitcoin? As the largest and most well-established cryptocurrency, it has faced many challenges and controversies over the years. But despite this, it continues to maintain its position as the dominant player in the crypto world. With the increasing mainstream adoption of cryptocurrencies and the development of new technology like decentralized finance, the future of Bitcoin looks promising. However, there are also concerns about its scalability and energy usage. So, what do you think the future holds for Bitcoin? Will it continue to thrive and dominate the market, or will it face new challenges that threaten its position?",Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/10vuhsr/daily_discussion_february_07_2023/) for unanswered questions.",Bitcoin
"When u buy from an exchange (coinbase, binance, robinhood etc.) then withdraw to your wallet you generate a new address every time (if your practicing safest techniques). How can someone know who that btc is going to. I get that the exchange has KYC but your hardware wallet doesn't especially if u generate a new address every time.",Bitcoin
"So since starting my bitcoin journey i have seen many different wallets and decided on having a ledger for my hardware wallet, muun for any quick payment i need to make and sparrow for connecting my ledger to it and using my own bitcoin node to validate transactions, i just want to know how do you guys see this setup? Can it be improved?",Bitcoin
" I have made the last (?) John McAfee interviews before he died, you can read the transcription here - [https://fuk.io/exclusive-interview-with-john-mcafee/](https://fuk.io/exclusive-interview-with-john-mcafee/) 

&#x200B;

Video version - [https://www.youtube.com/watch?v=28cVn\_xdb7k&list=PLH5AjN9R08KodnZdyFxbBuR3n1WBuXTFh](https://www.youtube.com/watch?v=28cVn_xdb7k&list=PLH5AjN9R08KodnZdyFxbBuR3n1WBuXTFh)",Bitcoin
"If there is real worry that the DC clowns will not increase the debt ceiling, why is there not more movement into crypto / bitcooin?  I see bitcoin as a hedge against inflation, but in the current inflationary environment it seems not to react as ""the whiteboard"" would have guessed.  Sorry if this is a rookie question.",Bitcoin
"Not everyone approaches Bitcoin the same, so the ideal list would be different for everyone. But in general, what are some things that would provide value for a paid subscriber? Some of my thoughts are:

* Monthly Giveaways
* Portfolio Allocation
* Trade Alerts
* Provide TL;DR for 2-hour crypto-related interviews / FED meeting etc.
* ELI5 Crypto concept
* Discounts on products
* Access to courses

What else?

I understand YOU may not be open to paying for a newsletter, but IF you were that kind of person, what kind of content would bring you value?",Bitcoin
"As  you most probably know, in version 0.3.3 of the BTC core there was introduced a significant update where the ""longest chain"" rule was replaces by ""the heaviest chain"" (the announcement [https://satoshi.nakamotoinstitute.org/posts/bitcointalk/268/](https://satoshi.nakamotoinstitute.org/posts/bitcointalk/268/), the code lines [https://github.com/bitcoin/bitcoin/commit/3b7cd5d89a226426df9c723d1f9ddfe08b7d1def#diff-608d8de3fba954c50110b6d7386988f27295de845e9d7174e40095ba5efcf1bbR1236](https://github.com/bitcoin/bitcoin/commit/3b7cd5d89a226426df9c723d1f9ddfe08b7d1def#diff-608d8de3fba954c50110b6d7386988f27295de845e9d7174e40095ba5efcf1bbR1236)).

What  I can't really get is what's the attack scenario it should have been preventing, which could allow a ""less than 50%"" attacker abuse the  disadvantage of the ""longest chain"" rule compared to the ""heaviest chain"", taking into account that in the same difficulty adjustment epoch  the ""longest"" and the ""heaviest"" means the same (if I'm not wrong about how the heaviness is calculated - via the accumulated work defined by the difficulty thresholds of every block in the past of the given one plus its own difficulty) and it could only probably differ at a moment of  difficulty adjustment?

The only  attack scenario I can think of is attacker mining for a really long  time, waiting for his local difficulty to drop so much that he alone  could mine at a rate of 1 block in 10 minutes, and then hoping for the main net to hit the unlucky strike of a line of slow discovered blocks,  while he in the same time is lucky enough to mine more blocks to a) compensate his initial waiting for the difficulty drop when he had been finding blocks much rarer than 1 in 10m (since he was only a part of the net then) plus b) manage to also overtake the other part of the net.

Can somebody explain this to me?",Bitcoin
"TLDR; animosity towards central banks is often ill-founded (doesn't mean you can't be angry at em, just let it be for the right reasons).  Central banks (The Fed especially) are not well positioned to inform the public of their activities, and can often be misleading in their communications. 

Central Banks have evolved (or rather, stagnated as the system evolved around them). They are not evil overseers. They don't inflate away private citizens' money. The Fed isn't the all-powerful, money printing, malevolent god that the majority lust for it to be. 

Central banks have limited tools, and a limited lens to manage *expectations*. 

....

Alright. A long one... to get some stuff out of the way, because I want people to know without having to dig through post history:

I work in banking, outside of the US. I'm not against Bitcoin in principle. I do post on some Reddits that are anti. When I say ""money"" here.. I mean *useable units of exchange*. This means that credit *is money*; as it should be, since it represents the source of units that make up everything from wholesale banking transactions, to paychecks, and grocery purchases. 

I see a lot of fog around what central banks actually do. I'll stick to the Fed generally here, ""banks"" mean ""commercial banks""…and I'll be specific if I'm speaking about any other entity. 

I'll try to use *Central Bank published information* to make most of my points here: Public communications, and meeting transcripts... with clarification (or opinion - watch out) injected. Some of this is information covered by others, some will be my own dabblings. 

What does the Fed do?

They can set rates, issue reserves, conduct open market operations, and have a few other ""tools' at their disposal. The purpose of these tools is largely for *expectation setting*. 

Money expanded beyond the Fed's ability to conventionally measure some time ago. Overseas markets for dollars, fueled by overseas deposits (called ""Eurodollars"", but are really just dollars held overseas), created a demand for dollars that was met by commercial banks (not governments). Keeping in mind this was seldom actual physical currency, and was usually just a ledger entry... existing only on a bank's balance sheet. Loans became issued on these deposits *outside of the jurisdiction of the Fed*, not tied to Fed issued reserves... and the resultant ""ledger dollars"" flowed back to the US and abroad. This was true money creation, and the true source of monetary inflation often incorrectly attributed to the de-pegging from gold. 

The money creation in this area was significant enough to merit some investigation in the 50's and 60's including a paper by [Milton Friedman - PDF](https://files.stlouisfed.org/files/htdocs/publications/review/71/07/Principles_Jul1971.pdf). The paper largely captures the space, but suggests a level of indirect influence by the Fed that may be an over-estimation. 

Noteworthy are pronouncements of:

>the Euro-dollar market has almost surely raised the world's nominal monetary supply... higher than it would otherwise be.

At the time, the impact wasn't fully clear. The eurodollar system evolved out of a necessity; bypassing local constraints to avoid liquidity issues, and potentially going as far as to solve Triffin's dilemma/sidestep balance of payments issues. The US didn't need to robustly export dollars, because dollars were being created [globally](https://www.economist.com/sites/default/files/20151003_world_econ.pdf) - PDF:

>The freewheeling Eurodollar market for banking in dollars outside America sprang up in the 1960s to get round red tape in America itself. It has been growing at a furious pace ever since.

Once the broader global monetary system got going (eurodollars, derivatives, repo, etc), M2 became a less meaningful statistic; per Alan Greenspan in [June 2000](https://www.federalreserve.gov/monetarypolicy/files/FOMC20000628meeting.pdf) (FOMC meeting - PDF):

>The problem is that we cannot extract from our statistical database what is true money conceptually, either in the transactions mode or the store-of-value mode. One of the reasons, obviously, is that the proliferation of products has been so extraordinary that the true underlying mix of money in our money and near money data is continuously changing. As a consequence, while of necessity it must be the case at the end of the day that inflation has to be a monetary phenomenon, **a decision to base policy on measures of money presupposes that we can locate money. And that has become an increasingly dubious proposition.**

The Fed realized this far earlier than 2000, and shifted their efforts towards facilitating the sense that money could be *tightened or loosened* via their policy. This is communicated to the broader banking/financial world, as well as the general public. 

When looking at a central bank's place in the global system; they interact in a similar fashion to other private entities. Long chains of transactions, connected by the ledgers/balance sheets of all the entities involved; the Fed's balance sheet is connected as well (to the degree that they participate). 

Let's look at quantitative easing (QE; often cited as a money printing activity), in a simplified way, explaining it through the balance sheet of the Fed and a bank:

QE is when the Fed purchases securities in an attempt to add liquidity to the system. Where does the Fed get the money for the purchases? They create reserves.. crediting the banks in their jurisdiction (borrowing from the balance sheet of the banks, by giving them an ""asset"" and increasing the Fed's liability). 

So, was money printed? The Fed added numbers under ""reserves"", increasing their liability.. then acquired an offsetting asset (securities ""purchased"" on the open market - from banks/primary dealers, etc.. they do not purchase US debt directly).  The idea is that a banking system with more reserves *will lend more*. 

But there is the question: what if the banks don't lend despite an abundance of reserves? Banks lend when it's prudent to do so, not necessarily because they have more reserves. 

Remember that reserves are a balance sheet item. Domestic US banks ~~are~~ were required to hold a specific amount vs. their lending. Reserves don't leave the banking system, and are usually only swapped between other banks under the Fed's jurisdiction. Technically a US domiciled bank may use reserves to purchase a treasury on the primary market (which would be used for fiscal spending), but otherwise there is no connection to reserves and broader money issuance. You and I have never spent a reserve. I'll also add a comment below re: another central bank that is clear about the nature of it's reserves (in a way the Fed often can't be, due to it's cultivated image).

If the Fed could just issue USD directly into the system (real money issuance), why use reserves?

The answer is that the Fed doesn't really do money, they do ""expectations"". They facilitate and regulate to a limited degree. 

Looking at a FOMC meeting from the [early 2000's -PDF](https://www.federalreserve.gov/monetarypolicy/files/FOMC20030625meeting.pdf), we can see the level of concern the Fed had over engaging in QE like Japan had done before them:

>CHAIRMAN GREENSPAN.  Governor Gramlich. 
MR. GRAMLICH.  Thank you, Mr. Chairman.  This discussion is going to be impossible for anybody to make sense out of because we’re all throwing our wisdom, or lack of it, out there!  I will do likewise, and let somebody else worry about where it goes.   

>... If we started doing more quantitative targeting, how would we do it?  That is, do we want 5, 6, or 7 percent money growth, and over what time period do we want that growth—for three months or six months?  There are a lot of issues involved, and I don’t know how to sort through them. 

Notice the language of *targeting* money growth using QE; not actually growing money, not printing.. and the fuzziness of whether QE would work for that purpose (QE had already not worked for Japan). 

>I think we’d have to be quite vague here, but one thing that gives me some optimism about this is that when it comes to vagueness I think this group is hard to beat.  [Laughter]   

Points for honesty I suppose (I did say they're misleading in their communications). 

>CHAIRMAN GREENSPAN.  Governor Kohn. 
 MR. KOHN.  Thank you, Mr. Chairman.  Let me try to react to some of what I’ve heard today....  We’re all learning in this process, and I think the process needs to go on.   One lesson that I drew from Japan was that not only did the Japanese get down to zero on the interest rate and not only did they try each new policy and say they were going to take it back, they didn’t give any sense of where they were going.  They were lurching from one policy to the next, each time saying that they didn’t think it would work.  So I do believe it’s important that we decide before we get to the point where such policies (QE) need to be triggered—and I’ll come to that issue next—at least on a very rough sequence of what we will do and **how we will talk to the public about it.  We don’t need to be very specific; but before we begin to use nontraditional techniques, I think we need to talk about them publicly and create a sense of continuity and confidence in our policymaking, which I believe was absent in Japan.**

So, the difference between QE in Japan and QE in the United States? Communication of expectation... the hope that communication of their policy will be the difference in real world outcome.. *not the policy itself*.

The Fed certainly isn't the all-powerful, money printing, evil god that many lust for it to be. Nor is the Fed a benevolent issuer of money and guardian of the system. 

...they're a janitor at best... At worst, a middling PR firm  representing market  interference in the short term. 

This post is not exhaustive, and I've left a lot out for simplicity. How US debt issuance interacts with the system (collateral) would need several TLDRs on its own.",Bitcoin
I've seen several videos of a people paying for goods using btc were the merchant creates a qr code for the customer to scan. My question is how do you know that the qr code you are about to scan represents the agreed amount price of product in fiat terms? Thanks to anyone that has this answer.,Bitcoin
"Hayek proposed that we wouldn't have good money ""until we took it out of the hands of government"" 👏 

See how Bitcoin applies to Hayek's work ""The Denationalization of Money"" and makes Austrian Economics actionable 👀",Bitcoin
"So one of my friends bought 5 bitcoins at one of those kiosk vending machine things about 10 years ago. He then texted them to my other friend. They then proceeded to forget all about it until recently. They have no idea how or if they can recover them. I’ve never used one of those kiosks, so I’m not sure how they work. Does anyone have any ideas on how to recover them?",Bitcoin
"I mean 0% chance of getting it stolen (by hackers) assuming the owner doesnt make any careless mistakes. is hardware wallet with the seed phrase stored no where else but the hardware wallet a guarantee that your funds can’t be stolen by hackers? if that’s the case, how come there are still major crypto exchanges that get hacked and lose hundreds of millions of dollars? how do the hackers usually exploit that? are there extra measures on top of hardware wallets and not storing your seed phrase anywhere else to ensure a greater level of security?",Bitcoin
"In the last 24 hours, bitcoind has died on me 5 times and I have to restart it again every time.

I see no errors in the debug.log file.  To run bitcoin, I use the command ./bitcoind -debug=1

I have checked the drive and ram for errors and none were found.  All other apps seem to work fine.

I have checked the internet connection and that is stable too.

It doesn't perform a safe stop, the process simply disappears from the system manager and the Terminal window just closes.

I am using an i3 with a 1tb drive and 8gb ram.

The os is ubuntu 22.04 with bitcoin-core 24.1.

For now, I have written a simple script to keep an eye on bitcoind and to start it again if it is not running, but this solution is not ideal.

Any idea why it keeps dying or any suggestions on how I can troubleshoot to find the issue?",Bitcoin
"Hi, I would appreciate some advice from you experts in here. I did not migrate from BRD wallet and now I want to recover from the seed backup (12 word).
I understand BRD used a M/0 derivation path and I need to use a wallet that supports this.
Bluewallet is one that is supposed to do that.
I guess I should create a new wallet with a B39 compatible wallet and transfer my funds to the new one?
What is your best advice?
Thank you,",Bitcoin
" Hey,

Does  anybody know any sites that provide bitcoin combined order books from  all the exchanges. Would like to see that if anyone can help me  please?...",Bitcoin
"Hello,  


I am new to crypto and this subreddit and want to know if it is 100% safe to share public addresses in a community like this? I think I understand that nobody can steal crypto just from a public address (as I've recently been researching), but can they get any other information that could harm someone such as personal/location information or bank details? 

I am looking for help in this sub on this matter but do not want to be harmed or scammed. Thanks!",Bitcoin
Is the difference that private nodes are running on tor? Any benefit to one or the other besides privacy of a private node operator?,Bitcoin
"Place your bets! Closest under guess wins 100k sats!

Looking for the highest mempool size during the month of feb 2023. 

Good Luck to us all!",Bitcoin
"I've seen some posts around here recently about people wanting to take custody of their own coins. THIS IS AWESOME. Good job. I've also seen quite a few people delaying this action because they are either unsure of which hardware wallet they want, don't have the funds to get a hardware wallet they want yet, are waiting to receive their hardware wallet in the mail, etc. 

There is **NO** reason you need a hardware wallet to take your coins into your own custody. There is **NO** reason to delay such action. If they are available to withdraw. Withdraw them. They are A LOT more secure in your custody than someone else's. It doesn't HAVE to be a hardware wallet. Put them in a reputable software wallet until a time you can afford or receive the hardware wallet you want. 

Don't delay, take custody of your coins TODAY!

Reputable Wallets: 
https://bitcoin.org/en/choose-your-wallet

There are many more and the community here is more than happy to help and recommend other free options, but as always DYOR.

Edit: I want to make this really easy for anyone.
There are no dumb questions and I will help anyone that has questions and is unsure. I also wanted to add that it's best practice to send a test transaction of a small amount and make sure it goes through before sending large sums of Bitcoin to and from addresses. 

Another good point brought up in the comments is that I personally don't even have a hardware wallet. I keep my stack of Bitcoin in a wallet whose seed is stamped on a steel plate in my safe. I only ever send BTC to that wallet, never from that wallet. I can always grab the seed out the safe, restore a wallet on some old hardware I have (old phone)  and broadcast a tx if I really need to send from it. I have a smaller amount, I'm okay with losing, in a hot wallet on my phone for spending. That is of course still backed up but it's a separate wallet, more public knowledge of addresses and identity. 

I hope this helps.  Please ask questions.",Bitcoin
"Theoretically if I plug a Trezor/Ledger/keepkey into an infected computer, is there any way a hacker can steal my funds? (I know double check the address on device to make sure I’m sending the funds correctly) but can a hacker with a script or virus funnel funds out of my physical device?",Bitcoin
"Hey guys, stock guy here. I'm looking to get into crypto and I'm wondering where you guys start? I noticed people have stated they have price targets but I have no clue how they can ever forecast that, as it's not a company with predictable cash flows you can model. Especially Bitcoin where the technology has stayed and will stay relatively the same forever. So how do you guys determine if it's a good buy? To me it seems like a purely speculative play, buying just hoping it becomes a mass adopted currency. 

Please enlighten me :)",Bitcoin
"Newbie question here, can a powerfull entity make tons of useless transactions to completly stop the regular transactions from happening ?  I know there is a fee system to prevent that a little bit, but if the entity have a lot of money to spend in this attack can it block btc transactions for a long period of time ?",Bitcoin
What are the long-term implications of the rise of cryptocurrency and decentralized finance on the global financial system? Will it lead to greater economic freedom and opportunity or further financial instability and inequality?,Bitcoin
"Please utilize this sticky thread for all general **Bitcoin** discussions! If you see posts on the front page or /r/Bitcoin/new which are better suited for this daily discussion thread, please help out by directing the OP to this thread instead. Thank you!

If you don't get an answer to your question, you can try phrasing it differently or commenting again tomorrow.

Please check the [previous discussion thread](https://www.reddit.com/r/Bitcoin/comments/10v0cgg/mentor_monday_february_06_2023_ask_all_your/) for unanswered questions.",Bitcoin
"Hello everyone, I was wondering if you could help me understand a few basic things about block explorers. In this case the transaction was made from 1 address to another, but as you can see, there are 60 senders in the list.

If you click on the TX hash you can see all the 60 senders and the amounts I transferred are separated into 3 or 4 different transactions with different amounts (but the sum is equal to what I've sent).

I wanted to know why do these 60 senders appear here (and not only my transaction); why are the amounts split; and if you have any good reads about this topic.

Thank you.

https://preview.redd.it/aze1a2hfnoga1.png?width=866&format=png&auto=webp&v=enabled&s=0dc4368e17e1e3dcb03653ab10adf5ae770ace3f",Bitcoin
"So tomorrow I have an interview as an intern data analyst at a huge fintech company that manages and helps startups. I just started researching the field. It’s nothing I’ve ever done or have experience with, but it honestly looks super challenging (which I love). 
Does anyone have advice on what I can say or ask during the interview to impress the interviewers? I am really looking forward to getting this job, but I’m afraid my lack of experience will show through this interview.",Bitcoin
If I believe in the long term future of BTC. And that my ROI would be greater investing my 401k in BTC rather than leaving it tell retirement. Would it make sense and wise to take out my 401k and invest it in BTC right now that we are still relatively early in the game. Convince me that I’m crazy and leave my 401k untouched tell retirement,Bitcoin
"In an old thread, I was complaining about fees when using Coinbase and people suggested Strike.  Well I finally registered a new account on Strike, and when I went to add the debit card I have to complete the 1-2 day background check.  I sent an image of the debit card and images of my state ID so just have to hope they indicate that its ok and wait now.  Just wondering about alternatives which wouldn't have the waiting period",Bitcoin
" When I buy crypto and I send it to someone's wallet, is it deposited  with all of my personal information? I'm trying to buy something online  using crypto as a method of payment but I don't want all of my personal  information being sent to the wallet holder. If I buy through an  exchange and the crypto is sent directly to their wallet would that  contain my personal info?",Bitcoin
"Lightning Ventures Q&A interview with IBEX CEO Jose Lemus is live now 📢

Check it out here 👇

[https://youtu.be/zpL6rBYVr14](https://youtu.be/zpL6rBYVr14)",Bitcoin
"Hey all, I’ve recently started to use blue wallet for lightning transactions. I’ve used the wallet normally before without any issues. I completed a lightning transaction and the payment has been in transition for over 24 hours. Any advice?",Bitcoin
I'm already in BTC mining but I would like to buy one or two more miners and host them under a \*green\* electricity grid... Like in Norway or Iceland or even New York state. Do you guys have any advice?,Bitcoin
How does a wallet restored from a seed know how many coins it has access to if the number of addresses is virtually infinite?  Does it scan the blockchain for its signature in blocks every time it connects? Do you have any video or link about it?,Bitcoin
"And please explain it to someone who isn't particularly technically proficient.  Basically, when you read the white paper, it doesn't say anything in there about the Bitcoin network storing messages, files, or images.  It talks only about digital cash and a new monetary system.  And about solving the Byzantine generals problem and the double spend problem, etc.  Nowhere is there any indication of any intention, to make the network into anything other than monetary.  Seriously, it's not meant to be an email system or a file repository/archive, the way I read the white paper.  So...why not design it to preclude that?  Or is it not possible to do so?  

I'm just trying to gain a full understanding of how it works, and trust me I've read quite a bit and watched many videos about it.  And nowhere can I seem to find an answer to this question.  Thank you for any insight.",Bitcoin
